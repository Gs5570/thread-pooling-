Empirical Software Engineering (2023) 28:30
https://doi.org/10.1007/s10664-022-10257-9
What really changes when developers intend to
improve their source code: a commit-level study of
static metric value and static analysis warning changes
Alexander Trautsch1 · Johannes Erbel2 · Steffen Herbold1 · Jens Grabowski2
Accepted: 2 November 2022
© The Author(s) 2023
Abstract
Many software metrics are designed to measure aspects that are believed to be related to
software quality. Static software metrics, e.g., size, complexity and coupling are used in
defect prediction research as well as software quality models to evaluate software quality.
Static analysis tools also include boundary values for complexity and size that generate
warnings for developers. While this indicates a relationship between quality and software
metrics, the extent of it is not well understood. Moreover, recent studies found that complexity
metrics may be unreliable indicators for understandability of the source code. To
explore this relationship, we leverage the intent of developers about what constitutes a quality
improvement in their own code base. We manually classify a randomized sample of
2,533 commits from 54 Java open source projects as quality improving depending on the
intent of the developer by inspecting the commit message. We distinguish between perfective
and corrective maintenance via predefined guidelines and use this data as ground truth
for the fine-tuning of a state-of-the art deep learning model for natural language processing.
The benchmark we provide with our ground truth indicates that the deep learning model
can be confidently used for commit intent classification. We use the model to increase our
data set to 125,482 commits. Based on the resulting data set, we investigate the differences
in size and 14 static source code metrics between changes that increase quality, as indicated
by the developer, and changes unrelated to quality. In addition, we investigate which files
Communicated by: Xin Xia
  Alexander Trautsch
alexander.trautsch@uni-passau.de
Johannes Erbel
johannes.erbel@cs.uni-goettingen.de
Steffen Herbold
steffen.herbold@uni-passau.de
Jens Grabowski
grabowski@cs.uni-goettingen.de
1 University of Passau, Passau, Germany
2 Institute of Computer Science, University of Goettingen, G¨ottingen, Germany
30 Page 2 of 40 Empir Software Eng (2023) 28:30
are targets of quality improvements. We find that quality improving commits are smaller
than non-quality improving commits. Perfective changes have a positive impact on static
source code metrics while corrective changes do tend to add complexity. Furthermore, we
find that files which are the target of perfective maintenance already have a lower median
complexity than files which are the target of non-pervective changes. Our study results provide
empirical evidence for which static source code metrics capture quality improvement
from the developers point of view. This has implications for program understanding as well
as code smell detection and recommender systems.
Keywords Static code analysis · Quality evolution · Software metrics · Software quality
1 Introduction
Software quality is notoriously hard to measure (Kitchenham and Pfleeger 1996). The main
reason is that quality is subjective and that it consists of multiple factors. This idea was
formalized by Boehm and McCall in the 70s (Boehm et al. 1976; McCall et al. 1977).
Both introduced a layered approach where software quality consists of multiple factors. The
standard ISO/IEC 9126 (2001) and successor ISO/IEC 25010 (2011) also approach software
quality in this fashion.
All these ideas contain abstract quality factors. However, the question remains what
concrete measurements can we perform to evaluate the abstract factors of which software
quality consists, i.e., how do we measure software quality. Some software quality
models recommend concrete measurements, e.g., ColumbusQM (Bakota et al. 2011) and
Quamoco (Wagner et al. 2012). Defect prediction researchers also try to build (machine
learning) models to find a function that can mapmeasurable metrics to the number of defects
in the source code. This can also be thought of as software quality evaluation, that tries to
map internal software quality, measured by code or process metrics, to external software
quality measured by defects (Fenton and Bieman 2014). The internal and external quality
categories can also be mapped to perfective and corrective maintenance categories after
Swanson (1976). Perfective maintenance should increase internal quality while corrective
maintenance should increase external quality. Both categories should increase the overall
quality of the software. To ease the readability, we adopt the perfective and corrective terms
defined by Swanson for the rest of the paper when referring to the categories. For general
assumptions, we adopt the internal and external quality terms. Internal quality represents
what the developer sees, e.g., structure, size, and complexity while external quality what the
user sees, e.g., defects.
Software quality models and defect prediction models use static source code metrics
as a proxy for quality (Hosseini et al. 2017). The intuition is that complex code, as measured
by static source code metrics, is harder to reason about and, therefore, is more prone
to errors. However, recent research by Peitek et al. (2021) showed that measured code
complexity is perceived very differently between developers and does not translate well to
code understanding. A similar result was found by Scalabrino et al. (2021) although their
work is focused on readability measured in a static way. Both studies, due to their nature,
observe developers in a controlled experiment with code snippets. To supplement these
results, it would be interesting to measure what developers change in their code “in the
Empir Software Eng (2023) 28:30 Page 3 of 40 30
wild” to improve software quality and if their intent matches what we can measure, e.g., if
complexity is reduced in a change that intends to improve quality.
While there are multiple publications on maintenance or change classification after
Swanson (1976), e.g.,Mockus (2000), Mauczka et al. (2012), Levin and Yehudai (2017) and
H¨onel et al. (2019), we are not aware of publications that investigate differences between
multiple software metrics for corrective and perfective maintenance as well as their counterparts,
i.e., non-perfective and non-corrective. The inclusion of these counterparts results
in computational effort as we need every metric for every file in every commit. However,
we are able to provide this data via the SmartSHARK ecosystem (Trautsch et al. 2017,
2020b). This additional effort allows us to infer if categories of changes are different when
regarding all changes of a software project. Most recent work focuses on certain aspects
instead of a generic overview, e.g., how software metric values change when code smells
are removed (Bavota et al. 2015) or refactorings are applied (Bavota et al. 2015; Alshayeb
2009; Pantiuchina et al. 2020).
However, we believe that taking a step back from focused approaches and investigating
generic quality improvements is worthwhile. A generic overview has the advantage of mitigating
possible problems that can occur for narrow meaning keywords of topically focused
approaches while at the same time providing a cohesive overview. Moreover, it allows
for generic statements about software quality evolution based on this information and can
complement focused approaches.
In this work, we find changes that increase the quality, while we measure current, previous
and delta of common source code metric values used in a current version (Bakota et al.
2014) of the Columbus quality model (Bakota et al. 2011). We use the commit message
contained in each change to find commits where the intent of the developer is to improve
software quality. This provides us with a view of corrective and perfective maintenance
commits.
Within our study, we first classify the commit intent for a sample of 2,533 commits from
54 open source projects manually. The manual classification is provided by two researchers
according to predefined guidelines. According to the overview of previous research in this
area provided by AlOmar et al. (2021) our study would be the largest manual classification
study of commits. We use this data as ground truth to fine-tune a state-of-the-art deep
learning model for natural language processing that was pre-trained exclusively on software
engineering data (von der Mosel et al. 2022). After we determine the performance of the
model, we classify all commits, increasing our data to 125,482 commits.
We use the automatically classified data to conduct a two part study. The first part is
a confirmatory study into the expected behavior of metric values for quality increasing
changes. Expected behaviour, e.g., complexity is reduced in quality increasing changes, is
derived as hypothesis from existing quality models and the related literature.
In case our data matches the expected behavior from the literature, we can confirm the
postulated theories and provide evidence in favor of using the measurements. Otherwise, we
try to establish which metrics may be unsuitable for quality estimation, including the potential
reasons. Even further, we determine whether metrics used in software quality models are
impacted by quality increasing maintenance, therefore providing an evaluation for software
quality measurement metrics.
The second part of our study is of exploratory nature. We investigate which files are
the target of quality improvements by the developers. We explore whether only complex
files are receiving perfective changes and which metric values are indicative of corrective
changes. This provides us with data for practitioners and static analysis tool vendors for
30 Page 4 of 40 Empir Software Eng (2023) 28:30
boundary values which are likely to have a positive impact on the quality of source code
from the perspective of the developers.
Overall, our work provides the following contributions:
– A large data set of manual classifications of commit intents with improving internal and
external quality categories.
– A confirmatory study of size and complexity metric value as well as static analysis
warning changes for quality improvements.
– An exploratory study of size and complexity metric values as well as static analysis
warnings of files that are the target of quality improvements.
– A fine-tuned state-of-the-art deep learning model for automatic classification of commit
intents.
The main findings of our study are the following:
– We confirm previous work that quality increasing commits are smaller than changes
unrelated to quality.
– While perfective changes have a positive impact on most static source code metric
values and static analysis warnings, corrective changes have a negative impact on size
and complexity.
– The files that are the target of perfective changes are already less complex and smaller
than files which are not the target of perfective changes.
– The files that are the target of corrective changes are more complex and larger than files
which are not the target of corrective changes.
The remainder of this paper is structured as follows. In Section 2, we define our research
questions and hypotheses. In Section 3, we discuss the previous work related to our study.
Section 4 contains our case study design with descriptions for subject selection as well as
data sources and analysis procedure. In Section 5, we present the results of our case study
and discuss them in Section 6. Section 7 lists our identified threats to validity and Section 8
closes with a conclusion of our work.
2 Research Questions and Hypotheses
In our study, we answer two research questions.
– RQ1: Does developer intent to improve internal or external quality have a positive
impact on software metric values?
Previous work provides us with certain indications about the impact on software metric
values. This is part of our confirmatory study, and we derive two hypotheses from
previous work regarding how size and software metric values should change for different
types of quality improvement.We formulate our assumptions as hypothesis and test
these in our case study.
– H1: Intended quality improvements are smaller than non-perfective and
non-corrective changes.
Mockus (2000) found that corrective changes modify fewer lines while perfective
changes delete more lines. Purushothaman and Perry (2005) also observed
more deletions for perfective maintenance and an overall smaller size of perfective
and corrective maintenance. Both studies provide measurements we
Empir Software Eng (2023) 28:30 Page 5 of 40 30
base our hypothesis on. While they are using the same closed source project
we will be able to see if our assumption holds for our multiple Java open
source projects.
H¨onel et al. (2019) used size-based metrics as additional features for
an automated approach to classify maintenance types. They found that the
size-based metric values increased the classification performance. Moreover,
just-in-time quality assurance (Kamei et al. 2013) builds on the assumption
that changes and metrics derived from these changes can predict bug introduction,
meaning there should be a difference. Therefore, we hypothesize that
corrective as well as perfective maintenance consist of smaller changes. Addition
of features should be larger than both, and therefore we assume that the
categories we are interested in, perfective and corrective, are smaller than
other non-perfective and non-corrective changes.
– H2: Intended quality improvements impact software quality metric values
in a positive way.
In this paper, we focus on metrics used in the Columbus Quality Model
(Bakota et al. 2011, 2014). The metrics are specifically chosen for a quality
model so they should provide different measurements based on their maintenance
category. Prior research, e.g., Ch’avez et al. (2017) and Stroggylos
and Spinellis (2007) found that refactorings, which are part of our classification,
have a measurable impact on software metric values.We hypothesize that
an improvement consciously applied by a developer via a perfective commit
has a measurable, positive impact on software metric values. Positive means
that we expect a value change direction of the metric value, e.g., complexity
is reduced. We note our expected direction for each metric together with a
description in Table 4.
Defect prediction research assumes a connection between software metrics
and external software quality in the form of bugs. While most publications in
defect prediction are not investigating the impact of single bug fixing changes
the most common datasets all contain coupling, size and complexity metrics
as independent variables, e.g., Jureczko and Madeyski (2010), NASA (2004),
and D’Ambros et al. (2012), see also the systematic literature review by Hosseini
et al. (2017).We hypothesize that fixing bugs via corrective commits has
a measurable, positive impact on software metric values. While a bug fix may
add complexity, our study compares bug fix changes with all non-corrective
changes including feature additions. Therefore, we do not hypothesize that
bug fixing decreases complexity generally, but that it is decreasing complexity
in comparison to all non-corrective changes. In contrast to H1 we are not
able to compare our results to concrete studies as we are not aware of a study
that investigates metric value changes of perfective and corrective changes and
compares them against all other non-perfective and non-corrective changes.
We are instead trying to validate the assumption that quality improvements
should have a positive impact on software quality metrics as they are found to
improve detection of defects (Gyimothy et al. 2005).
Our second research question is exploratory in nature.
– RQ2: What kind of files are the target of internal or external quality improvements?
The first part of our study provides us with information about metric value changes
30 Page 6 of 40 Empir Software Eng (2023) 28:30
for quality increasing commits. In this part, we are exploring which files are the target
of quality increasing commits. We are interested in how complex, e.g., via cyclomatic
complexity, a file is on average that receives perfective maintenance. Moreover, on the
external quality side we are interested in which files are receiving corrective changes.
Due to the exploratory nature of this research question, we do not derive hypotheses.
3 Related Work
We separate the discussion of the related work into publications on the classification of
changes, publications on the relation between quality improvements and software metrics
and publications with a focus on the commit message.
Most prior work that follows a similar approach to ours is concerned with specific types
of quality improving changes, e.g., refactoring and removal of code smells. We note that
some code smell detection is based on internal software quality metrics, which we use in
our study.
We first present previous research related to the first phase of our study, i.e., classification
of changes with respect to maintenance types. Mockus (2000) study changes in a
large system and identified reasons for changes. They find that a textual description of the
change can be used to identify the type of change with a keyword based approach which
they validated with a developer survey. The authors classified changes to Swansons maintenance
types. They find that corrective and perfective changes are smaller and that perfective
changes delete more lines than other changes. Mauczka et al. (2012) present an automatic
keyword based approach for classification into Swansons maintenance types. They evaluate
their approach and provide a keyword list for each maintenance type together with a weight.
Fu et al. (2015) present an approach for change classification that uses latent drichtlet
allocation. They study five open source projects and classify changes into Swansons maintenance
types together with a not sure type. The keyword list of their study is based on
Mauczka et al. (2012).
Mauczka et al. (2015) collect developer classifications for three different classification
schemes. Their data contains 967 commits from six open source projects. While the developers
themselves are the best source of information, we believe that within the guidelines
of our approach our classifications are similar to those of the developers. We evaluate this
assumption in Section 4.2.
Yan et al. (2016) use discriminative topic modeling also based on the keyword list by
Mauczka et al. (2012). They focus on changes with multiple categories. Levin and Yehudai
(2017) improve maintenance type classification by utilizing source code in addition to keywords.
This is an indication that metric values which are computed from source code are
impacted by different maintenance types.
H¨onel et al. (2019) use size metrics as additional features for automated classification
of changes. In our study, we first classify the change and then look at how this impacts
size and spread of the change. However, the differences we found in our study support the
assumption that size-based features can be used to distinguish change categories.
More recently, Wang et al. (2021) also analyze developer intents from the commit
messages. They focus on large review effort code changes instead of quality changes or
maintenance types. They also use a keyword based heuristic for the classification. They do
not, however, include a perfective maintenance classification.
Ghadhab et al. (2021) also use a deep learning model to classify commits. They use word
embeddings from the deep learning model in combination with fine-grained code changes
Empir Software Eng (2023) 28:30 Page 7 of 40 30
to classify into Swansons maintenance categories. In contrast to Ghadhab et al., we do not
include code changes in our automatic classifications and focus on the commit message.
The classification of changes for the ground truth in our study is based on manual inspection
by two researchers instead of a keyword list.We specify guidelines for the classification
procedure which enable other researchers to replicate our work. To accept or reject our
hypotheses, we only inspect internal and external quality improvements which would correspond
to the perfective and corrective maintenance types by Swanson. In contrast to the
previous studies, we relate our classified changes also to a set of static software metrics.
We now present research related to our second phase of our study, the relation between
intended quality improvements and software metrics. Stroggylos and Spinellis (2007) found
changes where the developers intended a refactoring via the commit message. The authors
then measured several source code metrics to evaluate the quality change. In contrast to the
work of Stroggylos and Spinellis (2007), we do not focus on refactoring keywords. Instead,
we consider refactoring as a part of our classification guidelines. Moreover, our aim is to
investigate whether the metrics most commonly used as internal quality metrics (see also
; Al Dallal and Abdin 2018) are the ones that are changing if developers perform quality
improving changes including refactoring.
Fakhoury et al. (2019) investigate the practical impact of software evolution with developer
perceived readability improvements on existing readability models. After finding
target commits via commit message filtering, they applied state-of-the-art readability models
before and after the change and investigate the impact of the change on the resulting
readability score.
Pantiuchina et al. (2018) analyze commit messages to extract the intent of the developer
to improve certain static source code metrics related to software quality. In contrast to their
work, we are not extracting the intent to improve certain static code metrics but instead
focus on overall improvement to measure the delta of a multitude of metrics between the
improving commit and its parents. Developers may not use the terminology Pantiuchina et
al. base their keywords on, e.g., instead of writing reduce coupling or increase cohesion the
developer may simply write refactoring or simplify code.
In contrast to the previous studies, we relate developer intents to improve the quality
either by perfective maintenance or by corrective maintenance to change size metrics and
static source code metrics. In addition, we also look at mean static source code metrics per
file which are the target of quality improvements.
As the commit message is used to extract the intent of the developer in our study, we
also briefly discuss related work on commit message contents. Most of that work that is
not already covered previous sections builds and evaluates a quality model for the commit
message. The proposed quality models are not suitable for our study as is, as they only
determine general commit message quality and we use the message to classify the commit
to one of three types. However, they still provide interesting data considering the content of
the commit messages.
Santos and Hindle (2016) investigate whether unusual commit messages correlate with
build failures using an n-gram language model. The authors find, that their language model
is able to identify unusual commit messages. However, they did not find a significant correlation
between unusualness of a commit message as determined by the cross-entropy of
their language model and build failures.
Chahal and Saini (2018) analyze the impact of community dynamics on syntactic quality
of commit messages. They define a commit message quality model and use the model to
relate community dynamic metrics to commit message quality. They find that a small group
of contributors active at the same time can lead to a high quality of commit messages.
30 Page 8 of 40 Empir Software Eng (2023) 28:30
Tian et al. (2022) study commit messages in five open source projects and find, that an
average of about 44% messages could be improved. They proposed a classification model
for quality of commit messages after manually classifying 1600 commits. In their multimethod
study the authors also provide a taxonomy of commit messages with expression
categories. They find, that between 0.9% and 7.5% of commit messages do neither contain
what was changed nor why the change was applied.
4 Case Study Design
The goal of our case study is to gather empirical data about what changes when a developer
intends to improve the quality of the code base in comparison to their counterpart, e.g., what
changes in perfective commits in comparison to all other, i.e., non-perfective commits.
To achieve this, we first sample a number of commits from our selected study subjects.
This sample is classified by two researchers into two categories of quality improving and
other changes. The classification into categories is only done via the commit message as it
expresses the intent of the developer on what the change should achieve.
This data is then used to train a model that can confidently classify the rest of our commit
messages. The classified commits are then used to investigate the static source code metric
value changes to accept or reject our hypotheses in the confirmatory part of our study. After
that, we investigate the metric values before the change is applied in the exploratory part of
our study.
4.1 Data and Study Subject Selection
The data used in our study is a SmartSHARK (Trautsch et al. 2017) database taken from
Trautsch et al. (2020a). We use all projects and commits in the database. However, only
commits that change production code and which are not empty are considered. For each
change in our data, we extract a list of changed files, the number of changed lines, the
number of hunks,1 and the delta as well as the previous and current value of source code
metrics from the changed files between the parent and the current commit. To create our
ground truth sample, we randomly sample 2% of commits per project rounded up for manual
classification.
The data consists of Java open source projects under the umbrella of the Apache Software
Foundation.2 All projects use an issue tracking system and were still active when the data
was collected. Each project consist of at least 100 files and 1000 commits and is at least
two years old. Table 1 shows every project, the number of commits and the years of data
we consider for sampling. In addition, we include the number of perfective and corrective
commits for our ground truth and final classification.
4.2 Change Type Classification Guidelines
As we are not relying on a keyword based approach and there is no existing guideline
for this kind of classification, we created a guideline based on Herzig et al. (2013). Our
ground truth consists of a sample of changes which we manually classified into perfective,
1An area within a file that is changed.
2https://www.apache.org
Empir Software Eng (2023) 28:30 Page 9 of 40 30
Table 1 Case study subjects with time frame and distribution of commits
Project Timeframe #C #S #SP #SC #AP #AC
archiva 2005–2018 3,914 79 35 17 1,478 1,005
calcite 2012–2018 1,987 40 8 14 565 665
cayenne 2007–2018 3,738 75 31 14 1,470 1,007
commons-bcel 2001–2019 884 18 9 6 588 171
commons-beanutils 2001–2018 577 12 5 2 317 130
commons-codec 2003–2018 828 17 12 1 619 76
commons-collections 2001–2018 1,827 37 27 3 1,185 200
commons-compress 2003–2018 1,598 32 17 6 873 317
commons-configuration 2003–2018 2,075 42 23 7 1,027 253
commons-dbcp 2001–2019 1,034 21 15 3 672 211
commons-digester 2001–2017 1,256 26 16 0 744 113
commons-imaging 2007–2018 682 14 10 2 476 96
commons-io 2002–2018 1,036 21 15 3 613 171
commons-jcs 2002–2018 788 16 10 1 400 162
commons-jexl 2002–2018 1,469 30 20 1 873 199
commons-lang 2002–2018 3,261 66 50 6 2,182 420
commons-math 2003–2018 4,675 94 66 10 2,981 574
commons-net 2002–2018 1,092 22 13 5 585 246
commons-rdf 2014–2018 529 11 9 0 341 35
commons-scxml 2005–2018 479 10 6 2 256 76
commons-validator 2002–2018 1,573 32 18 6 900 296
commons-vfs 2002–2018 1,136 23 11 8 628 207
eagle 2015–2018 582 12 5 4 104 199
falcon 2011–2018 1,547 31 7 13 255 676
flume 2011–2018 1,489 30 5 14 266 591
giraph 2010–2018 854 18 4 6 201 281
gora 2010–2019 569 12 3 4 182 141
helix 2011–2019 2,199 44 8 9 552 580
httpcomponents-client 2005–2019 2,399 48 22 16 1,113 639
httpcomponents-core 2005–2019 2,598 52 25 12 1,326 544
jena 2002–2019 8,698 174 88 34 4,163 1,424
jspwiki 2001–2018 4,326 87 32 25 1,523 941
knox 2012–2018 1,131 23 3 10 266 306
kylin 2014–2018 6,789 136 40 40 1,904 2,163
lens 2013–2018 1,370 28 9 9 321 479
mahout 2008–2018 2,075 42 16 15 836 467
manifoldcf 2010–2019 2,867 58 10 21 602 1,164
mina-sshd 2008–2019 1,281 26 10 6 381 396
nifi 2014–2018 3,299 66 12 18 592 1,052
opennlp 2008–2018 1,763 36 22 6 805 275
parquet-mr 2012–2018 1,228 25 7 9 439 316
pdfbox 2008–2018 8,256 166 81 69 3,934 2,904
phoenix 2014–2019 7,835 157 23 83 828 4,545
30 Page 10 of 40 Empir Software Eng (2023) 28:30
Table 1 (continued)
Project Timeframe #C #S #SP #SC #AP #AC
ranger 2014–2018 2,213 45 10 20 434 908
roller 2005–2019 2,435 49 15 13 869 723
santuario-java 2001–2019 1,455 30 14 5 627 406
storm 2011–2018 2,839 57 24 9 987 716
streams 2012–2019 911 19 7 2 264 196
struts 2006–2018 2,945 59 21 18 1,191 682
systemml 2012–2018 3,860 78 21 25 921 1,416
tez 2013–2018 2,359 48 8 27 443 1,223
tika 2007–2018 2,581 52 11 10 705 740
wss4j 2004–2018 2,455 50 22 10 712 702
zeppelin 2013–2018 1,836 37 11 6 333 699
125,482 2,533 1,022 685 47,852 35,124
All considered commits (#C), sample size (#S), sample perfective commits (#SP), sample corrective commits
(#SC), all perfective commits (#AP), all corrective commits (#AC)
corrective, and other changes. We do not consider adaptive changes as separate a category.
Instead, we include them in the other changes. The reason is that we focus on internal
and external quality improvements and map perfective to internal quality and corrective
to external quality. Every commit message is inspected independently by two researchers
with software development experience. The inspection is using a graphical frontend that
loads the sample and displays the commit message which can then be assigned a label by
each researcher independently. If the commit message does not provide enough information,
we inspect additional linked information in the form of bug reports or the change itself.
In case of a link between the commit message and the issue tracking system, we inspect
the bug report and determine if it is a bug according to the guidelines by Herzig et al.
(2013).We perform this step because the reporter of a bug sometimes assigns a wrong type.
We defined the guidelines listed in Table 2 used by both researchers for the classification
of changes. The deep learning model for our final classification of intents only receives
the commit messages. This is a conscious trade-off. On the one hand we want the ground
truth to be as exact as possible, on the other hand we want to keep the automatic intent
classification as simple as possible. The results of our fine-tuning evaluation (Table 3) show
that the model does not need the additional data from changes and issue reports to perform
well.
Both researchers achieve a substantial inter-rater agreement (Landis and Koch 1977) with
a Kappa score of 0.66 (Cohen 1960). Disagreements are discussed and assigned a label both
researchers agree upon after discussion. The disagreement front end shows both prior labels
anonymized in random order.
In contrast to the classification by Mauczka et al. (2015) and Hattori and Lanza (2008),
we do not categorize release tagging, license or copyright corrections as perfective. Our
rationale is that these changes are not related to the code quality, which is our main interest
in this study.
Empir Software Eng (2023) 28:30 Page 11 of 40 30
Table 2 Classification rules and examples, footnotes denote different commit messages from our data
A change is classified as perfective if. . .
1. the commit message says code is removed or marked as deprecated.
2. code is moved to new packages.
3. generics are introduced, new Java features are used, existing code is switched to collections, or class
members are switched to final.
4. documentation is improved or example code is updated.
5. static analysis warnings are fixed even though no related bug is reported.
6. code is reformatted or the readability is otherwise improved (e.g. whitespace fixes or tabs to spaces).
7. existing code is cleaned up, simplified, or its efficiency improved.
8. dependencies are updated.
9. developer tooling is improved, e.g., build scripts or logging facilities.
10. the repository layout is cleaned, e.g., by removing compiled code or maintaining .gitignore files.
11. tests are improved or added.
Examples: Eliminated unused private field. JIRA: DBCP-255a Because of other null
checks it was already impossible to use the field. Thus, this is clean up. [CODEC-127]
Non-ascii characters in source filesb While the linked issue is a bug, it only affects IDEs
for developers and not the compiled code. Thus, this is an improvement of developer
tooling. JEXL-240: Javadocc The message indicates that this commit only improved
the code comments. Therefore, it is classified as perfective.
A change is classified as corrective if. . .
1. the commit message mentions bug fixes.
2. the commit message or the linked issue mentions that a wrong behaviour is fixed.
3. the commit message or the linked issue mentions that a NullPointerException is fixed.
4. a bug report is linked via the commit message that is of type bug and is not just a
feature request in disguise (see Herzig et al. 2013).
Examples: KYLIN-940 ,fix NPE in monitor module, apply patch from Xiaoyu Wangd
This fixes a NullPointerException that is visible to the end user. owl syntax checker (bug
fixes)e Fixes a wrong behavior.
A change is classified as other if. . .
1. the commit message mentions feature or functionality addition.
2. the commit message mentions license information or copyrights changes.
3. the commit message mentions repository related information with unclear purpose,
e.g., merges of branches without information, tagging of releases.
4. the commit message mentions that a release is prepared.
5. an issue is linked via the commit message that requests a feature.
6. any of the 1-5 are tangled with a perfective or corrective classification.
Examples: KYLIN-715 fix license issuef License changes or additions are not direct
improvements of source code. Support the alpha channel for PAM files. Fix the alpha
channel order when reading and writing. Add various tests.g This change adds support
for a new feature, fixes something and adds tests, it is therefore highly tangled and we
do not classify it as either or both.
30 Page 12 of 40 Empir Software Eng (2023) 28:30
In Mauczka et al. (2015) the researchers selected six projects and seven developers with
personal commitment and provided the developers with the commit messages that they then
labeled according to different classification schemes. One of which is the Swanson classification
which matches our study. Each developer labeled a sample of commit messages
from their respective project. As we are focused on Java we also use the Java projects of the
Mauczka et al. (2015) dataset to validate our guidelines.
Two authors of this paper re-classified the Java projects from Mauczka et al. (2015):
Deltaspike, Mylyn-reviews and Tapiji. The commit messages were classified separately
first. Disagreements were then resolved together in a separate session. In the first session
both authors achieve a substantial inter-rater agreement (Landis and Koch 1977) with a
Kappa score of 0.62 (Cohen 1960).
Aside from the classification differences regarding release tagging, license or copyright
changes, we noticed further differences. Several commits contain some variation of “minor
bugfixes” which are classified as perfective maintenance by the developers or both corrective
and perfective, whereas we classify them as corrective. Additionally, code removal
or test additions were not classified as perfective changes by the developers, but rather as
corrective changes. This reveals a difference of perspective between researchers and developers.
We consider pure code removal and test additions as perfective instead of corrective
as we think of corrective changes as improving external quality, e.g., by fixing a customer
facing bug. The data also contains clean-up and removal messages without a hint of an
underlying bug which are classified as corrective by the developers. Based on the information
available to us, we cannot decide if these are misclassifications by the developers, the
result of differences in the classification guidelines, or misclassifications by us due to lack
of in-depth knowledge about the projects.
The authors achieve a substantial inter-rater agreement (Landis and Koch 1977) with the
developers yielding a Kappa score of 0.63 (Cohen 1960).
4.3 Deep Learning for Commit Intent Classification
In order to use all available data, we use a deep learning model that classifies all data which
is not manually classified into perfective, corrective or other. Due to the size of state-of-theart
deep learning models and the computing requirements for training them, a current best
Table 3 Change classification model performance comparison
Model Acc. F1 MCC Description
von der Mosel et al. (2022) 0.80 0.79 0.70 BERT model pre-trained on
software engineering data, fine-tuned
with only commit messages
Ghadhab et al. (2021) 0.78 0.80 – BERT model pre-trained on natural
language, includes code changes.
Gharbi et al. (2019) – 0.46 – Multi-label active learning, only
commit message
Levin and Yehudai (2017) 0.76 – – Keywords and code changes, Random
Forest model
H¨onel et al. (2019) 0.80 – – LogitBoost model, includes code density.
Empir Software Eng (2023) 28:30 Page 13 of 40 30
practice is to use a pre-trained model which was trained unsupervised on a large data set.
The model is then fine-tuned on labeled data for a specific task.
To achieve a high performance, we use seBERT (von der Mosel et al. 2022), a model that
is pre-trained on textual software engineering data in two common Natural Language Processing
(NLP) tasks. Masked Language Model (MLM) and Next Sentence Prediction (NSP)
which predict randomly masked words in a sentence and the next sentence respectively.
Combined, this allows the model to learn a contextual understanding of the language.While
von der Mosel et al. (2022) include a similar benchmark based on our ground truth data, it
only used the perfective label, i.e., a binary classification to demonstrate text classification
for software engineering data. In our study, we measure performance of the multi-class
case with all three labels, perfective, corrective and other.Within this study, we first use our
ground truth data to evaluate the multi-class performance of the model. We perform a 10 ×
10 cross-validation which splits our data into 10 parts and uses 9 for fine-tuning the model
and one for evaluating the performance. The fine-tuning itself splits the data into 80%
training and 20% validation. The model is then fine-tuned and evaluated on the validation
data for each epoch. At the end the best epoch is chosen to classify the test data of the fold.
This is repeated 10 times for every fold which yields 100 performance measurements.
Our experiment shows sufficient performance comparable to other state-of-the-art
models for commit classification. We provide the final fine-tuned model as well as the
fine-tuning code as part of our replication kit for other researchers. Performance wise our
model is comparable to Ghadhab et al. (2021) and improves performance compared other
studies, e.g., Gharbi et al. (2019) and Levin and Yehudai (2017). However, we note that
we fine-tuned the model with only the labels used in our study, i.e., perfective, corrective
and other. Therefore, it cannot be used or directly compared with models that support other
commit classification labels. This would require the same data and labels, we can only
compare the given model performance metrics, which we do in Table 3. If we look at the
overview of commit classification studies by AlOmar et al. (2021) we can see that our
model outperforms the other models for comparable tasks where accuracy or F-measure
is given. While this is evidence that our model can perform our required commit intent
classification a throughout comparison of different commit intent classification approaches
is not within the scope of this study.
4.4 Metric Selection
The metric selection is based on the Columbus software quality model by Bakota et al.
(2011). The metrics are selected from the current version of the model also in use as QualityGate
(Bakota et al. 2014). The current model consists of 14 static source code metrics
related to size, complexity, documentation, re-usability and fault-proneness. While the quality
model provides us with a selection of metrics, we do not use it directly as it requires a
baseline of projects before estimating quality of a candidate project.
Table 4 shows the metrics utilized in this study, a short description, and the direction
which we assume they change in quality improving commits. As most of the metrics are size
and complexity metrics, we expect that their values decrease in comparison to all other commits.
The metrics we expect to increase in quality improving commits are commented lines
of code, comment density, and API documentation, as added documentation should increase
these metrics. The three bottom rules consist of static analysis warnings from PMD3 aggre-
3https://pmd.github.io/
30 Page 14 of 40 Empir Software Eng (2023) 28:30
Table 4 Static source code metrics and static analysis warning severities used in this study including the
expected direction of their values in quality increasing commits
Name and description Abbrev  
Cyclomatic Complexity (McCabe 1976)
The number of independent control-flow paths. McCC ↓
Logical Lines of Code
Number of lines in a file without comments and empty lines. LLOC ↓
Nesting Level else-if
Maximum of nesting level in a file. NLE ↓
Number of parameters in a method
The sum of all parameters of all methods in a file. NUMPAR ↓
Clone Coverage
Ratio of code covered by duplicates. CC ↓
Comment lines of code
Sum of commented lines. CLOC ↑
Comment density
Ratio of CLOC to LLOC. CD ↑
API Documentation
Number of documented public methods, +1 if class is documented. AD ↑
Number of Ancestors
Number of classes, interfaces, enums from which the class is inherited. NOA ↓
Coupling between object classes
Number of used classes (inheritance, function call, type reference). CBO ↓
Number of Incoming Invocations
Other methods that call the current class. NII ↓
Minor static analysis warnings
E.g., brace rules, naming conventions. Minor ↓
Major static analysis warnings
E.g., type resolution rules, unnecessary/unused code rules. Major ↓
Critical static analysis warnings
E.g., equals for string comparison, catching null pointer exceptions. Critical ↓
gated by severity for every file. We are of the opinion that this selection strikes a good
balance of size, complexity, documentation, clone, and coupling based metrics.
As we are interested in static source code metrics in a commit granularity, we sum
the metrics values for all files that are changed within a commit. In addition, we extract
meta information about each change. The static source code metrics are provided by a
Empir Software Eng (2023) 28:30 Page 15 of 40 30
SmartSHARK plugin using the OpenStaticAnalyzer.4 To answer our research question, we
provide the delta of the metric value changes as well as their current and previous value.
4.5 Analysis Procedure
For our confirmatory study as part of RQ1, we compare the difference between two samples.
To choose a valid statistical test of whether there is a difference between both samples,
we first perform the Shapiro-Wilk test (Wilk and Shapiro 1965) to test for normality of
each sample. Since we found that the data is non-normal, we perform theMann-Whitney Utest
(Mann and Whitney 1947) to evaluate if the metric values of one population dominates
the other. Since we have an expectation about the direction of metric changes, we perform a
one-sided Mann-Whitney U test. The H0 hypothesis is that both samples are the same, the
alternative hypothesis is that one sample contains lower or higher values depending on our
expectation. The expected direction of the metric value change is noted in the last column
of Table 4.
As our data contains a large number of metrics, we cannot assume a statistical test with
p < 0.05 is a valid rejection of a H0 hypothesis. To mitigate the problem posed by a high
number of statistical tests, we perform Bonferroni correction (Abdi 2007). We choose a
significance level of α = 0.05 with Bonferroni correction for 192 statistical tests. They
consist of four size metrics with two groups and three statistical tests as well as 14 source
code metrics with two groups and three statistical tests (normality tests for two samples and
Mann-Whitney U for difference between samples). The second part is repeated for RQ2.
We reject the H0 hypothesis that there is no difference between samples atp < 0.00026.
To calculate the effect size of theMann-Whitney U test, we use Cliff’s d (Cliff 1993) as a
non-parametric effect size measure. We follow a common interpretation of d values (Griessom
and Kim 2005): d < 0.10 is negligible, 0.10 ≤ d < 0.33 is small, 0.33 ≤ d < 0.474
is medium and d ≥ 0.474 is large. We provide the effect size for every difference that is
statistically significant.
We report the results visually with box plots. The box plots shows three groups: all,
perfective and corrective, this allows us to show the values for each metric for each group
and serves to highlight the differences. Additionally, we report the differences between each
group and its counterpart, e.g., perfective and non-perfective in the tables where we report
the statistical differences.
A more detailed description of the procedure for each hypothesis follows. For H1, we
compare the structure of quality improving changes with every non-perfective and noncorrective
change. We compare the size (changed lines) and diffusion (number of hunks,
number of changed files) to evaluate the hypothesis. We visualize the results with box plots
and report results for statistical tests to determine if the difference in samples is statistically
significant.
For H2, we also visualize the results via box plots. As most of the differences hover
around zero, we transform the data before plotting via sign(x) · log(abs(x + 1)). As we
are interested in the differences between changes of metric values, we also require x  = 0 :
∀x ∈ X where X is the complete, non-transformed data set for the visualizations. Due to
the difference in changes, we provide our data size corrected, e.g., the delta of McCC is
divided by the modified lines. Additionally, we report the percentage of data that is nonzero
to indicate how often the measurements are changing in our data. In addition to the
visualization, we provide a table with differences between the samples and statistical test
results.
4https://openstaticanalyzer.github.io/
30 Page 16 of 40 Empir Software Eng (2023) 28:30
Table 5 Statistical test results for perfective and corrective commits, Mann-Whitney U test p-values (p-val)
and effect size (d) with category, n is negligible, s is small
Metric Perfective Corrective
p-value d p-val d
#lines added <0.0001 0.20 (s) <0.0001 0.21 (s)
#lines deleted <0.0001 0.15 (s) <0.0001 0.16 (s)
#files modified 0.2081 – <0.0001 0.22 (s)
#hunks <0.0001 0.01 (n) <0.0001 0.22 (s)
Statistically significant p-values are bolded
As part of our exploratory study for answering RQ2, we also provide box plots of our
metric values. Instead of transformed delta values, we provide the raw averages per file in a
change before the change was applied. In addition, we provide the median values of all of
our metrics before the change was applied. In this part, we apply a two-sidedMann-Whitney
U test as we have no expectation of the direction the metrics change into for the categories.
To complement the visualization, we also provide density plots for both categories. They
show the overlap between the perfective and corrective changes.
4.6 Replication Kit
All data and source code can be found in our replication kit (Trautsch et al. 2021). In addition,
we provide a small website for this publication that contains all information and where
the fine-tuned model can be tested live.5
5 Results
In this section, we first present the results for evaluating our hypotheses of our first research
question. After that, we describe the results of the exploratory part of our study for our
second research question.
5.1 Confirmatory Study
We first present the results of our confirmatory study and evaluate our hypotheses. These
results answer our first research question: Does developer intent to improve internal or
external quality have a positive impact on software metric values?
5.1.1 Results H1: Intended Quality Improvements are Smaller than Non-perfective
and Non-corrective Changes
Figure 1 shows the distribution of sizes between perfective, corrective, and all commits.
Table 5 shows the statistical test results for the differences between perfective and nonperfective
as well as corrective and non-corrective commits. We can see that perfective
commits tend to add fewer lines but instead remove more lines as the non-perfective commits.
When we calculate a median delta between all commits and perfective commits, we
5https://r.semistatic.space/emse2021/
Empir Software Eng (2023) 28:30 Page 17 of 40 30
Fig. 1 Commit size distribution over all projects for all, perfective and corrective commits. Fliers are omitted
find a difference of 28 for added lines and -2 for deleted lines. While the effect sizes are
negligible to small, we can see this difference also in Fig. 1. The diffusion of the change
over files is also different, however for the number of modified files the difference is not
significant for perfective commits.
Corrective commits also tend to add less code, while they do not delete as much, the
difference in added and deleted lines is also statistically significant. While the effect size
is small, we can see the difference in Fig. 1. For corrective commits, we can also see a
difference in the number of files changed and the number of hunks modified. This diffusion
of the change via the number of files and hunks is also statistically significant although,
again, with a small effect size.
We can conclude, that perfective commits tend to remove more lines, and are generally
adding fewer lines to the repository. Corrective commits delete fewer lines and add fewer
lines than non-corrective commits. Corrective commits are also distributed over fewer
hunks and fewer files than non-corrective commits.
We accept H1 that intended quality improvements are smaller than non-perfective
and non-corrective changes.
Perfective and corrective commits tend to add fewer lines, perfective commits
remove more lines. The effect size is negligible to small in all cases.
5.1.2 Results H2: Intended Quality Improvements Impact Software Quality Metric
Values in a Positive Way
We first note that no metric value changes for each instance of our data. This can be seen in
Table 6, which shows the percentages for each metric value for perfective, corrective, and
all changes. We can see some differences between changes, e.g., critical PMD warnings
only change in about 7% of commits while LLOC changes in about 75%. Some differences
are also between categories, e.g., McCC changes in 31% of perfective changes and in 57%
of corrective changes.
To evaluate H2, we present the differences in all changes visually as box plots in Fig. 2,
which shows the metric values for all commits, only perfective and only corrective.
In addition, we provide Table 7 which shows the Mann-Whitney U test (Mann andWhitney
1947) p-values, and effect sizes for differences between the types of commits. The
differences that are compared in Table 7 are between perfective and non-perfective as well
as corrective and non-corrective.We can see that most metric values are different depending
on whether they are measured in perfective, corrective, or non-perfective and non-corrective
commits. In the following, we discuss the differences for each measured metric value. A
description for each metric and the expected direction of metric value change is shown in
Table 4.
McCC: the cyclomatic complexity of perfective changes is smaller than for nonperfective
changes as well as a combination of all changes. Even when we do not account for
the size of the change. This is expected as some perfective commits mention simplification
30 Page 18 of 40 Empir Software Eng (2023) 28:30
Table 6 Percentage of commits
where the metric value does
change on all commits (%NZ),
perfective commits (%NZ P) and
corrective commits (%NZ C)
Metric %NZ %NZ P %NZ C
McCC 51.03 31.01 57.70
LLOC 74.69 60.93 77.99
NLE 36.76 23.92 34.28
NUMPAR 35.93 24.44 24.98
CC 49.41 37.81 55.14
CLOC 51.56 46.52 42.51
CD 76.07 66.48 77.35
AD 27.19 20.63 15.82
NOA 10.51 6.96 3.62
CBO 30.89 22.52 22.22
NII 27.08 17.78 21.09
Minor 36.15 27.02 29.77
Major 19.87 13.23 14.77
Critical 7.23 4.20 4.95
of code. For perfective commits the effect size is medium. Corrective commits however have
higher McCC than all commits. This can be seen in Fig. 2. The median of corrective commits
is higher than for all commits. Our assumption about McCC being lower in all quality
improving commits is not met in this case. While it makes sense that corrective commits add
Fig. 2 Static source code metric value changes in all, perfective and corrective commits divided by changed
lines. Fliers are omitted
Empir Software Eng (2023) 28:30 Page 19 of 40 30
Table 7 Statistical test results for perfective and corrective commits, Mann-Whitney U test p-values (p-val)
and effect size (d) with category, n is negligible, s is small, m is medium
Metric Perfective Corrective
p-val d p-val d
McCC <0.0001 0.39 (m) 1.0000 –
LLOC <0.0001 0.45 (m) 1.0000 –
NLE <0.0001 0.27 (s) 1.0000 –
NUMPAR <0.0001 0.25 (s) <0.0001 0.09 (n)
CC 1.0000 – <0.0001 0.12 (s)
CLOC <0.0001 0.16 (s) <0.0001 0.05 (n)
CD 1.0000 – <0.0001 0.16 (s)
AD <0.0001 0.02 (n) <0.0001 0.08 (n)
NOA <0.0001 0.08 (n) <0.0001 0.07 (n)
CBO <0.0001 0.19 (s) <0.0001 0.06 (n)
NII <0.0001 0.19 (s) <0.0001 0.02 (n)
Minor <0.0001 0.19 (s) <0.0001 0.05 (n)
Major <0.0001 0.12 (s) <0.0001 0.05 (n)
Critical <0.0001 0.05 (n) <0.0001 0.03 (n)
Statistically significant p-values are bolded. All values are normalized for changed lines
complexity, Table 7 provides a comparison of stochastic dominance between corrective and
non-corrective commits, not if corrective commits remove or add McCC. Thus, this means
that changes in corrective commits are more complex than those of non-corrective changes.
LLOC: the difference of LLOC is the most pronounced in our data. We find that
even when we do not correct for size of the change the difference between perfective
and non-perfective changes in LLOC is the most pronounced. While manually classifying
the commits, we found that often code is removed because it was marked as deprecated
before or it was no longer needed due to other reasons. The effect size for perfective commits
is medium. For corrective commits, we can see the same result as for McCC. While
we assumed that bug fixes usually add code, we did not expect them to dominate all
non-corrective commits including feature additions.
NLE: the nesting level if-else is smaller in perfective commits. We expect this is due
to simplification and removal of complex code. When we look at the box plot in Fig. 2 it
shows a noticeable difference. This means simplification is a high priority when improving
code quality in perfective commits. For corrective commits, we can see the same effect
as previously seen for McCC and LLOC. The NLE is not lower but higher for corrective
commits. This is more evidence for the fact that bug fixes add more complex code. There
may be a timing factor involved, e.g., if bug fixes are quick fixes, they would add more
complex code without a more complex refactoring which would decrease the complexity
again.
NUMPAR: the number of parameters in a method is also different for perfective commits.
This may be a hint of the type of perfective maintenance performed the most in
perfective commits. The manual classification showed a lot of commit messages that
claimed a simplification of the changed code. This metric would also be impacted by a simplification
or refactoring operations. Corrective commits also show less additions in this
30 Page 20 of 40 Empir Software Eng (2023) 28:30
metric, while it only has a negligible effect size it is still statistically significant. Fixing bugs
seems to include some code reduction or at least less addition of parameters for methods.
CC: the clone coverage is not different for perfective commits. We would have expected
that it is decreasing in perfective commits. However, it seems that clone removal is not a
big part of perfective maintenance in our study subjects, which contradicts our expectation.
Corrective commits contain a lower clone coverage, however. This could either be because
corrective commits introduce fewer new clones than non-corrective commits or because
they remove more. A possible reason for clone removal may be the correction of copy and
paste related bugs.
CLOC: the comment lines of code show a difference for perfective commits and corrective
commits. While we expected the CLOC to increase in both types of quality improving
commits the effect size is higher in perfective commits. It seems that bug fixing operations
do not add enough comment lines to show a larger difference here for corrective commits.
CD: the comment density of perfective commits is not statistically significantly different
from non-perfective commits. We would have expected a difference here because perfective
maintenance should include additional comments on new or previously uncommented
code. We can see a difference for corrective commits here. This shows that the density of
comments is also improving in bug fixing operations probably due to clarifications for parts
of the code that were fixed.
AD: the API documentation metric does change in perfective and corrective commits
compared to non-perfective and non-corrective commits. A reason could be that perfective
commits do add API documentation to make the difference significant. Corrective changes
that introduce code in our study subjects seem to almost always include API documentation,
therefore we can see a difference here. However, the effect size is negligible in both cases.
NOA: the number of ancestors is lower in perfective commits as expected. This metric
would be affected in simplification and clean up maintenance operations. For corrective
commits we can also see a lower value, this hints at some clean up operations happening
during bug fixing.
CBO: the coupling between objects is lower after perfective commits. This is expected
due to class removal and subsequent decoupling of classes. For corrective commits we can
also see a difference. While the effect size is negligible, there is some code clean up happening
during bug fixes, e.g., NOA and CC are also lower in corrective than in non-corrective
commits.
NII: the number of incoming invocations is lower in both perfective and corrective commits.
However, the effect size is small in perfective and negligible in corrective commits. It
seems reasonable to see a difference in this metric, because in the case of perfective commits,
we have lots of source code removal. However, there are also maintenance activities
which are decoupling classes which would also impact this metric. Corrective maintenance
seems to involve only limited decoupling operations, also seen in CBO.
Minor: The PMD warnings of minor severity are different in both types of changes.
However, we can see that the effect size is larger for perfective changes which makes sense
as those warnings can be part of perfective maintenance.
Major: The PMD warnings of major severity are also different in both types of changes.
We can see the difference in effect size again and we expect the reason is the same as for
Minor.
Critical: The PMD warnings of critical severity are different for both types of changes.
Here, the effect size is negligible for both types. However, as they are only changed in about
7% of our commits, they are not changing often regardless of commit type.
Empir Software Eng (2023) 28:30 Page 21 of 40 30
There are significant differences between perfective and corrective changes. Corrective
changes do not have a positive impact on all quality metrics. Therefore, we
reject H2 that intended quality improvements have a positive impact on quality
metric values.
5.2 Summary RQ1
In summary, we have the following results for RQ1.
RQ1 Summary
While intended quality improvements by developers yield measurable differences
in almost all metrics we find that not all metric values are changing in the expected
direction.
Perfective changes
Perfective commits have a positive effect on metric values that measure code
complexity through the size, conditional statements, number of parameters, and
coupling. For two metrics we do not find the expected difference to non-perfective
commits. Code clones and comment density metric values are not statistically
significantly different in perfective commits.
Corrective changes
Only for two metrics, we observe a non-negligible and statistically significant
change that we predicted. For LLOC, McCC and NLE, we observe the opposite of
the expectation, which indicates that bug fixes add complex code.
5.3 Exploratory Study
To answer our RQ2: What kind of files are the target of internal or external quality improvements?
We conduct an exploratory study. We present the results which files are changed in
which change category with respect to their metric values. The extracted metrics are considered
on a per-change basis, i.e., we divide the metrics by the number of changed files
to get an average metric value per file. We depict the average metric value per file before
the change is applied in Fig. 3 as box plot. The median for each metric per file is listed in
Table 8. This provides a view on the average metric values per file before a perfective or
corrective change is applied.
In addition to the per file metric values we include a kernel density estimation of the
metric values before the change is applied in Fig. 4. In Fig. 4 the metric values are depicted
per change. This provides an additional view on the differences in densities for metric values
before a perfective or corrective change is applied. Figure 3 shows box plots for the metric
values of files before the change is applied. We can see that, perfective changes are not
necessarily applied to complex files. If we compare the median values in Table 8 we can see
that perfective changes are applied to smaller, simpler files than the average or corrective
change. McCC, LLOC, NLE, NUMPAR and CBO are lower for the files which receive
perfective changes, while CLOC, CD, AD are higher. This means that less complex and
well documented files are often the target of perfective changes. If we look at corrective
changes we see that they are more complex and usually larger files. McCC, LLOC, NLE,
NUMPAR, CBO, NII as well as Minor, Major and Critical are higher than all changes, or
30 Page 22 of 40 Empir Software Eng (2023) 28:30
Fig. 3 Static source code metrics divided by the number of changed files before the change is applied. Fliers
are omitted
perfective changes. As we consider the metric values before the change is applied they can
be considered pre-bugfix. However, when we consider our results for RQ1 the corrective
changes usually increase the complexity even further.
Table 8 Median metric values
per file before the change is
applied
Metric All Perfective Corrective
McCC 21.78 18.78 33.23
LLOC 186.98 163.75 264.18
NLE 9.60 8.33 14.00
NUMPAR 16.06 15.00 22.00
CC 0.04 0.04 0.05
CLOC 46.25 55.00 54.00
CD 0.25 0.32 0.25
AD 0.50 0.67 0.46
NOA 1.00 1.00 1.00
CBO 9.67 8.00 14.00
NII 8.00 8.50 9.50
Minor 7.00 6.00 9.67
Major 2.00 1.25 3.00
Critical 0.00 0.00 0.00
Empir Software Eng (2023) 28:30 Page 23 of 40 30
Fig. 4 Kernel density estimation plot of metric values for perfective and corrective categories before the
change
Table 9 show the results of our statistical tests. Analogous to RQ1 we compare the difference
between perfective and non-perfective as well as corrective and non-corrective. While
Table 9 Statistical test results for
perfective and corrective
commits regarding their average
metrics before the change,
Mann-Whitney U test p-values
(p-val) and effect size (d) with
category, n is negligible, s is
small, m is medium
Metric Perfective Corrective
p-val d p-val d
McCC <0.0001 0.05 (n) <0.0001 0.08 (n)
LLOC <0.0001 0.05 (n) <0.0001 0.05 (n)
NLE <0.0001 0.04 (n) <0.0001 0.07 (n)
NUMPAR 0.6367 – 0.0218 –
CC <0.0001 0.01 (n) 0.0011 –
CLOC <0.0001 0.12 (s) <0.0001 0.06 (n)
CD <0.0001 0.15 (s) <0.0001 0.15 (s)
AD <0.0001 0.17 (s) <0.0001 0.15 (s)
NOA 0.5109 – <0.0001 0.02 (n)
CBO <0.0001 0.09 (n) <0.0001 0.07 (n)
NII <0.0001 0.05 (n) <0.0001 0.04 (n)
Minor <0.0001 0.04 (n) <0.0001 0.02 (n)
Major <0.0001 0.09 (n) <0.0001 0.04 (n)
Statistically significant p-values Critical <0.0001 0.05 (n) <0.0001 0.03 (n)
are bolded
30 Page 24 of 40 Empir Software Eng (2023) 28:30
most metric differences are statistically significant, we observe only some small effect sizes
for the comment related metrics while the rest is negligible.
Figure 4 shows another perspective on our data in the form of a direct comparison of the
density between perfective and corrective changes. We can see that McCC, NLE, LLOC,
NUMPAR, CD, CBO, NII and Minor have a lower density for perfective than for corrective.
While the differences are small they are noticeable.
RQ2 Summary
The files that are targets of perfective changes are in median not large and complex
even before the change is applied. Corrective changes are applied to files which are
in median already complex and large. In particular the median values of McCC,
LLOC, CBO and NLE show this behavior. This also holds for the median values
of static analysis warnings. The median number of static analysis warnings is
lower before a perfective change is applied and higher before a corrective change is
applied. The differences are statistically significant for most metrics, however the
effect sizes are negligible to small.
6 Discussion
Our results show that size is different in both types of commits in H1. The size difference
between all commits and perfective as well as corrective commits shows that both tend to
be smaller than non-perfective and non-corrective commits. In case of perfective commits,
code is statistically significantly more often deleted.
The differences in change size as well as the increased number of deletions for perfective
commits we found for H1 confirms previous research. The studies by Mockus (2000),
Purushothaman and Perry (2005) and Alali et al. (2008) found that perfective maintenance
activities are usually smaller. Mockus (2000) as well as Purushothaman and Perry (2005)
found that corrective maintenance is also smaller and that perfective maintenance deletes
more code. Another indication that size between maintenance types is different can be
seen in the work by H¨onel et al. (2019), which used size based metrics as predictors for
maintenance types and showed that it improved the performance of classification models.
Our results for H2 show statistically significant differences in metric measurements
between perfective commits and non-perfective commits. This result indicates a confirmation
of the measurements used by quality models, as the majority of metrics change as
expected when developers actively improve the internal code quality. This empirical confirmation
of the connection between quality metrics and developer intent is one of our main
contributions and was, to the best of our knowledge, not part of any prior study. However,
there are several examples of prior work that assumed this relationship.
The publications by McCabe (1976) and Chidamber and Kemerer (1994) assume that
reducing complexity and coupling metrics increases software quality which is in line
with our developer intents. While all metrics are included in a current ColumbusQM version
(Bakota et al. 2014) because we used it as a basis, the CBO, McCC, LLOC, NOA
metrics are also part of the SQUALE model (Mordal-Manet et al. 2009) AD, NLE, McCC,
and PMD warnings are also part of Quamoco (Wagner et al. 2012). It seems that developers
and the Columbus quality model agree with their view on software quality. We find that
most of the metrics used in the quality model change when developers perceive their change
as quality increasing. This is also true for most of the metrics shared with the SQUALE
Empir Software Eng (2023) 28:30 Page 25 of 40 30
model and with the Quamoco quality model. However, the implementation for the metrics
may differ between the models. Our work establishes that all these quality models are
directly related to intended improvements of the internal code quality by the developers.
Surprisingly, we found only few statistically significant and non-negligible differences
for corrective commits. Not all software metric values are changing into the expected direction
for corrective commits. For example, we can see that McCC, LLOC and NLE are
increasing in corrective changes compared to non-corrective commits. While we are not
expecting them to decrease for every corrective commit, we assumed that in comparison
to all non-corrective commits they would be decreasing. Even when considering software
aging (Parnas 2001) we would expect the aging to impact all kinds of changes not just corrective
changes. When we look at popular data sets used in the defect prediction domain
we often find coupling, size and complexity software metrics (Herbold et al. 2022). For
example, the popular (as per the literature review from Hosseini et al. (2017)) data set by
Jureczko and Madeyski (2010) uses such features, but they are also common in more recent
data sets, e.g., by Ferenc et al. (2020) or Yatish et al. (2019).
That the most significant difference is in the size of changes could explain various recent
findings from the literature, in which size was found to be a very good indicator both for
release level defect prediction (Zhou et al. 2018) and just-in-time defect prediction (Huang
et al. 2017). This could also be an explanation for possible ceiling effects (Menzies et al.
2008) when such criteria are used, as the difference to non-corrective changes are relatively
small. We believe that these aspects should be further considered by the defect prediction
community and believe that more research is required to establish causal relationships
between features and defectiveness.
While the work by Peitek et al. (2021) indicates that cyclomatic complexity may not be
as indicative of code understandability as expected, we show within our work that it often
changes in quality increasing commits. It seems that developers associate overall complexity
as measured by McCC, NLE, NUMPAR with code that needs quality improvement. However,
as we can see in the exploratory part of our study the most complex files are usually
not targeted for quality increasing changes.
Our exploratory study to answer RQ2 about files that are the target of quality increasing
commits reveals additional interesting data. We show that perfective maintenance does not
necessarily target files that are in need of it due to high complexity in comparison to nonperfective
changes. In fact, low complexity files as measured by McCC and NLE are more
often part of additional quality increasing work by the developers. This may hint at problems
regarding the prioritization of quality improvements in the source code. Maybe errors
could have been avoided when perfective changes would have targeted more complex files.
There could also be effects of different developers or a bias for perfective changes towards
simpler code, this warrants future investigation. Corrective changes, in contrast to perfective
changes, are applied to files which are large and complex. This was expected, however
combined with the results of RQ1 this means that bugs are fixed in complex and large files
and then the files get, on average, even more complex and even larger.
Future work could investigate boundary values according to our data. When we compare
the median values of our measurements in Table 8 with current boundary values from
PMD,6 we may think that the PMD warning value of 80 McCC per file may be too high. A
PMD warning triggered at 34 McCC per file would have warned about at least 50% of the
6https://pmd.github.io/pmd/pmd rules java design.html#cyclomaticcomplexity
30 Page 26 of 40 Empir Software Eng (2023) 28:30
files that were in need of a bug fix. However, lowering the boundary will also result in more
warnings for files that were not target of corrective changes.
6.1 Implications for Researchers
Our results for H1 increase the validity of previous research by confirming previous
results in our study on a larger data set of different projects. Our confirmation that quality
increasing changes are smaller than non-perfective and non-corrective changes shows
that researchers developing a change classification approach can benefit from including size
based metrics.
Our results for H2 show that perfective changes reduce size and complexity metrics
in comparison to non-perfective changes. Previous studies investigating refactorings also
found an impact on size and complexity metrics. We are able to generalize this finding by
providing results of a superset of refactoring operations, namely perfective changes. This
indicates that perfective changes generally reduce size and complexity metrics. This also
indicates that software quality models that use the affected metrics in their code quality
estimations agree with the developers on what impacts code quality.
Increasing the external quality by fixing bugs, i.e., corrective changes, decreases the
internal quality, i.e., complexity metric values. Defect prediction models may assign a
higher risk to parts of the code that contained a bug before as there is an assumption of latent
bugs still existing (Kim et al. 2007; Rahman et al. 2011). Our data provides a fine grained
perspective by providing empirical data which shows that the code quality as measured by
static source code metrics is actually decreasing.
This also has implications for researchers developing and deploying defect prediction
models in practice. The fact that fixing a bug increases the risk of the file can lead to problems
regarding the acceptance of the model by practitioners as they have no way of reducing
the risk (Lewis et al. 2013). The results of our study could help to explain the reasons to
developers. We can empirically show that fixing a bug is a complex operation that introduces
even more complexity than non-corrective changes, even feature additions. According
to our results, the main driver of complexity in a project are bug fixes and the only way to
combat the rising complexity is perfective maintenance which should especially target large
and complex files.
In our results for RQ2 we see a difference between files before corrective changes are
applied and before non-corrective changes are applied. This difference is one of the sources
of the predictive power of defect prediction models. However, the difference is smaller than
expected. Incorporating metrics that have a larger difference in our data, e.g., comment
density and API documentation into defect prediction models, may increase their prediction
performance.
6.2 Implications for Practitioners
Our results for H2 suggest that, for the most part, software quality models match the expectations
of the developers. If practitioners select a software quality model which uses static
source code metrics that show a difference in our data they can expect that the model
matches their intuition.
In combination with RQ2, our results indicate that bug fixing is the main driver of complexity
in a software project and perfective changes are the main reducer of complexity.
This has implications for developers. If more complex files were targeted for perfective
Empir Software Eng (2023) 28:30 Page 27 of 40 30
maintenance bugs could possibly have been prevented. As fixing bugs does not decrease
complexity, perfective maintenance is the best way to reduce it and combat rising complexity
of the project as a whole. However, given the results for RQ2, we see that large and
complex files are not the main target of perfective maintenance. This is an opportunity for
improvement by shifting priorities for perfective maintenance to large and complex files.
Moreover, our results indicate that a bug fix should be treated similar to technical debt
regarding its negative impact on complexity metrics. To mitigate this, practitioners should
be aware that it would be beneficial to clean up and simplify the code that is introduced as
part of the bug fix.
7 Threats to Validity
In this section, we discuss the threats to validity we identified for our work.We discuss four
basic types of validity separately as suggested byWohlin et al. (2000) and include reliability
due to our manual classification approach.
7.1 Reliability
We classify changes to a software retroactively and without the developers. This may introduce
a researcher bias to the data and subsequently the results. However, this is a necessity
given the size of the data and the unrestricted time frame for the sample and full data
because it would not be feasible to ask developers about a couple of commits from years
ago. To mitigate this threat, we perform the classification labeling according to guidelines
and every change is independently classified by two researchers. We also compare our differences
with a sample of changes classified by the developers themselves from Mauczka
et al. (2015) and confirm that we are agreeing on most changes. In addition, we measure the
inter-rater agreement between the researchers and find that it is substantial.
7.2 Construct Validity
Our definition of quality improving may be too broad. We aggregate different types of
quality improvement together, e.g., improving error messages, structure of the code or readability.
This may influence the changes we observe within our metric values. While these
differences should be studied as well, we believe that a broad overview of generic quality
improvements independent of their type has advantages.We avoid the risk of being focused
only on structural improvements, i.e., due to use of generics or new Java features without
missing bigger changes due to simplification of method code.
7.3 Conclusion Validity
We are reporting differences in metric value changes between perfective and corrective
changes of the software development history of our study subjects. We find a difference
for perfective commits and only some non-negligible, statistically significant difference for
corrective commits. This could be an effect of our sample used as ground truth, however
we chose to draw randomly from a list of commits in our study subjects so that our sample
should be representative.
30 Page 28 of 40 Empir Software Eng (2023) 28:30
We use a deep learning model to classify all of our commits based on the ground truth
we provide. This can introduce a bias or errors in the classification. We note however, that
the non-negligible effect sizes for our results do not change. The quality metric evaluation
of only the ground truth data is included in the Appendix and shows similar results.We note
that for the small effect sizes we observe, a large number of observations are needed to show
a significant difference as is demonstrated by the results in this article when compared to
the ground truth.
7.4 Internal Validity
A possible threat could be tangled commits which improve quality and at the same time
add a feature. We mitigate this in our ground truth, by manual inspection of the commit
message of every change considered. We excluded tangled commits if it was possible to
determine this by the commit message. As no automatic untangling approach is available
to us and available approaches to label tangled commits already use the commit message
to find tangled commits we determine that tangled commits which are not identifiable from
the commit message are a minor threat.
Another threat could be a lower number of feature additions in our study subjects. Maybe
feature additions happen too infrequently to influence the results, therefore, corrective commits
are seen as adding more complex code than non-corrective commits.While we include
some projects that are in development for a long period of time, we believe this threat is
mitigated by the unrestricted time frame of our study.
Bots which commit code (Dey et al. 2020) could be a possible threat to our study. We
mitigate this threat by matching our author data against the bot data set provided by Dey
et al. (2020). We did not find matches for bots in our data. We were able to detect a Jenkins
bot only when dropping the restriction of our case study data that a commit has to change
non-test code. We also implemented the detection mechanism by Dey et al. (2020) which
uses the username and email of the author of the commit, as used by Dey et al. to create
their bot data set. This also yielded no bots in our data. Manual inspection of the author data
yielded two bot-like accounts which turned out to be from a previous cvs2svn conversion
as well as asf-sync-process which allows user patches without an account. However, the
content of changes by the accounts we found are created by developers. We determine that
the threat of bots in our data is low.
Missing information in a commit message could impact our results. Commits which are
in our other category could still be perfective or corrective without it being apparent from the
commit message. The study conducted by Tian et al. (2022) found that between 0.9% and
7.5% of commits do not contain why a change was made nor what it was that was changed.
This can not be mapped to our study completely because we do not discern between why
and what. Morevoer, some of what we found could map top both, e.g., simplify, clean up.
We are not able to mitigate this threat as we extract the intent of the developers only from
the commit message.
7.5 External Validity
We focus on a convenience sample of data consisting of Java Open Source projects under
the umbrella of the Apache Software Foundation.We consider this a minor threat to external
validity. The reason is that although we are limited to one organization, we still have a wide
variety of different types of software in our data. We believe that this mitigates the missing
variety of project patronage.
Empir Software Eng (2023) 28:30 Page 29 of 40 30
Furthermore, we only include Java projects. However, Java is used in a wide variety of
projects and remains a popular language. Its age provides us with a long history of data we
can utilize in this study. However, we note that this study may not generalize to all Java
projects much less all software projects in other languages.
8 Conclusion
Numerous quality measurements exist, and numerous software quality models try to connect
concrete quality metrics with abstract quality factors and sub factors. Although it seems
clear that some static source code metrics influence software quality factors, the question
of which and how much remains. Instead of relying on necessarily limited developer and
expert evaluations of source code or changes we extract metrics from past changes where
developers intended to increase the quality extracted from the commit message.
Within this work, we performed a manual classification of developer intents on a sample
of 2,533 commits from 54 Java open source projects by two researchers independently and
guided by classification guidelines. We classify the commits into three categories, perfective
maintenance, corrective maintenance, or neither. We further evaluate our classification
guidelines by re-classifying of a developer labeled sample. We use the manually labeled
data as ground truth to evaluate and then fine tune a state-of-the-art deep learning model for
text classification. The fine-tuned model is then used to classify all available commits into
our categories increasing our data size to 125,482 commits. We extract static source code
metrics and static analysis warnings for all 125,482 commits which allows us to investigate
the impact of changes and the distribution of metric values before the changes are applied.
Based on the literature, we hypothesize that certain metric values change in a certain direction,
e.g., perfective changes reduce complexity. We find that perfective commits are more
often removing code and generally add fewer lines. Regarding the metric measurements,
we find that most metric value changes of perfective commits are significantly different to
non-perfective commits and have a positive, non-negligible impact on the majority of metric
values.
Surprisingly, we found that corrective changes are more complex and larger than noncorrective
changes. It seems that fixing a bug increases the size, but also the complexity
measured via McCC and NLE. As we compare against all non-corrective changes, we were
expecting less addition of complexity as e.g., feature additions.We conclude that the process
of performing a bug fix tends to add more complex code than non-corrective changes.
We find that complex files are not necessarily the primary target for quality increasing
work by developers, including refactoring. To the contrary, we find that perfective
quality changes are applied to files that are already less complex than files changed in nonperfective
or corrective commits. Files contained in corrective changes on the other hand are
more complex and usually larger than files contained in either perfective or non-corrective
changes. In combination with our first result this shows that corrective changes are applied
to files which are already complex and get even more complex after the change is applied.
While we explored a limited number of metrics and commits we think that this approach
can be used to evaluate more metrics connected with software quality in a meaningful way
and help practitioners and researchers with additional empirical data.
30 Page 30 of 40 Empir Software Eng (2023) 28:30
Appendix: Ground Truth Only Results
Fig. 5 Ground truth only. Commit size distribution over all projects for all, perfective and corrective commits.
Fliers are omitted
Table 10 Ground truth only. Statistical test results for perfective and corrective commits, Mann-Whitney U
test p-values (p-val) and effect size (d) with category n is negligible, s is small
Metric Perfective Corrective
p-value d p-val d
#lines added <0.0001 0.20 (s) <0.0001 0.20 (s)
#lines deleted <0.0001 0.13 (s) <0.0001 0.17 (s)
#files modified 0.2829 – <0.0001 0.22 (s)
#hunks 0.7009 – <0.0001 0.21 (s)
Statistically significant p-values are bolded
Fig. 6 Ground truth only. Static source code metric value changes in all, perfective and corrective commits
divided by changed lines. Fliers are omitted
Empir Software Eng (2023) 28:30 Page 31 of 40 30
Table 11 Ground truth only. Statistical test results for perfective and corrective commits, Mann-Whitney U
test p-values (p-val) and effect size (d) with category, n is negligible, s is small, m is medium
Metric Perfective Corrective
p-val d p-val d
McCC <0.0001 0.37 (m) 1.0000 –
LLOC <0.0001 0.42 (m) 1.0000 –
NLE <0.0001 0.26 (s) 0.9577 –
NUMPAR <0.0001 0.24 (s) <0.0001 0.09 (n)
CC 1.0000 – <0.0001 0.12 (s)
CLOC <0.0001 0.19 (s) 0.1906 –
CD 0.9303 – <0.0001 0.15 (s)
AD 0.1556 – <0.0001 0.10 (s)
NOA <0.0001 0.08 (n) <0.0001 0.09 (n)
CBO <0.0001 0.18 (s) 0.0145 –
NII <0.0001 0.19 (s) 0.0620 –
Minor <0.0001 0.18 (s) 0.0005 –
Major <0.0001 0.10 (s) 0.0002 0.06 (n)
Critical <0.0001 0.06 (n) 0.1111 –
Statistically significant p-values are bolded. All values are normalized for changed lines
Fig. 7 Ground truth only. Static source code metrics before the change is applied. Fliers are omitted
30 Page 32 of 40 Empir Software Eng (2023) 28:30
Table 12 Median metric values
before the change is applied Metric All Perfective Corrective
McCC 21.00 18.00 34.00
LLOC 187.22 160.38 270.00
NLE 9.50 7.67 15.20
NUMPAR 16.00 14.67 21.00
CC 0.04 0.04 0.04
CLOC 48.22 55.00 55.00
CD 0.25 0.31 0.24
AD 0.50 0.63 0.49
NOA 1.00 1.00 1.00
CBO 9.50 8.00 14.00
NII 8.00 8.00 9.00
Minor 7.00 5.43 10.00
Major 2.00 1.00 2.67
Critical 0.00 0.00 0.00
Table 13 Ground truth only.
Statistical test results for
perfective and corrective
commits regarding their average
metrics before the change,
Mann-Whitney U test p-values
(p-val) and effect size (d) with
category, n is negligible, s is
small, m is medium
Metric Perfective Corrective
p-val d p-val d
McCC 0.0003 – 0.0016 –
LLOC 0.0005 – 0.1138 –
NLE 0.0003 – 0.0072 –
NUMPAR 0.5344 – 0.4704 –
CC 0.4142 – 0.0210 –
CLOC <0.0001 0.10 (n) 0.0111 –
CD <0.0001 0.15 (s) <0.0001 0.16 (s)
AD <0.0001 0.15 (s) <0.0001 0.15 (s)
NOA 0.6847 – 0.2103 –
CBO <0.0001 0.11 (s) 0.0190 –
NII 0.0510 – 0.0105 –
Minor 0.0006 – 0.6288 –
Major <0.0001 0.12 (s) 0.0852 –
Statistically significant p-values Critical 0.0179 – 0.5730 –
are bolded
Empir Software Eng (2023) 28:30 Page 33 of 40 30
Table 14 Detailed statistical tests results for metric changes
Metric MWU statistic Median SHA statistic SHA p-val
Perfective changes
McCC 2579401012.5 0.02,0.00 0.55,0.27 <0.0001,<0.0001
LLOC 2691844899.5 0.25,0.00 0.57,0.20 <0.0001,<0.0001
NLE 2351847133.0 0.00,0.00 0.58,0.20 <0.0001,<0.0001
NUMPAR 2328626543.0 0.00,0.00 0.39,0.05 <0.0001,<0.0001
CC 1666541612.5 0.00,0.00 0.03,0.01 <0.0001,<0.0001
CLOC 2158356261.5 0.00,0.00 0.32,0.34 <0.0001,<0.0001
CD 1715608163.5 0.00,0.00 0.41,0.21 <0.0001,<0.0001
AD 1899339427.0 0.00,0.00 0.25,0.13 <0.0001,<0.0001
NOA 1997259809.5 0.00,0.00 0.06,0.01 <0.0001,<0.0001
CBO 2208901912.0 0.00,0.00 0.21,0.04 <0.0001,<0.0001
NII 2210463268.0 0.00,0.00 0.09,0.07 <0.0001,<0.0001
Minor 2201853734.0 0.00,0.00 0.04,0.01 <0.0001,<0.0001
Major 2077680338.5 0.00,0.00 0.04,0.00 <0.0001,<0.0001
Critical 1952568002.5 0.00,0.00 0.05,0.05 <0.0001,<0.0001
Corrective changes
McCC 1319862052.5 0.00,0.00 0.36,0.36 <0.0001,<0.0001
LLOC 1406100592.5 0.07,0.18 0.36,0.36 <0.0001,<0.0001
NLE 1538986445.5 0.00,0.00 0.35,0.35 <0.0001,<0.0001
NUMPAR 1736495605.5 0.00,0.00 0.14,0.14 <0.0001,<0.0001
CC 1781604826.0 0.00,0.00 0.01,0.01 <0.0001,<0.0001
CLOC 1665288104.5 0.00,0.00 0.38,0.38 <0.0001,<0.0001
CD 1833218654.0 0.00,0.00 0.28,0.28 <0.0001,<0.0001
AD 1719709796.5 0.00,0.00 0.19,0.19 <0.0001,<0.0001
NOA 1700427713.0 0.00,0.00 0.03,0.03 <0.0001,<0.0001
CBO 1687001103.5 0.00,0.00 0.09,0.09 <0.0001,<0.0001
NII 1621472694.0 0.00,0.00 0.11,0.11 <0.0001,<0.0001
Minor 1664776380.0 0.00,0.00 0.01,0.01 <0.0001,<0.0001
Major 1667877088.0 0.00,0.00 0.01,0.01 <0.0001,<0.0001
Critical 1631274846.5 0.00,0.00 0.07,0.07 <0.0001,<0.0001
Accompanies Table 7. SHA is Shapiro-Wilk, MWU is Mann-Whitney U test, the number of samples for
non-perfective is 77,630, for perfective the number is 47,852. The number of samples for non-corrective is
90,258 and for corrective 35,124. For both samples the median, Shapiro-Wilk test statistic and p-value are
given comma separated
30 Page 34 of 40 Empir Software Eng (2023) 28:30
Table 15 Detailed statistical tests results for metrics before the change is applied
Metric MWU statistic Median SHA statistic SHA p-val
Perfective changes
McCC 1946723702.0 47.00,39.00 0.27,0.21 <0.0001,<0.0001
LLOC 1946637361.5 397.00,335.00 0.26,0.21 <0.0001,<0.0001
NLE 1934702498.0 21.00,18.00 0.28,0.24 <0.0001,<0.0001
NUMPAR 1860319211.0 34.00,32.00 0.25,0.20 <0.0001,<0.0001
CC 1881849087.0 0.09,0.08 0.07,0.10 <0.0001,<0.0001
CLOC 1642584608.5 84.00,118.00 0.18,0.24 <0.0001,<0.0001
CD 1570226793.0 0.40,0.54 0.08,0.14 <0.0001,<0.0001
AD 1548982847.0 0.83,1.00 0.11,0.14 <0.0001,<0.0001
NOA 1861405551.0 2.00,2.00 0.13,0.09 <0.0001,<0.0001
CBO 2023896520.5 21.00,15.00 0.24,0.16 <0.0001,<0.0001
NII 1756171669.5 15.00,18.00 0.25,0.21 <0.0001,<0.0001
Minor 1926916681.5 15.00,13.00 0.15,0.13 <0.0001,<0.0001
Major 2025070328.5 4.00,3.00 0.23,0.17 <0.0001,<0.0001
Critical 1949852017.5 0.00,0.00 0.19,0.12 <0.0001,<0.0001
Corrective changes
McCC 1455448657.0 41.00,50.00 0.23,0.23 <0.0001,<0.0001
LLOC 1506999970.0 361.00,399.00 0.23,0.23 <0.0001,<0.0001
NLE 1477296467.5 18.00,22.00 0.25,0.25 <0.0001,<0.0001
NUMPAR 1573653093.5 33.00,33.00 0.22,0.22 <0.0001,<0.0001
CC 1605078855.0 0.09,0.08 0.09,0.09 <0.0001,<0.0001
CLOC 1683529860.5 101.00,83.00 0.22,0.22 <0.0001,<0.0001
CD 1832629967.5 0.51,0.35 0.12,0.12 <0.0001,<0.0001
AD 1822953682.5 1.00,0.75 0.13,0.13 <0.0001,<0.0001
NOA 1616227506.0 2.00,2.00 0.10,0.10 <0.0001,<0.0001
CBO 1476937730.0 17.00,21.00 0.19,0.19 <0.0001,<0.0001
NII 1655048856.5 17.00,14.00 0.22,0.22 <0.0001,<0.0001
Minor 1557520234.5 14.00,14.00 0.14,0.14 <0.0001,<0.0001
Major 1516141644.5 3.00,4.00 0.19,0.19 <0.0001,<0.0001
Critical 1546362144.0 0.00,0.00 0.15,0.15 <0.0001,<0.0001
Accompanies Table 9. SHA is Shapiro-Wilk, MWU is Mann-Whitney U test, the number of samples for
non-perfective is 77,630, for perfective the number is 47,852. The number of samples for non-corrective is
90,258 and for corrective 35,124. For both samples the median, Shapiro-Wilk test statistic and p-value are
given comma separated
Acknowledgements We want to thank the GWDG G¨ottingen7 for providing us with computing resources
within their HPC-Cluster.
Funding Open Access funding enabled and organized by Projekt DEAL. This work was partly funded by
the German Research Foundation (DFG) through the project DEFECTS, grant 402774445
Data Availability The datasets generated during and/or analysed during the current study are available in
the Zenodo repository, https://doi.org/10.5281/zenodo.7078179.
7https://www.gwdg.de
Empir Software Eng (2023) 28:30 Page 35 of 40 30
Declarations
Conflict of Interests The authors have no competing interests to declare that are relevant to the content of
this article.
Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which
permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,
and indicate if changes were made. The images or other third party material in this article are included in the
article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is
not included in the article’s Creative Commons licence and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.
To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
References
Abdi H (2007) Bonferroni and sidak corrections for multiple comparisons. In: Encyclopedia of measurement
and statistics. Sage, Thousand Oaks, pp 103–107
Al Dallal J, Abdin A (2018) Empirical evaluation of the impact of object-oriented code refactoring
on quality attributes: a systematic literature review. IEEE Trans Softw Eng 44(1):44–69.
https://doi.org/10.1109/TSE.2017.2658573
Alali A, Kagdi H, Maletic JI (2008) What’s a typical commit? A characterization of open source software
repositories. In: 2008 16th IEEE international conference on program comprehension, pp 182–191.
https://doi.org/10.1109/ICPC.2008.24
AlOmar EA, Mkaouer MW, Ouni A (2021) Toward the automatic classification of self-affirmed refactoring.
J Syst Softw 171:110821. https://doi.org/10.1016/j.jss.2020.110821. http://www.sciencedirect.com/
science/article/pii/S016412122030217X
Alshayeb M (2009) Empirical investigation of refactoring effect on software quality. Inf Softw Technol
51(9):1319–1326. https://doi.org/10.1016/j.infsof.2009.04.002. http://www.sciencedirect.com/science/
article/pii/S095058490900038X
Bakota T, Heged˝us P, K¨ortv´elyesi P, Ferenc R, Gyim´othy T (2011) A probabilistic software quality
model. In: 2011 27th IEEE international conference on software maintenance (ICSM), pp 243–252.
https://doi.org/10.1109/ICSM.2011.6080791
Bakota T, Heged˝us P, Siket I, Lad´anyi G, Ferenc R (2014) Qualitygate sourceaudit: a tool for
assessing the technical quality of software. In: 2014 Software evolution week—IEEE conference
on software maintenance, reengineering, and reverse engineering (CSMR-WCRE), pp 440–445.
https://doi.org/10.1109/CSMR-WCRE.2014.6747214
Bavota G, De Lucia A, Di Penta M, Oliveto R, Palomba F (2015) An experimental investigation
on the innate relationship between quality and refactoring. J Syst Softw 107:1–14.
https://doi.org/10.1016/j.jss.2015.05.024. http://www.sciencedirect.com/science/article/pii/
S0164121215001053
Boehm BW, Brown JR, Lipow M (1976) Quantitative evaluation of software quality. In: Proceedings of the
2nd international conference on software engineering, ICSE ’76. IEEE Computer Society Press, Los
Alamitos, pp 592–605. http://dl.acm.org/citation.cfm?id=800253.807736
Chahal KK, Saini M (2018) Developer dynamics and syntactic quality of commit messages in oss projects.
In: Stamelos I, Gonzalez-Baraho˜na JM, Varlamis I, Anagnostopoulos D (eds) Open source systems:
enterprise software and solutions. Springer International Publishing, Cham, pp 61–76
Ch’avez A, Ferreira I, Fernandes E, Cedrim D, Garcia A (2017) How does refactoring affect internal
quality attributes? A multi-project study. In: Proceedings of the 31st Brazilian symposium
on software engineering, SBES’17. Association for Computing Machinery, New York, pp 74–83.
https://doi.org/10.1145/3131151.3131171
Chidamber SR, Kemerer CF (1994) A metrics suite for object oriented design. IEEE Trans Softw Eng
20(6):476–493. https://doi.org/10.1109/32.295895
Cliff N (1993) Dominance statistics: ordinal analyses to answer ordinal questions. Psychol Bull
Cohen J (1960) A coefficient of agreement for nominal scales. Educ Psychol Meas 20(1):37–46.
https://doi.org/10.1177/001316446002000104
30 Page 36 of 40 Empir Software Eng (2023) 28:30
D’Ambros M, Lanza M, Robbes R (2012) Evaluating defect prediction approaches: a
benchmark and an extensive comparison. Empirical Softw Engg 17(4–5):531–577.
https://doi.org/10.1007/s10664-011-9173-9
Dey T, Mousavi S, Ponce E, Fry T, Vasilescu B, Filippova A, Mockus A (2020) Detecting and
characterizing bots that commit code. In: Proceedings of the 17th international conference on
mining software repositories. Association for Computing Machinery, New York, pp 209–219.
https://doi.org/10.1145/3379597.3387478
Fakhoury S, Roy D, Hassan A, Arnaoudova V (2019) Improving source code readability: theory and practice.
In: 2019 IEEE/ACM 27th international conference on program comprehension (ICPC), pp 2–12.
https://doi.org/10.1109/ICPC.2019.00014
Fenton N, Bieman J (2014) Software metrics: a rigorous and practical approach, 3rd edn. CRC Press, Inc.,
Boca Raton
Ferenc R, Gyimesi P, Gyimesi G, T´oth Z, Gyim´othy T (2020) An automatically created novel bug dataset
and its validation in bug prediction. J Syst Softw 169:110691. https://doi.org/10.1016/j.jss.2020.110691.
http://www.sciencedirect.com/science/article/pii/S0164121220301436
Fu Y, Yan M, Zhang X, Xu L, Yang D, Kymer JD (2015) Automated classification of
software change messages by semi-supervised latent dirichlet allocation. Inf Softw Technol
57:369–377. https://doi.org/10.1016/j.infsof.2014.05.017. http://www.sciencedirect.com/science/article/
pii/S0950584914001347
Ghadhab L, Jenhani I, Mkaouer MW, Ben Messaoud M (2021) Augmenting commit classification by
using fine-grained source code changes and a pre-trained deep neural language model. Inf Softw Technol
135:106566. https://doi.org/10.1016/j.infsof.2021.106566. https://www.sciencedirect.com/science/
article/pii/S0950584921000495
Gharbi S, Mkaouer MW, Jenhani I, Messaoud MB (2019) On the classification of software change
messages using multi-label active learning. In: Proceedings of the 34th ACM/SIGAPP symposium
on applied computing, SAC ’19. Association for Computing Machinery, New York, pp 1760–1767.
https://doi.org/10.1145/3297280.3297452
Griessom RJ, Kim JJ (2005) Effect sizes for research: a broad practical approach. Lawrence Erlbaum
Associates Publishers
Gyimothy T, Ferenc R, Siket I (2005) Empirical validation of object-orientedmetrics on open source software
for fault prediction. IEEE Trans Softw Eng 31(10):897–910. https://doi.org/10.1109/TSE.2005.112
Hattori LP, Lanza M (2008) On the nature of commits. In: Proceedings of the 23rd IEEE/ACM international
conference on automated software engineering, ASE’08. IEEE Press, Piscataway, pp III–63–III–71.
https://doi.org/10.1109/ASEW.2008.4686322
Herbold S, Trautsch A, Trautsch F, Ledel B (2022) Problems with SZZ and features: An empirical
study of the state of practice of defect prediction data collection. Empir Software Eng 27:42.
https://doi.org/10.1007/s10664-021-10092-4
Herzig K, Just S, Zeller A (2013) It’s not a bug, it’s a feature: how misclassification impacts bug prediction.
In: Proceedings of the 2013 international conference on software engineering, ICSE ’13. IEEE Press,
pp 392–401
H¨onel S, EricssonM, L¨oweW,Wingkvist A (2019) Importance and aptitude of source code density for commit
classification into maintenance activities. In: 2019 IEEE 19th international conference on software
quality, reliability and security (QRS), pp 109–120. https://doi.org/10.1109/QRS.2019.00027
Hosseini S, Turhan B, Gunarathna D (2017) A systematic literature review and meta-analysis on cross project
defect prediction. IEEE Trans Softw Eng PP(99):1–1. https://doi.org/10.1109/TSE.2017.2770124
Huang Q, Xia X, Lo D (2017) Supervised vs unsupervised models: a holistic look at effort-aware just-intime
defect prediction. In: 2017 IEEE International conference on software maintenance and evolution
(ICSME), pp 159–170. https://doi.org/10.1109/ICSME.2017.51
ISO/IEC (2001) Iso/iec 9126. software engineering—product quality
ISO/IEC (2011) ISO/IEC 25010:2011, systems and software engineering—systems and software quality
requirements and evaluation (square)—system and software quality models
Jureczko M, Madeyski L (2010) Towards identifying software project clusters with regard to
defect prediction. In: Proceedings of the 6th international conference on predictive models
in software engineering, PROMISE ’10. Association for Computing Machinery, New York.
https://doi.org/10.1145/1868328.1868342
Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N (2013) A largescale
empirical study of just-in-time quality assurance. IEEE Trans Softw Eng 39(6):757–773.
https://doi.org/10.1109/TSE.2012.70
Empir Software Eng (2023) 28:30 Page 37 of 40 30
Kim S, Zimmermann T, Whitehead EJ Jr, Zeller A (2007) Predicting faults from cached
history. In: 29th International conference on software engineering (ICSE’07), pp 489–498.
https://doi.org/10.1109/ICSE.2007.66
Kitchenham B, Pfleeger SL (1996) Software quality: the elusive target [special issues section]. IEEE Softw
13(1):12–21. https://doi.org/10.1109/52.476281
Landis JR, Koch GG (1977) An application of hierarchical kappa-type statistics in the assessment of majority
agreement among multiple observers. Biometrics 33(2):363–374. http://www.jstor.org/stable/2529786
Levin S, Yehudai A (2017) Boosting automatic commit classification into maintenance activities by utilizing
source code changes. In: Proceedings of the 13th international conference on predictive models and
data analytics in software engineering, PROMISE. Association for Computing Machinery, New York,
pp 97–106. https://doi.org/10.1145/3127005.3127016
Lewis C, Lin Z, Sadowski C, Zhu X, Ou R, Whitehead EJ (2013) Does bug prediction support human developers?
findings from a google case study. In: 2013 35th International conference on software engineering
(ICSE), pp 372–381. https://doi.org/10.1109/ICSE.2013.6606583
Mann HB, Whitney DR (1947) On a test of whether one of two random variables is stochastically larger than
the other. Ann Math Stat 18(1):50–60
Mauczka A, Huber M, Schanes C, Schramm W, Bernhart M, Grechenig T (2012) Tracing your maintenance
work—a cross-project validation of an automated classification dictionary for commit messages. In:
Proceedings of the 15th international conference on fundamental approaches to software engineering,
FASE’12. Springer, Berlin, pp 301–315. https://doi.org/10.1007/978-3-642-28872-2 21
Mauczka A, Brosch F, Schanes C, Grechenig T (2015) Dataset of developer-labeled commit messages. In:
Proceedings of the 12th working conference on mining software repositories, MSR ’15. IEEE Press,
Piscataway, pp 490–493. http://dl.acm.org/citation.cfm?id=2820518.2820595
McCabe TJ (1976) A complexity measure. IEEE Trans Softw Eng 2(4):308–320.
https://doi.org/10.1109/TSE.1976.233837
McCall JA, Richards PK, Walters GF (1977) Factors in software quality: concept and definitions of software
quality, vol 1(3). Rome Air Development Center, Air Force Systems Command, Griffiss Air Force Base,
New York
Menzies T, Turhan B, Bener A, Gay G, Cukic B, Jiang Y (2008) Implications of ceiling effects in
defect predictors. In: Proceedings of the 4th international workshop on predictor models in software
engineering, PROMISE ’08. Association for Computing Machinery, New York, pp 47–54.
https://doi.org/10.1145/1370788.1370801
Mockus Votta (2000) Identifying reasons for software changes using historic databases.
In: Proceedings 2000 international conference on software maintenance, pp 120–130.
https://doi.org/10.1109/ICSM.2000.883028
Mordal-Manet K, Balmas F, Denier S, Ducasse S, Wertz H, Laval J, Bellingard F, Vaillergues P (2009) The
squale model—a practice-based industrial quality model. In: 2009 IEEE International conference on
software maintenance, pp 531–534. https://doi.org/10.1109/ICSM.2009.5306381
NASA (2004) Nasa IV & V facility metrics data program. http://mdp.ivv.nasa.gov/repository.html
Pantiuchina J, Lanza M, Bavota G (2018) Improving code: the (mis) perception of quality metrics. In:
2018 IEEE International conference on software maintenance and evolution (ICSME), pp 80–91.
https://doi.org/10.1109/ICSME.2018.00017
Pantiuchina J, Zampetti F, Scalabrino S, Piantadosi V, Oliveto R, Bavota G, Penta MD (2020) Why
developers refactor source code: a mining-based study. ACM Trans Softw Eng Methodol 29(4).
https://doi.org/10.1145/3408302
Parnas DL (2001) Software aging. Addison-Wesley Longman Publishing Co., Inc, pp 551–567
Peitek N, Apel S, Parnin C, Brechmann A, Siegmund J (2021) Program comprehension and code complexity
metrics: an fmri study. In: 2021 IEEE/ACM 43rd international conference on software engineering
(ICSE), pp 524–536. https://doi.org/10.1109/ICSE43902.2021.00056
Purushothaman R, Perry DE (2005) Toward understanding the rhetoric of small source code changes. IEEE
Trans Softw Eng 31(6):511–526. https://doi.org/10.1109/TSE.2005.74
Rahman F, Posnett D, Hindle A, Barr E, Devanbu P (2011) Bugcache for inspections: hit or miss? In: Proceedings
of the 19th ACM SIGSOFT symposium and the 13th European conference on foundations of
software engineering, ESEC/FSE ’11. Association for Computing Machinery, New York, pp 322–331.
https://doi.org/10.1145/2025113.2025157
Santos EA, Hindle A (2016) Judging a commit by its cover: correlating commit message entropy
with build status on travis-ci. In: Proceedings of the 13th international conference on mining
software repositories, MSR ’16. Association for Computing Machinery, New York, pp 504–507.
https://doi.org/10.1145/2901739.2903493
30 Page 38 of 40 Empir Software Eng (2023) 28:30
Scalabrino S, Bavota G, Vendome C, Linares-V´asquez M, Poshyvanyk D, Oliveto R (2021)
Automatically assessing code understandability. IEEE Trans Softw Eng 47(3):595–613.
https://doi.org/10.1109/TSE.2019.2901468
Stroggylos K, Spinellis D (2007) Refactoring–does it improve software quality? In: Fifth international
workshop on software quality (woSQ’07: ICSE workshops 2007), pp 10–10.
https://doi.org/10.1109/WOSQ.2007.11
Swanson EB (1976) The dimensions of maintenance. In: Proceedings of the 2nd international conference on
software engineering. ICSE ’76. IEEE Computer Society Press, Washington, DC, pp 492–497
Tian Y, Zhang Y, Stol KJ, Jiang L, Liu H (2022) What makes a good commit message? In: Proceedings
of the 44th international conference on software engineering, ICSE ’22. Association for Computing
Machinery, New York, pp 2389–2401. https://doi.org/10.1145/3510003.3510205
Trautsch A, Herbold S, Grabowski J (2020a) A longitudinal study of static analysis warning evolution
and the effects of PMD on software quality in apache open source projects. Empir Softw Eng.
https://doi.org/10.1007/s10664-020-09880-1
Trautsch A, Trautsch F, Herbold S, Ledel B, Grabowski J (2020b) The smartshark ecosystem for software
repository mining. In: Proceedings of the 42st international conference on software engineering -
demonstrations. ACM
Trautsch A, Erbel J, Herbold S, Grabowski J (2021) Replication kit. https://github.com/atrautsch/emse2021
replication
Trautsch F, Herbold S, Makedonski P, Grabowski J (2017) Addressing problems with replicability
and validity of repository mining studies through a smart data platform. Empir Softw Eng.
https://doi.org/10.1007/s10664-017-9537-x
von der Mosel J, Trautsch A, Herbold S (2022) On the validity of pre-trained transformers for natural language
processing in the software engineering domain. IEEE Transactions on Software Engineering, 1–1.
https://doi.org/10.1109/TSE.2022.3178469
Wagner S, Lochmann K, Heinemann L, Kl¨as M, Trendowicz A, Pl¨osch R, Seidl A, Goeb A, Streit J (2012)
The quamoco product quality modelling and assessment approach. In: Proceedings of the 34th International
conference on software engineering, ICSE ’12. IEEE Press, Piscataway, pp 1133–1142. http://dl.
acm.org/citation.cfm?id=2337223.2337372
Wang S, Bansal C, Nagappan N (2021) Large-scale intent analysis for identifying large-revieweffort
code changes. Inf Softw Technol 130:106408. http://www.sciencedirect.com/science/article/pii/
S0950584920300033
Wilk MB, Shapiro SS (1965) An analysis of variance test for normality (complete samples). Biometrika
52(3-4):591–611. https://doi.org/10.1093/biomet/52.3-4.591
Wohlin C, Runeson P, H¨ost M, Ohlsson MC, Regnell B, Wessl´en A (2000) Experimentation in software
engineering: an introduction. Kluwer Academic Publishers, Norwell
Yan M, Fu Y, Zhang X, Yang D, Xu L, Kymer JD (2016) Automatically classifying software
changes via discriminative topic model: supporting multi-category and cross-project. J Syst Softw
113:296–308. https://doi.org/10.1016/j.jss.2015.12.019. http://www.sciencedirect.com/science/article/
pii/S016412121500285X
Yatish S, Jiarpakdee J, Thongtanunam P, Tantithamthavorn C (2019) Mining software defects: should we
consider affected releases? In: 2019 IEEE/ACM 41st international conference on software engineering
(ICSE), pp 654–665. https://doi.org/10.1109/ICSE.2019.00075
Zhou Y, Yang Y, Lu H, Chen L, Li Y, Zhao Y, Qian J, Xu B (2018) How far we have progressed in the
journey? An examination of cross-project defect prediction. ACM Trans Softw Eng Methodol 27(1).
https://doi.org/10.1145/3183339
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps
and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under
a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted
manuscript version of this article is solely governed by the terms of such publishing agreement and applicable
law.
Empir Software Eng (2023) 28:30 Page 39 of 40 30
Alexander Trautsch is a postdoctoral researcher at the AI Engineering
research group at the University of Passau. He received his
doctorate in 2022 from the Georg-August-Universit¨at G¨ottingen. His
research interests include empirical software engineering, software
evolution and the application of machine learning approaches to
improve software development.
Johannes Erbel is a postdoctoral researcher at the Georg-August-
Universit¨at G¨ottingen and works in the research group Software Engineering
for Distributed Systems at the Institute of Computer Science.
He received his doctorate in 2022 from the Georg-August-Universit¨at
G¨ottingen by investigating the reflection of scientific workflows
within a causally connected cloud runtime model. Dr. Erbel’s
research interests focus on model-driven engineering approaches, as
well as software evolution. For more information, see: https://swe.
informatik.uni-goettingen.de/staff/johannesmartin-erbel.
Steffen Herbold is professor and Chair of AI Engineering at the University
of Passau. His research is focused on the responsible and fair
solution of problems with machine learning, the quality assurance of
applications using machine learning, and their operation.
30 Page 40 of 40 Empir Software Eng (2023) 28:30
Jens Grabowski is professor at the Georg-August-Universitat
G¨ottingen and is heading the Software Engineering for Distributed
Systems Group. Prof. Grabowski is one of the developers of the standardized
testing languages TTCN-3 and UML Testing Profile. The
current research interests of Prof. Grabowski are directed towards
model-based development and testing, managed software evolution,
and empirical software engineering.
One Thousand and One Stories:
A Large-Scale Survey of Software Refactoring
Yaroslav Golubev
JetBrains Research
Saint Petersburg, Russia
yaroslav.golubev@jetbrains.com
Zarina Kurbatova
JetBrains Research
Saint Petersburg, Russia
zarina.kurbatova@jetbrains.com
Eman Abdullah AlOmar
Rochester Institute of Technology
Rochester, United States
eman.alomar@mail.rit.edu
Timofey Bryksin
JetBrains Research
Higher School of Economics
Saint Petersburg, Russia
timofey.bryksin@jetbrains.com
Mohamed Wiem Mkaouer
Rochester Institute of Technology
Rochester, United States
mwmvse@rit.edu
ABSTRACT
Despite the availability of refactoring as a feature in popular IDEs,
recent studies revealed that developers are reluctant to use them,
and still prefer the manual refactoring of their code. At JetBrains,
our goal is to fully support refactoring features in IntelliJ-based
IDEs and improve their adoption in practice. Therefore, we start
by raising the following main questions. How exactly do people
refactor code? What refactorings are the most popular? Why do
some developers tend not to use convenient IDE refactoring tools?
In this paper, we investigate the raised questions through the
design and implementation of a survey targeting 1,183 users of
IntelliJ-based IDEs. Our quantitative and qualitative analysis of the
survey results shows that almost two-thirds of developers spend
more than one hour in a single session refactoring their code; that
refactoring types vary greatly in popularity; and that a lot of developers
would like to know more about IDE refactoring features but
lack the means to do so. These results serve us internally to support
the next generation of refactoring features, as well as can help our
research community to establish new directions in the refactoring
usability research.
CCS CONCEPTS
• Software and its engineering → Software evolution; Maintaining
software.
KEYWORDS
Refactorings, IDE Refactoring Features, Software Maintenance
ACM Reference Format:
Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey
Bryksin, and Mohamed Wiem Mkaouer. 2021. One Thousand and One
Stories: A Large-Scale Survey of Software Refactoring. In Proceedings of the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8562-6/21/08. . . $15.00
https://doi.org/10.1145/3468264.3473924
29th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE ’21), August 23–28,
2021, Athens, Greece. ACM, New York, NY, USA, 11 pages. https://doi.org/
10.1145/3468264.3473924
1 INTRODUCTION
Refactoring [12] is traditionally defined as the process of improving
the internal code structure without altering its external behavior.
Since this practice had been introduced to a wide audience of software
engineers, it has become a crucial tool to maintain high-quality
software and to reduce its technical debt. Several refactoring types,
involving renaming, moving, and extracting elements have been
implemented as actionable tools in modern Integrated Development
Environments (IDEs), providing developers with an automatic and
safe way to apply these predefined code transformations [32, 33, 36].
Even though all modern IDEs usually have a top-level menu
with various options devoted to refactoring, several recent surveys
report that developers are often reluctant to adopt these features
and still manually refactor their code [17, 35]. Despite the highlevel
maturity of IDEs and the safety and interactivity of their
tools, manual refactoring is still widely adopted regardless of its
drawbacks and error-proneness.
While several studies have recently highlighted the lack of automated
refactoring usage [1, 15, 17, 30, 35, 40], little is known
regarding the reasons hindering the widespread adoption of automated
refactoring, especially when it is offered as a built-in feature
in modern IDEs. Furthermore, existing studies were limited by investigating
only a few types of refactoring, as well as by the number
of developers surveyed.
The goal of this paper is to share the results of a large-scale survey
about refactoring conducted by JetBrains Research, to reflect
on what refactorings developers actually use, as well as to raise
the awareness of the current usability challenges that developers
face when they use the IDE to refactor their code. In particular, we
designed a survey of 20 questions to investigate several dimensions
related to (1) general background information about respondents,
(2) developers’ familiarity with refactoring in general, (3) how developers
tend to refactor their code, (4) the extent to which they are
familiar with the IDE built-in refactoring functionalities, (5) along
with their degree of adoption and thoughts on specific refactoring
features of IDEs, namely the Undo and Preview. Our survey
1303
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin, and Mohamed Wiem Mkaouer
was sent to paid subscribers of the IntelliJ Platform-based IDEs
(IntelliJ IDEA, PyCharm, WebStorm, and others), and we received
1,183 complete responses, achieving a response rate of 6.0% and a
completion rate of 88.9%.
We provide the refactoring community with a variety of insights
that are currently being investigated by JetBrains IDE development
teams to plan future enhancements of the refactoring tools. The
raw data of the survey is available online.1 Notably, the survey has
revealed the following:
• While refactoring is a regular and recurrent practice supporting
the software development cycle, two-thirds of respondents
confirmed that their refactoring sessions can take
up to an hour or even longer.
• Some refactorings, such as Rename and Extract entities, are
more popular and intuitive than others, such as Pull Up and
Push Down entities. Despite the existence of built-in refactoring
tools, some developers opt for refactoring their code
using manual practices: for instance, Find and Replace is
popular for Renaming, Copy and Paste is typically used for
Moving.
• The familiarity of developers with IDE refactoring features
varies from one refactoring type to another. Also, the adoption
of an IDE feature correlates with the popularity of the
refactoring itself. For instance, Rename refactoring has been
found to be the most popular one, and its built-in Rename
tool is also heavily used. This is in contrast with Pull Up /
Push Down refactorings, which are the least popular, and consequently
their corresponding tools are used significantly
less frequently.
• The possibility of Preview and Undo helps in consolidating
the usage of refactoring tools, as developers tend to trust code
transformations more and understand their impact better.
Nevertheless, these functionalities can be considered too
complex when developers are performing what they believe
to be simple and intuitive refactorings.
2 RELATEDWORK
Some refactoring techniques and formalisms to guarantee program
preservation have been reported in an extensive survey study by
Mens and Tourwe [20]. The authors discussed the existing literature
in terms of refactoring activities and their automation techniques.
They reported different types of software artifacts being refactored,
along with existing refactoring tool support, and the impact of
refactoring in the software process. Murphy-Hill & Black [22] surveyed
112 Agile Open Northwest conference attendees and found
that refactoring tools are underused by professional programmers.
At Microsoft, Kim et al. [17] surveyed 328 professional software
engineers to investigate when and how they do refactoring. When
surveyed, the developers mentioned the main benefits of refactoring
to be: improved readability (43%), improved maintainability
(30%), improved extensibility (27%), and fewer bugs (27%). When
asked what provokes them to refactor, the main reason provided
was poor readability (22%). Only one code smell (i.e., code duplication)
was mentioned (13%). Sharma et al. [34] surveyed 39 software
architects to ask about the problems they are facing whenever they
1Raw data of the survey: https://zenodo.org/record/4923175
refactor their systems and the limitations of existing refactoring
tools they use. Their main findings are: (1) fear of breaking the code
restricts developers from adopting refactoring techniques, (2) lack
of awareness of the impact of refactoring on code quality is a major
obstacle to refactoring tasks, and (3) developers feel reluctant to
adopt refactoring because it might result in wasting their resources.
Oliveira et al. [25] surveyed 107 developers about their refactoring
output, using 7 refactoring types applied to pilot software systems.
They found significant differences in the outputs for the same tasks
due to the differences in the IDE refactorings they used. In their
extended work [26], Oliveira et al. confirmed that refactoring implementations
of various IDEs (Eclipse, NetBeans, etc.) have differences
in all refactoring types. They also reported that these IDEs have
different input parameters to apply refactorings.
Our study complements the ongoing effort of previous studies
by providing more in-depth insights regarding the challenges faced
by developers, specifically when using modern IDEs. Our study
is also the first to attract 1,183 developers, becoming the largest
refactoring survey in literature.
3 STUDY DESIGN
3.1 Pilot Survey
Since this survey was going to be distributed to a wide range of
participants, it was critical to ensure that our questions properly
convey the points we are seeking answers to. Therefore, we performed
a pilot study for the purpose of refining our questions and
survey protocol. The pilot version of the survey contained the
following questions: (1) What is your experience in software engineering
and what programming languages do you regularly use?
(2) Provided a list of popular refactorings, please select whether
you know each one of them in general and as an IDE feature. (3)
For the same refactorings, please select which ones you use and
how often. (4) What are your general thoughts about the current
state of automatic refactorings? (5) What are your negative experiences
with automatic refactorings? (6) If you had cases when you
wanted to perform a refactoring, but decided not to, what were
the reasons? (7) How often do you use the Preview IDE feature
when performing a refactoring? What are your main reasons for
using it? (8) How often do you use the Undo action after applying
a refactoring? What are your main reasons for using it?
The pilot version of the survey was reviewed by the members
of the Product Management team and the members of the Market
Research and Analytics team at JetBrains, who are experienced
with survey design and execution. We have received the following
feedback to refine the questions.
• In questions (2) and (3), we immediately show a list of refactoring
names to respondents, thus possibly alienating developers
who may perform refactorings but do not know
them by their names. Instead, we should tune our questions
according to the participants’ familiarity with refactorings.
• It is better to strictly divide questions about refactoring activities
in general and questions about IDE refactoring features
in order not to confuse less experienced participants.
• To avoid generic responses, it is recommended to tie respondents
to a specific time frame: asking about their experience
during the past month or during one programming session.
1304
One Thousand and One Stories: A Large-Scale Survey of Software Refactoring ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
Table 1: The summary of survey questions. Grey numbers near certain questions indicate that the presence of this question
conditionally depends on another question, specified by the number
I. Background
Question 1 How many years of coding experience do you have?
Question 2 What programming languages do you regularly use?
II. Familiarity with refactorings
Question 3 In the past month, how often have you performed any code refactoring?
Question 4 (3) Have you renamed anything in your code or project structure in the past month?
Question 5 (3) Have you moved any code from one location in the project to another?
Question 6 (3) During this time, did you ever refactor code for an hour or more in a single session?
III. Refactoring approaches
Question 7
For the following scenarios, please select all the approaches you have used in the past month.
(Renaming a class, method, variable, or symbol / Extracting a method or a variable from existing code / Moving code to another file)
Question 8 (7) In these scenarios, what were your main reasons for not using the IDE refactoring feature?
Question 9
For the following scenarios, please select all the approaches you have used in the past month.
(Inlining a variable or method / Changing the signature of an existing function / Moving a method up or down the class hierarchy)
Question 10 (9) In these scenarios, what were your main reasons for not using the IDE refactoring feature?
IV. IDE refactoring features
Question 11
How familiar are you with the following IDE refactoring features?
(Rename file, class, method, symbol, etc. / Extract method, variable, component, etc. / Move /
Inline variable or method / Change signature / Pull Up or Push Down member)
Question 12 (11) Please think about the last several times you used IDE refactorings. How happy were you with the overall experience?
Question 13 (12) Please tell us a bit more about your experience.
Question 14 (11) How often do you undo or revert an IDE refactoring action because you’re unhappy with the result?
Question 15 (14) The last few times you undid or reverted an IDE refactoring feature, what were the reasons?
V. Previewing refactorings
Question 16 When refactoring the code, do you find it useful to preview all of your changes before applying them?
Question 17 (16) For what types of changes do you find this feature most useful?
Question 18 (16) What are your main reasons for wanting to see a refactoring preview?
Question 19 (16) What are your main reasons for not finding a preview useful?
VI. Final thoughts
Question 20 Please share any thoughts or feedback you have about using the IDE refactorings.
• Question (6) is too broad, it is better to split it into more
specific questions. Question (7) is restricted to participants
who use previews, whereas it would be also interesting to
learn the reasons for not using that feature.
This feedback allowed us to reformulate some questions and
consolidate others. The resulting survey was then approved by the
Product Management team and the Market Research and Analytics
team, and its questions are enumerated in Table 1.
3.2 Final Survey
Now, let us describe all the research dimensions encapsulated in
the final version of the survey.
The complete list of questions is presented in Table 1. There
was a total of 20 questions in the survey: 9 single choice, 8 multiple
choice (in 4 of which a write-in Other answer was available),
and 3 open-ended. Also, since our survey targeted different groups
of developers, it contained conditional questions and one case of
branching. Conditional questions allow us to gather deeper insights
and target specific groups of respondents, and branching allows
tailoring questions to a specific group in case groups do not intersect
(for example, when some developers like a feature, and the
others do not). According to the survey methodology and the guidelines
proposed by Kitchenham and Pfleeger [10, 14, 18], conditional
questions and branching ensure that respondents are asked only
those questions that apply to them while allowing us to gather
deeper insights. You can find more information about conditional
questions and branching in our supplementary data package [13].
Let us now briefly overview the main sections of the survey.
I Background. The first introductory section had just two basic
questions related to the respondent’s coding experience and
the programming languages they use.
II Familiarity with refactorings. The second section aimed to
determine the respondent’s familiarity with refactorings. We
asked them how often they have been recently refactoring code.
In case the respondents said that they do it rarely or never at
all, we followed up by asking how often they have renamed or
moved code elements to ensure that their previous selection
was not due to their unfamiliarity with the term refactoring.
Finally, we asked the respondents whether they have spent one
hour or more in a single session refactoring their code.
III Refactoring approaches. The third section was dedicated to
discovering whether developers actually carry out refactorings
with IDE refactoring tools or manually. In the cases when they
did not refactor using IDE tools, we asked a follow-up question
to inquire about the reasons for that.
IV IDE refactoring features. In the fourth section, we concentrated
on the automatic refactoring tools of IDEs. We asked
the respondents how often they use each type of refactoring
features (e.g., Rename). If at least one refactoring feature was
regularly used, we presented more questions to gauge their
satisfaction with these features. We also inquired about how
often respondents had to undo an applied refactoring.
V Previewing refactorings. The fifth section discussed the usage
of previews when refactoring code.We divided respondents
into those who find previews useful and those who do not, and
then dived deeper into the reasons for both of these views.
1305
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin, and Mohamed Wiem Mkaouer
VI Final thoughts. Finally, in the last section of the survey, we
gave the respondents the opportunity to share any refactoringrelated
feedback.
3.3 Surveying Process
We selected respondents from the list of paid subscribers of IntelliJbased
IDEs (IntelliJ IDEA, PyCharm, CLion, WebStorm, etc.) [31]
who had previously agreed to receive invitations to such surveys
by email. In total, the mailing list consisted of 20,000 developers,
compiled evenly from users of different IntelliJ-based IDEs. This allowed
us to mitigate possible skews towards a given IDE or specific
languages and inquire about broader aspects of refactoring.
19,860 emails were successfully delivered, respondents were
given three weeks to fill the survey. In the end, we received 1,330
responses, out of which 1,183 were complete. This corresponds
to a response rate of 6.0% and a completion rate of 88.9%. We
considered the obtained response rate to be satisfactory, as it is
close to other studies in the field that had reported a response rate
between 5.7% [29] and 7.9% [19]
The first two questions in the survey gave us some background
information on the developers. The question about coding experience
(Question 1) demonstrated that 27.8% of the participants have
more than 16 years of experience, a total of 48.1% have more than
10 years of experience, and as much as 79% have more than 5 years
of experience. Only 2 respondents (0.2%) in our survey answered
I do not have any coding experience, and were therefore taken to the
end of the survey.
Question 2 allowed our participants to select up to 3 languages
that they are familiar with. The most popular languages by the
percentage of respondents in our survey are: JavaScript (46.6%),
Python (28.3%), PHP (27.2%), Java (24.4%), and SQL (21.4%). These
languages constitute some of the most popular programming languages
used in different software engineering domains [11]. Among
other languages selected by the participants are Typescript, C/C++,
C#, Go, Ruby, and others.
Overall, the results of this survey represent the opinions of more
than a thousand developers with a high average experience in
different popular programming languages, which can serve as a
practical window to the current general state of refactoring usage.
4 RESULTS
In this section, we describe the results of the survey, divided into
five parts, corresponding to sections II–VI mentioned in Section 3.2
and shown in Table 1.
Because of the conditional questions and branching, questions
in our survey have a different number of respondents answering
them. To avoid creating confusion with numbers not summing
up, we decided to present the results of all questions in the form
of percentages with respect to the number of respondents who
answered them. For all survey questions, the number of respondents
is explicitly stated in captions of the figures and tables. Also, to
avoid confusion in similar terms, we mark refactorings in general
in cursive and mark IDE refactoring features in small caps.
To process the open-ended questions, we used the open coding
technique based on guidelines provided by Cruzes et al. [9]. For
each question, firstly, the list of possible categories of answers was
compiled. This was done independently by the first two authors
in two iterations. On the first iteration, each author studied the responses
and gave them possible labels, and on the second iteration,
each author reduced the overlap between the labels and drew up
the final list of categories. After this, the first two authors compared
their lists of obtained categories and compiled the final list together.
Finally, using this final list, they independently did a third iteration
and labeled each response (no category, one specific category, or
several categories in the case of long responses). After this, they
compared and discussed the resulting labeling. In all the questionable
cases, the authors had a discussion, until a perfect consensus
was reached. This was done for all open-ended questions.
4.1 Familiarity with Refactorings
We started the main body of the survey with the basic Question 3
that asked how often the respondents performed any code refactoring
recently. Figure 1 shows the breakdown of the answers. It
can be seen that refactoring is an omnipresent practice in software
development: 40.6% of developers indicated that they refactor code
Almost every day and 36.9% more said that they refactor code Every
week. Only about 20% of users said that they refactor code Once or
twice a month, and just 2.5% Never refactored code.
Never
2.5%
Once or twice a month
20%
Every week
36.9%
Almost every day
40.6%
Figure 1: The answers to Question 3: In the past month, how
often have you performed any code refactoring? (Out of 1,181
respondents)
However, it might be the case that developers may have a different
perception of the word refactor. To account for this, if the
participant selected Once or twice a month or Never in the previous
question (266 respondents), we additionally showed them Questions
4 and 5, asking whether they renamed or moved anything
in the code recently. We found out that 84.2% of these 266 respondents
renamed entities in the code and 78.2% of them moved code
elements. Overall, only 14 respondents did not answer positively to
either of these questions, and therefore, we did not consider their
answers further, because they are not a target audience for any
questions about refactorings. The remaining survey considered the
answers of all the remaining 1,167 participants.
Respondents who are familiar with refactoring (1,145 respondents)
were exposed to the last question in this block, Question
6, asking whether they have been recently refactoring for an hour
or more in a single session. The results are presented in Figure 2.
Surprisingly, almost two-thirds of developers answered positively.
1306
One Thousand and One Stories: A Large-Scale Survey of Software Refactoring ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
No
25.9%
Yes
66.3%
Don't remember
7.8%
Figure 2: The answers to Question 6: During this time, did
you ever refactor code for an hour or more in a single session?
(Out of 1,145 respondents)
While it is widely accepted that refactoring helps to enforce
better design practices or to cope with design defects [12], recent
studies have shown that developers interleave refactoring activities
with other maintenance and development-related tasks in
practice, including feature updates, bug fixes, and API types migration
[2, 4, 16, 23, 28, 38]. Yet, little is known about the overhead
that refactorings exhibit, especially when they are executed in conjunction
with these development tasks. According to the survey
results, the majority of developers state that they spent over an
hour while refactoring their code, which is interesting, considering
that refactoring tools were designed to be executed independently,
applying small edits. This finding was interesting to us, and we
plan to monitor how refactoring features are being used for such
time frame in the future. Researchers should also pay close attention
to such refactoring sessions and study whether our current
refactoring tools remain efficient during continuous refactoring.
Summary: Intuitively, refactorings are a key element in the
software development cycle, according to the participants of the
survey. Nearly four out of five developers indicated that they
refactored code every week or even almost every day recently.
Interestingly, two-thirds of respondents said that they had refactoring
sessions of an hour or longer during this time.
4.2 Refactoring Approaches
The next section in the survey aimed to analyze how developers
execute their intended refactoring, and the degree of their reliance
on the IDE tools to do that. The results of survey questions corresponding
to this investigation are presented in Figure 3.
The first question (Question 7) aimed to verify whether developers
refactor code using built-in automated tools of IDEs or they
perform it manually. First, we clustered refactorings into 3 main
categories, namely the Rename, the Extract, and the Move categories.
Each category can target various code entities (e.g., rename
class, method, attribute, etc.). Then, we designed our question to
let respondents choose for each refactoring category whether they
refactor their code using the appropriate IDE feature, or they rely
on intuitive programming practices, such as Copy and Paste or Find
and Replace. Since developers may select more than one choice, we
report the percentage of developers selecting each option for each
refactoring category. All percentages are showcased in Figure 3a.
In general, it can be seen that a significant number of respondents
use IDE features to perform refactorings. Rename is in the
big lead, with as much as 85.8% of participants saying that they
used the IDE feature for it. This is a strong indicator that Rename
refactorings are implemented well as an IDE feature, their usage is
intuitive, and they produce stable, reliable output. However, since
renaming is a relatively simple change (at least compared to other
refactorings), almost half the respondents also used the Find and
Replace feature to locate all instances of a given code element and
rename it. Also, Rename can be seen as the most universal of all
the studied refactorings, because virtually no one said that they did
not encounter a scenario for its use.
As for the Extract refactoring, significantly fewer people used
the IDE feature, about 54.7%, especially when compared to the
Rename feature. However, the Extract feature is still solicited by
more than half of the respondents. Also, 10.7% of the respondents
indicated that they did not have a scenario where they had to
extract something. These findings are in line with the recent study
of Alcocer et al. [1] where the authors reported the existence of
certain usability issues of the Extract Method refactoring when
using IntelliJ IDEA.
Finally, Moving code appears to be the first refactoring where
the use of the IDE feature is not the most popular answer. 38.6% of
developers answered that they use the IDE feature and more than
half the respondents (57.5%) answered that they simply Copy and
Paste. Occasionally, moving elements can be basic, and in this case,
simply copying and pasting suffices. However, when the move is
more difficult, when it involves dependencies and complex relationships
between objects, then the IDE feature might be simpler
to use. Still, in general, Move refactorings are almost as popular as
Renames, with only 4.4% of participants saying that they did not
encounter this scenario.
It is also alarming to notice that for all three refactorings, about
one-third of respondents performed the refactoring by manual
editing (typing).
In the next Question 8, we selected all respondents who answered
Used Find and Replace, Used Copy and Paste, or Edited manually
for at least one refactoring (1,014 respondents), and asked
them about the reasons for not using the IDE refactoring features.
The results are presented in Figure 3b.
Two answers are the most popular. 48.4% of respondents said
that they were not sure that the automated refactoring would
work the way they wanted. This falls in line with the popular
notion that developers often do not trust automated refactoring
tools [6, 7, 23, 35, 39]. Also, 45.6% of respondents said that certain
refactorings were easier to conduct manually. This puts the results
of the previous question into perspective: a lot of developers use
“manual” ways of conducting refactorings, because sometimes it is
just more straightforward to do so. Therefore, one of the main takeaways
for our IDE development teams is the importance of making
the refactoring tools simpler and more intuitive. Since the Rename
feature is the most successful one in terms of adoption, analyzing
how developers activate it would help in understanding its success,
and potentially replicating it to other types of refactorings.
Fewer participants selected other options: 24.7% said that they
did not think about using IDE refactoring tools at the time, 21.7%
said that they did know about the IDE tools, and 15.9% said that
1307
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin, and Mohamed Wiem Mkaouer
85.8% 46.2% 21.3% 29.8% 0.3%
54.7% 20.7% 30.4% 33.2% 10.7%
38.6% 12.2% 57.5% 30.8% 4.4%
Used IDE
refactoring
Used Find
and Replace
Used Copy
and Paste
Edited
manually
Didn't have
this
scenario
Renaming a
class, method,
variable, or
symbol
Extracting a
method or a
variable from
existing code
Moving code to
another file
(a) The answers to Question 7: For the following scenarios, please select
all the approaches you have used in the past month. Several approaches
can be selected for each refactoring, the percentages show
the proportion of respondents who chose each option. (Out of 1,167
respondents)
45.6%
21.7%
24.7%
48.4%
15.9%
11.3%
It was easier to do it
manually
I didn't know about IDE
refactorings
I didn't think about using
IDE refactorings at the
time
I was not sure the
automated refactoring would
work the way I wanted
I had had some bad
experiences with IDE
refactorings before
Other
0 10 20 30 40 50
Respondents (%)
(b) The answers to Question 8: In these scenarios (Renaming, Extracting,
Moving), what were your main reasons for not using the IDE
refactoring feature? (Out of 1,014 respondents)
50.3% 11.7% 12.8% 25.4% 24.8%
53.6% 14.3% 9.8% 42.1% 10.6%
27.0% 6.4% 26.9% 27.2% 30.0%
Used IDE
refactoring
Used Find
and Replace
Used Copy
and Paste
Edited
manually
Didn't have
this
scenario
Inlining a
variable or
method
Changing the
signature of an
existing
function
Moving a method
up or down the
class hierarchy
(c) The answers to Question 9: For the following scenarios, please
select the approaches you’ve used in the past month. Several approaches
can be selected for each refactoring, the percentages show
the proportion of respondents who chose each option. (Out of 1,167
respondents)
48.2%
28.2%
24.4%
32.8%
10.4%
5.6%
It was easier to do it
manually
I didn't know about IDE
refactorings
I didn't think about using
IDE refactorings at the
time
I was not sure the
automated refactoring would
work the way I wanted
I had had some bad
experiences with IDE
refactorings before
Other
0 10 20 30 40 50
Respondents (%)
(d) The answers to Question 10: In these scenarios (Inlining, Changing
the Signature, Pulling/Pushing), whatwere your main reasons for
not using the IDE refactoring feature? (Out of 868 respondents)
Figure 3: Answers to Questions 7–10 about different ways of using refactorings.
they had certain negative experiences with refactoring features
before. When given a prompt to answer freely, some respondents
mentioned several issues with refactoring features: performance
issues, namespace confusion, difficulties with refactorings that involve
significant changes to the logic of the program. This feedback
is critical to our IDE development teams, and therefore, there will
be a follow up with these respondents to seek more technical details
with regard to their issues.
After this, we repeated the same line of questioning in regards
to three other refactorings, namely, Inlining a variable or a method,
Changing the signature of a function, and Moving a method up and
down the class hierarchy (known also as Pull up and Push down).
The answers to Question 9 are demonstrated in Figure 3c.
It can be seen that Changing the signature is approximately as
popular as Extracting in Question 7, with 10.6% of respondents
saying that they did not recently have this scenario in their work
and 53.6% saying that they used automated IDE tools. For this
refactoring, editing manually was also a very popular answer. As
for the other two refactorings, they seem to be less popular than the
previous ones. For Inlining, a quarter of the respondents said that
they did not recently encounter this scenario in their work. Still,
half the respondents said that they use IDE features for Inlining.
Pulling up / Pushing down appears to be the least popular of the
chosen refactorings. 30% of respondents did not recently encounter
this refactoring in their work, and only 27% used IDE features for
these refactorings.
In Question 10, we once again selected participants who answered
Used Find and Replace, Used Copy and Paste, or Edited manually
for at least one refactoring from Question 9 (868 respondents)
and asked them about their reasons for not using the IDE
tools. The results are presented in Figure 3d. The general distribution
of answers is similar to Figure 3b, but there are some differences
between them. There are more answers It was easier to do it manually
and I did not know about IDE refactorings. It is possible that
1308
One Thousand and One Stories: A Large-Scale Survey of Software Refactoring ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
5.1% 4.5% 2.1% 22.6% 65.7%
22.5% 18.8% 2.7% 25.7% 30.3%
27.4% 15.7% 3.4% 27.5% 26.0%
30.3% 18.9% 2.1% 25.3% 23.4%
26.8% 15.0% 2.7% 27.0% 28.5%
42.8% 20.7% 2.7% 19.3% 14.5%
Don't know about it
Know about it, but don't
use it
Used it in the past, but
will not use it again Use it sometimes Use it regularly
Rename file, class,
method, symbol, etc.
Extract method, variable,
component, etc.
Move
Inline variable or method
Change signature
Pull Up / Push Down
member
Figure 4: The answers to Question 11: How familiar are you with the following IDE refactoring features? (Out of 1,167 respondents)
these refactorings are less universally known. Another takeaway
for our IDE development teams is to attract developers’ attention to
the possibility of using these refactorings, perhaps by implementing
special tooltips or IDE notifications.
Summary: Renames are the most universal refactoring, with
virtually everyone using them. 85.8% of participants renamed
elements using IDE tools. On the other side, 30% of respondents
stated that they did not recently perform Pull Up and Push Down
refactorings. Despite the existence of IDE refactoring features,
developers still manually refactor their code, including Find and
Replace for Rename, Copy and Paste for Move, Editing manually
for Changing signature. Respondents justified their reluctance
to use IDE refactoring features with not knowing the outcomes,
along with manual refactoring being more intuitive.
4.3 IDE Refactoring Features
In this section, we focused our questions specifically on IDE refactoring
features. Question 11 asked the developers about their familiarity
with IDE refactoring features for the same six refactorings
studied in Section 4.2. The heatmap with all the results is demonstrated
in Figure 4.
Our first observation from the figure is the middle column. For
all the refactorings, there were equally very few respondents who
said that they refactored in the past, but will not do it again. This
is a very positive insight meaning that even with all the concerns
raised about automatic refactoring features, developers generally
do not give up on them.
Coming to the other answers, they correlate fairly well with
the previous questions. Once again, we can see the same two main
outliers. Rename is the most popular refactoring feature, with 65.7%
of respondents saying that they use it regularly and 22.6% more
saying that they use it sometimes. On the other hand, only 5.1%
of participants said they are unaware of the Rename IDE feature.
Meanwhile, Pull up / Push down remains the least popular: up to
42.8% of developers did not know about the Pull up / push down
IDE feature, only 19.3% of respondents used it sometimes, and only
14.5% used it regularly. All the other automated refactoring features
(Extract, Move, Inline, and Change signature) are distributed
more similarly between Rename and Pull up / Push down.
1,078 respondents (92.4%) selected Use it sometimes or Use it
regularly for at least one refactoring feature. For these participants,
the next block of questions was unlocked. In Question 12, we
asked the participants about their overall experience when using
the last several IDE refactoring features. The results are presented
in Table 2.
Table 2: The answers to Question 12: Please think about the
last several times you used IDE refactorings. How happy were
you with the overall experience? (Out of 1,078 respondents)
Answer % of respondents
Not at all happy 0.3%
Not happy 1.6%
Neither happy nor unhappy 12.6%
Happy 56.9%
Very happy 28.6%
It can be seen that the overall experience is overwhelmingly
positive, with 85.5% of developers giving positive responses, and
as much as 28.6% saying that they were Very happy with the last
several uses of IDE refactoring features.
Still, even though only 1.9% of developers answered either Not
happy or Not at all happy, there were 12.6% respondents who answered
Neither happy nor unhappy. It is very important to focus on
developers who do not give positive answers to get a deeper understanding
of underlying shortcomings in IDE refactoring features.
To do this, in Question 13, we asked all the respondents who did
not give a positive answer to share their experience in an open
form. Several issues were brought up by several participants:
(1) 22 developers mentioned that their main problem was a certain
negative experience when a refactoring was performed
inaccurately. 6 developers specifically mentioned that a refactoring
broke the code or introduced new errors.
1309
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin, and Mohamed Wiem Mkaouer
(2) 7 developers mentioned that using an IDE refactoring tool
might be slower than doing the refactoring manually. On
the other hand, for very large projects, the problem of correctly
taking care of all namespaces for various variables
and functions becomes more difficult.
(3) 3 developers mentioned that IDE refactoring tools have a
steep learning curve for them. As seen in previous questions,
almost all developers understand what Rename is, but may
never use some more specific features such as Pull up /
Push down simply because they have no environment to
understand what they do and what they are for. 5 more
developers mentioned that the existing features that they
know are confusing for them.
(4) Related to the last point, 5 developers mentioned that they
would want a more direct and more visible representation
of IDE refactoring tools: special tips, or maybe a notification
when a refactoring is possible.
(5) 3 developers mentioned a general lack of trust in all automated
tools.
(6) 2 developers brought up another interesting issue: applying
an automated refactoring can violate a specific coding style
guideline or formatting. Therefore, even if the refactoring is
done correctly, it might still need some editing afterward.
This list is not comprehensive, but it can serve as a blueprint
to get an idea of the current challenges that developers are facing
with regard to the usage of automated IDE refactoring tools.
Similar to other IDE features, refactorings can be reverted in case
their outcome seems unexpected to the developer. To target this
event, we asked Question 14 about how often developers had to
undo a refactoring performed by the IDE. In order for the answers
not to be vague, we specified the approximate percentage of time
for each answer. The results are presented in Table 3.
Table 3: The answers to Question 14: How often do you undo
or revert an IDE refactoring action because you’re unhappy
with the result? (Out of 1,091 respondents)
Answer % of respondents
Often (>75% of the time) 3.0%
Every other time (≈50% of the time) 6.2%
Sometimes (≈25% of the time) 32.3%
Rarely (≈5% of the time) 50.8%
Never 7.7%
Naturally, it can be seen that in this question, extreme answers
were rare. Indeed, only 7.7% of respondents said that they Never
revert, but at the same time, only 6.2% said that they do it Every
other time (≈50% of the time) and only 3% said that they do it Often
(>75% of the time). The vast majority of our respondents lied
in between these poles, with 32.3% saying they undo refactoring
actions Sometimes (≈25% of the time) and half of all participants
(50.8%) saying they do it Rarely (≈5% of the time).
In general, Undo is one of the most fundamental actions in
programming, it happens all the time for various reasons. In a
way, it is an inherent part of the creative process. Nevertheless,
it is captivating for us to discover the reasons behind developers
31.4%
71.2%
14.3%
4.9%
I changed my mind
The result was not
what I expected
I just wanted to
see how the
refactoring works
Other
0 10 20 30 40 50 60 70 80
Respondents (%)
Figure 5: The answers to Question 15: The last few times you
undid or reverted an IDE refactoring feature, what were the
reasons? (Out of 1,007 respondents)
undoing IDE refactoring actions. We asked this in Question 15
of all the respondents who did not choose Never in Question 14
(1007 respondents). The answers are presented in Figure 5.
The most popular answer by far is that the produced result of
the IDE feature was not what the developer expected, with 71.2%
of participants selecting this option. 31.4% of respondents said that
they changed their mind, and also 14.3% said that they just wanted
to see how the refactoring would work. Among other reasons that
the developers shared were their own mistakes, mishaps, and wrong
configuration of refactoring (like namespaces of formats).
Summary: 85.5% of the respondents said that their experience
with the IDE refactoring features is positive. A deeper inquiry
about the existing challenges resulted in a list of possible issues to
consider, including the difficulty of refactoring large projects, and
the learning curve of some refactoring types. The vast majority
of developers occasionally undo or revert refactorings, with the
main reason being that the refactoring produced the result that
was unexpected.
4.4 Previewing Refactorings
Previewing the outcome of refactoring before its execution is one
of the main features of IntelliJ IDEs. Previously, in Question 13,
some developers mentioned that their reliance on the refactoring
feature highly depends on the preview, and that for some of them,
the preview makes the biggest difference.We decided to investigate
this further, so our next block of questions is aimed specifically at
the process of previewing the refactoring.
In Question 16, we asked all the respondents whether they find
the previewing useful. The distribution of the results is presented in
Figure 6. The opinion about previewing refactorings is also largely
positive. 44.5% of the respondents said that the previews are Very
useful, and 37.7% said that they are Useful for certain changes. Overall,
this constitutes more than 80% of participants. On the other
hand, 12.3% said that the previews are Not very useful and 5.5%
more did not have an opinion on the subject.
While this in itself proves the importance of the previewing
function, it is of interest to dive deeper into both sides of this
question. For this reason, we divided the respondents into those
1310
One Thousand and One Stories: A Large-Scale Survey of Software Refactoring ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
Not very useful
12.3%
Useful for certain changes
37.7%
Very useful
44.5%
Don't have an opinion
5.5%
Figure 6: The answers to Question 16: When refactoring the
code, do you find it useful to previewall of your changes before
applying them? (Out of 1,167 respondents)
who gave positive answers and those who did not, and asked them
separate questions.
For participants who answered positively, Question 17 asked
for specific types of refactorings, for which they find the preview
to be especially useful. The distribution of answers correlated well
with the overall distribution of the popularity of the refactoring
feature usage (as indicated by the last two columns in Figure 4), so it
might be the case that the preview is used more or less similarly for
different refactorings, at least no obvious anomalies were present.
Then, in Question 18, we asked the respondents about their
reasons for wanting to see the preview. The most popular answer,
indicated by as many as 227 developers, is making sure that everything
is done as they want it to be, with 22 developers specifically
saying they are making sure that the code does not break, and 13
developers mentioning using this to combat a distrust in automatic
features. However, other specific aspects were brought up.
(1) 78 developers specifically mentioned making sure that the
IDE will not do anything extra and checking the impact of
the changes on the whole codebase. Often, this has to do
with renaming, and developers check namespaces, imports,
and occurrences in comments.
(2) 8 developers check the preview carefully because they say it
prevents them from wasting time later to check the Version
Control System (VCS) diff or undo the unwanted changes.
(3) 5 developers said that they check the readability of the code
during the change, making sure that applying refactoring
will not worsen it, as well as check code style. They mentioned
that in this regard, the visual aspect of the preview is
very important.
(4) One developer mentioned that they are interested in the
preview as a means to comprehend their code. Basically, they
look at the preview not to check if the IDE missed something,
but rather to check whether they forgot something. Several
respondents mentioned that the preview allows them to
think about the necessity of the change one more time.
It can be seen that apart from its main purpose, the preview
sometimes serves different other goals. We also observed that the
reasons for seeing the preview correlate well with the general
shortcomings of refactoring features that developers mentioned in
Section 4.3. The preview can elevate many of the concerns that have
to do with refactorings, even such specific ones as, for example, the
uniformity of code style.
It is also important to understand the reasoning of participants
who did not answer positively to the question about the usefulness
of the preview. To such respondents, we showed Question 19
asking them for their reasons. The distribution of the results is
shown in Figure 7.
37.5%
29.3%
49.5%
27.9%
It would slow down
the process too
much
It would make the
process more
complex
I can predict the
effects of the
change on my own
Other
0 10 20 30 40 50
Respondents (%)
Figure 7: The answers to Question 19: What are your main
reasons for not finding a preview useful? (Out of 208 respondents)
It can be seen that almost half of the respondents who were
asked this question selected the answer I can predict the effects of
the change on my own. The other responses are a little less popular:
the previewing slowing down the IDE too much or adding too much
to the complexity of the process.
When given a prompt to write the Other answer, respondents
mentioned several reasons.
(1) 13 developers said that they prefer to use the VCS diff postfactum,
motivating it with the fact that it produces the same
functionality but also merges it with the possibility to verify
a wider set of changes.
(2) 10 developers were simply very different from the ones in
the previous questions, saying that they would rather simply
run the code, and if it fails, they can always undo the change.
(3) 6 developers mentioned that the preview function is too
complex for them or confuses them.
(4) 5 developers did not know about or did not encounter the
preview feature.
In general, it seems that the opinion strongly depends on the
developers, and in our survey, more developers demonstrated a
cautious approach.
Summary: Previewing is a critical feature for the IDE refactoring
process. 82.2% of our respondents found it useful at least for
specific changes. Apart from simply making sure that the refactoring
is what is intended, it is useful in other situations, such as
a fail-safe against a fast decision, a reason to review the change,
as well as a verification for code readability and compliance with
the code style of the project. A minority of our respondents were
not positive about the preview, with half of them justifying this
with being able to predict the refactoring outcome without the
preview.
1311
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin, and Mohamed Wiem Mkaouer
4.5 Final Thoughts
The last question in our survey, Question 20, was an open-ended
question for describing general thoughts about refactorings and IDE
refactoring features. In general, the respondents’ messages were
positive. 177 developers expressed their compliments, respondents
very often mentioned that refactorings are a cornerstone of the
development process, and for some of them — the main reason to
use an IDE in the first place. Let us enumerate the key findings in
their feedback.
Refactoring knowledge. 52 developers reiterated a previous
point about refactoring tools having a steep learning curve. They
suggested that refactorings should be directly enabled in IDEs,
through tips and popups. Some developers even suggested having
interactive tutorials within the IDE that showup for first-time usage.
Interestingly, several respondents mentioned that they are quite
familiar with some refactoring types and hardly know anything
about others.
Language difference. 9 developers commented on how refactorings
behave differently in different languages. The biggest difference
is noted between statically typed languages like Java and
dynamically typed languages like Python, with refactorings in the
latter ones being much less predictable. This is understandable from
the nature of the language itself, however, tool developers should
pay closer attention to refactorings in dynamically typed languages.
This supports the findings in other works [24, 30] observing that
developers are interested in refactorings for more languages, including
the dynamically typed ones.
Scope. As mentioned before, one common problem the respondents
have with refactorings is how they treat complex namespaces.
Developers also mentioned the ability for any refactoring to be easily
undone as a useful feature. Furthermore, developers suggested
a more explicit and clear way to mark certain source files or code
elements as a no-go zone for refactorings.
Complex features. Finally, several developers mentioned their
thoughts on the complexity of tasks that refactorings solve. Some developers
expressed the idea that most of the automatic refactorings,
like the ones we study in this survey, are useful and time saviors,
but they represent basic atomic actions. They suggested supporting
more complex features like splitting a large complex method or
class into several smaller ones. On the other hand, several developers
mentioned that for simpler changes, it is often much faster to
apply the refactorings manually, and that automated refactorings
do not need to strive to replace the simpler changes completely.
5 THREATS TO VALIDITY
A large percentage of our participants (48.1%) have more than 10
years of professional experience. Moreover, the respondents might
be more experienced with refactorings because IntelliJ-based IDEs
are known for their rich support of refactorings. In our questionnaire,
we focused only on a part of refactorings the IntelliJ-based
IDEs support, which are the most well-known and studied [23].
However, in the Final Thoughts section of the questionnaire, the
respondents were free to provide feedback about their experience
with any refactoring.
Our survey involved participants who use a large variety of
languages and IDEs, which could influence the way they plan their
refactorings.We did not classify our findings by a specific language
or an IDE. Further research aimed at investigating the possible
differences is necessary.
Since all participants in our survey are paid subscribers of Jet-
Brains IDEs, our results may not generalize to other contexts and
other companies. Also, concerning the correctness of our interpretation
of open responses, we did not discuss all responses because
some of them are open to various interpretations, and follow-up
surveys or interviews are needed to clarify them.
6 IMPLICATIONS
Educating about refactorings. The results show that developers
are not familiar with some of the refactoring types that IDEs support
(see Figure 4). Also, some developers might be cautious about the
side-affects of refactoring, so they are not likely ready to perform
any refactoring unless they get to knowhowitworks (see Figure 3b).
To encourage users to use automatic refactorings tools in IDEs,
it might be useful to start with educating users about them. For
example, along with showing a possibility to perform a refactoring,
IDEs could provide some information about its purpose as well. The
knowledge about how refactoring works will help IDEs to respond
to user expectations (see Table 3 and Figure 5).
Increasing awareness about refactoring possibilities. One
of the reasons the developers do refactorings manually is that
they do not know about the possibility to perform them automatically
[35]. It could be helpful if IDEs suggested possible refactoring
opportunities for the user. The results show that users often refactor
their code, sometimes for more than an hour, so automatic
refactoring recommendations could save them time.
Supporting complex refactorings. Since some developers expressed
their need in support of complex refactorings in IDEs, it
would be exciting to support batch refactorings [5, 37], even if they
are less intuitive. Such series of transformations would result in
more atomic methods that better optimize structural metrics, such
as complexity and lines of code. A lot of research efforts have focused
on exploring the impact of refactoring on software quality
using metrics [3, 8, 21, 27], and it would be interesting to turn this
into actual IDE features.
7 CONCLUSION
In this paper, we presented the results of a large-scale survey about
refactoring usage, conducted by JetBrains Research among the
users of IntelliJ-based IDEs. The survey consists of several blocks of
questions: familiarity with refactorings, using different approaches
to conduct refactorings, making use of specific IDE refactoring
features, and using previews when employing refactorings. Together
with the background information at the start and some final
thoughts at the end, the results of this survey represent a unique
perspective of 1,183 experienced developers.
We hope that our results can be useful for both researchers and
practitioners, and the presented opinions of more than a thousand
developers can be used to perfect our methods and our tools.
ACKNOWLEDGEMENTS
We would like to thank the Product Management team and the
Market Research and Analytics team of JetBrains for their advice
and guidance during the designing and conducting of the survey.
1312
One Thousand and One Stories: A Large-Scale Survey of Software Refactoring ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
REFERENCES
[1] Juan Pablo Sandoval Alcocer, Alejandra Siles Antezana, Gustavo Santos, and
Alexandre Bergel. 2020. Improving the Success Rate of Applying the Extract
Method Refactoring. Science of Computer Programming 195 (2020), 102475. https:
//doi.org/10.1016/j.scico.2020.102475
[2] Eman Abdullah AlOmar, Hussein AlRubaye, Mohamed Wiem Mkaouer, Ali Ouni,
and Marouane Kessentini. 2021. Refactoring Practices in the Context of Modern
Code Review: An Industrial Case Study at Xerox. (2021). https://doi.org/10.1109/
ICSE-SEIP52600.2021.00044
[3] Eman Abdullah AlOmar, Mohamed Wiem Mkaouer, Ali Ouni, and Marouane
Kessentini. 2019. On the Impact of Refactoring on the Relationship Between
Quality Attributes and Design Metrics. In 2019 ACM/IEEE International Symposium
on Empirical Software Engineering and Measurement (ESEM). 1–11. https:
//doi.org/10.1109/ESEM.2019.8870177
[4] Eman Abdullah AlOmar, Anthony Peruma, Mohamed Wiem Mkaouer, Christian
Newman, Ali Ouni, and Marouane Kessentini. 2021. How We Refactor and How
We Document It? On the Use of Supervised Machine Learning Algorithms to
Classify Refactoring Documentation. Expert Systems with Applications 167 (2021),
114176. https://doi.org/10.1016/j.eswa.2020.114176
[5] Ana Carla Bibiano, Eduardo Fernandes, Daniel Oliveira, Alessandro Garcia,
Marcos Kalinowski, Baldoino Fonseca, Roberto Oliveira, Anderson Oliveira,
and Diego Cedrim. 2019. A Quantitative Study On Characteristics and Effect
of Batch Refactoring on Code Smells. In 2019 ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement (ESEM). 1–11.
https://doi.org/10.1109/ESEM.2019.8870183
[6] Alex Bogart, Eman Abdullah AlOmar, Mohamed Wiem Mkaouer, and Ali Ouni.
2020. Increasing the Trust In Refactoring Through Visualization. In Proceedings of
the IEEE/ACM 42nd International Conference on Software Engineering Workshops.
334–341. https://doi.org/10.1145/3387940.3392190
[7] John Brant and Friedrich Steimann. 2015. Refactoring Tools Are Trustworthy
Enough and Trust Must Be Earned. IEEE Software 32, 6 (2015), 80–83. https:
//doi.org/10.1109/MS.2015.145
[8] Diego Cedrim, Alessandro Garcia, Melina Mongiovi, Rohit Gheyi, Leonardo Sousa,
Rafael de Mello, Baldoino Fonseca, Márcio Ribeiro, and Alexander Chávez. 2017.
Understanding the Impact of Refactoring on Smells: A Longitudinal Study of 23
Software Projects. In Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering. 465–475. https://doi.org/10.1145/3106237.3106259
[9] Daniela S Cruzes and Tore Dyba. 2011. Recommended Steps for Thematic Synthesis
in Software Engineering. In 2011 international symposium on empirical software
engineering and measurement. 275–284. https://doi.org/10.1109/ESEM.2011.36
[10] Steve Easterbrook, Janice Singer, Margaret-Anne Storey, and Daniela Damian.
2008. Selecting Empirical Methods for Software Engineering Research. In Guide
to advanced empirical software engineering. 285–311. https://doi.org/10.1007/978-
1-84800-044-5_11
[11] Developer Economics. accessed: 05.07.2021. State of the Developer Nation — 2021
Q1, https://www.developereconomics.com/resources/reports/. https://www.
developereconomics.com/resources/reports/
[12] Martin Fowler, Kent Beck, John Brant, William Opdyke, and don Roberts. 1999.
Refactoring: Improving the Design of Existing Code. Addison-Wesley Longman
Publishing Co., Inc. http://dl.acm.org/citation.cfm?id=311424
[13] Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin,
and Mohamed Wiem Mkaouer. accessed: 05.07.2021. Supplementary Data Package,
https://zenodo.org/record/4923175. https://zenodo.org/record/4923175
[14] Robert M Groves, Floyd J Fowler Jr, Mick P Couper, James M Lepkowski, Eleanor
Singer, and Roger Tourangeau. 2011. Survey Methodology. Vol. 561. John Wiley
& Sons.
[15] Benedikt Hauptmann, Sebastian Eder, Maximilian Junker, Elmar Juergens, and
VolkmarWoinke. 2015. Generating Refactoring Proposals to Remove Clones from
Automated System Tests. In 2015 IEEE 23rd International Conference on Program
Comprehension. 115–124. https://doi.org/10.1109/ICPC.2015.20
[16] Ameya Ketkar, Nikolaos Tsantalis, and Danny Dig. 2020. Understanding Type
Changes in Java. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 629–641. https://doi.org/10.1145/3368089.3409725
[17] Miryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. 2014. An
Empirical Study of Refactoring Challenges and Benefits at Microsoft. IEEE
Transactions on Software Engineering 40, 7 (2014), 633–649. https://doi.org/10.
1109/TSE.2014.2318734
[18] Barbara A Kitchenham and Shari L Pfleeger. 2008. Personal Opinion Surveys. In
Guide to advanced empirical software engineering. 63–92. https://doi.org/10.1007/
978-1-84800-044-5_3
[19] Flávio Medeiros, Márcio Ribeiro, Rohit Gheyi, Sven Apel, Christian Kästner,
Bruno Ferreira, Luiz Carvalho, and Baldoino Fonseca. 2018. Discipline Matters:
Refactoring of Preprocessor Directives in the #ifdef Hell. IEEE Transactions on
Software Engineering 44, 5 (2018), 453–469. https://doi.org/10.1109/TSE.2017.
2688333
[20] Tom Mens and Tom Tourwé. 2004. A Survey of Software Refactoring. IEEE
Transactions on software engineering 30, 2 (2004), 126–139. https://doi.org/10.
1109/TSE.2004.1265817
[21] Wiem Mkaouer, Marouane Kessentini, Adnan Shaout, Patrice Koligheu, Slim
Bechikh, Kalyanmoy Deb, and Ali Ouni. 2015. Many-Objective Software Remodularization
Using NSGA-III. ACM Transactions on Software Engineering and
Methodology (TOSEM) 24, 3 (2015), 17. https://doi.org/10.1145/2729974
[22] Emerson Murphy-Hill and Andrew P Black. 2008. Refactoring Tools: Fitness for
Purpose. IEEE software 25, 5 (2008), 38–44. https://doi.org/10.1109/MS.2008.123
[23] Emerson Murphy-Hill, Chris Parnin, and AndrewP Black. 2011. HowWe Refactor,
and How We Know It. IEEE Transactions on Software Engineering 38, 1 (2011),
5–18. https://doi.org/10.1109/TSE.2011.41
[24] Christian D. Newman, Mohamed Wiem Mkaouer, Michael L. Collard, and
Jonathan I. Maletic. 2018. A Study on Developer Perception of Transformation
Languages for Refactoring. In Proceedings of the 2Nd International Workshop
on Refactoring (IWoR 2018). 34–41. https://doi.org/10.1145/3242163.3242170
[25] Jonhnanthan Oliveira, Rohit Gheyi, Melina Mongiovi, Gustavo Soares, Márcio
Ribeiro, and Alessandro Garcia. 2019. Revisiting the Refactoring Mechanics.
Information and Software Technology 110 (2019), 136–138. https://doi.org/10.
1016/j.infsof.2019.03.002
[26] Jonhnanthan Oliveira, Rohit Gheyi, Felipe Pontes, Melina Mongiovi, Márcio
Ribeiro, and Alessandro Garcia. 2020. Revisiting Refactoring Mechanics from
Tool Developers’ Perspective. In Brazilian Symposium on Formal Methods. 25–42.
https://doi.org/10.1007/978-3-030-63882-5_3
[27] Jevgenija Pantiuchina, Michele Lanza, and Gabriele Bavota. 2018. Improving Code:
The (Mis) Perception of Quality Metrics. In 2018 IEEE International Conference
on Software Maintenance and Evolution (ICSME). 80–91. https://doi.org/10.1109/
ICSME.2018.00017
[28] Jevgenija Pantiuchina, Fiorella Zampetti, Simone Scalabrino, Valentina Piantadosi,
Rocco Oliveto, Gabriele Bavota, and Massimiliano Di Penta. 2020. Why
Developers Refactor Source Code: A Mining-Based Study. ACM Transactions
on Software Engineering and Methodology (TOSEM) 29, 4 (2020), 1–30. https:
//doi.org/10.1145/3408302
[29] Leonardo Passos, Rodrigo Queiroz, Mukelabai Mukelabai, Thorsten Berger, Sven
Apel, Krzysztof Czarnecki, and Jesus Alejandro Padilla. 2018. A Study of Feature
Scattering in the Linux Kernel. IEEE Transactions on Software Engineering, Early
Access (2018), 146–164. https://doi.org/10.1109/TSE.2018.2884911
[30] Gustavo H Pinto and Fernando Kamei. 2013. What Programmers Say About
Refactoring Tools? An Empirical Investigation of StackOverflow. In Proceedings
of the 2013 ACM workshop on Workshop on refactoring tools. 33–36. https://doi.
org/10.1145/2541348.2541357
[31] IntelliJ Platform. accessed: 05.07.2021. Open Source Platform for Building IDEs
and Developer Tools, https://www.jetbrains.com/opensource/idea/. https://
www.jetbrains.com/opensource/idea/
[32] Veselin Raychev, Max Schäfer, Manu Sridharan, and Martin Vechev. 2013. Refactoring
with Synthesis. ACM SIGPLAN Notices 48, 10 (2013), 339–354. https:
//doi.org/10.1145/2544173.2509544
[33] Max Schäfer and Oege De Moor. 2010. Specifying and Implementing Refactorings.
In Proceedings of the ACM international conference on Object oriented programming
systems languages and applications. 286–301. https://doi.org/10.1145/1869459.
1869485
[34] Tushar Sharma, Girish Suryanarayana, and Ganesh Samarthyam. 2015. Challenges
to and Solutions for Refactoring Adoption: An Industrial Perspective. IEEE
Software 32, 6 (2015), 44–51. https://doi.org/10.1109/MS.2015.105
[35] Danilo Silva, Nikolaos Tsantalis, and Marco Tulio Valente. 2016. Why We Refactor?
Confessions of GitHub Contributors. In Proceedings of the 2016 24th ACM
SIGSOFT International Symposium on Foundations of Software Engineering. 858–
870. https://doi.org/10.1145/2950290.2950305
[36] Gustavo Soares, Rohit Gheyi, and Tiago Massoni. 2012. Automated Behavioral
Testing of Refactoring Engines. IEEE Transactions on Software Engineering 39, 2
(2012), 147–162. https://doi.org/10.1109/TSE.2012.19
[37] Leonardo Sousa, Diego Cedrim, Alessandro Garcia, Willian Oizumi, Ana C Bibiano,
Daniel Oliveira, Miryung Kim, and Anderson Oliveira. 2020. Characterizing
and Identifying Composite Refactorings: Concepts, Heuristics and Patterns. In
Proceedings of the 17th International Conference on Mining Software Repositories.
186–197. https://doi.org/10.1145/3379597.3387477
[38] Nikolaos Tsantalis, Matin Mansouri, Laleh M Eshkevari, Davood Mazinanian,
and Danny Dig. 2018. Accurate and Efficient Refactoring Detection in Commit
History. In Proceedings of the 40th International Conference on Software Engineering.
483–494. https://doi.org/10.1145/3180155.3180206
[39] Mohsen Vakilian, Nicholas Chen, Stas Negara, Balaji Ambresh Rajkumar, Brian P
Bailey, and Ralph E Johnson. 2012. Use, Disuse, and Misuse of Automated
Refactorings. In 2012 34th International Conference on Software Engineering (ICSE).
233–243. https://doi.org/10.1109/ICSE.2012.6227190
[40] Mohsen Vakilian and Ralph E Johnson. 2014. Alternate Refactoring Paths Reveal
Usability Problems. In Proceedings of the 36th international conference on software
engineering. 1106–1116. https://doi.org/10.1145/2568225.2568282
1313


THE ART OF
SOFTWARE
TESTING

THE ART OF
SOFTWARE
TESTING
Third Edition
GLENFORD J. MYERS
TOM BADGETT
COREY SANDLER
John Wiley & Sons, Inc.
Copyright#2012 byWord Association, Inc. All rights reserved.
Published by JohnWiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means, electronic, mechanical, photocopying, recording, scanning, or
otherwise, except as permitted under Section 107 or 108 of the 1976 United States
Copyright Act, without either the prior written permission of the Publisher, or authorization
through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc.,
222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-8600, or on the
web at www.copyright.com. Requests to the Publisher for permission should be addressed
to the Permissions Department, JohnWiley & Sons, Inc., 111 River Street, Hoboken, NJ
07030, (201) 748-6011, fax (201) 748-6008, or online at www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their
best efforts in preparing this book, they make no representations or warranties with respect
to the accuracy or completeness of the contents of this book and specifically disclaim any
implied warranties of merchantability or fitness for a particular purpose. No warranty may
be created or extended by sales representatives or written sales materials. The advice and
strategies contained herein may not be suitable for your situation. You should consult with a
professional where appropriate. Neither the publisher nor author shall be liable for any loss
of profit or any other commercial damages, including but not limited to special, incidental,
consequential, or other damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside
the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears
in print may not be available in electronic books. For more information about Wiley
products, visit our website at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Myers, Glenford J., 1946-
The art of software testing / Glenford J. Myers, Corey Sandler, Tom Badgett.—3rd ed.
p. cm.
Includes index.
ISBN 978-1-118-03196-4 (cloth); ISBN 978-1-118-13313-2 (ebk); ISBN 978-1-118-13314-9
(ebk); ISBN 978-1-118-13315-6 (ebk)
1. Computer software—Testing. 2. Debugging in computer science. I. Sandler,
Corey, 1950- II. Badgett, Tom. III. Title.
QA76.76.T48M894 2011
005.1 0
4—dc23
2011017548
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1
Contents
Preface vii
Introduction ix
1 A Self-Assessment Test 1
2 The Psychology and Economics of Software Testing 5
3 Program Inspections, Walkthroughs, and Reviews 19
4 Test-Case Design 41
5 Module (Unit) Testing 85
6 Higher-Order Testing 113
7 Usability (User) Testing 143
8 Debugging 157
9 Testing in the Agile Environment 175
10 Testing Internet Applications 193
11 Mobile Application Testing 213
Appendix Sample Extreme Testing Application 227
Index 233
v

Preface
In 1979, Glenford Myers published a book that turned out to be a classic.
The Art of Software Testing has stood the test of time—25 years on the
publisher’s list of available books. This fact alone is a testament to the
solid, essential, and valuable nature of his work.
During that same time, the authors of this edition (the third) of The Art
of Software Testing published, collectively, more than 200 books, most of
them on computer software topics. Some of these titles sold very well and,
like this one, have gone through multiple versions. Corey Sandler’s Fix
Your Own PC, for example, is in its eighth edition as this book goes to
press; and Tom Badgett’s books on Microsoft PowerPoint and other Office
titles have gone through four or more editions. However, unlike Myers’s
book, none of these remained current for more than a few years.
What is the difference? The newer books covered more transient
topics—operating systems, applications software, security, communications
technology, and hardware configurations. Rapid changes in computer
hardware and software technology during the 1980s and 1990s necessitated
frequent changes and updates to these topics.
Also during that period hundreds of books about software testing were
published. They, too, took a more transient approach to the topic. The Art
of Software Testing alone gave the industry a long-lasting, foundational
guide to one of the most important computer topics: How do you ensure
that all of the software you produce does what it was designed to do, and—
just as important—doesn’t do what it isn’t supposed to do?
The edition you are reading today retains the foundational philosophy
laid by Myers more than three decades ago. But we have updated the
examples to include more current programming languages, and we have
addressed topics that were not yet topics when Myers wrote the first
edition: Web programming, e-commerce, Extreme (Agile) programming
and testing, and testing applications for mobile devices.
vii
Along the way, we never lost sight of the fact that a new classic must stay
true to its roots, so our version also offers you a software testing philosophy,
and a process that works across current and unforeseeable future
hardware and software platforms. We hope that the third edition of The
Art of Software Testing, too, will span a generation of software designers
and developers.
viii Preface
Introduction
At the time this book was first published, in 1979, it was a well-known
rule of thumb that in a typical programming project approximately
50 percent of the elapsed time and more than 50 percent of the total cost
were expended in testing the program or system being developed.
Today, a third of a century and two book updates later, the same holds
true. There are new development systems, languages with built-in tools,
and programmers who are used to developing more on the fly. But testing
continues to play an important part in any software development project.
Given these facts, you might expect that by this time program testing
would have been refined into an exact science. This is far from the case. In
fact, less seems to be known about software testing than about any other
aspect of software development. Furthermore, testing has been an out-ofvogue
subject; it was so when this book was first published and, unfortunately,
this has not changed. Today there are more books and articles
about software testing—meaning that, at least, the topic has greater visibility
than it did when this book was first published—but testing remains
among the ‘‘dark arts’’ of software development.
This would be more than enough reason to update this book on the art
of software testing, but we have additional motivations. At various times,
we have heard professors and teaching assistants say, ‘‘Our students graduate
and move into industry without any substantial knowledge of how to
go about testing a program. Moreover, we rarely have any advice to offer
in our introductory courses on how a student should go about testing and
debugging his or her exercises.’’
Thus, the purpose of this updated edition of The Art of Software Testing
is the same as it was in 1979 and in 2004: to fill these knowledge gaps for
the professional programmer and the student of computer science. As the
title implies, the book is a practical, rather than theoretical, discussion of
the subject, complete with updated language and process discussions.
ix
Although it is possible to discuss program testing in a theoretical vein, this
book is intended to be a practical, ‘‘both feet on the ground’’ handbook.
Hence, many subjects related to program testing, such as the idea of mathematically
proving the correctness of a program, were purposefully
excluded.
Chapter 1 ‘‘assigns’’ a short self-assessment test that every reader should
take before reading further. It turns out that the most important practical
information you must understand about program testing is a set of philosophical
and economic issues; these are discussed in Chapter 2. Chapter 3
introduces the important concept of noncomputer-based code walkthroughs,
or inspections. Rather than focus attention on the procedural or
managerial aspects of this concept, as most such discussions do, this chapter
addresses it from a technical, how-to-find-errors point of view.
The alert reader will realize that the most important component in a
program tester’s bag of tricks is the knowledge of how to write effective
test cases; this is the subject of Chapter 3. Chapter 4 discusses the testing
of individual modules or subroutines, followed in Chapter 5 by the testing
of larger entities. Chapter 6 takes on the concept of user or usability testing,
a component of software testing that always has been important, but is
even more relevant today due to the advent of more complex software
targeted at an ever broadening audience. Chapter 7 offers some practical
advice on program debugging, while Chapter 8 delves into the concepts of
extreme programming testing with emphasis on what has come to be
called the ‘‘agile environment.’’ Chapter 9 shows how to use other features
of program testing, which are detailed elsewhere in this book, with Web
programming, including e-commerce systems, and the all new, highly interactive
social networking sites. Chapter 10 describes how to test software
developed for the mobile environment.
We direct this book at three major audiences. First, the professional
programmer. Although we hope that not everything in this book will be
new information to this audience, we believe it will add to the professional’s
knowledge of testing techniques. If the material allows this group
to detect just one more bug in one program, the price of the book will have
been recovered many times over.
The second audience is the project manager, who will benefit from the
book’s practical information on the management of the testing process.
The third audience is the programming and computer science student,
and our goal for them is twofold: to expose them to the problems of
x Introduction
program testing, and provide a set of effective techniques. For this third
group, we suggest the book be used as a supplement in programming
courses such that students are exposed to the subject of software testing
early in their education.
Introduction xi

1 A Self-Assessment
Test
Since this book was first published over 30 years ago, software testing
has become more difficult and easier than ever.
Software testing is more difficult because of the vast array of programming
languages, operating systems, and hardware platforms that have
evolved in the intervening decades. And while relatively few people used
computers in the 1970s, today virtually no one can complete a day’s work
without using a computer. Not only do computers exist on your desk, but
a ‘‘computer,’’ and consequently software, is present in almost every device
we use. Just try to think of the devices today that society relies on that are
not software driven. Sure there are some—hammers and wheelbarrows
come to mind—but the vast majority use some form of software to operate.
Software is pervasive, which raises the value of testing it. The machines
themselves are hundreds of times more powerful, and smaller, than those
early devices, and today’s concept of ‘‘computer’’ is much broader and
more difficult to define. Televisions, telephones, gaming systems, and automobiles
all contain computers and computer software, and in some cases
can even be considered computers themselves.
Therefore, the software we write today potentially touches millions of
people, either enabling them to do their jobs effectively and efficiently, or
causing them untold frustration and costing them in the form of lost work
or lost business. This is not to say that software is more important today
than it was when the first edition of this book was published, but it is safe
to say that computers—and the software that drives them—certainly affect
more people and more businesses now than ever before.
1
Software testing is easier, too, in some ways, because the array of software
and operating systems is much more sophisticated than in the past,
providing intrinsic, well-tested routines that can be incorporated into
applications without the need for a programmer to develop them from
scratch. Graphical User Interfaces (GUIs), for example, can be built from a
development language’s libraries, and since they are preprogrammed objects
that have been debugged and tested previously, the need for testing
them as part of a custom application is much reduced.
And, despite the plethora of software testing tomes available on the
market today, many developers seem to have an attitude that is counter
to extensive testing. Better development tools, pretested GUIs, and the
pressure of tight deadlines in an ever more complex development environment
can lead to avoidance of all but the most obvious testing
protocols. Whereas low-level impacts of bugs may only inconvenience
the end user, the worst impacts can result in large financial loses, or even
cause harm to people. The procedures in this book can help designers,
developers, and project managers better understand the value of comprehensive
testing, and provide guidelines to help them achieve required
testing goals.
Software testing is a process, or a series of processes, designed to make
sure computer code does what it was designed to do and, conversely, that it
does not do anything unintended. Software should be predictable and consistent,
presenting no surprises to users. In this book, we will look at many
approaches to achieving this goal.
Now, before we start the book, we’d like you to take a short exam. We
want you to write a set of test cases—specific sets of data—to test properly
a relatively simple program. Create a set of test data for the program—data
the program must handle correctly to be considered a successful program.
Here’s a description of the program:
The program reads three integer values from an input dialog. The
three values represent the lengths of the sides of a triangle. The program
displays a message that states whether the triangle is scalene,
isosceles, or equilateral.
Remember that a scalene triangle is one where no two sides are equal,
whereas an isosceles triangle has two equal sides, and an equilateral
triangle has three sides of equal length. Moreover, the angles opposite the
2 The Art of Software Testing
equal sides in an isosceles triangle also are equal (it also follows that the
sides opposite equal angles in a triangle are equal), and all angles in an
equilateral triangle are equal.
Evaluate your set of test cases by using it to answer the following
questions. Give yourself one point for each yes answer.
1. Do you have a test case that represents a valid scalene triangle?
(Note that test cases such as 1, 2, 3 and 2, 5, 10 do not warrant a yes
answer because a triangle having these dimensions is not valid.)
2. Do you have a test case that represents a valid equilateral triangle?
3. Do you have a test case that represents a valid isosceles triangle?
(Note that a test case representing 2, 2, 4 would not count because it
is not a valid triangle.)
4. Do you have at least three test cases that represent valid isosceles
triangles such that you have tried all three permutations of two equal
sides (such as, 3, 3, 4; 3, 4, 3; and 4, 3, 3)?
5. Do you have a test case in which one side has a zero value?
6. Do you have a test case in which one side has a negative value?
7. Do you have a test case with three integers greater than zero such that
the sum of two of the numbers is equal to the third? (That is, if the
program said that 1, 2, 3 represents a scalene triangle, it would contain
a bug.)
8. Do you have at least three test cases in category 7 such that you have
tried all three permutations where the length of one side is equal to
the sum of the lengths of the other two sides (e.g., 1, 2, 3; 1, 3, 2; and
3, 1, 2)?
9. Do you have a test case with three integers greater than zero such that
the sum of two of the numbers is less than the third (such as 1, 2, 4 or
12, 15, 30)?
10. Do you have at least three test cases in category 9 such that you have
tried all three permutations (e.g., 1, 2, 4; 1, 4, 2; and 4, 1, 2)?
11. Do you have a test case in which all sides are zero (0, 0, 0)?
12. Do you have at least one test case specifying noninteger values
(such as 2.5, 3.5, 5.5)?
13. Do you have at least one test case specifying the wrong number of
values (two rather than three integers, for example)?
14. For each test case did you specify the expected output from the
program in addition to the input values?
A Self-Assessment Test 3
Of course, a set of test cases that satisfies these conditions does not guarantee
that you will find all possible errors, but since questions 1 through
13 represent errors that actually have occurred in different versions of this
program, an adequate test of this program should expose at least these
errors.
Now, before you become concerned about your score, consider this: In
our experience, highly qualified professional programmers score, on the
average, only 7.8 out of a possible 14. If you’ve done better, congratulations;
if not, we’re here to help.
The point of the exercise is to illustrate that the testing of even a trivial
program such as this is not an easy task. Given this is true, consider the difficulty
of testing a 100,000-statement air traffic control system, a compiler, or
even a mundane payroll program. Testing also becomes more difficult with
the object-oriented languages, such as Java and Cþþ. For example, your test
cases for applications built with these languages must expose errors associated
with object instantiation and memory management.
It might seem from working with this example that thoroughly testing a
complex, real-world program would be impossible. Not so! Although the
task can be daunting, adequate program testing is a very necessary—and
achievable—part of software development, as you will learn in this book.
4 The Art of Software Testing
2 The Psychology
and Economics of
Software Testing
Software testing is a technical task, yes, but it also involves some important
considerations of economics and human psychology.
In an ideal world, we would want to test every possible permutation of a
program. In most cases, however, this simply is not possible. Even a seemingly
simple program can have hundreds or thousands of possible input
and output combinations. Creating test cases for all of these possibilities is
impractical. Complete testing of a complex application would take too
long and require too many human resources to be economically feasible.
In addition, the software tester needs the proper attitude (perhaps
‘‘vision’’ is a better word) to successfully test a software application. In
some cases, the tester’s attitude may be more important than the actual process
itself. Therefore, we will start our discussion of software testing with
these issues before we delve into the more technical nature of the topic.
The Psychology of Testing
One of the primary causes of poor application testing is the fact that most
programmers begin with a false definition of the term. They might say:
‘‘Testing is the process of demonstrating that errors are not present.’’
‘‘The purpose of testing is to show that a program performs its intended
functions correctly.’’
‘‘Testing is the process of establishing confidence that a program does
what it is supposed to do.’’
5
These definitions are upside down.
When you test a program, you want to add some value to it. Adding
value through testing means raising the quality or reliability of the program.
Raising the reliability of the program means finding and removing errors.
Therefore, don’t test a program to show that it works; rather, start with
the assumption that the program contains errors (a valid assumption for
almost any program) and then test the program to find as many of the
errors as possible.
Thus, a more appropriate definition is this:
Testing is the process of executing a program with the intent of finding
errors.
Although this may sound like a game of subtle semantics, it’s really an
important distinction. Understanding the true definition of software testing
can make a profound difference in the success of your efforts.
Human beings tend to be highly goal-oriented, and establishing the
proper goal has an important psychological effect on them. If our goal is to
demonstrate that a program has no errors, then we will be steered subconsciously
toward this goal; that is, we tend to select test data that have a
low probability of causing the program to fail. On the other hand, if our
goal is to demonstrate that a program has errors, our test data will have a
higher probability of finding errors. The latter approach will add more
value to the program than the former.
This definition of testing has myriad implications, many of which are
scattered throughout this book. For instance, it implies that testing is a
destructive, even sadistic, process, which explains why most people find it
difficult. That may go against our grain; with good fortune, most of us have
a constructive, rather than a destructive, outlook on life. Most people are
inclined toward making objects rather than ripping them apart. The definition
also has implications for how test cases (test data) should be designed,
and who should and who should not test a given program.
Another way of reinforcing the proper definition of testing is to analyze
the use of the words ‘‘successful’’ and ‘‘unsuccessful’’—in particular, their use
by project managers in categorizing the results of test cases. Most project
managers refer to a test case that did not find an error a ‘‘successful test run,’’
whereas a test that discovers a new error is usually called ‘‘unsuccessful.’’
Once again, this is upside down. ‘‘Unsuccessful’’ denotes something undesirable
or disappointing. To our way of thinking, a well-constructed and
6 The Art of Software Testing
executed software test is successful when it finds errors that can be fixed.
That same test is also successful when it eventually establishes that there
are no more errors to be found. The only unsuccessful test is one that does
not properly examine the software; and, in the majority of cases, a test that
found no errors likely would be considered unsuccessful, since the concept
of a program without errors is basically unrealistic.
A test case that finds a new error can hardly be considered unsuccessful;
rather, it has proven to be a valuable investment. An unsuccessful test case
is one that causes a program to produce the correct result without finding
any errors.
Consider the analogy of a person visiting a doctor because of an overall
feeling of malaise. If the doctor runs some laboratory tests that do not locate
the problem, we do not call the laboratory tests ‘‘successful’’; they were unsuccessful
tests in that the patient’s net worth has been reduced by the expensive
laboratory fees, the patient is still ill, and the patient may question the
doctor’s ability as a diagnostician. However, if a laboratory test determines
that the patient has a peptic ulcer, the test is successful because the doctor
can now begin the appropriate treatment. Hence, the medical profession
seems to use these words in the proper sense. The analogy, of course, is that
we should think of the program, as we begin testing it, as the sick patient.
A second problem with such definitions as ‘‘testing is the process of
demonstrating that errors are not present’’ is that such a goal is impossible
to achieve for virtually all programs, even trivial programs.
Again, psychological studies tell us that people perform poorly when
they set out on a task that they know to be infeasible or impossible. For
instance, if you were instructed to solve the crossword puzzle in the
Sunday New York Times in 15 minutes, you probably would achieve little,
if any, progress after 10 minutes because, if you are like most people, you
would be resigned to the fact that the task seems impossible. If you were
asked for a solution in four hours, however, we could reasonably expect to
see more progress in the initial 10 minutes. Defining program testing as the
process of uncovering errors in a program makes it a feasible task, thus
overcoming this psychological problem.
A third problem with the common definitions such as ‘‘testing is the
process of demonstrating that a program does what it is supposed to do’’ is
that programs that do what they are supposed to do still can contain
errors. That is, an error is clearly present if a program does not do what it is
supposed to do; but errors are also present if a program does what it is not
supposed to do. Consider the triangle program of Chapter 1. Even if we
The Psychology and Economics of Software Testing 7
could demonstrate that the program correctly distinguishes among all scalene,
isosceles, and equilateral triangles, the program still would be in
error if it does something it is not supposed to do (such as representing 1,
2, 3 as a scalene triangle or saying that 0, 0, 0 represents an equilateral
triangle). We are more likely to discover the latter class of errors if we
view program testing as the process of finding errors than if we view it as
the process of showing that a program does what it is supposed to do.
To summarize, program testing is more properly viewed as the destructive
process of trying to find the errors in a program (whose presence is
assumed). A successful test case is one that furthers progress in this direction
by causing the program to fail. Of course, you eventually want to use
program testing to establish some degree of confidence that a program
does what it is supposed to do and does not do what it is not supposed to
do, but this purpose is best achieved by a diligent exploration for errors.
Consider someone approaching you with the claim that ‘‘my program is
perfect’’ (i.e., error free). The best way to establish some confidence in this
claim is to try to refute it, that is, to try to find imperfections rather than
just confirm that the program works correctly for some set of input data.
The Economics of Testing
Given our definition of program testing, an appropriate next step is to determine
whether it is possible to test a program to find all of its errors. We
will show you that the answer is negative, even for trivial programs. In
general, it is impractical, often impossible, to find all the errors in a program.
This fundamental problem will, in turn, have implications for the
economics of testing, assumptions that the tester will have to make about
the program, and the manner in which test cases are designed.
To combat the challenges associated with testing economics, you should
establish some strategies before beginning. Two of the most prevalent strategies
include black-box testing and white-box testing, which we will explore
in the next two sections.
Black-Box Testing
One important testing strategy is black-box testing (also known as datadriven
or input/output-driven testing). To use this method, view the program
as a black box. Your goal is to be completely unconcerned about the
8 The Art of Software Testing
internal behavior and structure of the program. Instead, concentrate on
finding circumstances in which the program does not behave according to
its specifications.
In this approach, test data are derived solely from the specifications
(i.e., without taking advantage of knowledge of the internal structure of
the program).
If you want to use this approach to find all errors in the program, the
criterion is exhaustive input testing, making use of every possible input condition
as a test case. Why? If you tried three equilateral-triangle test cases
for the triangle program, that in no way guarantees the correct detection of
all equilateral triangles. The program could contain a special check for values
3842, 3842, 3842 and denote such a triangle as a scalene triangle.
Since the program is a black box, the only way to be sure of detecting the
presence of such a statement is by trying every input condition.
To test the triangle program exhaustively, you would have to create test
cases for all valid triangles up to the maximum integer size of the development
language. This in itself is an astronomical number of test cases, but it
is in no way exhaustive: It would not find errors where the program said
that  3, 4, 5 is a scalene triangle and that 2, A, 2 is an isosceles triangle.
To be sure of finding all such errors, you have to test using not only all
valid inputs, but all possible inputs. Hence, to test the triangle program
exhaustively, you would have to produce virtually an infinite number of
test cases, which, of course, is not possible.
If this sounds difficult, exhaustive input testing of larger programs is even
more problematic. Consider attempting an exhaustive black-box test of a
Cþþ compiler. Not only would you have to create test cases representing all
valid Cþþ programs (again, virtually an infinite number), but you would
have to create test cases for all invalid Cþþ programs (an infinite number)
to ensure that the compiler detects them as being invalid. That is, the compiler
has to be tested to ensure that it does not do what it is not supposed to
do—for example, successfully compile a syntactically incorrect program.
The problem is even more onerous for transaction-base programs such
as database applications. For example, in a database application such as an
airline reservation system, the execution of a transaction (such as a database
query or a reservation for a plane flight) is dependent upon what happened
in previous transactions. Hence, not only would you have to try all
unique valid and invalid transactions, but also all possible sequences of
transactions.
The Psychology and Economics of Software Testing 9
This discussion shows that exhaustive input testing is impossible. Two
important implications of this: (1) You cannot test a program to guarantee
that it is error free; and (2) a fundamental consideration in program testing
is one of economics. Thus, since exhaustive testing is out of the question,
the objective should be to maximize the yield on the testing investment by
maximizing the number of errors found by a finite number of test cases.
Doing so will involve, among other things, being able to peer inside the
program and make certain reasonable, but not airtight, assumptions about
the program (e.g., if the triangle program detects 2, 2, 2 as an equilateral
triangle, it seems reasonable that it will do the same for 3, 3, 3). This will
form part of the test case design strategy in Chapter 4.
White-Box Testing
Another testing strategy, white-box (or logic-driven) testing, permits you to
examine the internal structure of the program. This strategy derives test
data from an examination of the program’s logic (and often, unfortunately,
at the neglect of the specification).
The goal at this point is to establish for this strategy the analog to exhaustive
input testing in the black-box approach. Causing every statement in the
program to execute at least once might appear to be the answer, but it is not
difficult to show that this is highly inadequate. Without belaboring the point
here, since this matter is discussed in greater depth in Chapter 4, the analog
is usually considered to be exhaustive path testing. That is, if you execute, via
test cases, all possible paths of control flow through the program, then possibly
the program has been completely tested.
There are two flaws in this statement, however. One is that the number
of unique logic paths through a program could be astronomically large. To
see this, consider the trivial program represented in Figure 2.1. The diagram
is a control-flow graph. Each node or circle represents a segment of
statements that execute sequentially, possibly terminating with a branching
statement. Each edge or arc represents a transfer of control (branch) between
segments. The diagram, then, depicts a 10- to 20-statement program
consisting of a DO loop that iterates up to 20 times. Within the body of the
DO loop is a set of nested IF statements. Determining the number of unique
logic paths is the same as determining the total number of unique ways of
moving from point a to point b (assuming that all decisions in the program
are independent from one another). This number is approximately 1014, or
10 The Art of Software Testing
100 trillion. It is computed from 520 þ 519 þ . . . 51, where 5 is the
number of paths through the loop body. Most people have a difficult time
visualizing such a number, so consider it this way: If you could write, execute,
and verify a test case every five minutes, it would take approximately
1 billion years to try every path. If you were 300 times faster, completing a
test once per second, you could complete the job in 3.2 million years, give
or take a few leap years and centuries.
Of course, in actual programs every decision is not independent from
every other decision, meaning that the number of possible execution paths
would be somewhat fewer. On the other hand, actual programs are much
larger than the simple program depicted in Figure 2.1. Hence, exhaustive
path testing, like exhaustive input testing, appears to be impractical, if not
impossible.
FIGURE 2.1 Control-Flow Graph of a Small Program.
The Psychology and Economics of Software Testing 11
The second flaw in the statement ‘‘exhaustive path testing means a complete
test’’ is that every path in a program could be tested, yet the program
might still be loaded with errors. There are three explanations for this.
The first is that an exhaustive path test in no way guarantees that a program
matches its specification. For example, if you were asked to write an
ascending-order sorting routine but mistakenly produced a descendingorder
sorting routine, exhaustive path testing would be of little value; the
program still has one bug: It is the wrong program, as it does not meet the
specification.
Second, a program may be incorrect because of missing paths. Exhaustive
path testing, of course, would not detect the absence of necessary paths.
Third, an exhaustive path test might not uncover data-sensitivity errors.
There are many examples of such errors, but a simple one should suffice.
Suppose that in a program you have to compare two numbers for convergence,
that is, to see if the difference between the two numbers is less than
some predetermined value. For example, you might write a Java IF statement
as
if (a-b<c)
System.out.println("a-b<c");
Of course, the statement contains an error because it should compare c
to the absolute value of a-b. Detection of this error, however, is dependent
upon the values used for a and b and would not necessarily be detected by
just executing every path through the program.
In conclusion, although exhaustive input testing is superior to exhaustive
path testing, neither proves to be useful because both are infeasible.
Perhaps, then, there are ways of combining elements of black-box and
white-box testing to derive a reasonable, but not airtight, testing strategy.
This matter is pursued further in Chapter 4.
Software Testing Principles
Continuing with the major premise of this chapter, that the most important
considerations in software testing are issues of psychology, we can
identify a set of vital testing principles or guidelines. Most of these principles
may seem obvious, yet they are all too often overlooked. Table 2.1
summarizes these important principles, and each is discussed in more
detail in the paragraphs that follow.
12 The Art of Software Testing
Principle 1: A necessary part of a test case is a definition of the
expected output or result.
This principle, though obvious, when overlooked is the cause of
one of the most frequent mistakes in program testing. Again, it is
something that is based on human psychology. If the expected result
of a test case has not been predefined, chances are that a plausible,
but erroneous, result will be interpreted as a correct result because of
the phenomenon of ‘‘the eye seeing what it wants to see.’’ In other
words, in spite of the proper destructive definition of testing, there is
still a subconscious desire to see the correct result. One way of
TABLE 2.1 Vital Program Testing Guidelines
Principle
Number Principle
1 A necessary part of a test case is a definition of the expected
output or result.
2 A programmer should avoid attempting to test his or her own
program.
3 A programming organization should not test its own programs.
4 Any testing process should include a thorough inspection of the
results of each test.
5 Test cases must be written for input conditions that are invalid
and unexpected, as well as for those that are valid and expected.
6 Examining a program to see if it does not do what it is supposed
to do is only half the battle; the other half is seeing whether the
program does what it is not supposed to do.
7 Avoid throwaway test cases unless the program is truly a
throwaway program.
8 Do not plan a testing effort under the tacit assumption that no
errors will be found.
9 The probability of the existence of more errors in a section of a
program is proportional to the number of errors already found in
that section.
10 Testing is an extremely creative and intellectually challenging
task.
The Psychology and Economics of Software Testing 13
combating this is to encourage a detailed examination of all output
by precisely spelling out, in advance, the expected output of the program.
Therefore, a test case must consist of two components:
1. A description of the input data to the program.
2. A precise description of the correct output of the program for
that set of input data.
A problem may be characterized as a fact or group of facts for
which we have no acceptable explanation, that seem unusual, or that
fail to fit in with our expectations or preconceptions. It should be
obvious that some prior beliefs are required if anything is to appear
problematic. If there are no expectations, there can be no surprises.
Principle 2: A programmer should avoid attempting to test his or her
own program.
Any writer knows—or should know—that it’s a bad idea to attempt
to edit or proofread his or her own work. They know what the
piece is supposed to say, hence may not recognize when it says otherwise.
And they really don’t want to find errors in their own work. The
same applies to software authors.
Another problem arises with a change in focus on a software project.
After a programmer has constructively designed and coded a program,
it is extremely difficult to suddenly change perspective to look
at the program with a destructive eye.
As many homeowners know, removing wallpaper (a destructive
process) is not easy, but it is almost unbearably depressing if it was
your hands that hung the paper in the first place. Similarly, most programmers
cannot effectively test their own programs because they
cannot bring themselves to shift mental gears to attempt to expose
errors. Furthermore, a programmer may subconsciously avoid finding
errors for fear of retribution from peers or a supervisor, a client,
or the owner of the program or system being developed.
In addition to these psychological issues, there is a second significant
problem: The program may contain errors due to the programmer’s
misunderstanding of the problem statement or specification. If
this is the case, it is likely that the programmer will carry the same
misunderstanding into tests of his or her own program.
This does not mean that it is impossible for a programmer to test
his or her own program. Rather, it implies that testing is more effective
and successful if someone else does it. However, as we will
14 The Art of Software Testing
discuss in more detail in Chapter 3, developers can be valuable members
of the testing team when the program specification and the program
code itself are being evaluated.
Note that this argument does not apply to debugging (correcting
known errors); debugging is more efficiently performed by the original
programmer.
Principle 3: A programming organization should not test its own
programs.
The argument here is similar to that made in the previous principle.
A project or programming organization is, in many senses, a living
organization with psychological problems similar to those of
individual programmers. Furthermore, in most environments, a programming
organization or a project manager is largely measured on
the ability to produce a program by a given date and for a certain cost.
One reason for this is that it is easy to measure time and cost objectives,
whereas it is extremely difficult to quantify the reliability of a
program. Therefore, it is difficult for a programming organization to
be objective in testing its own programs, because the testing process,
if approached with the proper definition, may be viewed as decreasing
the probability of meeting the schedule and the cost objectives.
Again, this does not say that it is impossible for a programming
organization to find some of its errors, because organizations do
accomplish this with some degree of success. Rather, it implies that it
is more economical for testing to be performed by an objective, independent
party.
Principle 4: Any testing process should include a thorough inspection
of the results of each test.
This is probably the most obvious principle, but again it is something
that is often overlooked. We’ve seen numerous experiments
that show many subjects failed to detect certain errors, even when
symptoms of those errors were clearly observable on the output listings.
Put another way, errors that are found in later tests were often
missed in the results from earlier tests.
Principle 5: Test cases must be written for input conditions that are
invalid and unexpected, as well as for those that are valid
and expected.
There is a natural tendency when testing a program to concentrate
on the valid and expected input conditions, to the neglect of the
The Psychology and Economics of Software Testing 15
invalid and unexpected conditions. For instance, this tendency frequently
appears in the testing of the triangle program in Chapter 1.
Few people, for instance, feed the program the numbers 1, 2, 5 to
ensure that the program does not erroneously interpret this as an
equalateral triangle instead of a scalene triangle. Also, many errors that
are suddenly discovered in production software turn up when it is used
in some new or unexpected way. It is hard, if not impossible, to define
all the use cases for software testing. Therefore, test cases representing
unexpected and invalid input conditions seem to have a higher errordetection
yield than do test cases for valid input conditions.
Principle 6: Examining a programto see if it does not do what it is supposed
to do is only half the battle; the other half is seeing
whether the program does what it is not supposed to do.
This is a corollary to the previous principle. Programs must be
examined for unwanted side effects. For instance, a payroll program
that produces the correct paychecks is still an erroneous program if it
also produces extra checks for nonexistent employees, or if it overwrites
the first record of the personnel file.
Principle 7: Avoid throwaway test cases unless the program is truly a
throwaway program.
This problem is seen most often with interactive systems to test
programs. A common practice is to sit at a terminal and invent test
cases on the fly, and then send these test cases through the program.
The major issue is that test cases represent a valuable investment
that, in this environment, disappears after the testing has been completed.
Whenever the program has to be tested again (e.g., after correcting
an error or making an improvement), the test cases must be
reinvented. More often than not, since this reinvention requires a
considerable amount of work, people tend to avoid it. Therefore, the
retest of the program is rarely as rigorous as the original test, meaning
that if the modification causes a previously functional part of the
program to fail, this error often goes undetected. Saving test cases
and running them again after changes to other components of the
program is known as regression testing.
Principle 8: Do not plan a testing effort under the tacit assumption
that no errors will be found.
This is a mistake project managers often make and is a sign of the
use of the incorrect definition of testing—that is, the assumption that
16 The Art of Software Testing
testing is the process of showing that the program functions correctly.
Once again, the definition of testing is the process of executing a program
with the intent of finding errors. And it should be obvious from
our previous discussions that it is impossible to develop a program
that is completely error free. Even after extensive testing and error
correction, it is safe to assume that errors still exist; they simply have
not yet been found.
Principle 9: The probability of the existence of more errors in a section
of a program is proportional to the number of errors already
found in that section.
This phenomenon is illustrated in Figure 2.2. At first glance this
concept may seem nonsensical, but it is a phenomenon present in
many programs. For instance, if a program consists of two modules,
classes, or subroutines, A and B, and five errors have been found in
module A, and only one error has been found in module B, and if
module A has not been purposely subjected to a more rigorous test,
then this principle tells us that the likelihood of more errors in module
A is greater than the likelihood of more errors in module B.
Another way of stating this principle is to say that errors tend to
come in clusters and that, in the typical program, some sections
seem to be much more prone to errors than other sections, although
nobody has supplied a good explanation of why this occurs. The phenomenon
is useful in that it gives us insight or feedback in the testing
process. If a particular section of a program seems to be much more
prone to errors than other sections, then this phenomenon tells us
FIGURE 2.2 The Surprising Relationship between Errors Remaining and
Errors Found.
The Psychology and Economics of Software Testing 17
that, in terms of yield on our testing investment, additional testing
efforts are best focused against this error-prone section.
Principle 10: Testing is an extremely creative and intellectually challenging
task.
It is probably true that the creativity required in testing a large
program exceeds the creativity required in designing that program.
We already have seen that it is impossible to test a program sufficiently
to guarantee the absence of all errors. Methodologies discussed
later in this book help you develop a reasonable set of test
cases for a program, but these methodologies still require a significant
amount of creativity.
Summary
As you proceed through this book, keep in mind these important principles
of testing:
  Testing is the process of executing a program with the intent of finding
errors.
  Testing is more successful when not performed by the developer(s).
  A good test case is one that has a high probability of detecting an
undiscovered error.
  A successful test case is one that detects an undiscovered error.
  Successful testing includes carefully defining expected output as well
as input.
  Successful testing includes carefully studying test results.
18 The Art of Software Testing
3 Program Inspections,
Walkthroughs, and
Reviews
For many years, most of us in the programming community worked under
the assumptions that programs are written solely for machine execution,
and are not intended for people to read, and that the only way to
test a program is to execute it on a machine. This attitude began to change
in the early 1970s through the efforts of program developers who first saw
the value in reading code as part of a comprehensive testing and debugging
regimen.
Today, not all testers of software applications read code, but the concept
of studying program code as part of a testing effort certainly is widely accepted.
Several factors may affect the likelihood that a given testing and
debugging effort will include people actually reading program code: the
size or complexity of the application, the size of the development team,
the timeline for application development (whether the schedule is relaxed
or intense, for example), and, of course, the background and culture of the
programming team.
For these reasons, we will discuss the process of noncomputer-based
testing (‘‘human testing’’) before we delve into the more traditional
computer-based testing techniques. Human testing techniques are quite
effective in finding errors—so much so that every programming project
should use one or more of these techniques. You should apply these
methods between the time the program is coded and when computerbased
testing begins. You also can develop and apply analogous methods
19
at earlier stages in the programming process (such as at the end of each
design stage), but these are outside the scope of this book.
Before we begin the discussion of human testing techniques, take note
of this important point: Because the involvement of humans results in less
formal methods than mathematical proofs conducted by a computer, you
may feel skeptical that something so simple and informal can be useful.
Just the opposite is true. These informal techniques don’t get in the way of
successful testing; rather, they contribute substantially to productivity and
reliability in two major ways.
First, it is generally recognized that the earlier errors are found, the lower
the costs of correcting the errors and the higher the probability of correcting
them correctly. Second, programmers seem to experience a psychological
shift when computer-based testing commences. Internally induced pressures
seem to build rapidly and there is a tendency to want to ‘‘fix this darn bug
as soon as possible.’’ Because of these pressures, programmers tend to make
more mistakes when correcting an error found during computer-based testing
than they make when correcting an error found earlier.
Inspections and Walkthroughs
The three primary human testing methods are code inspections, walkthroughs
and user (or usability) testing. We cover the first two of these,
which are code-oriented methods, in this chapter. These methods can be
used at virtually any stage of software development, after an application is
deemed to be complete or as each module or unit is complete (see Chapter
5 for more information on module testing). We discuss user testing in
detail in Chapter 7.
The two code inspection methods have a lot in common, so we will discuss
their similarities together. Their differences are enumerated in subsequent
sections.
Inspections and walkthroughs involve a team of people reading or
visually inspecting a program. With either method, participants must
conduct some preparatory work. The climax is a ‘‘meeting of the minds,’’
at a participant conference. The objective of the meeting is to find errors
but not to find solutions to the errors—that is, to test, not debug.
Code inspections and walkthroughs have been widely used for some
time. In our opinion, the reason for their success is related to some of the
principles identified in Chapter 2.
20 The Art of Software Testing
In a walkthrough, a group of developers—with three or four being an
optimal number—performs the review. Only one of the participants is the
author of the program. Therefore, the majority of program testing is conducted
by people other than the author, which follows testing principle 2,
which states that an individual is usually ineffective in testing his or her
own program. (Refer to Chapter 2, Table 2.1, and the subsequent discussion
for all 10 program testing principles.)
An inspection or walkthrough is an improvement over the older deskchecking
process (whereby a programmer reads his or her own program
before testing it). Inspections and walkthroughs are more effective, again
because people other than the program’s author are involved in the
process.
Another advantage of walkthroughs, resulting in lower debugging
(error-correction) costs, is the fact that when an error is found it usually is
located precisely in the code as opposed to black box testing where you
only receive an unexpected result. Moreover, this process frequently
exposes a batch of errors, allowing the errors to be corrected later en
masse. Computer-based testing, on the other hand, normally exposes only
a symptom of the error (e.g., the program does not terminate or the
program prints a meaningless result), and errors are usually detected and
corrected one by one.
These human testing methods generally are effective in finding from
30 to 70 percent of the logic-design and coding errors in typical programs.
They are not effective, however, in detecting high-level design errors, such
as errors made in the requirements analysis process. Note that a success
rate of 30 to 70 percent doesn’t mean that up to 70 percent of all errors
might be found. Recall from Chapter 2 that we can never know the total
number of errors in a program. Thus, what this means is that these methods
are effective in finding up to 70 percent of all errors found by the end
of the testing process.
Of course, a possible criticism of these statistics is that the human processes
find only the ‘‘easy’’ errors (those that would be trivial to find with
computer-based testing) and that the difficult, obscure, or tricky errors
can be found only by computer-based testing. However, some testers
using these techniques have found that the human processes tend to
be more effective than the computer-based testing processes in finding
certain types of errors, while the opposite is true for other types of
errors (e.g., uninitialized variables versus divide by zero errors).
Program Inspections, Walkthroughs, and Reviews 21
The implication is that inspections/walkthroughs and computer-based
testing are complementary; error-detection efficiency will suffer if one or
the other is not present.
Finally, although these processes are invaluable for testing new programs,
they are of equal, or even higher, value in testing modifications
to programs. In our experience, modifying an existing program is a process
that is more error prone (in terms of errors per statement written) than
writing a new program. Therefore, program modifications also should
be subjected to these testing processes as well as regression testing
techniques.
Code Inspections
A code inspection is a set of procedures and error-detection techniques
for group code reading. Most discussions of code inspections focus on the
procedures, forms to be filled out, and so on. Here, after a short summary
of the general procedure, we will focus on the actual error-detection
techniques.
Inspection Team
An inspection team usually consists of four people. The first of the four
plays the role of moderator, which in this context is tantamount to
that of a quality-control engineer. The moderator is expected to be a
competent programmer, but he or she is not the author of the program
and need not be acquainted with the details of the program. Moderator
duties include:
  Distributing materials for, and scheduling, the inspection session.
  Leading the session.
  Recording all errors found.
  Ensuring that the errors are subsequently corrected.
The second team member is the programmer. The remaining team
members usually are the program’s designer (if different from the programmer)
and a test specialist. The specialist should be well versed in software
testing and familiar with the most common programming errors, which we
discuss later in this chapter.
22 The Art of Software Testing
Inspection Agenda
Several days in advance of the inspection session, the moderator distributes
the program’s listing and design specification to the other participants.
The participants are expected to familiarize themselves with the material
prior to the session. During the session, two activities occur:
1. The programmer narrates, statement by statement, the logic of the
program. During the discourse, other participants should raise questions,
which should be pursued to determine whether errors exist. It
is likely that the programmer, rather than the other team members,
will find many of the errors identified during this narration. In other
words, the simple act of reading aloud a program to an audience seems
to be a remarkably effective error-detection technique.
2. The program is analyzed with respect to checklists of historically
common programming errors (such a checklist is discussed in the
next section).
The moderator is responsible for ensuring that the discussions proceed
along productive lines and that the participants focus their attention on
finding errors, not correcting them. (The programmer corrects errors after
the inspection session.)
Upon the conclusion of the inspection session, the programmer is given
a list of the errors uncovered. If more than a few errors were found, or if
any of the errors require a substantial correction, the moderator might
make arrangements to reinspect the program after those errors have been
corrected. This subsequent list of errors is also analyzed, categorized, and
used to refine the error checklist to improve the effectiveness of future
inspections.
As stated, this inspection process usually concentrates on discovering
errors, not correcting them. That said, some teams may find that when a
minor problem is discovered, two or three people, including the programmer
responsible for the code, may propose design changes to handle this
special case. The discussion of this minor problem may, in turn, focus
the group’s attention on that particular area of the design. During the discussion
of the best way to alter the design to handle this minor problem,
someone may notice a second problem. Now that the group has seen two
problems related to the same aspect of the design, comments likely will
Program Inspections, Walkthroughs, and Reviews 23
come thick and fast, with interruptions every few sentences. In a few minutes,
this whole area of the design could be thoroughly explored, and any
problems made obvious.
The time and location of the inspection should be planned to prevent all
outside interruptions. The optimal amount of time for the inspection session
appears to be from 90 to 120 minutes. The session is a mentally taxing
experience, thus longer sessions tend to be less productive. Most inspections
proceed at a rate of approximately 150 program statements per hour.
For that reason, large programs should be examined over multiple inspections,
each dealing with one or several modules or subroutines.
Human Agenda
Note that for the inspection process to be effective, the testing group must
adopt an appropriate attitude. If, for example, the programmer views the
inspection as an attack on his or her character and adopts a defensive posture,
the process will be ineffective. Rather, the programmer must a leave
his or her ego at the door and place the process in a positive and constructive
light, keeping in mind that the objective of the inspection is to find
errors in the program and, thus, improve the quality of the work. For this
reason, most people recommend that the results of an inspection be a
confidential matter, shared only among the participants. In particular, if
managers somehow make use of the inspection results (to assume or imply
that the programmer is inefficient or incompetent, for example), the purpose
of the process may be defeated.
Side Benefits of the Inspection Process
The inspection process has several beneficial side effects, in addition to its
main effect of finding errors. For one, the programmer usually receives
valuable feedback concerning programming style, choice of algorithms,
and programming techniques. The other participants gain in a similar way
by being exposed to another programmer’s errors and programming style.
In general, this type of software testing helps reinforce a team approach to
this particular project and to projects that involve these participants in
general. Reducing the potential for the evolution of an adversarial relationship,
in favor of a cooperative, team approach to projects, can lead to more
efficient and reliable program development.
24 The Art of Software Testing
Finally, the inspection process is a way of identifying early the most
error-prone sections of the program, helping to focus attention more
directly on these sections during the computer-based testing processes
(number 9 of the testing principles given in Chapter 2).
An Error Checklist for Inspections
An important part of the inspection process is the use of a checklist to
examine the program for common errors. Unfortunately, some checklists
concentrate more on issues of style than on errors (e.g., ‘‘Are comments
accurate and meaningful?’’ and ‘‘Are if-else code blocks, and do-while
groups aligned?’’), and the error checks are too nebulous to be useful
(such as, ‘‘Does the code meet the design requirements?’’). The checklist in
this section, divided into six categories, was compiled after many years of
study of software errors. It is largely language-independent, meaning that
most of the errors can occur with any programming language. You may
wish to supplement this list with errors peculiar to your programming language
and with errors detected after completing the inspection process.
Data Reference Errors
Does a referenced variable have a value that is unset or uninitialized?
This probably is the most frequent programming error, occurring in
a wide variety of circumstances. For each reference to a data item
(variable, array element, field in a structure), attempt to ‘‘prove’’ informally
that the item has a value at that point.
For all array references, is each subscript value within the defined
bounds of the corresponding dimension?
For all array references, does each subscript have an integer value?
This is not necessarily an error in all languages, but, in general,
working with noninteger array references is a dangerous practice.
For all references through pointer or reference variables, is the referenced
memory currently allocated? This is known as the ‘‘dangling
reference’’ problem. It occurs in situations where the lifetime of a
pointer is greater than the lifetime of the referenced memory. One
instance occurs where a pointer references a local variable within
a procedure, the pointer value is assigned to an output parameter
or a global variable, the procedure returns (freeing the referenced
Program Inspections, Walkthroughs, and Reviews 25
location), and later the program attempts to use the pointer value.
In a manner similar to checking for the prior errors, try to prove
informally that, in each reference using a pointer variable, the referenced
memory exists.
When a memory area has alias names with differing attributes, does
the data value in this area have the correct attributes when referenced
via one of these names? Situations to look for are the use of
the EQUIVALENCE statement in Fortran and the REDEFINES clause in
COBOL. As an example, a Fortran program contains a real variable A
and an integer variable B; both are made aliases for the same memory
area by using an EQUIVALENCE statement. If the program stores a
value into A and then references variable B, an error is likely present
since the machine would use the floating-point bit representation in
the memory area as an integer.
Sidebar 3.1: History of COBOL and Fortran
COBOL and Fortran are older programming languages that have
fueled business and scientific software development for generations
of computer hardware, operating systems and programmers.
COBOL (an acronym for COmmon Business Oriented Language)
first was defined about 1959 or 1960, and was designed to support
business application development on mainframe class computers.
The original specification included aspects of other existing languages
at the time. Big-name computer manufacturers and representatives of
the federal government participated in this effort to create a businessoriented
programming language that could run on a variety of hardware
and operating system platforms.
COBOL language standards have been reviewed and updated over
the years. By 2002, COBOL was available for most current operating
platforms and object-oriented versions supporting the .NET development
environment.
As the time of this writing, the latest version of COBOL is Visual
COBOL 2010.
Fortran (originally FORTRAN, but modern references generally
follow the uppercase/lowercase syntax) is a little older than COBOL,
26 The Art of Software Testing
Does a variable’s value have a type or attribute other than what
the compiler expects? This situation might occur where a C or Cþþ
program reads a record into memory and references it by using a
structure, but the physical representation of the record differs from
the structure definition.
Are there any explicit or implicit addressing problems if, on the computer
being used, the units of memory allocation are smaller than the
units of addressable memory? For instance, in some environments,
fixed-length bit strings do not necessarily begin on byte boundaries,
but address only point-to-byte boundaries. If a program computes
the address of a bit string and later refers to the string through this
address, the wrong memory location may be referenced. This situation
also could occur when passing a bit-string argument to a
subroutine.
If pointer or reference variables are used, does the referenced memory
location have the attributes the compiler expects? An example of
such an error is where a Cþþ pointer upon which a data structure is
based is assigned the address of a different data structure.
If a data structure is referenced in multiple procedures or subroutines,
is the structure defined identically in each procedure?
When indexing into a string, are the limits of the string off by one
in indexing operations or in subscript references to arrays?
with early specifications defined in the early to middle 1950s. Like
COBOL, Fortran was designed for specific types of mainframe application
development, but in the scientific and numerical management
arenas. The name derives from an existing IBM system at the time,
Mathematical FORmula TRANslating System. Although the original
Fortran contained only 32 statements, it marked a significant improvement
over assembly-level programming that preceded it.
The current version as of the publication date of this book is Fortran
2008, formally approved by the appropriate standard committees
in 2010. Like COBOL, the evolution of Fortran added support for a
broad range of hardware and operating system platforms. However,
Fortran is probably used more in current development—as well as
older system maintenance—than COBOL.
Program Inspections, Walkthroughs, and Reviews 27
For object-oriented languages, are all inheritance requirements met
in the implementing class?
Data Declaration Errors
Have all variables been explicitly declared? A failure to do so is not
necessarily an error, but is, nevertheless, a common source of trouble.
For instance, if a program subroutine receives an array parameter,
and fails to define the parameter as an array (as in a DIMENSION
statement), a reference to the array (such as C¼A(I)) is interpreted
as a function call, leading to the machine’s attempting to execute the
array as a program. Also, if a variable is not explicitly declared in an
inner procedure or block, is it understood that the variable is shared
with the enclosing block?
If all attributes of a variable are not explicitly stated in the declaration,
are the defaults well understood? For instance, the default
attributes received in Java are often a source of surprise when not
properly declared.
Where a variable is initialized in a declarative statement, is it properly
initialized? In many languages, initialization of arrays and
strings is somewhat complicated and, hence, error prone.
Is each variable assigned the correct length and data type?
Is the initialization of a variable consistent with its memory type?
For instance, if a variable in a Fortran subroutine needs to be reinitialized
each time the subroutine is called, it must be initialized with
an assignment statement rather than a DATA statement.
Are there any variables with similar names (e.g., VOLT and VOLTS)?
This is not necessarily an error, but it should be seen as a warning
that the names may have been confused somewhere within the
program.
Computation Errors
Are there any computations using variables having inconsistent
(such as nonarithmetic) data types?
Are there any mixed-mode computations? An example is when
working with floating-point and integer variables. Such occurrences
are not necessarily errors, but they should be explored carefully to
ensure that the conversion rules of the language are understood.
28 The Art of Software Testing
Consider the following Java snippet showing the rounding error that
can occur when working with integers:
int x¼1;
int y¼2;
int z¼0;
z¼x/y;
System.out.println ("z¼ " þz);
OUTPUT:
z¼0
Are there any computations using variables having the same data
type but of different lengths?
Is the data type of the target variable of an assignment smaller than
the data type or a result of the right-hand expression?
Is an overflow or underflow expression possible during the computation
of an expression? That is, the end result may appear to have
valid value, but an intermediate result might be too big or too small
for the programming language’s data types.
Is it possible for the divisor in a division operation to be zero?
If the underlying machine represents variables in base-2 form, are
there any sequences of the resulting inaccuracy? That is, 10   0.1 is
rarely equal to 1.0 on a binary machine.
Where applicable, can the value of a variable go outside the meaningful
range? For example, statements assigning a value to the variable
PROBABILITY might be checked to ensure that the assigned value
will always be positive and not greater than 1.0.
For expressions containing more than one operator, are the assumptions
about the order of evaluation and precedence of operators
correct?
Are there any invalid uses of integer arithmetic, particularly divisions?
For instance, if i is an integer variable, whether the expression
2 i/2¼¼i depends on whether i has an odd or an even value
and whether the multiplication or division is performed first.
Comparison Errors
Are there any comparisons between variables having different data types,
such as comparing a character string to an address, date, or number?
Program Inspections, Walkthroughs, and Reviews 29
Are there any mixed-mode comparisons or comparisons between
variables of different lengths? If so, ensure that the conversion rules
are well understood.
Are the comparison operators correct? Programmers frequently confuse
such relations as at most, at least, greater than, not less than, and
less than or equal.
Does each Boolean expression state what it is supposed to state? Programmers
often make mistakes when writing logical expressions involving
and, or, and not.
Are the operands of a Boolean operator Boolean? Have comparison
and Boolean operators been erroneously mixed together? This represents
another frequent class of mistakes. Examples of a few typical
mistakes are illustrated here:
  If you want to determine whether i is between 2 and 10, the
expression 2<i<10 is incorrect. Instead, it should be (2<i)&&
(i<10).
  If you want to determine whether i is greater than x or y, i>xjjy
is incorrect. Instead, it should be (i>x)jj(i>y).
  If you want to compare three numbers for equality, if(a¼¼b¼¼c)
does something quite different.
  If you want to test the mathematical relation x>y>z, the correct
expression is (x>y)&&(y>z).
Are there any comparisons between fractional or floating-point numbers
that are represented in base-2 by the underlying machine? This
is an occasional source of errors because of truncation and base-2
approximations of base-10 numbers.
For expressions containing more than one Boolean operator, are
the assumptions about the order of evaluation and the precedence
of operators correct? That is, if you see an expression
such as if((a¼¼2)&&(b¼¼2)jj(c¼¼3)), is it well understood
whether the and or the or is performed first?
Does the way in which the compiler evaluates Boolean expressions
affect the program? For instance, the statement
if(x¼¼0&&(x/y)>z)
may be acceptable for compilers that end the test as soon as one side
of an and is false, but may cause a division-by-zero error with other
compilers.
30 The Art of Software Testing
Control-Flow Errors
If the program contains a multipath branch such as a computed
GOTO, can the index variable ever exceed the number of branch possibilities?
For example, in the statement
GOTO(200,300,400),i
will i always have the value of 1, 2, or 3?
Will every loop eventually terminate? Devise an informal proof or
argument showing that each loop will terminate.
Will the program, module, or subroutine eventually terminate?
Is it possible that, because of the conditions upon entry, a loop will
never execute? If so, does this represent an oversight? For instance,
if you had the following for loop and while loop headed by the following
statements:
for (i¼x;i<¼z;iþþ){
...
}
or . . .
while (NOTFOUND) {
...
}
what happens if x is greater than z or if NOTFOUND is initially false?
For a loop controlled by both iteration and a Boolean condition (e.g.,
a searching loop) what are the consequences of loop fall-through?
For example, for the psuedo-code loop headed by
DO I¼1 to TABLESIZE WHILE (NOTFOUND)
what happens if NOTFOUND never becomes false?
Are there any off-by-one errors, such as one too many or too few
iterations? This is a common error in zero-based loops. You will often
forget to count 0 as a number. For example, if you want to create
Java code for a loop that iterates 10 times, the following would be
wrong, as it performs 11 iterations:
for (int i¼0;i<¼10;iþþ){
System.out.println(i);
}
Program Inspections, Walkthroughs, and Reviews 31
Correct, the loop is iterated 10 times:
for (int i¼0; i<10;iþþ) {
System.out.println(i);
}
If the language contains a concept of statement groups or code
blocks (e.g., do-while or {...}), is there an explicit while for each
group, and do the instances of do correspond to their appropriate
groups? Is there a closing bracket for each open bracket? Most modern
compilers will complain of such mismatches.
Are there any nonexhaustive decisions? For instance, if an input parameter’s
expected values are 1, 2, or 3, does the logic assume that it
must be 3 if it is not 1 or 2? If so, is the assumption valid?
Interface Errors
Does the number of parameters received by this module equal the
number of arguments sent by each of the calling modules? Also, is
the order correct?
Do the attributes (e.g., data type and size) of each parameter match
the attributes of each corresponding argument?
Does the units system of each parameter match the units system of
each corresponding argument? For example, is the parameter
expressed in degrees but the argument expressed in radians?
Does the number of arguments passed by this module to another
module equal the number of parameters expected by that module?
Do the attributes of each argument passed to another module match
the attributes of the corresponding parameter in that module?
Does the units system of each argument passed to another module
match the units system of the corresponding parameter in that
module?
If built-in functions are invoked, are the number, attributes, and order
of the arguments correct?
If a module or class has multiple entry points, is a parameter ever
referenced that is not associated with the current point of entry?
Such an error exists in the second assignment statement in the
following PL/1 program:
32 The Art of Software Testing
A: PROCEDURE (W,X);
W¼Xþ1;
RETURN
B: ENTRY (Y,Z);
Y¼XþZ;
END;
Does a subroutine alter a parameter that is intended to be only an
input value?
If global variables are present, do they have the same definition and
attributes in all modules that reference them?
Are constants ever passed as arguments? In some Fortran implementations
a statement such as
CALL SUBX(J,3)
is dangerous, because if the subroutine SUBX assigns a value to its
second parameter, the value of the constant 3 will be altered.
Input/Output Errors
If files are explicitly declared, are their attributes correct?
Are the attributes on the file’s OPEN statement correct?
Does the format specification agree with the information in the I/O
statement? For instance, in Fortran, does each FORMAT statement
agree (in terms of the number and attributes of the items) with the
corresponding READ or WRITE statement?
Is sufficient memory available to hold the file your program will read?
Have all files been opened before use?
Have all files been closed after use?
Are end-of-file conditions detected and handled correctly?
Are I/O error conditions handled correctly?
Are there spelling or grammatical errors in any text that is printed or
displayed by the program?
Does the program properly handle ‘‘File not Found’’ errors?
Other Checks
If the compiler produces a cross-reference listing of identifiers, examine
it for variables that are never referenced or are referenced only once.
Program Inspections, Walkthroughs, and Reviews 33
If the compiler produces an attribute listing, check the attributes of
each variable to ensure that no unexpected default attributes have
been assigned.
If the program compiled successfully, but the computer produced
one or more ‘‘warning’’ or ‘‘informational’’ messages, check each one
carefully. Warning messages are indications that the compiler suspects
you are doing something of questionable validity: Review all of
these suspicions. Informational messages may list undeclared variables
or language uses that impede code optimization.
Is the program or module sufficiently robust? That is, does it check
its input for validity?
Is a function missing from the program?
This checklist is summarized in Tables 3.1 and 3.2.
Walkthroughs
The code walkthrough, like the inspection, is a set of procedures and
error-detection techniques for group code reading. It shares much in common
with the inspection process, but the procedures are slightly different,
and a different error-detection technique is employed.
Like the inspection, the walkthrough is an uninterrupted meeting of
one to two hours in duration. The walkthrough team consists of three to
five people. One of these people plays a role similar to that of the moderator
in the inspection process; another person plays the role of a secretary
(a person who records all errors found); and a third person plays the role
of a tester. Suggestions as to who the three to five people should be vary. Of
course, the programmer is one of those people. Suggestions for the other
participants include:
  A highly experienced programmer
  A programming-language expert
  A new programmer (to give a fresh, unbiased outlook)
  The person who will eventually maintain the program
  Someone from a different project
  Someone from the same programming team as the programmer
34 The Art of Software Testing
TABLE 3.1 Inspection Error Checklist Summary, Part I
Data Reference Computation
1. Unset variable used? 1. Computations on nonarithmetic
variables?
2. Subscripts within bounds? 2. Mixed-mode computations?
3. Noninteger subscripts? 3. Computations on variables of
different lengths?
4. Dangling references? 4. Target size less than size of
assigned value?
5. Correct attributes when aliasing? 5. Intermediate result overflow or
underflow?
6. Record and structure attributes match? 6. Division by zero?
7. Computing addresses of bit strings?
Passing bit-string arguments?
7. Base-2 inaccuracies?
8. Based storage attributes correct? 8. Variable’s value outside of
meaningful range?
9. Structure definitions match across
procedures?
9. Operator precedence
understood?
10. Off-by-one errors in indexing or
subscripting operations?
10. Integer divisions correct?
11. Inheritance requirements met?
Data Declaration Comparison
1. All variables declared? 1. Comparisons between
inconsistent variables?
2. Default attributes understood? 2. Mixed-mode comparisons?
3. Arrays and strings initialized properly? 3. Comparison relationships correct?
4. Correct lengths, types, and storage
classes assigned?
4. Boolean expressions correct?
5. Initialization consistent with storage
class?
5. Comparison and Boolean
expressions mixed?
6. Any variables with similar names? 6. Comparisons of base-2 fractional
values?
7. Operator precedence understood?
8. Compiler evaluation of Boolean
expressions understood?
Program Inspections, Walkthroughs, and Reviews 35
TABLE 3.2 Inspection Error Checklist Summary, Part II
Control Flow Input/Output
1. Multiway branches exceeded? 1. File attributes correct?
2. Will each loop terminate? 2. OPEN statements correct?
3. Will program terminate? 3. Format specification
matches I/O statement?
4. Any loop bypasses because of entry conditions? 4. Buffer size matches record
size?
5. Possible loop fall-throughs correct? 5. Files opened before use?
6. Off-by-one iteration errors? 6. Files closed after use?
7. DO/END statements match? 7. End-of-file conditions
handled?
8. Any nonexhaustive decisions? 8. I/O errors handled?
9. Any textual or grammatical errors in output
information?
Interfaces Other Checks
1. Number of input parameters equal to number
of arguments?
1. Any unreferenced variables
in cross-reference listing?
2. Parameter and argument attributes match? 2. Attribute list what was
expected?
3. Parameter and argument units system match? 3. Any warning or
informational messages?
4. Number of arguments transmitted to called
modules equal to number of parameters?
4. Input checked for validity?
5. Attributes of arguments transmitted to called
modules equal to attributes of parameters?
5. Missing function?
6. Units system of arguments transmitted to called
modules equal to units system of parameters?
7. Number, attributes, and order of arguments to
built-in functions correct?
8. Any references to parameters not associated
with current point of entry?
9. Input-only arguments altered?
10. Global variable definitions consistent across
modules?
11. Constants passed as arguments?
36 The Art of Software Testing
The initial procedure is identical to that of the inspection process: The
participants are given the materials several days in advance, to allow
them time to bone up on the program. However, the procedure in the
meeting is different. Rather than simply reading the program or using
error checklists, the participants ‘‘play computer.’’ The person designated
as the tester comes to the meeting armed with a small set of paper test
cases—representative sets of inputs (and expected outputs) for the program
or module. During the meeting, each test case is mentally executed;
that is, the test data are ‘‘walked through’’ the logic of the program. The
state of the program (i.e., the values of the variables) is monitored on
paper or a whiteboard.
Of course, the test cases must be simple in nature and few in number,
because people execute programs at a rate that is many orders of magnitude
slower than a machine. Hence, the test cases themselves do not
play a critical role; rather, they serve as a vehicle for getting started and
for questioning the programmer about his or her logic and assumptions.
In most walkthroughs, more errors are found during the process of
questioning the programmer than are found directly by the test cases
themselves.
As in the inspection, the attitude of the participants is critical. Comments
should be directed toward the program rather than the programmer.
In other words, errors are not regarded as weaknesses in the person who
committed them. Rather, they are viewed as inherent to the difficulty of
the program development.
The walkthrough should have a follow-up process similar to that described
for the inspection process. Also, the side effects observed from inspections
(identification of error-prone sections and education in errors,
style, and techniques) also apply to the walkthrough process.
Desk Checking
A third human error-detection process is the older practice of desk checking.
A desk check can be viewed as a one-person inspection or walkthrough:
A person reads a program, checks it with respect to an error list,
and/or walks test data through it.
For most people, desk checking is relatively unproductive. One reason
is that it is a completely undisciplined process. A second, and more important,
reason is that it runs counter to testing principle 2 (see Chapter 2),
Program Inspections, Walkthroughs, and Reviews 37
which states that people are generally ineffective in testing their own
programs. For this reason, you could deduce that desk checking is
best performed by a person other than the author of the program (e.g.,
two programmers might swap programs rather than desk check their
own), but even this is less effective than the walkthrough or inspection
process. The reason is the synergistic effect of the walkthrough
or inspection team. The team session fosters a healthy environment
of competition; people like to show off by finding errors. In a deskchecking
process, there is no one to whom you can show off, thereby
precluding this apparently valuable effect. In short, desk checking
may be more valuable than doing nothing at all, but it is much less
effective than the inspection or walkthrough.
Peer Ratings
The last human review process is not associated with program testing
(i.e., its objective is not to find errors). Nevertheless, we include this process
here because it is related to the idea of code reading.
Peer rating is a technique of evaluating anonymous programs in
terms of their overall quality, maintainability, extensibility, usability,
and clarity. The purpose of the technique is to provide programmer selfevaluation.
A programmer is selected to serve as an administrator of the process.
The administrator, in turn, selects approximately 6 to 20 participants (6 is
the minimum to preserve anonymity). The participants are expected to
have similar backgrounds (e.g., don’t group Java application programmers
with assembly language system programmers). Each participant is asked to
select two of his or her own programs to be reviewed. One program should
be representative of what the participant considers to be his or her finest
work; the other should be a program that the programmer considers to be
poorer in quality.
Once the programs have been collected, they are randomly distributed
to the participants. Each participant is given four programs to review. Two
of the programs are the ‘‘finest’’ programs and two are ‘‘poorer’’ programs,
but the reviewer is not told which is which. Each participant spends
30 minutes reviewing each program and then completes an evaluation
form. After reviewing all four programs, each participant rates the relative
quality of the four programs. The evaluation form asks the reviewer to
38 The Art of Software Testing
answer, on a scale from 1 to 10 (1 meaning definitely yes and 10 meaning
definitely no), such questions as:
  Was the program easy to understand?
  Was the high-level design visible and reasonable?
  Was the low-level design visible and reasonable?
  Would it be easy for you to modify this program?
  Would you be proud to have written this program?
The reviewer also is asked for general comments and suggested
improvements.
After the review, the participants are given the anonymous evaluation
forms for their two contributed programs. They also are given a statistical
summary showing the overall and detailed ranking of their original programs
across the entire set of programs, as well as an analysis of how their
ratings of other programs compared with those ratings of other reviewers
of the same program. The purpose of the process is to allow programmers
to self-assess their programming skills. As such, the process appears to be
useful in both industrial and classroom environments.
Summary
This chapter discussed a form of testing that developers do not often consider:
human code testing. Most people assume that because programs are
written for machine execution, machines should test programs as well.
This assumption is invalid. Human testing techniques are very effective at
revealing errors. In fact, most programming projects should include the
following human testing techniques:
  Code inspections using checklists
  Group walkthroughs
  Desk checking
  Peer reviews
Another form of human testing is user or usability testing, a black-box
technique that evaluates software from a hands-on, end-user perspective.
We cover this topic in detail in Chapter 7.
Program Inspections, Walkthroughs, and Reviews 39

4 Test-Case Design
Moving beyond the psychological issues discussed in Chapter 2, the
most important consideration in program testing is the design and
creation of effective test cases.
Testing, however creative and seemingly complete, cannot guarantee
the absence of all errors. Test-case design is so important because complete
testing is impossible. Put another way, a test of any program must be necessarily
incomplete. The obvious strategy, then, is to try to make tests as
complete as possible.
Given constraints on time and cost, the key issue of testing becomes:
What subset of all possible test cases has the highest probability of
detecting the most errors?
The study of test-case design methodologies supplies answers to this
question.
In general, the least effective methodology of all is random-input
testing—the process of testing a program by selecting, at random, some
subset of all possible input values. In terms of the likelihood of detecting
the most errors, a randomly selected collection of test cases has little
chance of being an optimal, or even close to optimal, subset. Therefore, in
this chapter, we want to develop a set of thought processes that enable you
to select test data more intelligently.
41
Chapter 2 showed that exhaustive black-box and white-box testing are,
in general, impossible; at the same time, it suggested that a reasonable testing
strategy might feature elements of both. This is the strategy developed
in this chapter. You can develop a reasonably rigorous test by using certain
black-box–oriented test-case design methodologies and then supplementing
these test cases by examining the logic of the program, using white-box
methods.
The methodologies discussed in this chapter are:
Black Box White Box
Equivalence partitioning Statement coverage
Boundary value analysis Decision coverage
Cause-effect graphing Condition coverage
Error guessing Decision/condition coverage
Multiple-condition coverage
Although we will discuss these methods separately, we recommend that
you use a combination of most, if not all, of them to design a rigorous test
of a program, since each method has distinct strengths and weaknesses.
One method may find errors another method overlooks, for example.
Nobody ever promised that software testing would be easy. To quote an
old sage, ‘‘If you thought designing and coding that program was hard, you
ain’t seen nothing yet.’’
The recommended procedure is to develop test cases using the blackbox
methods and then develop supplementary test cases, as necessary,
with white-box methods. We’ll discuss the more widely known white-box
methods first.
White-Box Testing
White-box testing is concerned with the degree to which test cases exercise
or cover the logic (source code) of the program. As we saw in Chapter
2, the ultimate white-box test is the execution of every path in the
program; but complete path testing is not a realistic goal for a program
with loops.
42 The Art of Software Testing
Logic Coverage Testing
If you back completely away from path testing, it may seem that a worthy
goal would be to execute every statement in the program at least once.
Unfortunately, this is a weak criterion for a reasonable white-box test. This
concept is illustrated in Figure 4.1. Assume that this figure represents a
small program to be tested. The equivalent Java code snippet follows:
public void foo(int A,int B,int X) {
if(A>1 && B¼¼0) {
X¼X/A;
}
if(A¼¼2 jj X>1) {
X¼Xþ1;
}
}
FIGURE 4.1 A Small Program to Be Tested.
Test-Case Design 43
You could execute every statement by writing a single test case that
traverses path ace. That is, by setting A¼2, B¼0, and X¼3 at point a, every
statement would be executed once (actually, X could be assigned any integer
value >1).
Unfortunately, this criterion is a rather poor one. For instance, perhaps
the first decision should be an or rather than an and. If so, this error would
go undetected. Perhaps the second decision should have stated X>0; this
error would not be detected. Also, there is a path through the program in
which X goes unchanged (the path abd). If this were an error, it would go
undetected. In other words, the statement coverage criterion is so weak
that it generally is useless.
A stronger logic coverage criterion is known as decision coverage or
branch coverage. This criterion states that you must write enough test cases
that each decision has a true and a false outcome at least once. In other
words, each branch direction must be traversed at least once. Examples of
branch or decision statements are switch-case, do-while, and if-else
statements. Multipath GOTO statements qualify in some programming
languages such as Fortran.
Decision coverage usually can satisfy statement coverage. Since every
statement is on some subpath emanating either from a branch statement
or from the entry point of the program, every statement must be executed
if every branch direction is executed. There are, however, at least three
exceptions:
  Programs with no decisions.
  Programs or subroutines/methods with multiple entry points. A given
statement might be executed only if the program is entered at a
particular entry point.
  Statements within ON-units. Traversing every branch direction will
not necessarily cause all ON-units to be executed.
Since we have deemed statement coverage to be a necessary condition,
decision coverage, a seemingly better criterion, should be defined to
include statement coverage. Hence, decision coverage requires that each
decision have a true and a false outcome, and that each statement be executed
at least once. An alternative and easier way of expressing it is that
each decision has a true and a false outcome, and that each point of entry
(including ON-units) be invoked at least once.
44 The Art of Software Testing
This discussion considers only two-way decisions or branches and has to
be modified for programs that contain multipath decisions. Examples are
Java programs containing switch-case statements, Fortran programs containing
arithmetic (three-way) IF statements or computed or arithmetic
GOTO statements, and COBOL programs containing altered GOTO statements
or GO-TO-DEPENDING-ON statements. For such programs, the criterion is exercising
each possible outcome of all decisions at least once and invoking each
point of entry to the program or subroutine at least once.
In Figure 4.1, decision coverage can be met by two test cases covering
paths ace and abd or, alternatively, acd and abe. If we choose the latter
alternative, the two test-case inputs are A¼3, B¼0, X¼3 and A¼2, B¼1,
and X¼1.
Decision coverage is a stronger criterion than statement coverage, but it
still is rather weak. For instance, there is only a 50 percent chance that
we would explore the path where x is not changed (i.e., only if we chose
the former alternative). If the second decision were in error (if it should
have said X<1 instead of X>1), the mistake would not be detected by the
two test cases in the previous example.
A criterion that is sometimes stronger than decision coverage is condition
coverage. In this case, you write enough test cases to ensure that each
condition in a decision takes on all possible outcomes at least once. But, as
with decision coverage, this does not always lead to the execution of each
statement, so an addition to the criterion is that each point of entry to the
program or subroutine, as well as ON-units, be invoked at least once. For
instance, the branching statement:
DO K¼0 to 50 WHILE (JþK<QUEST)
contains two conditions: Is K less than or equal to 50, and is JþK less than
QUEST? Hence, test cases would be required for the situations K<¼50, K>50
(to reach the last iteration of the loop), JþK<QUEST, and JþK>¼QUEST.
Figure 4.1 has four conditions: A>1, B¼0, A¼2, and X>1. Hence, enough
test cases are needed to force the situations where A>1, A<¼1, B¼0, and
B<>0 are present at point a and where A¼2, A<>2, X>1, and X<¼1 are
present at point b. A sufficient number of test cases satisfying the criterion,
and the paths traversed by each, are:
A¼2, B¼0, X¼4 ace
A¼1, B¼1, X¼1 adb
Test-Case Design 45
Note that although the same number of test cases was generated for this
example, condition coverage usually is superior to decision coverage in
that it may (but does not always) cause every individual condition in a
decision to be executed with both outcomes, whereas decision coverage
does not. For instance, in the same branching statement
DO K¼0 to 50 WHILE (JþK<QUEST)
is a two-way branch (execute the loop body or skip it). If you are using
decision testing, the criterion can be satisfied by letting the loop run from
K¼0 to 51, without ever exploring the circumstance where the WHILE clause
becomes false. With the condition criterion, however, a test case would be
needed to generate a false outcome for the conditions JþK<QUEST.
Although the condition coverage criterion appears, at first glance, to
satisfy the decision coverage criterion, it does not always do so. If the decision
IF(A & B) is being tested, the condition coverage criterion would let
you write two test cases—A is true, B is false, and A is false, B is true—but
this would not cause the THEN clause of the IF to execute. The condition
coverage tests for the earlier example covered all decision outcomes, but
this was only by chance. For instance, two alternative test cases
A¼1, B¼0, X¼3
A¼2, B¼1, X¼1
cover all condition outcomes but only two of the four decision outcomes
(both of them cover path abe and, hence, do not exercise the true outcome
of the first decision and the false outcome of the second decision).
The obvious way out of this dilemma is a criterion called decision/
condition coverage. It requires sufficient test cases such that each condition
in a decision takes on all possible outcomes at least once, each decision
takes on all possible outcomes at least once, and each point of entry is
invoked at least once.
A weakness with decision/condition coverage is that although it may appear
to exercise all outcomes of all conditions, it frequently does not, because
certain conditions mask other conditions. To see this, examine
Figure 4.2. The flowchart in this figure is the way a compiler would generate
machine code for the program in Figure 4.1. The multicondition decisions
in the source program have been broken into individual decisions
and branches because most machines do not have a single instruction that
makes multicondition decisions. A more thorough test coverage, then,
46 The Art of Software Testing
appears to be the exercising of all possible outcomes of each primitive decision.
The two previous decision coverage test cases do not accomplish
this; they fail to exercise the false outcome of decision H and the true outcome
of decision K.
The reason, as shown in Figure 4.2, is that results of conditions in the
and and the or expressions can mask or block the evaluation of other conditions.
For instance, if an and condition is false, none of the subsequent
conditions in the expression need be evaluated. Likewise, if an or condition
is true, none of the subsequent conditions need be evaluated. Hence,
errors in logical expressions are not necessarily revealed by the condition
coverage and decision/condition coverage criteria.
A criterion that covers this problem, and then some, is multiple-condition
coverage. This criterion requires that you write sufficient test cases such that
all possible combinations of condition outcomes in each decision, and all
FIGURE 4.2 Machine Code for the Program in Figure 4.1.
Test-Case Design 47
points of entry, are invoked at least once. For instance, consider the following
sequence of pseudo-code.
NOTFOUND¼TRUE;
DO I¼1 to TABSIZE WHILE (NOTFOUND); /*SEARCH TABLE*/
.. . searching logic.. . ;
END
The four situations to be tested are:
1. I<¼TABSIZE and NOTFOUND is true.
2. I<¼TABSIZE and NOTFOUND is false (finding the entry before
hitting the end of the table).
3. I>TABSIZE and NOTFOUND is true (hitting the end of the table without
finding the entry).
4. I>TABSIZE and NOTFOUND is false (the entry is the last one in the
table).
It should be easy to see that a set of test cases satisfying the multiplecondition
criterion also satisfies the decision coverage, condition coverage,
and decision/condition coverage criteria.
Returning to Figure 4.1, test cases must cover eight combinations:
1. A>1, B¼0 5. A¼2, X>1
2. A>1, B<>0 6. A¼2, X<¼1
3. A<¼1, B¼0 7. A<>2, X>1
4. A<¼1, B<>0 8. A<>2, X<¼1
Note Recall from the Java code snippet presented earlier that test cases
5 through 8 express values at the point of the second if statement. Since X
may be altered above this if statement, the values needed at this if statement
must be backed up through the logic to find the corresponding input
values.
These combinations to be tested do not necessarily imply that eight test
cases are needed. In fact, they can be covered by four test cases. The testcase
input values, and the combinations they cover, are as follows:
A¼2, B¼0, X¼4 Covers 1, 5
A¼2, B¼1, X¼1 Covers 2, 6
48 The Art of Software Testing
A¼1, B¼0, X¼2 Covers 3, 7
A¼1, B¼1, X¼1 Covers 4, 8
The fact that there are four test cases and four distinct paths in Figure 4.1
is just coincidence. In fact, these four test cases do not cover every path; they
miss the path acd. For instance, you would need eight test cases for the
following decision:
if(x¼¼y && length(z)¼¼0 && FLAG) {
j¼1;
else
i¼1;
}
although it contains only two paths. In the case of loops, the number of
test cases required by the multiple-condition criterion is normally much
less than the number of paths.
In summary, for programs containing only one condition per decision,
a minimum test criterion is a sufficient number of test cases to: (1) invoke
all outcomes of each decision at least once, and (2) invoke each point of
entry (such as entry point or ON-unit) at least once, to ensure that all statements
are executed at least once. For programs containing decisions having
multiple conditions, the minimum criterion is a sufficient number of
test cases to invoke all possible combinations of condition outcomes
in each decision, and all points of entry to the program, at least once.
(The word ‘‘possible’’ is inserted because some combinations may be found
to be impossible to create.)
Black-Box Testing
As we discussed in Chapter 2, black-box (data-driven or input/output
driven) testing is based on program specifications. The goal is to find areas
wherein the program does not behave according to its specifications.
Equivalence Partitioning
Chapter 2 described a good test case as one that has a reasonable probability
of finding an error; it also stated that an exhaustive input test of a program
is impossible. Hence, when testing a program, you are limited to a
Test-Case Design 49
small subset of all possible inputs. Of course, then, you want to select the
‘‘right’’ subset, that is, the subset with the highest probability of finding the
most errors.
One way of locating this subset is to realize that a well-selected test case
also should have two other properties:
1. It reduces, by more than a count of one, the number of other test
cases that must be developed to achieve some predefined goal of ‘‘reasonable’’
testing.
2. It covers a large set of other possible test cases. That is, it tells us
something about the presence or absence of errors over and above
this specific set of input values.
These properties, although they appear to be similar, describe two distinct
considerations. The first implies that each test case should invoke as
many different input considerations as possible to minimize the total number
of test cases necessary. The second implies that you should try to partition
the input domain of a program into a finite number of equivalence
classes such that you can reasonably assume (but, of course, not be absolutely
sure) that a test of a representative value of each class is equivalent
to a test of any other value. That is, if one test case in an equivalence class
detects an error, all other test cases in the equivalence class would be
expected to find the same error. Conversely, if a test case did not detect an
error, we would expect that no other test cases in the equivalence class
would fall within another equivalence class, since equivalence classes may
overlap one another.
These two considerations form a black-box methodology known as
equivalence partitioning. The second consideration is used to develop a set
of ‘‘interesting’’ conditions to be tested. The first consideration is then used
to develop a minimal set of test cases covering these conditions.
An example of an equivalence class in the triangle program of Chapter 1
is the set ‘‘three equal-valued numbers having integer values greater than
zero.’’ By identifying this as an equivalence class, we are stating that if no
error is found by a test of one element of the set, it is unlikely that an error
would be found by a test of another element of the set. In other words, our
testing time is best spent elsewhere: in different equivalence classes.
Test-case design by equivalence partitioning proceeds in two steps:
(1) identifying the equivalence classes and (2) defining the test cases.
50 The Art of Software Testing
Identifying the Equivalence Classes The equivalence classes are identified
by taking each input condition (usually a sentence or phrase in the
specification) and partitioning it into two or more groups. You can use the
table in Figure 4.3 to do this. Notice that two types of equivalence classes
are identified: valid equivalence classes represent valid inputs to the program,
and invalid equivalence classes represent all other possible states
of the condition (i.e., erroneous input values). Thus, we are adhering to
principle 5, discussed in Chapter 2, which stated you must focus attention
on invalid or unexpected conditions.
Given an input or external condition, identifying the equivalence classes
is largely a heuristic process. Follow these guidelines:
1. If an input condition specifies a range of values (e.g., ‘‘the item
count can be from 1 to 999’’), identify one valid equivalence class
(1<item count<999) and two invalid equivalence classes
(item count<1 and item count>999).
2. If an input condition specifies the number of values (e.g., ‘‘one
through six owners can be listed for the automobile’’), identify one
valid equivalence class and two invalid equivalence classes (no owners
and more than six owners).
3. If an input condition specifies a set of input values, and there is
reason to believe that the program handles each differently (‘‘type
External
condition
Valid equivalence
classes
Invalid equivalence
classes
FIGURE 4.3 A Form for Enumerating Equivalence Classes.
Test-Case Design 51
of vehicle must be BUS, TRUCK, TAXICAB, PASSENGER, or
MOTORCYCLE’’), identify a valid equivalence class for each and one
invalid equivalence class (‘‘TRAILER,’’ for example).
4. If an input condition specifies a ‘‘must-be’’ situation, such as ‘‘first
character of the identifier must be a letter,’’ identify one valid equivalence
class (it is a letter) and one invalid equivalence class (it is not a
letter).
If there is any reason to believe that the program does not handle
elements in an equivalence class identically, split the equivalence class
into smaller equivalence classes. We will illustrate an example of this
process shortly.
Identifying the Test Cases The second step is the use of equivalence
classes to identify the test cases. The process is as follows:
1. Assign a unique number to each equivalence class.
2. Until all valid equivalence classes have been covered by (incorporated
into) test cases, write a new test case covering as many of the
uncovered valid equivalence classes as possible.
3. Until your test cases have covered all invalid equivalence classes,
write a test case that covers one, and only one, of the uncovered
invalid equivalence classes.
The reason that individual test cases cover invalid cases is that certain
erroneous-input checks mask or supersede other erroneous-input checks.
For instance, if the specification states ‘‘enter book type (HARDCOVER,
SOFTCOVER, or LOOSE) and amount (1–999),’’ the test case, (XYZ 0),
expressing two error conditions (invalid book type and amount) will probably
not exercise the check for the amount, since the program may say
‘‘XYZ IS UNKNOWN BOOK TYPE’’ and not bother to examine the remainder
of the input.
An Example
As an example, assume that we are developing a compiler for a subset
of the Fortran language, and we wish to test the syntax checking of the
DIMENSION statement. The specification is listed below. (Note: This is not
52 The Art of Software Testing
the full Fortran DIMENSION statement; it has been edited considerably to
make it textbook size. Do not be deluded into thinking that the testing of
actual programs is as easy as the examples in this book.) In the specification,
items in italics indicate syntactic units for which specific entities
must be substituted in actual statements; brackets are used to indicate option
items; and an ellipsis indicates that the preceding item may appear
multiple times in succession.
A DIMENSION statement is used to specify the dimensions of arrays.
The form of the DIMENSION statement is
DIMENSION ad[,ad]...
where ad is an array descriptor of the form
n(d[,d]...)
where n is the symbolic name of the array and d is a dimension
declarator. Symbolic names can be one to six letters or digits, the first
of which must be a letter. The minimum and maximum numbers of
dimension declarations that can be specified for an array are one and
seven, respectively. The form of a dimension declarator is
[lb: ]ub
where lb and ub are the lower and upper dimension bounds. A
bound may be a constant in the range  65534 to 65535 or the name
of an integer variable (but not an array element name). If lb is not
specified, it is assumed to be 1. The value of ub must be greater than
or equal to lb. If lb is specified, its value may be negative, 0, or positive.
As for all statements, the DIMENSION statement may be continued
over multiple lines.
The first step is to identify the input conditions and, from these, locate
the equivalence classes. These are tabulated in Table 4.1. The numbers in
the table are unique identifiers of the equivalence classes.
The next step is to write a test case covering one or more valid equivalence
classes. For instance, the test case
DIMENSION A(2)
covers classes 1, 4, 7, 10, 12, 15, 24, 28, 29, and 43.
Test-Case Design 53
The next step is to devise one or more test cases covering the remaining
valid equivalence classes. One test case of the form
DIMENSION A 12345 (I,9,J4XXXX,65535,1,KLM,
X,1000, BBB(-65534:100,0:1000,10:10, I:65535)
covers the remaining classes. The invalid input equivalence classes, and a
test case representing each, are:
TABLE 4.1 Equivalence Classes
Input Condition
Valid Equivalence
Classes
Invalid Equivalence
Classes
Number of array
descriptors
one (1), > one (2) none (3)
Size of array name 1–6 (4) 0 (5), >6 (6)
Array name has letters (7),
has digits (8)
has something
else (9)
Array name starts with letter yes (10) no (11)
Number of dimensions 1–7 (12) 0 (13), >7 (14)
Upper bound is constant (15),
integer variable (16)
array element name (17),
something else (18)
Integer variable name has letter (19), has
digits (20)
has something else (21)
Integer variable starts with
letter
yes (22) no (23)
Constant –65534–65535
(24)
<–65534 (25),
>65535 (26)
Lower bound specified yes (27), no (28)
Upper bound to lower
bound
greater than (29),
equal (30)
less than (31)
Specified lower bound negative (32), zero
(33), > 0 (34)
Lower bound is constant (35),
integer variable (36)
array element name (37),
something else (38)
Lower bound is one (39) ub>¼1 (40), ub<1 (41)
Multiple lines yes (42), no (43)
54 The Art of Software Testing
(3): DIMENSION
(5): DIMENSION (10)
(6): DIMENSION A234567(2)
(9): DIMENSION A.1(2)
(11): DIMENSION 1A(10)
(13): DIMENSION B
(14): DIMENSION B(4,4,4,4,4,4,4,4)
(17): DIMENSION B(4,A(2))
(18): DIMENSION B(4,,7)
(21): DIMENSION C(I.,10)
(23): DIMENSION C(10,1J)
(25): DIMENSION D(- 65535:1)
(26): DIMENSION D(65536)
(31): DIMENSION D(4:3)
(37): DIMENSION D(A(2):4)
(38): D(.:4)
(43): DIMENSION D(0)
Hence, the equivalence classes have been covered by 17 test cases. You
may want to consider how these test cases would compare to a set of test
cases derived in an ad hoc manner.
Although equivalence partitioning is vastly superior to a random selection
of test cases, it still has deficiencies. It overlooks certain types of highyield
test cases, for example. The next two methodologies, boundary value
analysis and cause-effect graphing, cover many of these deficiencies.
Boundary Value Analysis
Experience shows that test cases that explore boundary conditions have a
higher payoff than test cases that do not. Boundary conditions are those
situations directly on, above, and beneath the edges of input equivalence
classes and output equivalence classes. Boundary value analysis differs
from equivalence partitioning in two respects:
1. Rather than selecting any element in an equivalence class as being
representative, boundary value analysis requires that one or more elements
be selected such that each edge of the equivalence class is the
subject of a test.
2. Rather than just focusing attention on the input conditions (input
space), test cases are also derived by considering the result space
(output equivalence classes).
Test-Case Design 55
It is difficult to present a ‘‘cookbook’’ for boundary value analysis, since
it requires a degree of creativity and a certain amount of specialization
toward the problem at hand. (Hence, like many other aspects of testing, it
is more a state of mind than anything else.) However, a few general guidelines
are in order:
1. If an input condition specifies a range of values, write test cases for
the ends of the range, and invalid-input test cases for situations just
beyond the ends. For instance, if the valid domain of an input value
is –1.0 to 1.0, write test cases for the situations –1.0, 1.0, –1.001,
and 1.001.
2. If an input condition specifies a number of values, write test cases for
the minimum and maximum number of values and one beneath and
beyond these values. For instance, if an input file can contain 1–255
records, write test cases for 0, 1, 255, and 256 records.
3. Use guideline 1 for each output condition. For instance, if a payroll
program computes the monthly FICA deduction, and if the minimum
is $0.00 and the maximum is $1,165.25, write test cases that cause
$0.00 and $1,165.25 to be deducted. Also, see whether it is possible
to invent test cases that might cause a negative deduction or a deduction
of more than $1,165.25.
Note that it is important to examine the boundaries of the result
space because it is not always the case that the boundaries of the input
domains represent the same set of circumstances as the boundaries
of the output ranges (e.g., consider a sine subroutine). Also, it is
not always possible to generate a result outside of the output range;
nonetheless, it is worth considering the possibility.
4. Use guideline 2 for each output condition. If an information retrieval
system displays the most relevant abstracts based on an input request,
but never more than four abstracts, write test cases such that
the program displays zero, one, and four abstracts, and write a test
case that might cause the program to erroneously display five
abstracts.
5. If the input or output of a program is an ordered set (a sequential file,
for example, or a linear list or a table), focus attention on the first and
last elements of the set.
6. In addition, use your ingenuity to search for other boundary
conditions.
56 The Art of Software Testing
The triangle analysis program of Chapter 1 can illustrate the need for
boundary value analysis. For the input values to represent a triangle, they
must be integers greater than 0 where the sum of any two is greater than
the third. If you were defining equivalent partitions, you might define one
where this condition is met and another where the sum of two of the integers
is not greater than the third. Hence, two possible test cases might be
3–4–5 and 1–2–4. However, we have missed a likely error. That is, if an
expression in the program were coded as AþB>¼C instead of AþB>C, the
program would erroneously tell us that 1–2–3 represents a valid scalene
triangle. Hence, the important difference between boundary value analysis
and equivalence partitioning is that boundary value analysis explores situations
on and around the edges of the equivalence partitions.
As an example of a boundary value analysis, consider the following
program specification:
MTEST is a program that grades multiple-choice examinations. The
input is a data file named OCR, with multiple records that are 80
characters long. Per the file specification, the first record is a title
used as a title on each output report. The next set of records
describes the correct answers on the exam. These records contain a
‘‘2’’ as the last character in column 80. In the first record of this set,
the number of questions is listed in columns 1–3 (a value of 1–999).
Columns 10–59 contain the correct answers for questions 1–50
(any character is valid as an answer). Subsequent records contain, in
columns 10–59, the correct answers for questions 51–100, 101–150,
and so on.
The third set of records describes the answers of each student;
each of these records contains a ‘‘3’’ in column 80. For each student,
the first record contains the student’s name or number in columns 1–
9 (any characters); columns 10–59 contain the student’s answers for
questions 1–50. If the test has more than 50 questions, subsequent
records for the student contain answers 51–100, 101–150, and so on,
in columns 10–59. The maximum number of students is 200. The
input data are illustrated in Figure 4.4. The four output records are:
1. A report, sorted by student identifier, showing each student’s
grade (percentage of answers correct) and rank.
2. A similar report, but sorted by grade.
Test-Case Design 57
3. A report indicating the mean, median, and standard deviation
of the grades.
4. A report, ordered by question number, showing the percentage
of students answering each question correctly.
We can begin by methodically reading the specification, looking for input
conditions. The first boundary input condition is an empty input file.
The second input condition is the title record; boundary conditions are a
missing title record and the shortest and longest possible titles. The next
input conditions are the presence of correct-answer records and the
number-of-questions field on the first answer record. The equivalence class
1 80
Title
1 3 4 9 10 59 60 79 80
Correct answers 1–50 2
No. of
questions
1 9 10 59 60 79 80
Correct answers 51–100 2
1 9 10 59 60 79 80
Student identifier Correct answers 1–50 3
1 9 10 59 60 79 80
Correct answers 51–100 3
1 9 10 59 60 79 80
Student identifier Correct answers 1–50 3
FIGURE 4.4 Input to the MTEST Program.
58 The Art of Software Testing
for the number of questions is not 1–999, because something special happens
at each multiple of 50 (i.e., multiple records are needed). A reasonable
partitioning of this into equivalence classes is 1–50 and 51–999.
Hence, we need test cases where the number-of-questions field is set to 0,
1, 50, 51, and 999. This covers most of the boundary conditions for the
number of correct-answer records; however, three more interesting situations
are the absence of answer records and having one too many and one
too few answer records (e.g., the number of questions is 60, but there are
three answer records in one case and one answer record in the other case).
The unique test cases identified so far are:
1. Empty input file
2. Missing title record
3. 1-character title
4. 80-character title
5. 1-question exam
6. 50-question exam
7. 51-question exam
8. 999-question exam
9. 0-question exam
10. Number-of-questions field with nonnumeric value
11. No correct-answer records after title record
12. One too many correct-answer records
13. One too few correct-answer records
The next input conditions are related to the students’ answers. The
boundary value test cases here appear to be:
14. 0 students
15. 1 student
16. 200 students
17. 201 students
18. A student has one answer record, but there are two correct-answer
records.
19. The above student is the first student in the file.
20. The above student is the last student in the file.
21. A student has two answer records, but there is just one correctanswer
record.
Test-Case Design 59
22. The above student is the first student in the file.
23. The above student is the last student in the file.
You also can derive a useful set of test cases by examining the output
boundaries, although some of the output boundaries (e.g., empty report 1)
are covered by the existing test cases. The boundary conditions of reports
1 and 2 are:
0 students (same as test 14)
1 student (same as test 15)
200 students (same as test 16)
24. All students receive the same grade.
25. All students receive a different grade.
26. Some, but not all, students receive the same grade (to see if ranks are
computed correctly).
27. A student receives a grade of 0.
28. A student receives a grade of 10.
29. A student has the lowest possible identifier value (to check the sort).
30. A student has the highest possible identifier value.
31. The number of students is such that the report is just large enough to
fit on one page (to see if an extraneous page is printed).
32. The number of students is such that all students but one fit on one
page.
The boundary conditions from report 3 (mean, median, and standard
deviation) are:
33. The mean is at its maximum (all students have a perfect score).
34. The mean is 0 (all students receive a grade of 0).
35. The standard deviation is at its maximum (one student receives a
0 and the other receives a 100).
36. The standard deviation is 0 (all students receive the same grade).
Tests 33 and 34 also cover the boundaries of the median. Another useful
test case is the situation where there are 0 students (looking for a division
by 0 in computing the mean), but this is identical to test case 14.
60 The Art of Software Testing
An examination of report 4 yields the following boundary value tests:
37. All students answer question 1 correctly.
38. All students answer question 1 incorrectly.
39. All students answer the last question correctly.
40. All students answer the last question incorrectly.
41. The number of questions is such that the report is just large enough
to fit on one page.
42. The number of questions is such that all questions but one fit on one
page.
An experienced programmer would probably agree at this point that
many of these 42 test cases represent common errors that might have been
made in developing this program, yet most of these errors probably would
go undetected if a random or ad hoc test-case generation method were
used. Boundary value analysis, if practiced correctly, is one of the most
useful test-case design methods. However, it often is used ineffectively because
the technique, on the surface, sounds simple. You should understand
that boundary conditions may be very subtle and, hence, identification of
them requires a lot of thought.
Cause-Effect Graphing
One weakness of boundary value analysis and equivalence partitioning is
that they do not explore combinations of input circumstances. For instance,
perhaps the MTEST program of the previous section fails when the product
of the number of questions and the number of students exceeds some
limit (the program runs out of memory, for example). Boundary value testing
would not necessarily detect such an error.
The testing of input combinations is not a simple task because even if
you equivalence-partition the input conditions, the number of combinations
usually is astronomical. If you have no systematic way of selecting a
subset of input conditions, you’ll probably select an arbitrary subset of
conditions, which could lead to an ineffective test.
Cause-effect graphing aids in selecting, in a systematic way, a high-yield
set of test cases. It has a beneficial side effect in pointing out incompleteness
and ambiguities in the specification.
Test-Case Design 61
A cause-effect graph is a formal language into which a natural-language
specification is translated. The graph actually is a digital logic circuit (a
combinatorial logic network), but instead of standard electronics notation,
a somewhat simpler notation is used. No knowledge of electronics is necessary
other than an understanding of Boolean logic (i.e., of the logic operators
and, or, and not).
The following process is used to derive test cases:
1. The specification is divided into workable pieces. This is necessary because
cause-effect graphing becomes unwieldy when used on large specifications.
For instance, when testing an e-commerce system, a workable
piece might be the specification for choosing and verifying a single item
placed in a shopping cart. When testing a Web page design, you might
test a single menu tree or even a less complex navigation sequence.
2. The causes and effects in the specification are identified. A cause is a
distinct input condition or an equivalence class of input conditions.
An effect is an output condition or a system transformation (a lingering
effect that an input has on the state of the program or system).
For instance, if a transaction causes a file or database record to be
updated, the alteration is a system transformation; a confirmation
message would be an output condition.
You identify causes and effects by reading the specification word by
word and underlining words or phrases that describe causes and effects.
Once identified, each cause and effect is assigned a unique number.
3. The semantic content of the specification is analyzed and transformed
into a Boolean graph linking the causes and effects. This is
the cause-effect graph.
4. The graph is annotated with constraints describing combinations of
causes and/or effects that are impossible because of syntactic or environmental
constraints.
5. By methodically tracing state conditions in the graph, you convert
the graph into a limited-entry decision table. Each column in the
table represents a test case.
6. The columns in the decision table are converted into test cases.
The basic notation for the graph is shown in Figure 4.5. Think of each
node as having the value 0 or 1; 0 represents the ‘‘absent’’ state and 1 represents
the ‘‘present’’ state.
62 The Art of Software Testing
  The identity function states that if a is 1, b is 1; else b is 0.
  The not function states that if a is 1, b is 0, else b is 1.
  The or function states that if a or b or c is 1, d is 1; else d is 0.
  The and function states that if both a and b are 1, c is 1; else c is 0.
The latter two functions (or and and) are allowed to have any number of
inputs.
To illustrate a small graph, consider the following specification:
The character in column 1 must be an ‘‘A’’ or a ‘‘B.’’ The character in
column 2 must be a digit. In this situation, the file update is made. If
the first character is incorrect, message X12 is issued. If the second character
is not a digit, message X13 is issued.
The causes are:
1—character in column 1 is ‘‘A’’
2—character in column 1 is ‘‘B’’
3—character in column 2 is a digit
FIGURE 4.5 Basic Cause-Effect Graph Symbols.
Test-Case Design 63
and the effects are:
70—update made
71—message X12 is issued
72—message X13 is issued
The cause-effect graph is shown in Figure 4.6. Notice the intermediate
node 11 that was created. You should confirm that the graph represents the
specification by setting all possible states of the causes and verifying that
the effects are set to the correct values. For readers familiar with logic diagrams,
Figure 4.7 is the equivalent logic circuit.
Although the graph in Figure 4.6 represents the specification, it does
contain an impossible combination of causes—it is impossible for both
causes 1 and 2 to be set to 1 simultaneously. In most programs, certain
combinations of causes are impossible because of syntactic or environmental
considerations (a character cannot be an ‘‘A’’ and a ‘‘B’’ simultaneously).
FIGURE 4.6 Sample Cause-Effect Graph.
FIGURE 4.7 Logic Diagram Equivalent to Figure 4.6.
64 The Art of Software Testing
To account for these, the notation in Figure 4.8 is used. The E constraint
states that it must always be true that, at most, one of a and b can be 1 (a
and b cannot be 1 simultaneously). The I constraint states that at least one
of a, b, and c must always be 1 (a, b, and c cannot be 0 simultaneously).
The O constraint states that one, and only one, of a and b must be 1. The R
constraint states that for a to be 1, b must be 1 (i.e., it is impossible for a to
be 1 and b to be 0).
There frequently is a need for a constraint among effects. The M constraint
in Figure 4.9 states that if effect a is 1, effect b is forced to 0.
FIGURE 4.8 Constraint Symbols.
FIGURE 4.9 Symbol for ‘‘Masks’’ Constraint.
Test-Case Design 65
Returning to the preceding simple example, we see that it is physically
impossible for causes 1 and 2 to be present simultaneously, but it is possible
for neither to be present. Hence, they are linked with the E constraint,
as shown in Figure 4.10.
To illustrate how cause-effect graphing is used to derive test cases, we
use the following specification for a debugging command in an interactive
system.
The DISPLAY command is used to view from a terminal window
the contents of memory locations. The command syntax is shown in
Figure 4.11. Brackets represent alternative optional operands. Capital
letters represent operand keywords. Lowercase letters represent
operand values (actual values are to be substituted). Underlined operands
represent the default values (i.e., the value used when the operand
is omitted).
FIGURE 4.10 Sample Cause-Effect Graph with ‘‘Exclusive’’ Constraint.
DISPLAY hexloc1
0
-hexloc2
-END
-bytecount
-1
FIGURE 4.11 Syntax of the DISPLAY Command.
66 The Art of Software Testing
The first operand (hexloc1) specifies the address of the first byte
whose contents are to be displayed. The address may be one to six
hexadecimal digits (0–9, A–F) in length. If it is not specified, the address
0 is assumed. The address must be within the actual memory
range of the machine.
The second operand specifies the amount of memory to be
displayed. If hexloc2 is specified, it defines the address of the
last byte in the range of locations to be displayed. It may be
one to six hexadecimal digits in length. The address must be
greater than or equal to the starting address (hexloc1). Also,
hexloc2 must be within the actual memory range of the machine.
If END is specified, memory is displayed up through the
last actual byte in the machine. If bytecount is specified, it defines
the number of bytes of memory to be displayed (starting
with the location specified in hexloc1). The operand bytecount
is a hexadecimal integer (one to six digits). The sum of
bytecount and hexloc1 must not exceed the actual memory
size plus 1, and bytecount must have a value of at least 1.
When memory contents are displayed, the output format on the
screen is one or more lines of the format
xxxxxx ¼ word1 word2 word3 word4
where xxxxxx is the hexadecimal address of word1. An integral
number of words (four-byte sequences, where the address of the
first byte in the word is a multiple of 4) is always displayed, regardless
of the value of hexloc1 or the amount of memory to be
displayed. All output lines will always contain four words (16
bytes). The first byte of the displayed range will fall within the
first word.
The error messages that can be produced are
M1 is invalid command syntax.
M2 memory requested is beyond actual memory limit.
M3 memory requested is a zero or negative range.
Test-Case Design 67
As examples:
DISPLAY
displays the first four words in memory (default starting address of 0,
default byte count of 1);
DISPLAY 77F
displays the word containing the byte at address 77F, and the three
subsequent words;
DISPLAY 77F-407A
displays the words containing the bytes in the address range
775–407A;
DISPLAY 77F.6
displays the words containing the six bytes starting at location 77F; and
DISPLAY 50FF-END
displays the words containing the bytes in the address range 50FF to
the end of memory.
The first step is a careful analysis of the specification to identify the
causes and effects. The causes are as follows:
1. First operand is present.
2. The hexloc1 operand contains only hexadecimal digits.
3. The hexloc1 operand contains one to six characters.
4. The hexloc1 operand is within the actual memory range of the machine.
5. Second operand is END.
6. Second operand is hexloc.
7. Second operand is bytecount.
8. Second operand is omitted.
9. The hexloc2 operand contains only hexadecimal digits.
10. The hexloc2 operand contains one to six characters.
11. The hexloc2 operand is within the actual memory range of the machine.
12. The hexloc2 operand is greater than or equal to the hexloc1 operand.
13. The bytecount operand contains only hexadecimal digits.
14. The bytecount operand contains one to six characters.
68 The Art of Software Testing
15. bytecount þ hexloc1 <¼ memory size þ 1.
16. bytecount >¼ 1.
17. Specified range is large enough to require multiple output lines.
18. Start of range does not fall on a word boundary.
Each cause has been given an arbitrary unique number. Notice that four
causes (5 through 8) are necessary for the second operand because the second
operand could be (1) END, (2) hexloc2, (3) byte-count, (4) absent, and
(5) none of the above. The effects are as follows:
91. Message M1 is displayed.
92. Message M2 is displayed.
93. Message M3 is displayed.
94. Memory is displayed on one line.
95. Memory is displayed on multiple lines.
96. First byte of displayed range falls on a word boundary.
97. First byte of displayed range does not fall on a word boundary.
The next step is the development of the graph. The cause nodes are
listed vertically on the left side of the sheet of paper; the effect nodes are
listed vertically on the right side. The semantic content of the specification
is carefully analyzed to interconnect the causes and effects (i.e., to show
under what conditions an effect is present).
Figure 4.12 shows an initial version of the graph. Intermediate node 32
represents a syntactically valid first operand; node 35 represents a syntactically
valid second operand. Node 36 represents a syntactically valid command.
If node 36 is 1, effect 91 (the error message) does not appear. If
node 36 is 0, effect 91 is present.
The full graph is shown in Figure 4.13. You should explore it carefully
to convince yourself that it accurately reflects the specification.
If Figure 4.13 were used to derive the test cases, many impossible-tocreate
test cases would be derived. The reason is that certain combinations of
causes are impossible because of syntactic constraints. For instance, causes 2
and 3 cannot be present unless cause 1 is present. Cause 4 cannot be present
unless both causes 2 and 3 are present. Figure 4.14 contains the complete
graph with the constraint conditions. Notice that, at most, one of the causes
5, 6, 7, and 8 can be present. All other cause constraints are the requires condition.
Notice that cause 17 (multiple output lines) requires the not of cause 8
Test-Case Design 69
FIGURE 4.12 Beginning of the Graph for the DISPLAY Command.
70 The Art of Software Testing
FIGURE 4.13 Full Cause-Effect Graph without Constraints.
Test-Case Design 71
(second operand is omitted); cause 17 can be present only when cause 8 is
absent. Again, you should explore the constraint conditions carefully.
The next step is the generation of a limited-entry decision table. For
readers familiar with decision tables, the causes are the conditions and the
effects are the actions. The procedure used is as follows:
1. Select an effect to be the present (1) state.
2. Tracing back through the graph, find all combinations of causes (subject
to the constraints) that will set this effect to 1.
3. Create a column in the decision table for each combination of causes.
5
4
3
2
1
10
9
8
7
6
15
14
13
12
11
18
17
16
31
32
35
36
91
40
39
38
34
33
97
96
95
94
93
92
V
V
37
V
V
V
V
V
V
V
V
V
V
V
V
R
R R
R
R
E R R
R
R
R
R
R
R
R
R
R
R
R
FIGURE 4.14 Complete Cause-Effect Graph of the DISPLAY Command.
72 The Art of Software Testing
4. For each combination, determine the states of all other effects and
place these in each column.
In performing step 2, the considerations are as follows:
1. When tracing back through an or node whose output should be 1,
never set more than one input to the or to 1 simultaneously. This is
called path sensitizing. Its objective is to prevent the failure to detect
certain errors because of one cause masking another cause.
2. When tracing back through an and node whose output should be 0,
all combinations of inputs leading to 0 output must, of course, be
enumerated. However, if you are exploring the situation where one
input is 0 and one or more of the others are 1, it is not necessary to
enumerate all conditions under which the other inputs can be 1.
3. When tracing back through an and node whose output should be 0,
only one condition where all inputs are zero need be enumerated. (If
the and is in the middle of the graph such that its inputs come from
other intermediate nodes, there may be an excessively large number
of situations under which all of its inputs are 0.)
These complicated considerations are summarized in Figure 4.15, and
Figure 4.16 is used as an example.
FIGURE 4.15 Considerations Used When Tracing the Graph.
Test-Case Design 73
Assume that we want to locate all input conditions that cause the output
state to be 0. Consideration 3 states that we should list only one circumstance
where nodes 5 and 6 are 0. Consideration 2 states that for the state
where node 5 is 1 and node 6 is 0, we should list only one circumstance
where node 5 is 1, rather than enumerating all possible ways that node 5
can be 1. Likewise, for the state where node 5 is 0 and node 6 is 1, we
should list only one circumstance where node 6 is 1 (although there is
only one in this example). Consideration 1 states that where node 5 should
be set to 1, we should not set nodes 1 and 2 to 1 simultaneously. Hence, we
would arrive at five states of nodes 1 through 4; for example, the values:
0 0 0 0 (5¼ 0, 6¼0)
1 0 0 0 (5¼ 1, 6¼0)
1 0 0 1 (5¼ 1, 6¼0)
1 0 1 0 (5¼ 1, 6¼0)
0 0 1 1 (5¼ 0, 6¼1)
rather than the 13 possible states of nodes 1 through 4 that lead to a 0
output state.
These considerations may appear to be capricious, but they have an important
purpose: to lessen the combined effects of the graph. They eliminate
situations that tend to be low-yield test cases. If low-yield test cases
are not eliminated, a large cause-effect graph will produce an astronomical
number of test cases. If the number of test cases is too large to be practical,
FIGURE 4.16 Sample Graph to Illustrate the Tracing Considerations.
74 The Art of Software Testing
you will select some subset, but there is no guarantee that the low-yield
test cases will be the ones eliminated. Hence, it is better to eliminate them
during the analysis of the graph.
We will now convert the cause-effect graph in Figure 4.14 into the decision
table. Effect 91 will be selected first. Effect 91 is present if node 36 is
0. Node 36 is 0 if nodes 32 and 35 are 0,0; 0,1; or 1,0; and considerations 2
and 3 apply here. By tracing back to the causes, and considering the constraints
among causes, you can find the combinations of causes that lead to
effect 91 being present, although doing so is a laborious process.
The resultant decision table, under the condition that effect 91 is present,
is shown in Figure 4.17 (columns 1 through 11). Columns (tests) 1 through
3 represent the conditions where node 32 is 0 and node 35 is 1. Columns 4
through 10 represent the conditions where node 32 is 1 and node 35 is 0.
Using consideration 3, only one situation (column 11) out of a possible 21
situations where nodes 32 and 35 are 0 is identified. Blanks in the table represent
‘‘don’t care’’ situations (i.e., the state of the cause is irrelevant) or indicate
that the state of a cause is obvious because of the states of other
dependent causes (e.g., in column 1, we know that causes 5, 7, and 8 must
be 0 because they exist in an ‘‘at most one’’ situation with cause 6).
Columns 12 through 15 represent the situations where effect 92 is present.
Columns 16 and 17 represent the situations where effect 93 is present.
Figure 4.18 represents the remainder of the decision table.
The last step is to convert the decision table into 38 test cases. A set of
38 test cases is listed here. The number or numbers beside each test case
designate the effects that are expected to be present. Assume that the last
location in memory on the machine being used is 7FFF.
1 DISPLAY 234AF74–123 (91)
2 DISPLAY 2ZX4–3000 (91)
3 DISPLAY HHHHHHHH-2000 (91)
4 DISPLAY 200 200 (91)
5 DISPLAY 0–22222222 (91)
6 DISPLAY 1–2X (91)
7 DISPLAY 2-ABCDEFGHI (91)
8 DISPLAY 3.1111111 (91)
9 DISPLAY 44.$42 (91)
10 DISPLAY 100.$$$$$$$ (91)
Test-Case Design 75
1
1
2
1
3
1
4
1
5
1
6
1
7
1
8
1
9
1
10
1
11
1
12
1
13
1
14
1
15
1
16
1
17
1 1
2 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 1 0 1
0
1 1 1 1 1 1 0 1 1 1
1
1 1 1
1 1 0 0 1 1
3
91 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0
92 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0
93 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
94 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
95 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
97 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
7 0 1 1 1 1 1
13 1 0 0 1 1
0 1 0 1
0
1
0
14
15
16
17
18
8 0
4
5
6 1 1 1 0 1 1 1 1 1 1 1
9 1 1 1 1 0 0 0 1 1 1
10 1 1 1 0 1 0 1 1 1 1
11 0 0 1
12 0
FIGURE 4.17 First Half of the Resultant Decision Table.
76 The Art of Software Testing
11 DISPLAY 10000000-M (91)
12 DISPLAY FF-8000 (92)
13 DISPLAY FFF.7001 (92)
14 DISPLAY 8000-END (92)
15 DISPLAY 8000–8001 (92)
16 DISPLAY AA-A9 (93)
17 DISPLAY 7000.0 (93)
18 DISPLAY 7FF9-END (94, 97)
18
1
19 20
1
21
1
22
0
23
0
24 25
0
26
0 1
27
1
28 29
1
30
1
31
1
32
1 1
33
0
34 35
0
36
0
37
1 1
38
1 1 1
2 1 1 1 1 1 1 1 1 1 1 1 1 1 1
3 1 1 1 1 1 1 1 1 1 1 1 1 1 1
4 1 1 1 1 1 1 1 1 1 1 1 1 1 1
5 1 1 1 1 1 1
6 1 1 1 1 1 1
7 1 1 1 1 1 1
8 1 1 1
9 1 1 1 1 1 1
10 1 1 1 1 1 1
11 1 1 1 1 1 1
12 1 1 1 1 1 1
13 1 1 1 1 1 1
14 1 1 1 1 1 1
15 1 1 1 1 1 1
16 1 1 1 1 1 1
17 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
18 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0
91 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
93 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
92 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
94 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0
95 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
96 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1
97 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0
FIGURE 4.18 Second Half of the Resultant Decision Table.
Test-Case Design 77
19 DISPLAY 1 (94, 97)
20 DISPLAY 21–29 (94, 97)
21 DISPLAY 4021.A (94, 97)
22 DISPLAY -END (94, 96)
23 DISPLAY (94, 96)
24 DISPLAY -F (94, 96)
25 DISPLAY .E (94, 96)
26 DISPLAY 7FF8-END (94, 96)
27 DISPLAY 6000 (94, 96)
28 DISPLAY A0-A4 (94, 96)
29 DISPLAY 20.8 (94, 96)
30 DISPLAY 7001-END (95, 97)
31 DISPLAY 5–15 (95, 97)
32w DISPLAY 4FF.100 (95, 97)
33 DISPLAY -END (95, 96)
34 DISPLAY -20 (95, 96)
35 DISPLAY .11 (95, 96)
36 DISPLAY 7000-END (95, 96)
37 DISPLAY 4–14 (95, 96)
38 DISPLAY 500.11 (95, 96)
Note that where two or more different test cases invoked, for the most
part, the same set of causes, different values for the causes were selected to
slightly improve the yield of the test cases. Also note that, because of the
actual storage size, test case 22 is impossible (it will yield effect 95 instead
of 94, as noted in test case 33). Hence, 37 test cases have been identified.
Remarks Cause-effect graphing is a systematic method of generating test
cases representing combinations of conditions. The alternative would be to
make an ad hoc selection of combinations; but in doing so, it is likely that
you would overlook many of the ‘‘interesting’’ test cases identified by the
cause-effect graph.
Since cause-effect graphing requires the translation of a specification into
a Boolean logic network, it gives you a different perspective on, and additional
insight into, the specification. In fact, the development of a cause-
78 The Art of Software Testing
effect graph is a good way to uncover ambiguities and incompleteness
in specifications. For instance, the astute reader may have noticed that
this process has uncovered a problem in the specification of the DISPLAY
command. The specification states that all output lines contain four words.
This cannot be true in all cases; it cannot occur for test cases 18 and 26
because the starting address is less than 16 bytes away from the end of
memory.
Although cause-effect graphing does produce a set of useful test cases, it
normally does not produce all of the useful test cases that might be identified.
For instance, in the example we said nothing about verifying that the
displayed memory values are identical to the values in memory and determining
whether the program can display every possible value in a memory
location. Also, the cause-effect graph does not adequately explore boundary
conditions. Of course, you could attempt to cover boundary conditions
during the process. For instance, instead of identifying the single cause
hexloc2>¼hexloc1
you could identify two causes:
hexloc2 ¼ hexloc1
hexloc2 > hexloc1
The problem in doing this, however, is that it complicates the graph
tremendously and leads to an excessively large number of test cases. For
this reason it is best to consider a separate boundary value analysis. For
instance, the following boundary conditions can be identified for the
DISPLAY specification:
1. hexloc1 has one digit
2. hexloc1 has six digits
3. hexloc1 has seven digits
4. hexloc1 ¼ 0
5. hexloc1 ¼ 7FFF
6. hexloc1 ¼ 8000
7. hexloc2 has one digit
8. hexloc2 has six digits
9. hexloc2 has seven digits
10. hexloc2 ¼ 0
Test-Case Design 79
11. hexloc2 ¼ 7FFF
12. hexloc2 ¼ 8000
13. hexloc2 ¼ hexloc
14. hexloc2 ¼ hexloc1 þ 1
15. hexloc2 ¼ hexloc1   1
16. bytecount has one digit
17. bytecount has six digits
18. bytecount has seven digits
19. bytecount ¼ 1
20. hexloc1 þ bytecount ¼ 8000
21. hexloc1 þ bytecount ¼ 8001
22. display 16 bytes (one line)
23. display 17 bytes (two lines)
Note that this does not imply that you would write 60 (37 þ 23) test
cases. Since the cause-effect graph gives us leeway in selecting specific values
for operands, the boundary conditions could be blended into the test
cases derived from the cause-effect graph. In this example, by rewriting
some of the original 37 test cases, all 23 boundary conditions could be
covered without any additional test cases. Thus, we arrive at a small but
potent set of test cases that satisfy both objectives.
Note that cause-effect graphing is consistent with several of the testing
principles in Chapter 2. Identifying the expected output of each test case is
an inherent part of the technique (each column in the decision table indicates
the expected effects). Also note that it encourages us to look for unwanted
side effects. For instance, column (test) 1 specifies that we should
expect effect 91 to be present and that effects 92 through 97 should be
absent.
The most difficult aspect of the technique is the conversion of the graph
into the decision table. This process is algorithmic, implying that you
could automate it by writing a program; several commercial programs exist
to help with the conversion.
Error Guessing
It has often been noted that some people seem to be naturally adept at program
testing. Without using any particular methodology such as boundary
80 The Art of Software Testing
value analysis of cause-effect graphing, these people seem to have a knack
for sniffing out errors.
One explanation for this is that these people are practicing—subconsciously
more often than not—a test-case design technique that could
be termed error guessing. Given a particular program, they surmise—both
by intuition and experience—certain probable types of errors and then
write test cases to expose those errors.
It is difficult to give a procedure for the error-guessing technique since
it is largely an intuitive and ad hoc process. The basic idea is to enumerate
a list of possible errors or error-prone situations and then write test
cases based on the list. For instance, the presence of the value 0 in a program’s
input is an error-prone situation. Therefore, you might write test
cases for which particular input values have a 0 value and for which particular
output values are forced to 0. Also, where a variable number of
inputs or outputs can be present (e.g., the number of entries in a list to
be searched), the cases of ‘‘none’’ and ‘‘one’’ (e.g., empty list, list containing
just one entry) are error-prone situations. Another idea is to identify
test cases associated with assumptions that the programmer might have
made when reading the specification (i.e., factors that were omitted from
the specification, either by accident or because the writer felt them to be
obvious).
Since a procedure for error guessing cannot be given, the next-best alternative
is to discuss the spirit of the practice, and the best way to do this
is by presenting examples. If you are testing a sorting subroutine, the following
are situations to explore:
  The input list is empty.
  The input list contains one entry.
  All entries in the input list have the same value.
  The input list is already sorted.
In other words, you enumerate those special cases that may have been
overlooked when the program was designed. If you are testing a binary
search subroutine, you might try the situations where: (1) there is only
one entry in the table being searched; (2) the table size is a power of 2
(e.g., 16); and (3) the table size is one less than and one greater than a
power of 2 (e.g., 15 or 17).
Test-Case Design 81
Consider the MTEST program in the section on boundary value
analysis. The following additional tests come to mind when using the
error-guessing technique:
  Does the program accept ‘‘blank’’ as an answer?
  A type-2 (answer) record appears in the set of type-3 (student)
records.
  A record without a 2 or 3 in the last column appears as other than the
initial (title) record.
  Two students have the same name or number.
  Since a median is computed differently depending on whether there is
an odd or an even number of items, test the program for an even
number of students and an odd number of students.
  The number-of-questions field has a negative value.
Error-guessing tests that come to mind for the DISPLAY command of the
previous section are as follows:
DISPLAY 100- (partial second operand)
DISPLAY 100. (partial second operand)
DISPLAY 100–10A 42 (extra operand)
DISPLAY 000–0000FF (leading zeros)
The Strategy
The test-case design methodologies discussed in this chapter can be combined
into an overall strategy. The reason for combining them should be
obvious by now: Each contributes a particular set of useful test cases, but
none of them by itself contributes a thorough set of test cases. A reasonable
strategy is as follows:
1. If the specification contains combinations of input conditions, start
with cause-effect graphing.
2. In any event, use boundary value analysis. Remember that this is an
analysis of input and output boundaries. The boundary value analysis
yields a set of supplemental test conditions, but as noted in the section
on cause-effect graphing, many or all of these can be incorporated
into the cause-effect tests.
82 The Art of Software Testing
3. Identify the valid and invalid equivalence classes for the input and
output, and supplement the test cases identified above, if necessary.
4. Use the error-guessing technique to add additional test cases.
5. Examine the program’s logic with regard to the set of test cases. Use
the decision coverage, condition coverage, decision/condition coverage,
or multiple-condition coverage criterion (the last being the most
complete). If the coverage criterion has not been met by the test cases
identified in the prior four steps, and if meeting the criterion is not
impossible (i.e., certain combinations of conditions may be impossible
to create because of the nature of the program), add sufficient
test cases to cause the criterion to be satisfied.
Again, the use of this strategy will not guarantee that all errors will be
found, but it has been found to represent a reasonable compromise. Also, it
represents a considerable amount of hard work, but as we said at the beginning
of this chapter, no one has ever claimed that program testing is easy.
Summary
Once you have agreed that aggressive software testing is a worthy addition
to your development efforts, the next step is to design test cases that will
exercise your application sufficiently to produce satisfactory test results. In
most cases, consider a combination of black-box and white-box methodologies
to ensure that you have designed rigorous program testing.
Test case design techniques discussed in this chapter include:
  Logic coverage. Tests that exercise all decision point outcomes at least
once, and ensure that all statements or entry points are executed at
least once.
  Equivalence partitioning. Defines condition or error classes to help reduce
the number of finite tests. Assumes that a test of a representative
value within a class also tests all values or conditions within that class.
  Boundary value analysis. Tests each edge condition of an equivalence
class; also considers output equivalence classes as well as input
classes.
  Cause-effect graphing. Produces Boolean graphical representations of
potential test case results to aid in selecting efficient and complete
test cases.
Test-Case Design 83
  Error guessing. Produces test cases based on intuitive and expert
knowledge of test team members to define potential software errors
to facilitate efficient test case design.
Extensive, in-depth testing is not easy; nor will the most extensive test
case design assure that every error will be uncovered. That said, developers
willing to go beyond cursory testing, who will dedicate sufficient time to
test case design, analyze carefully the test results, and act decisively on the
findings, will be rewarded with functional, reliable software that is reasonably
error free.
84 The Art of Software Testing
5 Module (Unit) Testing
Up to this point we have largely ignored the mechanics of testing and
the size of the program being tested. However, because large programs
(say, of 500 statements or 50-plus classes) require special testing treatment,
in this chapter we consider an initial step in structuring the testing of a large
program: module testing. Chapters 6 and 7 enumerate the remaining steps.
Module testing (or unit testing) is a process of testing the individual subprograms,
subroutines, classes, or procedures in a program. More specifically,
rather than initially testing the program as a whole, testing is first
focused on the smaller building blocks of the program. The motivations for
doing this are threefold. First, module testing is a way of managing the
combined elements of testing, since attention is focused initially on smaller
units of the program. Second, module testing eases the task of debugging
(the process of pinpointing and correcting a discovered error), since, when
an error is found, it is known to exist in a particular module. Finally, module
testing introduces parallelism into the program testing process by presenting
us with the opportunity to test multiple modules simultaneously.
The purpose of module testing is to compare the function of a module to
some functional or interface specification defining the module. To reemphasize
the goal of all testing processes, the objective here is not to show that the
module meets its specification, but that the module contradicts the specification.
In this chapter, we address module testing from three points of view:
1. The manner in which test cases are designed.
2. The order in which modules should be tested and integrated.
3. Advice about performing the tests.
85
Test-Case Design
You need two types of information when designing test cases for a module
test: a specification for the module and the module’s source code. The
specification typically defines the module’s input and output parameters
and its function.
Module testing is largely white-box oriented. One reason is that as you
test larger entities, such as entire programs (which will be the case for subsequent
testing processes), white-box testing becomes less feasible. A second
reason is that the subsequent testing processes are oriented toward
finding different types of errors (e.g., errors not necessarily associated
with the program’s logic, such as the program failing to meet its users’
requirements). Hence, the test-case design procedure for a module test is
the following:
Analyze the module’s logic using one or more of the white-box methods,
and then supplement these test cases by applying black-box
methods to the module’s specification.
The test-case design methods we will use were defined in Chapter 4; we
will illustrate their use in a module test here through an example.
Assume that we wish to test a module named BONUS, and its function is
to add $2,000 to the salary of all employees in the department or departments
having the largest sales revenue. However, if an eligible employee’s
current salary is $150,000 or more, or if the employee is a manager, the
salary is to be increased by only $1,000.
The inputs to the module are shown in the tables in Figure 5.1. If the
module performs its function correctly, it returns an error code of 0. If
either the employee or the department table contains no entries, it returns
an error code of 1. If it finds no employees in an eligible department, it
returns an error code of 2.
The module’s source code is shown in Figure 5.2. Input parameters
ESIZE and DSIZE contain the number of entries in the employee and department
tables. Note that though the module is written in PL/1, the following
discussion is largely language independent; the techniques are
applicable to programs coded in other languages. Also, because the PL/1
logic in the module is fairly simple, virtually any reader, even those not
familiar with PL/1, should be able to understand it.
86 The Art of Software Testing
Dept. Salary
Job
Name code Dept.
Department table
Employee table
Sales
FIGURE 5.1 Input Tables to Module BONUS.
BONUS : PROCEDURE(EMPTAB,DEPTTAB,ESIZE,DSIZE,ERRCODE);
DECLARE 1 EMPTAB (*),
2 NAME CHAR(6),
2 CODE CHAR(1),
2 DEPT CHAR(3),
2 SALARY FIXED DECIMAL(7,2);
DECLARE 1 DEPTTAB (*),
2 DEPT CHAR(3),
2 SALES FIXED DECIMAL(8,2);
DECLARE (ESIZE,DSIZE) FIXED BINARY;
DECLARE ERRCODE FIXED DECIMAL(1);
DECLARE MAXSALES FIXED DECIMAL(8,2) INIT(0); /*MAX. SALES IN DEPTTAB*/
DECLARE (I,J,K) FIXED BINARY; /*COUNTERS*/
DECLARE FOUND BIT(1); /*TRUE IF ELIGIBLE DEPT. HAS EMPLOYEES*/
DECLARE SINC FIXED DECIMAL(7,2) INIT(200.00); /*STANDARD INCREMENT*/
DECLARE LINC FIXED DECIMAL(7,2) INIT(100.00); /*LOWER INCREMENT*/
DECLARE LSALARY FIXED DECIMAL(7,2) INIT(15000.00); /*SALARY BOUNDARY*/
DECLARE MGR CHAR(1) INIT('M'); (continued)
FIGURE 5.2 Module BONUS.
Module (Unit) Testing 87
1 ERRCODE=0;
2 IF(ESIZE<=0)|(DSIZE<=0)
3 THEN ERRCODE=1; /*EMPTAB OR DEPTTAB ARE EMPTY*/
4 ELSE DO;
5 DO I = 1 TO DSIZE; /*FIND MAXSALES AND MAXDEPTS*/
6 IF(SALES(I)>=MAXSALES) THEN MAXSALES=SALES(I);
7 END;
8 DO J = 1 TO DSIZE;
9 IF(SALES(J)=MAXSALES) /*ELIGIBLE DEPARTMENT*/
10 THEN DO;
11 FOUND='0'B;
12 DO K = 1 TO ESIZE;
13 IF(EMPTAB.DEPT(K)=DEPTTAB.DEPT(J))
14 THEN DO;
15 FOUND='1'B;
16 IF(SALARY(K)>=LSALARY)|CODE(K)=MGR)
17 THEN SALARY(K)=SALARY(K)+LINC;
18 ELSE SALARY(K)=SALARY(K)+SINC;
19 END;
20 END;
21 IF(-FOUND) THEN ERRCODE=2;
22 END;
23 END;
24 END;
25 END;
FIGURE 5.2 (continued)
Sidebar 5.1: PL/1 Background
Readers new to software development may be unfamiliar with PL/1
and think of it is a ‘‘dead’’ language. True, there probably is very little
new development using PL/1, but maintenance of existing systems
continues, and the PL/1 constructs still are a pretty good way to learn
about programming procedures.
88 The Art of Software Testing
Regardless of which of the logic coverage techniques you use, the first step
is to list the conditional decisions in the program. Candidates in this program
are all IF and DO statements. By inspecting the program, we can see that all of
the DO statements are simple iterations, and each iteration limit will be equal
to or greater than the initial value (meaning that each loop body always will
execute at least once); and the only way of exiting each loop is via the DO
statement. Thus, the DO statements in this program need no special attention,
since any test case that causes a DO statement to execute will eventually cause
it to branch in both directions (i.e., enter the loop body and skip the loop
body). Therefore, the statements that must be analyzed are:
2 IF (ESIZE<¼O) j (DSIZE<¼0)
6 IF (SALES(I)>¼MAXSALES)
9 IF (SALES(J)¼MAXSALES)
13 IF (EMPTAB.DEPT(K)¼DEPTTAB.DEPT(J))
16 IF (SALARY(K)>¼LSALARY) j (CODE(K)¼MGR)
21 IF(-FOUND) THEN ERRCODE¼2
PL/1, which stands for Programming Language One, was developed
in the 1960s by IBM to provide an English-like development
environment for its mainframe class machines, beginning with the
IBM System/360. At this time in computer history, many programmers
were migrating toward specialty languages such as COBOL, designed
for business application development, and Fortran, designed
for scientific applications. (See Sidebar 3.1 in Chapter 3 for a little
background on these languages.)
One of the main goals for PL/1 designers was a development language
that could compete successfully with COBOL and Fortran while
providing a development environment that would be easier to learn
with a more natural language. All of the early goals for PL/1 likely
never were achieved, but those early designers obviously did their
homework, because PL/1 has been refined and upgraded over the
years and still is in use in some environments today.
By the mid-1990s PL/1 had been extended to other computer
platforms, including OS/2, Linux, UNIX, and Windows. New operating
system support brought language extensions to provide more flexibility
and functionality.
Module (Unit) Testing 89
Given the small number of decisions, we probably should opt for multicondition
coverage, but we will examine all the logic coverage criteria
(except statement coverage, which always is too limited to be of use) to
see their effects.
To satisfy the decision coverage criterion, we need sufficient test cases to
invoke both outcomes of each of the six decisions. The required input situations
to invoke all decision outcomes are listed in Table 5.1. Since two of
the outcomes will always occur, there are 10 situations that need to be
forced by test cases. Note that to construct Table 5.1, decision-outcome
circumstances had to be traced back through the logic of the program to
determine the proper corresponding input circumstances. For instance,
decision 16 is not invoked by any employee meeting the conditions; the
employee must be in an eligible department.
The 10 situations of interest in Table 5.1 could be invoked by the two test
cases shown in Figure 5.3. Note that each test case includes a definition of
the expected output, in adherence to the principles discussed in Chapter 2.
Although these two test cases meet the decision coverage criterion, it
should be obvious that there could be many types of errors in the module
that are not detected by these two test cases. For instance, the test cases do
not explore the circumstances where the error code is 0, an employee is a
manager, or the department table is empty (DSIZE<¼0).
TABLE 5.1 Situations Corresponding to the Decision Outcomes
Decision True Outcome False Outcome
2 ESIZE or DSIZE 0 ESIZE and DSIZE>0
6 Will always occur at least once. Order DEPTTAB so that a
department with lower sales occurs
after a department with higher sales.
9 Will always occur at least once. All departments do not have the
same sales.
13 There is an employee in an
eligible department.
There is an employee who is not in
an eligible department.
16 An eligible employee is either a
manager or earns LSALARY or
more.
An eligible employee is not a
manager and earns less than
LSALARY.
21 All eligible departments
contain no employees.
An eligible department contains at
least one employee.
90 The Art of Software Testing
A more satisfactory test can be obtained by using the condition coverage
criterion. Here we need sufficient test cases to invoke both outcomes of
each condition in the decisions. The conditions and required input situations
to invoke all outcomes are listed in Table 5.2. Since two of the outcomes
will always occur, there are 14 situations that must be forced by test
cases. Again, these situations can be invoked by only two test cases, as
shown in Figure 5.4.
The test cases in Figure 5.4 were designed to illustrate a problem. Since
they do invoke all the outcomes in Table 5.2, they satisfy the condition
coverage criterion, but they are probably a poorer set of test cases than
those in Figure 5.3 in terms of satisfying the decision coverage criterion.
The reason is that they do not execute every statement. For example, statement
18 is never executed. Moreover, they do not accomplish much more
than the test cases in Figure 5.3. They do not cause the output situation
ERRORCODE¼0. If statement 2 had erroneously set ESIZE¼0 and DSIZE¼0,
this error would go undetected. Of course, an alternative set of test cases
might solve these problems, but the fact remains that the two test cases in
Figure 5.4 do satisfy the condition coverage criterion.
Using the decision/condition coverage criterion would eliminate the major
weakness in the test cases in Figure 5.4. Here we would provide sufficient
test cases such that all outcomes of all conditions and decisions would
be invoked at least once. Making Jones a manager and making Lorin a nonmanager
could accomplish this. This would have the result of generating
both outcomes of decision 16, thus causing us to execute statement 18.
Test Input Expected output
case
ESIZE = 0
All other inputs are irrelevant
ERRCODE = 1
ESIZE, DSIZE, EMPTAB, and DEPTTAB
are unchanged
1
ESIZE = DSIZE = 3
DEPTTAB
ERRCODE = 2
EMPTAB
JONES
SMITH
LORIN
E D42
D32
D42
E
E
21,000.00
14,000.00
10,000.00
D42
D32
D95
10,000.00
8,000.00
10,000.00
ESIZE, DSIZE, and DEPTTAB are
unchanged
2
EMPTAB
JONES
SMITH
LORIN
E D42
D32
D42
E
E
21,100.00
14,000.00
10,200.00
FIGURE 5.3 Test Cases to Satisfy the Decision-Coverage Criterion.
Module (Unit) Testing 91
TABLE 5.2 Situations Corresponding to the Condition Outcomes
Decision Condition True Outcome False Outcome
2 ESIZE 0 ESIZE 0 ESIZE>0
2 DSIZE 0 DSIZE 0 DSIZE>0
6 SALES(I) 
MAXSALES
Will always occur at
least once.
Order DEPTTAB so that a
department with lower sales
occurs after a department with
higher sales.
9 SALES(J)¼
MAXSALES
Will always occur at
least once.
All departments do not
have the same sales.
13 EMPTAB.DEPT
(K)¼
DEPTTAB.
DEPT(J)
There is an employee
in an eligible
department.
There is an employee who
is not in an eligible
department.
16 SALARY(K) 
LSALARY
An eligible employee
earns LSALARY or
more.
An eligible employee earns
less than LSALARY.
16 CODE(K)¼MGR An eligible employee
is a manager.
An eligible employee is
not a manager.
21 —FOUND An eligible
department contains
no employees.
An eligible department
contains at least one
employee.
Test Input Expected output
case
ESIZE = DSIZE = 0
All other inputs are irrelevant
ERRCODE = 1
ESIZE, DSIZE, EMPTAB, and
DEPTTAB are unchanged
1
ESIZE = DSIZE = 3
DEPTTAB
ERRCODE = 2
EMPTAB
JONES
SMITH
LORIN
E D42
D32
D42
E
M
21,000.00
14,000.00
10,000.00
D42
D32
D95
10,000.00
8,000.00
10,000.00
ESIZE, DSIZE, and DEPTTAB are
unchanged
2
EMPTAB
JONES
SMITH
LORIN
E D42
D32
D42
E
M
21,000.00
14,000.00
10,100.00
FIGURE 5.4 Test Cases to Satisfy the Condition Coverage Criterion.
92 The Art of Software Testing
One problem with this, however, is that it is essentially no better than
the test cases in Figure 5.3. If the compiler being used stops evaluating an
or expression as soon as it determines that one operand is true, this modification
would result in the expression CODE(K)¼MGR in statement 16 never
having a true outcome. Hence, if this expression were coded incorrectly,
the test cases would not detect the error.
The last criterion to explore is multicondition coverage. This criterion
requires sufficient test cases such that all possible combinations of conditions
in each decision are invoked at least once. This can be accomplished
by working from Table 5.2. Decisions 6, 9, 13, and 21 have two combinations
each; decisions 2 and 16 have four combinations each. The methodology
to design the test cases is to select one that covers as many of the
combinations as possible, select another that covers as many of the remaining
combinations as possible, and so on. A set of test cases satisfying the
multicondition coverage criterion is shown in Figure 5.5. The set is more
Input Expected output
Same as above
Same as above
Test
case
ESIZE = 0 DSIZE = 0
All other inputs are irrelevant
ERRCODE = 1
ESIZE, DSIZE, EMPTAB, and
DEPTTAB are unchanged
1
ESIZE = 0 DSIZE > 0
All other inputs are irrelevant
ESIZE = 5 DSIZE = 4
DEPTTAB
ERRCODE = 2
EMPTAB
JONES
WARNS
LORIN
M D42
D95
D42
M
E
21,000.00
12,000.00
10,000.00
D42
D32
D95
10,000.00
8,000.00
10,000.00
TOY E D95 16,000.00
SMITH E D32 14,000.00
D44 10,000.00
ESIZE, DSIZE, and DEPTTAB are
unchanged
2
ESIZE > 0 DSIZE = 0
All other inputs are irrelevant
3
4
JONES
WARNS
LORIN
M D42
D95
D42
M
E
21,100.00
12,100.00
10,200.00
TOY E D95 16,100.00
SMITH E D32 14,000.00
EMPTAB
FIGURE 5.5 Test Cases to Catisfy the Multicondition
Coverage Criterion.
Module (Unit) Testing 93
comprehensive than the previous sets of test cases, implying that we
should have selected this criterion at the beginning.
It is important to realize that module BONUS could have such a large
number of errors that even the tests satisfying the multicondition coverage
criterion would not detect them all. For instance, no test cases generate the
situation where ERRORCODE is returned with a value of 0; thus, if statement
1 were missing, the error would go undetected. If LSALARY were erroneously
initialized to $150,000.01, the mistake would go unnoticed. If statement
16 stated SALARY(K)>LSALARY instead of SALARY(K) >¼LSALARY,
this error would not be found. Also, whether a variety of off-by-one errors
(such as not handling the last entry in DEPTTAB or EMPTAB correctly) would
be detected would depend largely on chance.
Two points should be apparent now: One, the multicondition criterion
is superior to the other criteria, and, two, any logic coverage criterion is
not good enough to serve as the only means of deriving module tests.
Hence, the next step is to supplement the tests in Figure 5.5 with a set of
black-box tests. To do so, the interface specifications of BONUS are shown
in the following:
BONUS, a PL/1 module, receives five parameters, symbolically referred to
here as EMPTAB, DEPTTAB, ESIZE, DSIZE, and ERRORCODE. The attributes
of these parameters are:
DECLARE 1 EMPTAB(*), /*INPUT AND OUTPUT*/
2 NAME CHARACTER(6),
2 CODE CHARACTER(1),
2 DEPT CHARACTER(3),
2 SALARY FIXED DECIMAL(7,2);
DECLARE 1 DEPTTAB(*), /*INPUT*/
2 DEPT CHARACTER(3),
2 SALES FIXED DECIMAL(8,2);
DECLARE (ESIZE, DSIZE) FIXED BINARY; /*INPUT*/
DECLARE ERRCODE FIXED DECIMAL(1); /*OUTPUT*/
The module assumes that the transmitted arguments have these attributes.
ESIZE and DSIZE indicate the number of entries in EMPTAB and
DEPTTAB, respectively. No assumptions should be made about the order of
entries in EMPTAB and DEPTTAB. The function of the module is to increment
the salary (EMPTAB.SALARY) of those employees in the department or departments
having the largest sales amount (DEPTTAB.SALES). If an eligible
94 The Art of Software Testing
employee’s current salary is $150,000 or more, or if the employee is a manager
(EMPTAB.CODE¼‘M’), the increment is $1,000; if not, the increment for
the eligible employee is $2,000. The module assumes that the incremented
salary will fit into field EMPTAB.SALARY. If ESIZE and DSIZE are not greater
than 0, ERRCODE is set to 1 and no further action is taken. In all other cases,
the function is completely performed. However, if a maximum-sales department
is found to have no employee, processing continues but ERRCODE will
have the value 2; otherwise, it is set to 0.
This specification is not suited to cause-effect graphing (there is not a discernible
set of input conditions whose combinations should be explored);
thus, boundary value analysis will be used. The input boundaries identified
are as follows:
1. EMPTAB has 1 entry.
2. EMPTAB has the maximum number of entries (65,535).
3. EMPTAB has 0 entries.
4. DEPTTAB has 1 entry.
5. DEPTTAB has 65,535 entries.
6. DEPTTAB has 0 entries.
7. A maximum-sales department has 1 employee.
8. A maximum-sales department has 65,535 employees.
9. A maximum-sales department has no employees.
10. All departments in DEPTTAB have the same sales.
11. The maximum-sales department is the first entry in DEPTTAB.
12. The maximum-sales department is the last entry in DEPTTAB.
13. An eligible employee is the first entry in EMPTAB.
14. An eligible employee is the last entry in EMPTAB.
15. An eligible employee is a manager.
16. An eligible employee is not a manager.
17. An eligible employee who is not a manager has a salary of $149,999.99.
18. An eligible employee who is not a manager has a salary of $150,000.
19. An eligible employee who is not a manager has a salary of $150,000.01.
The output boundaries are as follows:
20. ERRCODE¼0
21. ERRCODE¼1
22. ERRCODE¼2
23. The incremented salary of an eligible employee is $299,999.99.
Module (Unit) Testing 95
A further test condition based on the error-guessing technique is
as follows:
24. A maximum-sales department with no employees is followed
in DEPTTAB with another maximum-sales department having
employees.
This is used to determine whether the module erroneously terminates
processing of the input when it encounters an ERRCODE¼2 situation.
Reviewing these 24 conditions, numbers 2, 5, and 8 seem like impractical
test cases. Since they also represent conditions that will never occur (usually
a dangerous assumption to make when testing, but seemingly safe here), we
exclude them. The next step is to compare the remaining 21 conditions to
the current set of test cases (Figure 5.5) to determine which boundary conditions
are not already covered. Doing so, we see that conditions 1, 4, 7, 10, 14,
17, 18, 19, 20, 23, and 24 require test cases beyond those in Figure 5.5.
The next step is to design additional test cases to cover the 11 boundary
conditions. One approach is to merge these conditions into the existing
test cases (i.e., by modifying test case 4 in Figure 5.5), but this is not recommended
because doing so could inadvertently upset the complete multicondition
coverage of the existing test cases. Hence, the safest approach
is to add test cases to those of Figure 5.5. In doing this, the goal is to design
the smallest number of test cases necessary to cover the boundary conditions.
The three test cases in Figure 5.6 accomplish this. Test case 5 covers
conditions 7, 10, 14, 17, 18, 19, and 20; test case 6 covers conditions 1, 4,
and 23; and test case 7 covers condition 24.
The premise here is that the logic coverage, or white-box, test cases in
Figure 5.6 form a reasonable module test for procedure BONUS.
Incremental Testing
In performing the process of module testing, there are two key considerations:
the design of an effective set of test cases, which was discussed in the
previous section, and the manner in which the modules are combined to
form a working program. The second consideration is important because
it has these implications:
  The form in which module test cases are written
  The types of test tools that might be used
96 The Art of Software Testing
  The order in which modules are coded and tested
  The cost of generating test cases
  The cost of debugging (locating and repairing detected errors)
In short, then, it is a consideration of substantial importance. In this
section, we discuss two approaches, incremental and nonincremental testing;
in the next, we explore two incremental approaches, top-down and
bottom-up development or testing.
The question pondered here is the following: Should you test a program
by testing each module independently and then combining the modules to
Test Input Expected output
case
5 ESIZE = 3 DSIZE = 2
DEPTTAB
ERRCODE = 0
EMPTAB
ALLY
BEST
CELTO
E D36
D33
D33
E
E
14,999.99
15,000.00
15,000.01
D33
D36
55,400.01
55,400.01
ESIZE, DSIZE, and DEPTTAB are
unchanged
ALLY
BEST
CELTO
E D36
D33
D33
E
E
15,199.99
15,100.00
15,100.01
EMPTAB
6 ESIZE = 1 DSIZE = 1
DEPTTAB
ERRCODE = 0
EMPTAB
CHIEF M D99 99,899.99 D99 99,000.00
ESIZE, DSIZE, and DEPTTAB are
unchanged
ERRCODE = 2
ESIZE, DSIZE, and DEPTTAB are
unchanged
CHIEF M D99 99,999.99
EMPTAB
7 ESIZE = 2 DSIZE = 2
EMPTAB DEPTTAB
DOLE E D67 10,000.00 D66 20,000.00
FORD E D22 33,333.33 D67 20,000.00
EMPTAB
DOLE E D67 10,000.00
FORD E D22 33,333.33
FIGURE 5.6 Supplemental Boundary Value Analysis Test Cases
for BONUS.
Module (Unit) Testing 97
form the program, or should you combine the next module to be tested
with the set of previously tested modules before it is tested? The first approach
is called nonincremental, or ‘‘big-bang,’’ testing or integration; the
second approach is known as incremental testing or integration.
The program in Figure 5.7 is used as an example. The rectangles represent
the six modules (subroutines or procedures) in the program. The lines
connecting the modules represent the control hierarchy of the program;
that is, module A calls modules B, C, and D; module B calls module E; and
so on. Nonincremental testing, the traditional approach, is performed in
the following manner. First, a module test is performed on each of the six
modules, testing each module as a stand-alone entity. The modules might
be tested at the same time or in succession, depending on the environment
(e.g., interactive versus batch-processing computing facilities) and the
number of people involved. Finally, the modules are combined or integrated
(e.g., ‘‘link edited’’) to form the program.
The testing of each module requires a special driver module and one or
more stub modules. For instance, to test module B, test cases are first designed
and then fed to module B by passing it input arguments from a driver module,
a small module that must be coded to ‘‘drive,’’ or transmit, test cases through
the module under test. (Alternatively, a test tool could be used.) The driver
module must also display, to the tester, the results produced by B. In addition,
since module B calls module E, something must be present to receive control
when B calls E. A stub module, a special module given the name ‘‘E ’’ that must
be coded to simulate the function of module E, accomplishes this.
FIGURE 5.7 Sample Six-Module Program.
98 The Art of Software Testing
When the module testing of all six modules has been completed, the
modules are combined to form the program.
The alternative approach is incremental testing. Rather than testing
each module in isolation, the next module to be tested is first combined
with the set of modules that have been tested already.
It is premature to give a procedure for incrementally testing the program
in Figure 5.7, because there is a large number of possible incremental
approaches. A key issue is whether we should begin at the top or bottom of
the program. However, since we discuss this issue in the next section, let
us assume for the moment that we are beginning from the bottom.
The first step is to test modules E, C, and F, either in parallel (by three
people) or serially. Notice that we must prepare a driver for each module,
but not a stub. The next step is to test B and D; but rather than testing them
in isolation, they are combined with modules E and F, respectively. In other
words, to test module B, a driver is written, incorporating the test cases,
and the pair B-E is tested. The incremental process, adding the next module
to the set or subset of previously tested modules, is continued until the
last module (module A in this case) is tested. Note that this procedure
could have alternatively progressed from the top to the bottom.
Several observations should be apparent at this point:
1. Nonincremental testing requires more work. For the program in Figure
5.7, five drivers and five stubs must be prepared (assuming we do
not need a driver module for the top module). The bottom-up incremental
test would require five drivers but no stubs. A top-down incremental
test would require five stubs but no drivers. Less work is
required because previously tested modules are used instead of the
driver modules (if you start from the top) or stub modules (if you start
from the bottom) needed in the nonincremental approach.
2. Programming errors related to mismatching interfaces or incorrect
assumptions among modules will be detected earlier when incremental
testing is used. The reason is that combinations of modules are
tested together at an early point in time. However, when nonincremental
testing is used, modules do not ‘‘see one another’’ until the
end of the process.
3. As a result, debugging should be easier if incremental testing is used.
If we assume that errors related to intermodule interfaces and
assumptions do exist (a good assumption, from experience), then, if
Module (Unit) Testing 99
nonincremental testing has been used, the errors will not surface until
the entire program has been combined. At this time, we may have
difficulty pinpointing the error, since it could be anywhere within the
program. Conversely, if incremental testing is used, an error of this
type should be easier to pinpoint, because it is likely that the error is
associated with the most recently added module.
4. Incremental testing might result in more thorough testing. If you are
testing module B, either module E or A (depending on whether you
started from the bottom or the top) is executed as a result. Although
E or A should have been thoroughly tested previously, perhaps executing
it as a result of B’s module test will invoke a new condition,
perhaps one that represents a deficiency in the original test of E or A.
On the other hand, if nonincremental testing is used, the testing of B
will affect only module B. In other words, incremental testing substitutes
previously tested modules for the stubs or drivers needed in the
nonincremental test. As a result, the actual modules receive more
exposure by the completion of the last module test.
5. The nonincremental approach appears to use less machine time. If
module A of Figure 5.7 is being tested using the bottom-up approach,
modules B, C, D, E, and F probably execute during the execution of A.
In a nonincremental test of A, only stubs for B, C, and E are executed.
The same is true for a top-down incremental test. If module F is being
tested, modules A, B, C, D, and E may be executed during the test of F ;
in the nonincremental test of F, only the driver for F, plus F itself, executes.
Hence, the number of machine instructions executed during a
test run using the incremental approach is apparently greater than that
for the nonincremental approach. Offsetting this is the fact that the
nonincremental test requires more drivers and stubs than the incremental
test; machine time is needed to develop the drivers and stubs.
6. At the beginning of the module testing phase, there is more opportunity
for parallel activities when nonincremental testing is used
(that is, all the modules can be tested simultaneously). This might
be of significance in a large project (many modules and people),
since the head count of a project is usually at its peak at the start of
the module test phase.
In summary, observations 1 through 4 are advantages of incremental
testing, while observations 5 and 6 are disadvantages. Given current trends
100 The Art of Software Testing
in the computing industry (hardware costs have been decreasing, and seem
destined to continue to do so, while hardware capability increases, and labor
costs and the consequences of software errors are increasing), and
given the fact that the earlier an error is found, the lower the cost of repairing
it, you can see that observations 1 through 4 are growing in importance,
whereas observation 5 is becoming less important. Observation 6
seems to be a weak disadvantage, if one at all. This leads to the conclusion
that incremental testing is superior.
Top-Down versus Bottom-Up Testing
Given the conclusion of the previous section—that incremental testing is
superior to nonincremental testing—we next explore two incremental
strategies: top-down and bottom-up testing. Before getting into them, however,
we should clarify several misconceptions. First, the terms top-down
testing, top-down development, and top-down design often are used as synonyms.
Top-down testing and top-down development are synonyms (they
represent a strategy of ordering the coding and testing of modules), but
top-down design is something quite different and independent. A program
that was designed in top-down fashion can be incrementally tested in
either a top-down or a bottom-up fashion.
Second, bottom-up testing (or bottom-up development) is often mistakenly
equated with nonincremental testing. The reason is that bottom-up
testing begins in a manner that is identical to a nonincremental test (i.e.,
when the bottom, or terminal, modules are tested), but as we saw in the
previous section, bottom-up testing is an incremental strategy. Finally,
since both strategies are incremental, we won’t repeat here the advantages
of incremental testing; we will discuss only the differences between topdown
and bottom-up testing.
Top-Down Testing
The top-down strategy starts with the top, or initial, module in the program.
After this, there is no single ‘‘right’’ procedure for selecting the next
module to be incrementally tested; the only rule is that to be eligible to be
the next module, at least one of the module’s subordinate (calling) modules
must have been tested previously.
Module (Unit) Testing 101
Figure 5.8 is used to illustrate this strategy. A through L are the 12 modules
in the program. Assume that module J contains the program’s I/O read
operations and module I contains the write operations.
The first step is to test module A. To accomplish this, stub modules representing
B, C, and D must be written. Unfortunately, the production of stub
modules is often misunderstood; as evidence, you may often see such statements
as ‘‘a stub module need only write a message stating ‘we got this far’ ’’;
and, ‘‘in many cases, the dummy module (stub) simply exits—without doing
any work at all.’’ In most situations, these statements are false. Since module
A calls module B, A is expecting B to perform some work; this work most
likely is some result (output arguments) returned to A. If the stub simply
returns control or writes an error message without returning a meaningful
result, module A will fail, not because of an error in A, but because of a failure
of the stub to simulate the corresponding module. Moreover, returning a
‘‘wired-in’’ output from a stub module is often insufficient. For instance, consider
the task of writing a stub representing a square-root routine, a database
table-search routine, an ‘‘obtain corresponding master-file record’’ routine, or
the like. If the stub returns a fixed wired-in output, but doesn’t have the particular
value expected by the calling module during this invocation, the calling
module may fail or produce a confusing result. Hence, the production of
stubs is not a trivial task.
FIGURE 5.8 Sample 12-Module Program.
102 The Art of Software Testing
Another consideration is the form in which test cases are presented to
the program, an important consideration that is not even mentioned in
most discussions of top-down testing. In our example, the question is: How
do you feed test cases to module A? The top module in typical programs
neither receives input arguments nor performs input/output operations, so
the answer is not immediately obvious. The answer is that the test data are
fed to the module (module A in this situation) from one or more of its stubs.
To illustrate, assume that the functions of B, C, and D are as follows:
B —Obtain summary of transaction file.
C —Determine whether weekly status meets quota.
D —Produce weekly summary report.
A test case for A, then, is a transaction summary returned from stub B.
Stub D might contain statements to write its input data to a printer, allowing
the results of each test to be examined.
In this program, another problem exists. Presumably, module A calls module
B only once; therefore the problem is how to feed more than one test case
to A. One solution is to develop multiple versions of stub B, each with a different
wired-in set of test data to be returned to A. To execute the test cases,
the program is executed multiple times, each time with a different version of
stub B. Another alternative is to place test data on external files and have stub
B read the test data and return them to A. In either case, keeping in mind the
previous discussion, you should see that the development of stub modules is
more difficult than it is often made out to be. Furthermore, it often is necessary,
because of the characteristics of the program, to represent a test case
across multiple stubs beneath the module under test (i.e., where the module
receives data to be acted upon by calling multiple modules).
After A has been tested, an actual module replaces one of the stubs, and
the stubs required by that module are added. For instance, Figure 5.9
might represent the next version of the program.
After testing the top module, numerous sequences are possible. For instance,
if we are performing all the testing sequences, four examples of the
many possible sequences of modules are:
1.A B C D E F G H I J K L
2.A B E F J C G K D H L I
3.A D H I K L C G B F J E
4.A B F J D I E C G K H L
Module (Unit) Testing 103
If parallel testing occurs, other alternatives are possible. For instance,
after module A has been tested, one programmer could take module A and
test the combination A-B; another programmer could test A-C; and a third
could test A-D. In general, there is no best sequence, but here are two
guidelines to consider:
1. If there are critical sections of the program (perhaps module G), design
the sequence such that these sections are added as early as possible.
A ‘‘critical section’’ might be a complex module, a module with a
new algorithm, or a module suspected to be error prone.
2. Design the sequence such that the I/O modules are added as early as
possible.
The motivation for the first should be obvious, but the motivation for
the second deserves further discussion. Recall that a problem with stubs is
that some of them must contain the test cases, and others must write their
input to a printer or display. However, as soon as the module accepting the
program’s input is added, the representation of test cases is considerably
simplified; their form is identical to the input accepted by the final program
(e.g., from a transaction file or a terminal). Likewise, once the module
performing the program’s output function is added, the placement of
code in stub modules to write results of test cases might no longer be necessary.
Thus, if modules J and I are the I/O modules, and if module G performs
some critical function, the incremental sequence might be
A B F J D I C G E K H L
and the form of the program after the sixth increment would be that shown
in Figure 5.10.
FIGURE 5.9 Second Step in the Top-Down Test.
104 The Art of Software Testing
Once the intermediate state in Figure 5.10 has been reached, the representation
of test cases and the inspection of results are simplified. It has another
advantage, in that you have a working skeletal version of the program,
that is, a version that performs actual input and output operations. However,
stubs are still simulating some of the ‘‘insides.’’ This early skeletal version:
  Allows you to find human-factor errors and problems.
  Makes it possible to demonstrate the program to the eventual user.
  Serves as evidence that the overall design of the program is sound.
  Serves as a morale booster.
These points represent the major advantage of the top-down strategy.
On the other hand, the top-down approach has some serious shortcomings.
Assume that our current state of testing is that of Figure 5.10 and that
our next step is to replace stub H with module H. What we should do at this
point (or earlier) is use the methods described earlier in this chapter to design
a set of test cases for H. Note, however, that the test cases are in the
form of actual program inputs to module J. This presents several problems.
First, because of the intervening modules between J and H (F, B, A, and D),
we might find it impossible to represent certain test cases to module J that
test every predefined situation in H. For instance, if H is the BONUS module
of Figure 5.2, it might be impossible, because of the nature of intervening
module D, to create some of the seven test cases of Figures 5.5 and 5.6.
FIGURE 5.10 Intermediate State in the Top-Down Test.
Module (Unit) Testing 105
Second, because of the ‘‘distance’’ between H and the point at which the
test data enter the program, even if it were possible to test every situation,
determining which data to feed to J to test these situations in H is often a
difficult mental task.
Third, because the displayed output of a test might come from a module
that is a large distance away from the module being tested, correlating the
displayed output to what went on in the module may be difficult or impossible.
Consider adding module E to Figure 5.10. The results of each
test case are determined by examining the output written by module I, but
because of the intervening modules, it may be difficult to deduce the actual
output of E (that is, the data returned to B).
The top-down strategy, depending on how it is approached, may have
two further problems. People occasionally feel that the strategy can be
overlapped with the program’s design phase. For instance, if you are in the
process of designing the program in Figure 5.8, you might believe that after
the first two levels are designed, modules A through D can be coded and
tested while the design of the lower levels progresses. As we have emphasized
elsewhere, this is usually an unwise decision. Program design is an
iterative process, meaning that when we are designing the lower levels of a
program’s structure, we may discover desirable changes or improvements
to the upper levels. If the upper levels have already been coded and tested,
the desirable improvements will most likely be discarded, an unwise decision
in the long run.
A final problem that often arises in practice is failing to completely test a
module before proceeding to another module. This occurs for two reasons:
because of the difficulty of embedding test data in stub modules, and because
the upper levels of a program usually provide resources to lower levels.
In Figure 5.8 we saw that testing module A might require multiple
versions of the stub for module B. In practice, there is a tendency to say,
‘‘Because this represents a lot of work, I won’t execute all of A’s test cases
now. I’ll wait until I place module J in the program, at which time the
representation of test cases will be easier, and remember at this point to
finish testing module A.’’ Of course, the problem here is that we may forget
to test the remainder of module A at this later point in time. Also, because
upper levels often provide resources for use by lower levels (e.g., opening
of files), it is difficult sometimes to determine whether the resources have
been provided correctly (e.g., whether a file has been opened with the
proper attributes) until the lower modules that use them are tested.
106 The Art of Software Testing
Bottom-Up Testing
The next step is to examine the bottom-up incremental testing strategy.
For the most part, bottom-up testing is the opposite of top-down testing;
thus, the advantages of top-down testing become the disadvantages of
bottom-up testing, and the disadvantages of top-down testing become
the advantages of bottom-up testing. Because of this, the discussion of
bottom-up testing is shorter.
The bottom-up strategy begins with the terminal modules in the program
(the modules that do not call other modules). After these modules
have been tested, again there is no best procedure for selecting the next
module to be incrementally tested; the only rule is that to be eligible to be
the next module, all of the module’s subordinate modules (the modules it
calls) must have been tested previously.
Returning to Figure 5.8, the first step is to test some or all of modules E,
J, G, K, L, and I, either serially or in parallel. To do so, each module needs a
special driver module: a module that contains wired-in test inputs, calls
the module being tested, and displays the outputs (or compares the actual
outputs with the expected outputs). Unlike the situation with stubs, multiple
versions of a driver are not needed, since the driver module can iteratively
call the module being tested. In most cases, driver modules are easier
to produce than stub modules.
As was the case earlier, a factor influencing the sequence of testing is the
critical nature of the modules. If we decide that modules D and F are most
critical, an intermediate state of the bottom-up incremental test might be
that of Figure 5.11. The next steps might be to test E and then test B, combining
B with the previously tested modules E, F, and J.
A drawback of the bottom-up strategy is that there is no concept of an
early skeletal program. In fact, the working program does not exist until
the last module (module A) is added, and this working program is the complete
program. Although the I/O functions can be tested before the whole
program has been integrated (the I/O modules are being used in Figure
5.11), the advantages of the early skeletal program are not present.
The problems associated with the impossibility, or difficulty, of creating
all test situations in the top-down approach do not exist here. If you think
of a driver module as a test probe, the probe is being placed directly on the
module being tested; there are no intervening modules to worry about.
Examining other problems associated with the top-down approach, you
Module (Unit) Testing 107
can’t make the unwise decision to overlap design and testing, since the
bottom-up test cannot begin until the bottom of the program has been designed.
Also, the problem of not completing the test of a module before
starting another, because of the difficulty of encoding test data in versions
of a stub, does not exist when using bottom-up testing.
A Comparison
It would be convenient if the top-down versus bottom-up issue were as
clear-cut as the incremental versus nonincremental issue, but unfortunately
it is not. Table 5.3 summarizes the relative advantages and disadvantages
of the two approaches (excluding the previously discussed
advantages shared by both—those of incremental testing). The first advantage
of each approach might appear to be the deciding factor, but there is
no evidence showing that major flaws occur more often at the top or bottom
levels of the typical program. The safest way to make a decision is to
weigh the factors in Table 5.3 with respect to the particular program being
tested. Lacking such a program here, the serious consequences of the
fourth disadvantage—of top-down testing and the availability of test tools
that eliminate the need for drivers but not stubs—seems to give the
bottom-up strategy the edge.
Furthermore, it may be apparent that top-down and bottom-up testing
are not the only possible incremental strategies.
FIGURE 5.11 Intermediate State in the Bottom-Up Test.
108 The Art of Software Testing
Performing the Test
The remaining part of the module test is the act of actually carrying out the
test. A set of hints and guidelines for doing this is included here.
When a test case produces a situation where the module’s actual results
do not match the expected results, there are two possible explanations:
either the module contains an error, or the expected results are incorrect
(the test case is incorrect). To minimize this confusion, the set of test cases
TABLE 5.3 Comparison of Top-Down and Bottom-Up Testing
Top-Down Testing
Advantages Disadvantages
1. Advantageous when major flaws
occur toward the top of the
program.
1. Stub modules must be produced.
2. Once the I/O functions are added,
representation of cases is easier.
2. Stub modules are often more
complicated than they first appear to
be.
3. Early skeletal program allows
demonstrations and boosts morale.
3. Before the I/O functions are added,
the representation of test cases in
stubs can be difficult.
4. Test conditions may be impossible, or
very difficult, to create.
5. Observation of test output is more
difficult.
6. Leads to the conclusion that design
and testing can be overlapped.
7. Defers the completion of testing
certain modules.
Bottom-Up Testing
Advantages Disadvantages
1. Advantageous when major flaws
occur toward the bottom of the
program.
1. Driver modules must be produced.
2. Test conditions are easier to create.
2. The program as an entity does not exist
until the last module is added.
3. Observation of test results is easier.
Module (Unit) Testing 109
should be reviewed or inspected before the test is performed (that is, the
test cases should be tested).
The use of automated test tools can minimize part of the drudgery of the
testing process. For instance, test tools exist that eliminate the need for
driver modules. Flow-analysis tools enumerate the paths through a program,
find statements that can never be executed (‘‘unreachable’’ code),
and identify instances where a variable is used before it is assigned a value.
As was the practice earlier in this chapter, remember that a definition of
the expected result is a necessary part of a test case. When executing a test,
remember to look for side effects (instances where a module does something
it is not supposed to do). In general, these situations are difficult to
detect, but some of them may be found by checking, after execution of the
test case, the inputs to the module that are not supposed to be altered. For
instance, test case 7 in Figure 5.6 states that as part of the expected result,
ESIZE, DSIZE, and DEPTTAB should be unchanged. When running this test
case, not only should the output be examined for the correct result, but
ESIZE, DSIZE, and DEPTTAB should be examined to determine whether
they were erroneously altered.
The psychological problems associated with a person attempting to test
his or her own programs apply as well to module testing. Rather than testing
their own modules, programmers might swap them; more specifically,
the programmer of the calling module is always a good candidate to test
the called module. Note that this applies only to testing; the debugging of
a module always should be performed by the original programmer.
Avoid throwaway test cases; represent them in such a form that they can
be reused in the future. Recall the counterintuitive phenomenon in Figure
2.2. If an abnormally high number of errors is found in a subset of the modules,
it is likely that these modules contain even more, as yet undetected,
errors. Such modules should be subjected to further module testing, and
possibly an additional code walkthrough or inspection. Finally, remember
that the purpose of a module test is not to demonstrate that the module
functions correctly, but to demonstrate the presence of errors in the module.
Summary
In this chapter we introduced you to some of the mechanics of testing,
especially as it relates to large programs. This is a process of testing individual
program components—subroutines, subprograms, classes, and
110 The Art of Software Testing
procedures. In module testing you compare software functionality with the
specification that defines its intended function. Module or unit testing can
be an important part of a developer’s toolbox to help achieve a reliable
application, especially with object-oriented languages such as Java and
C#. The goal in module testing is the same as for any other type of software
testing: attempt to show how the program contradicts the specification. In
addition to the software specification, you will need each module’s source
code to effect a module test.
Module testing is largely white-box testing. (See Chapter 4 for more information
on white-box procedures and designing test cases for testing.) A
thorough module test design will include incremental strategies such as
top-down as well as bottom-up techniques.
It is helpful, when preparing for a module test, to review the psychological
and economic principles laid out in Chapter 2.
One more point: Module testing software is only the beginning of an
exhaustive testing procedure. You will need to move on to higher-order testing,
which we address in Chapter 6, and user testing, covered in Chapter 7.
Module (Unit) Testing 111

6 Higher-Order Testing
When you finish module-testing a program, you have really only just
begun the testing process. This is especially true of large or complex
programs. Consider this important concept:
A software error occurs when the program does not do what its end
user reasonably expects it to do.
Applying this definition, even if you could perform an absolutely perfect
module test, you still couldn’t guarantee that you have found all software
errors. To complete testing, then, some form of further testing is necessary.
We call this new form higher-order testing.
Software development is largely a process of communicating information
about the eventual program and translating this information from one
form to another. In essence, it is moving from the conceptual to the concrete.
For that reason, the vast majority of software errors can be attributed
to breakdowns, mistakes, and ‘‘noise’’ during the communication and
translation of information.
This view of software development is illustrated in Figure 6.1, a model
of the development cycle for a software product. The flow of the process
can be summarized in seven steps:
1. Translate the program user’s needs into a set of written requirements.
These are the goals for the product.
113
2. Translate the requirements into specific objectives by assessing feasibility,
time, and cost, resolving conflicting requirements, and establishing
priorities and trade-offs.
3. Translate the objectives into a precise product specification, viewing
the product as a black box and considering only its interfaces and
interactions with the end user. This description is called the external
specification.
4. If the product is a system such as an operating system, flight-control
system, database system, or employee personnel management system,
rather than an application (e.g., compiler, payroll program,
word processor), the next process is system design. This step
FIGURE 6.1 The Software Development Process.
114 The Art of Software Testing
partitions the system into individual programs, components, or subsystems,
and defines their interfaces.
5. Design the structure of the program or programs by specifying the
function of each module, the hierarchical structure of the modules,
and the interfaces between modules.
6. Develop a precise specification that defines the interface to, and function
of, each module.
7. Translate, through one or more substeps, the module interface specification
into the source code algorithm of each module.
Here’s another way of looking at these forms of documentation:
  Requirements specify why the program is needed.
  Objectives specify what the program should do and how well the program
should do it.
  External specifications define the exact representation of the program
to users.
  Documentation associated with the subsequent processes specifies, in
increasing levels of detail, how the program is constructed.
Given the premise that the seven steps of the development cycle involve
communication, comprehension, and translation of information, and the
premise that most software errors stem from breakdowns in information
handling, there are three complementary approaches to prevent and/or detect
these errors.
First, we can introduce more precision into the development process to
prevent many of the errors. Second, we can introduce, at the end of each
process, a separate verification step to locate as many errors as possible
before proceeding to the next process. This approach is illustrated in
Figure 6.2. For instance, the external specification is verified by comparing
it to the output of the prior stage (the statement of objectives) and feeding
back any discovered mistakes to the external specification process. (Use
the code inspection and walkthrough methods discussed in Chapter 3 in
the verification step at the end of the seventh process.)
The third approach is to orient distinct testing processes toward distinct
development processes. That is, focus each testing process on a particular
translation step—thus on a particular class of errors. This approach is
illustrated in Figure 6.3.
Higher-Order Testing 115
The testing cycle is structured to model the development cycle. In other
words, you should be able to establish a one-to-one correspondence between
development and testing processes. For instance:
  The purpose of a module test is to find discrepancies between the program’s
modules and their interface specifications.
  The purpose of a function test is to show that a program does not
match its external specifications.
  The purpose of a system test is to show that the product is inconsistent
with its original objectives.
FIGURE 6.2 The Development Process with Intermediate Verification
Steps.
116 The Art of Software Testing
Notice how we have structured these statements: ‘‘find discrepancies,’’
‘‘does not match,’’ ‘‘is inconsistent.’’ Remember that the goal of software
testing is to find problems (because we know there will be problems!). If
you set out to prove that some form of inputs work properly, or assume
that the program is true to its specification and objectives, your testing will
be incomplete. Only by setting out to prove that some form of inputs work
improperly, and assume that the program is untrue to its specification and
objectives, will your testing be complete. This is an important concept we
iterate throughout this book.
FIGURE 6.3 The Correspondence Between Development and Testing
Processes.
Higher-Order Testing 117
The advantages of this structure are that it avoids unproductive redundant
testing and prevents you from overlooking large classes
of errors. For instance, rather than simply labeling system testing as
‘‘the testing of the whole system’’ and possibly repeating earlier tests,
system testing is oriented toward a distinct class of errors (those made
during the translation of the objectives to the external specification) and
measured with respect to a distinct type of documentation in the development
process.
The higher-order testing methods shown in Figure 6.3 are most applicable
to software products (programs written as a result of a contract or intended
for wide usage, as opposed to experimental programs or those
written for use only by the program’s author). Programs not written as
products often do not have formal requirements and objectives; for such
programs, the function test might be the only higher-order test. Also, the
need for higher-order testing increases along with the size of the program.
The reason is that the ratio of design errors (errors made in the earlier
development processes) to coding errors is considerably higher in large
programs than in small programs.
Note that the sequence of testing processes in Figure 6.3 does not necessarily
imply a time sequence. For instance, since system testing is not defined
as ‘‘the kind of testing you do after function testing,’’ but instead as a
distinct type of testing focused on a distinct class of errors, it could very
well be partially overlapped in time with other testing processes.
In this chapter, we discuss the processes of function, system, acceptance,
and installation testing. We omit integration testing because it is often
not regarded as a separate testing step; and, when incremental module
testing is used, it is an implicit part of the module test.
We will keep the discussions of these testing processes brief, general,
and, for the most part, without examples because specific techniques used
in these higher-order tests are highly dependent on the specific program
being tested. For instance, the characteristics of a system test (the types of
test cases, the manner in which test cases are designed, the test tools used)
for an operating system will differ considerably from a system test of a
compiler, a program controlling a nuclear reactor, or a database application
program.
In the last few sections in this chapter we address planning and organizational
issues, along with the important question of determining when to
stop testing.
118 The Art of Software Testing
Function Testing
As indicated in Figure 6.3, function testing is a process of attempting to
find discrepancies between the program and the external specification. An
external specification is a precise description of the program’s behavior
from the end-user point of view.
Except when used on small programs, function testing is normally a
black-box activity. That is, you rely on the earlier module-testing process
to achieve the desired white-box logic coverage criteria.
To perform a function test, you analyze the specification to derive a set
of test cases. The equivalence partitioning, boundary value analysis,
cause-effect graphing, and error-guessing methods described in Chapter
4 are especially pertinent to function testing. In fact, the examples in
Chapter 4 are examples of function tests. The descriptions of the Fortran
DIMENSION statement, the examination scoring program, and the
DISPLAY command actually are examples of external specifications. They
are not, however, completely realistic examples; for instance, an actual
external specification for the scoring program would include a precise
description of the format of the reports. (Note: Since we discussed function
testing in Chapter 4, we present no examples of function tests in this
section.)
Many of the guidelines we provided in Chapter 2 also are particularly
pertinent to function testing. In particular, keep track of which functions
have exhibited the greatest number of errors; this information is valuable
because it tells you that these functions probably also contain the preponderance
of as-yet undetected errors. Also, remember to focus a sufficient
amount of attention on invalid and unexpected input conditions. (Recall
that the definition of the expected result is a vital part of a test case.)
Finally, as always, keep in mind that the purpose of the function test is to
expose errors and discrepancies with the specification, not to demonstrate
that the program matches its external specification.
System Testing
System testing is the most misunderstood and most difficult testing process.
System testing is not a process of testing the functions of the complete
system or program, because this would be redundant with the process of
function testing. Rather, as shown in Figure 6.3, system testing has a
Higher-Order Testing 119
particular purpose: to compare the system or program to its original objectives.
Given this purpose, consider these two implications:
1. System testing is not limited to systems. If the product is a program,
system testing is the process of attempting to demonstrate how the
program, as a whole, fails to meet its objectives.
2. System testing, by definition, is impossible if there is no set of written,
measurable objectives for the product.
In looking for discrepancies between the program and its objectives,
focus on translation errors made during the process of designing the
external specification. This makes the system test a vital test process, because
in terms of the product, the number of errors made, and the severity
of those errors, this step in the development cycle usually is the most
error prone.
It also implies that, unlike the function test, the external specification
cannot be used as the basis for deriving the system test cases, since this
would subvert the purpose of the system test. On the other hand, the
objectives document cannot be used by itself to formulate test cases, since
it does not, by definition, contain precise descriptions of the program’s
external interfaces. We solve this dilemma by using the program’s user
documentation or publications—design the system test by analyzing the
objectives; formulate test cases by analyzing the user documentation. This
has the useful side effect of comparing the program to its objectives and to
the user documentation, as well as comparing the user documentation to
the objectives, as shown in Figure 6.4.
Figure 6.4 illustrates why system testing is the most difficult testing process.
The leftmost arrow in the figure, comparing the program to its objectives,
is the central purpose of the system test, but there are no known testcase
design methodologies. The reason for this is that objectives state what
a program should do and how well the program should do it, but they do
not state the representation of the program’s functions. For instance, the
objectives for the DISPLAY command specified in Chapter 4 might have
read as follows:
A command will be provided to view, from a terminal, the contents of
main storage locations. Its syntax should be consistent with the syntax
of all other system commands. The user should be able to specify
120 The Art of Software Testing
a range of locations, via an address range or an address and a count.
Sensible defaults should be provided for command operands.
Output should be displayed as multiple lines of multiple words (in
hexadecimal), with spacing between the words. Each line should
contain the address of the first word of that line. The command is a
‘‘trivial’’ command, meaning that under reasonable system loads, it
should begin displaying output within two seconds, and there should
be no observable delay time between output lines. A programming
error in the command processor should, at the worst, cause the command
to fail; the system and the user’s session must not be affected.
The command processor should have no more than one userdetected
error after the system is put into production.
Given the statement of objectives, there is no identifiable methodology
that would yield a set of test cases, other than the vague but useful guideline
of writing test cases to attempt to show that the program is inconsistent
with each sentence in the objectives statement. Hence, a
different approach to test-case design is taken here: Rather than describing
a methodology, distinct categories of system test cases are discussed. Because
of the absence of a methodology, system testing requires a substantial
FIGURE 6.4 The System Test.
Higher-Order Testing 121
amount of creativity; in fact, the design of good system test cases requires
more creativity, intelligence, and experience than are required to design
the system or program itself.
Table 6.1 lists 15 categories of test cases, along with a brief description.
We discuss the categories in turn here. We don’t claim that all 15 categories
apply to every program, but to avoid overlooking something, we recommend
that you explore all of them when designing test cases.
TABLE 6.1 15 Categories of Test Cases
Category Description
Facility Ensure that the functionality in the objectives is implemented.
Volume Subject the program to abnormally large volumes of data to
process.
Stress Subject the program to abnormally large loads, generally
concurrent processing.
Usability Determine how well the end user can interact with the program.
Security Try to subvert the program’s security measures.
Performance Determine whether the program meets response and
throughput requirements.
Storage Ensure the program correctly manages its storage needs, both
system and physical.
Configuration Check that the program performs adequately on the
recommended configurations.
Compatibility/
Conversion
Determine whether new versions of the program are
compatible with previous releases.
Installation Ensure the installation methods work on all supported
platforms.
Reliability Determine whether the program meets reliability specifications
such as uptime and MTBF.
Recovery Test whether the system’s recovery facilities work as designed.
Serviceability/
Maintenance
Determine whether the application correctly provides
mechanisms to yield data on events requiring technical support.
Documentation Validate the accuracy of all user documentation.
Procedure Determine the accuracy of special procedures required to use or
maintain the program.
122 The Art of Software Testing
Facility Testing
The most obvious type of system testing is to determine whether each facility
(or function; but the word ‘‘function’’ is not used here to avoid confusing
this with function testing) mentioned in the objectives was actually
implemented. The procedure is to scan the objectives sentence by sentence,
and when a sentence specifies a what (e.g., ‘‘syntax should be consistent
. . . ,’’ ‘‘user should be able to specify a range of locations . . .’’),
determine that the program satisfies the ‘‘what.’’ This type of testing often
can be performed without a computer; a mental comparison of the objectives
with the user documentation is sometimes sufficient. Nonetheless, a
checklist is helpful to ensure that you mentally verify the same objectives
the next time you perform the test.
Volume Testing
A second type of system testing is to subject the program to heavy volumes
of data. For instance, a compiler could be fed an absurdly large source program
to compile. A linkage editor might be fed a program containing thousands
of modules. An electronic circuit simulator could be given a circuit
containing millions of components. An operating system’s job queue might
be filled to capacity. If a program is supposed to handle files spanning multiple
volumes, enough data is created to cause the program to switch from
one volume to another. In other words, the purpose of volume testing is to
show that the program cannot handle the volume of data specified in its
objectives.
Obviously, volume testing can require significant resources, therefore,
in terms of machine and people time, you shouldn’t go overboard. Still,
every program must be exposed to at least a few volume tests.
Stress Testing
Stress testing subjects the program to heavy loads, or stresses. This should
not be confused with volume testing; a heavy stress is a peak volume of
data, or activity, encountered over a short span of time. An analogy would be
evaluating a typist: A volume test would determine whether the typist
could cope with a draft of a large report; a stress test would determine
whether the typist could type at a rate of 50 words per minute.
Higher-Order Testing 123
Because stress testing involves an element of time, it is not applicable to
many programs—for example, a compiler or a batch-processing payroll
program. It is applicable, however, to programs that operate under varying
loads, or interactive, real-time, and process control programs. If an air traffic
control system is supposed to keep track of up to 200 planes in its sector,
you could stress-test it by simulating the presence of 200 planes. Since
there is nothing to physically keep a 201st plane from entering the sector, a
further stress test would explore the system’s reaction to this unexpected
plane. An additional stress test might simulate the simultaneous entry of a
large number of planes into the sector.
If an operating system is supposed to support a maximum of 15 concurrent
jobs, the system could be stressed by attempting to run 15 jobs simultaneously.
You could stress a pilot training aircraft simulator by
determining the system’s reaction to a trainee who forces the rudder left,
pulls back on the throttle, lowers the flaps, lifts the nose, lowers the landing
gear, turns on the landing lights, and banks left, all at the same time.
(Such test cases might require a four-handed pilot or, realistically, two test
specialists in the cockpit.) You might stress-test a process control system
by causing all of the monitored processes to generate signals simultaneously,
or a telephone switching system by routing to it a large number of
simultaneous phone calls.
Web-based applications are common subjects of stress testing. Here,
you want to ensure that your application, and hardware, can handle a target
volume of concurrent users. You could argue that you may have millions
of people accessing the site at one time, but that is not realistic. You
need to define your audience then design a stress test to represent the maximum
number of users you think will use your site. (Chapter 10 provides
more information on testing Web-based applications.)
Similarly, you could stress a mobile device application—a mobile phone
operating system, for example—by launching multiple applications that
run and stay resident, then making or receiving one or more telephone
calls. You could launch a GPS navigation program, an application that uses
CPU and radio frequency (RF) resources almost continuously, then
attempt to use other applications or engage telephone calls. (Chapter 11
discusses testing mobile applications in more detail.)
Although many stress tests do represent conditions that the program
likely will experience during its operation, others may truly represent
‘‘never will occur’’ situations; but this does not imply that these tests are
124 The Art of Software Testing
not useful. If these impossible conditions detect errors, the test is valuable
because it is likely that the same errors might also occur in realistic, less
stressful situations.
Usability Testing
Another important test case area is usability, or user testing. Although this
testing technique is nearly 30 years old, it has become more important
with the advent of more GUI-based software and the deep penetration of
computer hardware and software into all aspects of our society. By tasking
the ultimate end user of an application with testing the software in a realworld
environment, potential problems can be discovered that even the
most aggressive automated testing routing likely wouldn’t find. This area
of software testing is so important we will cover it further in the next
chapter.
Security Testing
In response to society’s growing concern about privacy, many programs
now have specific security objectives. Security testing is the process of attempting
to devise test cases that subvert the program’s security checks.
For example, you could try to formulate test cases that get around an operating
system’s memory protection mechanism. Similarly, you could try to
subvert a database system’s data security mechanisms. One way to devise
such test cases is to study known security problems in similar systems and
generate test cases that attempt to demonstrate comparable problems in
the system you are testing. For example, published sources in magazines,
chat rooms, or newsgroups frequently cover known bugs in operating systems
or other software systems. By searching for security holes in existing
programs that provide services similar to the one you are testing, you can
devise test cases to determine whether your program suffers from the same
kind of problems.
Web-based applications often need a higher level of security testing
than do most applications. This is especially true of e-commerce sites.
Although sufficient technology, namely encryption, exists to allow customers
to complete transactions securely over the Internet, you should not
rely on the mere application of technology to ensure safety. In addition,
you will need to convince your customer base that your application is safe,
Higher-Order Testing 125
or you risk losing customers. Again, Chapter 10 provides more information
on security testing in Internet-based applications.
Performance Testing
Many programs have specific performance or efficiency objectives, stating
such properties as response times and throughput rates under certain
workload and configuration conditions. Again, since the purpose of a system
test is to demonstrate that the program does not meet its objectives,
test cases must be designed to show that the program does not satisfy its
performance objectives.
Storage Testing
Similarly, programs occasionally have storage objectives that state, for
example, the amount of system memory the program uses and the size of
temporary or log files. You need to verify that your program can control its
use of system memory so it does not negatively impact other processes
running on the host. The same holds for physical files on the file system.
Filling a disk drive can cause significant downtime. You should design test
cases to show that these storage objectives have not been met.
Configuration Testing
Programs such as operating systems, database management systems, and
messaging programs support a variety of hardware configurations, including
various types and numbers of I/O devices and communications lines,
or different memory sizes. Often, the number of possible configurations is
too large to test each one, but at the least, you should test the program with
each type of hardware device and with the minimum and maximum configuration.
If the program itself can be configured to omit program components,
or if the program can run on different computers, each possible
configuration of the program should be tested.
Today, many programs are designed for multiple operating systems.
Thus, when testing such a program, you should do so on all of the operating
systems for which it was designed. Programs designed to execute
within a Web browser require special attention, since there are numerous
Web browsers available and they don’t all function the same way. In
126 The Art of Software Testing
addition, the same Web browser will operate differently on different operating
systems.
Compatibility/Conversion Testing
Most programs that are developed are not completely new; they often
are replacements for some deficient system. As such, programs often have
specific objectives concerning their compatibility with, and conversion
procedures from, the existing system. Again, in testing the program against
these objectives, the orientation of the test cases is to demonstrate that the
compatibility objectives have not been met and that the conversion procedures
do not work. Here you try to generate errors while moving data from
one system to another. An example would be upgrading a database system.
You want to ensure that the new release supports your existing data, just as
you need to validate that a new version of a word processing application
supports its previous document formats. Various methods exist to test
this process; however, they are highly dependent on the database system
you employ.
Installation Testing
Some types of software systems have complicated installation procedures.
Testing the installation procedure is an important part of the system testing
process. This is particularly true of an automated installation system that is
part of the program package. A malfunctioning installation program could
prevent the user from ever having a successful experience with the main
system you are testing. A user’s first experience is when he or she installs
the application. If this phase performs poorly, then the user/customer may
find another product, or have little confidence in the application’s validity.
Reliability Testing
Of course, the goal of all types of testing is the improvement of the program
reliability, but if the program’s objectives contain specific statements
about reliability, specific reliability tests might be devised. Testing reliability
objectives can be difficult. For example, a modern online system such
as a corporate wide area network (WAN) or an Internet service provider
(ISP) generally has a targeted uptime of 99.97 percent over the life of the
Higher-Order Testing 127
system. There is no known way that you could test this objective within a
test period of months or even years. Today’s critical software systems have
even higher reliability standards, and today’s hardware conceivably should
support these objectives. You potentially can test programs or systems with
more modest mean time between failures (MTBF) objectives or reasonable
(in terms of testing) operational error objectives.
An MTBF of no more than 20 hours, or an objective that a program
should experience no more than 12 unique errors after it is placed into
production, for example, presents testing possibilities, particularly for statistical,
program-proving, or model-based testing methodologies. These
methods are beyond the scope of this book, but the technical literature
(online and otherwise) offers ample guidance in this area. For example, if
this area of program testing is of interest to you, research the concept of
inductive assertions. The goal of this method is the development of a set of
theorems about the program in question, the proof of which guarantees the
absence of errors in the program. The method begins by writing assertions
about the program’s input conditions and correct results. The assertions
are expressed symbolically in a formal logic system, usually the first-order
predicate calculus. You then locate each loop in the program and, for each
loop, write an assertion stating the invariant (always true) conditions at an
arbitrary point in the loop. The program now has been partitioned into a
fixed number of fixed-length paths (all possible paths between a pair of
assertions). For each path, you then take the semantics of the intervening
program statements to modify the assertion, and eventually reach the end
of the path. At this point, two assertions exist at the end of the path: the
original one and the one derived from the assertion at the opposite end.
You then write a theorem stating that the original assertion implies the derived
assertion, and attempt to prove the theorem. If the theorems can be
proved, you could assume the program is error free—as long as the program
eventually terminates. A separate proof is required to show that the
program will always eventually terminate.
As complex as this sort of software proving or prediction sounds, reliability
testing and, indeed, the concept of software reliability engineering
(SRE) are with us today and are increasingly important for systems
that must maintain very high uptimes. To illustrate this point, examine
Table 6.2 to see the number of hours per year a system must be up to
support various uptime requirements. These values should indicate the
need for SRE.
128 The Art of Software Testing
Recovery Testing
Programs such as operating systems, database management systems, and
teleprocessing programs often have recovery objectives that state how the
system is to recover from programming errors, hardware failures, and data
errors. One objective of the system test is to show that these recovery functions
do not work correctly. Programming errors can be purposely injected
into a system to determine whether it can recover from them. Hardware
failures such as memory parity errors or I/O device errors can be simulated.
Data errors such as noise on a communications line or an invalid
pointer in a database can be created purposely or simulated to analyze the
system’s reaction.
One design goal of such systems is to minimize the mean time to recovery
(MTTR). Downtime often causes a company to lose revenue because
the system is inoperable. One testing objective is to show that the system
fails to meet the service-level agreement for MTTR. Often, the MTTR will
have an upper and lower boundary, so your test cases should reflect these
bounds.
Serviceability/Maintenance Testing
The program also may have objectives for its serviceability or maintainability
characteristics. All objectives of this sort must be tested. Such objectives
might define the service aids to be provided with the system,
including storage dump programs or diagnostics, the mean time to debug
an apparent problem, the maintenance procedures, and the quality of internal
logic documentation.
TABLE 6.2 Hours per Year for Various Uptime Requirements
Uptime Percent Requirements Operational Hours per Year
100 8760.0
99.9 8751.2
98 8584.8
97 8497.2
96 8409.6
95 8322.0
Higher-Order Testing 129
Documentation Testing
As we illustrated in Figure 6.4, the system test also is concerned with the
accuracy of the user documentation. The principal way of accomplishing
this test is to use the documentation to determine the representation of the
prior system test cases. That is, once a particular stress case is devised, you
would use the documentation as a guide for writing the actual test case.
Also, the user documentation itself should be the subject of an inspection
(similar to the concept of the code inspection in Chapter 3), to check it for
accuracy and clarity. Any examples illustrated in the documentation
should be encoded into test cases and fed to the program.
Procedure Testing
Finally, many programs are parts of larger, not completely automated systems
involving procedures people perform. Any prescribed human procedures,
such as those for the system operator, database administrator, or
end user, should be tested during the system test.
For example, a database administrator should document procedures for
backing up and recovering the database system. If possible, a person not
associated with the administration of the database should test the procedures.
However, a company must create the resources needed to adequately
test the procedures. These resources often include hardware and
additional software licensing.
Performing the System Test
One of the most vital considerations in implementing the system test is
determining who should do it. To answer this in a negative way, (1) programmers
should not perform a system test; and (2) of all the testing
phases, this is the one that the organization responsible for developing the
programs definitely should not perform.
The first point stems from the fact that a person performing a system
test must be capable of thinking like an end user, which implies a thorough
understanding of the attitudes and environment of the end user and
of how the program will be used. Obviously, then, if feasible, a good testing
candidate is one or more end users. However, because the typical end
user will not have the ability or expertise to perform many of the
130 The Art of Software Testing
categories of tests described earlier, an ideal system test team might be
composed of a few professional system test experts (people who spend
their lives performing system tests), a representative end user or two, a
human-factors engineer, and the key original analysts or designers of the
program. Including the original designers does not violate principle 2
from Table 2.1, ‘‘Vital Program Testing Guidelines,’’ recommending
against testing your own program, since the program has probably passed
through many hands since it was conceived. Therefore, the original designers
do not have the troublesome psychological ties to the program
that motivated this principle.
The second point stems from the fact that a system test is an ‘‘anything
goes, no holds barred’’ activity. Again, the development organization has
psychological ties to the program that are counter to this type of activity.
Also, most development organizations are most interested in having the
system test proceed as smoothly as possible and on schedule, hence
are not truly motivated to demonstrate that the program does not meet
its objectives. At the least, the system test should be performed by an
independent group of people with few, if any, ties to the development
organization.
Perhaps the most economical way of conducting a system test (economical
in terms of finding the most errors with a given amount of money, or
spending less money to find the same number of errors), is to subcontract
the test to a separate company. We talk about this more in the last section
of this chapter.
Acceptance Testing
Returning to the overall model of the development process shown in
Figure 6.3, you can see that acceptance testing is the process of comparing
the program to its initial requirements and the current needs of its
end users. It is an unusual type of test in that it usually is performed by
the program’s customer or end user and normally is not considered the
responsibility of the development organization. In the case of a contracted
program, the contracting (user) organization performs the acceptance
test by comparing the program’s operation to the original contract.
As is the case for other types of testing, the best way to do this is to devise
test cases that attempt to show that the program does not meet the contract;
if these test cases are unsuccessful, the program is accepted. In the
Higher-Order Testing 131
case of a program product, such as a computer manufacturer’s operating
system, or a software company’s database system, the sensible customer
first performs an acceptance test to determine whether the product satisfies
its needs.
Although the ultimate acceptance test is, indeed, the responsibility of
the customer or end user, the savvy developer will conduct user tests during
the development cycle and prior to delivering the finished product to
the end user or contract customer. See Chapter 7 for more information on
user or usability testing.
Installation Testing
The remaining testing process in Figure 6.3 is the installation test. Its position
in the figure is a bit unusual, since it is not related, as all of the other
testing processes are, to specific phases in the design process. It is an unusual
type of testing because its purpose is not to find software errors but
to find errors that occur during the installation process.
Many events occur when installing software systems. A short list of
examples includes the following:
  User must select a variety of options.
  Files and libraries must be allocated and loaded.
  Valid hardware configurations must be present.
  Programs may need network connectivity to connect to other
programs.
The organization that produced the system should develop the installation
tests, which should be delivered as part of the system, and run after
the system is installed. Among other things, the test cases might check to
ensure that a compatible set of options has been selected, that all parts of
the system exist, that all files have been created and have the necessary
contents, and that the hardware configuration is appropriate.
Test Planning and Control
If you consider that the testing of a large system could entail writing, executing,
and verifying tens of thousands of test cases, handling thousands
132 The Art of Software Testing
of modules, repairing thousands of errors, and employing hundreds of
people over a time span of a year or more, it is apparent that you are faced
with an immense project management challenge in planning, monitoring,
and controlling the testing process. In fact, the problem is so enormous
that we could devote an entire book to just the management of software
testing. The intent of this section is to summarize some of these
considerations.
As mentioned in Chapter 2, the major mistake most often made in planning
a testing process is the tacit assumption that no errors will be found.
The obvious result of this mistake is that the planned resources (people,
calendar time, and computer time) will be grossly underestimated, a notorious
problem in the computing industry. Compounding the problem is
the fact that the testing process falls at the end of the development cycle,
meaning that resource changes are difficult. A second, perhaps more insidious
problem is that the wrong definition of testing is being used, since it is
difficult to see how someone using the correct definition of testing (the
goal being to find errors) would plan a test using the assumption that no
errors will be found.
As is the case for most undertakings, the plan is the crucial part of the
management of the testing process. The components of a good test plan are
as follows:
1. Objectives. The objectives of each testing phase must be defined.
2. Completion criteria. Criteria must be designed to specify when each
testing phase will be judged to be complete. This matter is discussed
in the next section.
3. Schedules. Calendar time schedules are needed for each phase. They
should indicate when test cases will be designed, written, and executed.
Some software methodologies such as Extreme Programming
(discussed in Chapter 9) require that you design the test cases and
unit tests before application coding begins.
4. Responsibilities. For each phase, the people who will design, write,
execute, and verify test cases, and the people who will repair discovered
errors, should be identified. And, because in large projects disputes
inevitably arise over whether particular test results represent
errors, an arbitrator should be identified.
5. Test case libraries and standards. In a large project, systematic methods
of identifying, writing, and storing test cases are necessary.
Higher-Order Testing 133
6. Tools. The required test tools must be identified, including a plan for
who will develop or acquire them, how they will be used, and when
they will be needed.
7. Computer time. This is a plan for the amount of computer time
needed for each testing phase. It would include servers used for compiling
applications, if required; desktop machines required for installation
testing; Web servers for Web-based applications; networked
devices, if required; and so forth.
8. Hardware configuration. If special hardware configurations or devices
are needed, a plan is required that describes the requirements, how
they will be met, and when they will be needed.
9. Integration. Part of the test plan is a definition of how the program
will be pieced together (e.g., incremental top-down testing). A system
containing major subsystems or programs might be pieced together
incrementally, using the top-down or bottom-up approach, for
instance, but where the building blocks are programs or subsystems,
rather than modules. If this is the case, a system integration plan is
necessary. The system integration plan defines the order of integration,
the functional capability of each version of the system, and responsibilities
for producing ‘‘scaffolding,’’ code that simulates the
function of nonexistent components.
10. Tracking procedures. You must identify means to track various aspects
of the testing progress, including the location of error-prone modules
and estimation of progress with respect to the schedule, resources,
and completion criteria.
11. Debugging procedures. You must define mechanisms for reporting detected
errors, tracking the progress of corrections, and adding the
corrections to the system. Schedules, responsibilities, tools, and computer
time/resources also must be part of the debugging plan.
12. Regression testing. Regression testing is performed after making a
functional improvement or repair to the program. Its purpose is to
determine whether the change has regressed other aspects of the program.
It usually is performed by rerunning some subset of the program’s
test cases. Regression testing is important because changes
and error corrections tend to be much more error prone than the
original program code (in much the same way that most typographical
errors in newspapers are the result of last-minute editorial
134 The Art of Software Testing
changes, rather than changes in the original copy). A plan for regression
testing—who, how, when—also is necessary.
Test Completion Criteria
One of the most difficult questions to answer when testing a program is
determining when to stop, since there is no way of knowing if the error
just detected is the last remaining error. In fact, in anything but a small
program, it is unreasonable to expect that all errors will eventually be detected.
Given this dilemma, and given the fact that economics dictate that
testing must eventually terminate, you might wonder if the question has to
be answered in a purely arbitrary way, or if there are some useful stopping
criteria.
The completion criteria typically used in practice are both meaningless
and counterproductive. The two most common criteria are these:
1. Stop when the scheduled time for testing expires.
2. Stop when all the test cases execute without detecting errors—that is,
stop when the test cases are unsuccessful.
The first criterion is useless because you can satisfy it by doing absolutely
nothing. It does not measure the quality of the testing. The second
criterion is equally useless because it also is independent of the quality of
the test cases. Furthermore, it is counterproductive because it subconsciously
encourages you to write test cases that have a low probability
of detecting errors.
As discussed in Chapter 2, humans are highly goal oriented. If you are
told that you have finished a task when the test cases are unsuccessful, you
will subconsciously write test cases that lead to this goal, avoiding the useful,
high-yield, destructive test cases.
There are three categories of more useful criteria. The first category, but
not the best, is to base completion on the use of specific test-case design
methodologies. For instance, you might define the completion of module
testing as the following:
The test cases are derived from (1) satisfying the multiconditioncoverage
criterion and (2) a boundary value analysis of the module
Higher-Order Testing 135
interface specification, and all resultant test cases are eventually
unsuccessful.
You might define the function test as being complete when the following
conditions are satisfied:
The test cases are derived from (1) cause-effect graphing, (2) boundary
value analysis, and (3) error guessing, and all resultant test cases are
eventually unsuccessful.
Although this type of criterion is superior to the two mentioned earlier,
it has three problems. First, it is not helpful in a test phase in which specific
methodologies are not available, such as the system test phase. Second,
it is a subjective measurement, since there is no way to guarantee that
a person has used a particular methodology, such as boundary value analysis,
properly and rigorously. Third, rather than setting a goal and then letting
the tester choose the best way of achieving it, it does the opposite; testcase-
design methodologies are dictated, but no goal is given. Hence, this
type of criterion is useful sometimes for some testing phases, but it should
be applied only when the tester has proven his or her abilities in the past in
applying the test-case design methodologies successfully.
The second category of criteria—perhaps the most valuable one—is to
state the completion requirements in positive terms. Since the goal of testing
is to find errors, why not make the completion criterion the detection
of some predefined number of errors? For instance, you might state that a
module test of a particular module is not complete until three errors have
been discovered. Perhaps the completion criterion for a system test should
be defined as the detection and repair of 70 errors, or an elapsed time of
three months, whichever comes later.
Notice that, although this type of criterion reinforces the definition of
testing, it does have two problems, both of which are surmountable. One
problem is determining how to obtain the number of errors to be detected.
Obtaining this number requires the following three estimates:
1. An estimate of the total number of errors in the program.
2. An estimate of what percentage of these errors can feasibly be found
through testing.
136 The Art of Software Testing
3. An estimate of what fraction of the errors originated in particular design
processes, and during which testing phases these errors are
likely to be detected.
You can get a rough estimate of the total number of errors in several
ways. One method is to obtain them through experience with previous
programs. Also, a variety of predictive modules exist. Some of these require
you to test the program for some period of time, record the elapsed
times between the detection of successive errors, and insert these times
into parameters in a formula. Other modules involve the seeding of
known, but unpublicized, errors into the program, testing the program for
a while, and then examining the ratio of detected seeded errors to detected
unseeded errors. Another model employs two independent test teams
whose members test for a while, examine the errors found by each and the
errors detected in common by both teams, and use these parameters to
estimate the total number of errors. Another gross method to obtain this
estimate is to use industrywide averages. For instance, the number of
errors that exist in typical programs at the time that coding is completed
(before a code walkthrough or inspection is employed) is approximately 4
to 8 errors per 100 program statements.
The second estimate from the preceding list (the percentage of errors
that can be feasibly found through testing) involves a somewhat arbitrary
guess, taking into consideration the nature of the program and the consequences
of undetected errors.
Given the current paucity of information about how and when errors
are made, the third estimate is the most difficult. The data that exist indicate
that in large programs, approximately 40 percent of the errors are coding
and logic design mistakes, and that the remainder are generated in the
earlier design processes.
To use this criterion, you must develop your own estimates that are pertinent
to the program at hand. A simple example is presented here. Assume
we are about to begin testing a 10,000-statement program, that the number
of errors remaining after code inspections are performed is estimated at
5 per 100 statements, and we establish, as an objective the detection of
98 percent of the coding and logic design errors and 95 percent of the
design errors. The total number of errors is thus estimated at 500. Of the
500 errors, we assume that 200 are coding and logic design errors and
Higher-Order Testing 137
300 are design flaws. Hence, the goal is to find 196 coding and logic design
errors and 285 design errors. A plausible estimate of when the errors are
likely to be detected is shown in Table 6.3.
If we have scheduled four months for function testing and three months
for system testing, the following three completion criteria might be
established:
1. Module testing is complete when 130 errors are found and corrected
(65 percent of the estimated 200 coding and logic design errors).
2. Function testing is complete when 240 errors (30 percent of 200
plus 60 percent of 300) are found and corrected, or when four
months of function testing have been completed, whichever occurs
later. The reason for the second clause is that if we find 240 errors
quickly, it is probably an indication that we have underestimated
the total number of errors and thus should not stop function testing
early.
3. System testing is complete when 111 errors are found and corrected,
or when three months of system testing have been completed, whichever
occurs later.
The other obvious problem with this type of criterion is one of overestimation.
What if, in the preceding example, there are fewer than 240 errors
remaining when function testing starts? Based on the criterion, we could
never complete the function test phase.
This is a strange problem if you think about it: We do not have enough
errors; the program is too good. You could label it as not a problem because
it is the kind of problem a lot of people would love to have. If it does
occur, a bit of common sense can solve it. If we cannot find 240 errors in
four months, the project manager can employ an outsider to analyze the
TABLE 6.3 Hypothetical Estimate of When the Errors Might Be Found
Coding and Logic Design Errors Design Errors
Module test 65% 0%
Function test 30% 60%
System test 3% 35%
Total 98% 95%
138 The Art of Software Testing
test cases to judge whether the problem is (1) inadequate test cases or (2)
excellent test cases but a lack of errors to detect.
The third type of completion criterion is an easy one on the surface, but
it involves a lot of judgment and intuition. It requires you to plot the number
of errors found per unit time during the test phase. By examining the
shape of the curve, you can often determine whether to continue the test
phase or end it and begin the next test phase.
Suppose a program is being function-tested and the number of errors
found per week is being plotted. If, in the seventh week, the curve is the
top one of Figure 6.5, it would be imprudent to stop the function test,
even if we had reached our criterion for the number of errors to be found.
Since in the seventh week we still seem to be in high gear (finding many
errors), the wisest decision (remembering that our goal is to find errors) is
to continue function testing, designing additional test cases if necessary.
On the other hand, suppose the curve is the bottom one in Figure 6.5.
The error-detection efficiency has dropped significantly, implying that we
have perhaps picked the function test bone clean and that perhaps the best
move is to terminate function testing and begin a new type of testing (a
system test, perhaps). Of course, we must also consider other factors, such
as whether the drop in error-detection efficiency was due to a lack of computer
time or exhaustion of the available test cases.
Figure 6.6 is an illustration of what happens when you fail to plot the
number of errors being detected. The graph represents three testing phases
of an extremely large software system. An obvious conclusion is that the
project should not have switched to a different testing phase after period
6. During period 6, the error-detection rate was good (to a tester, the
higher the rate, the better), but switching to a second phase at this point
caused the error-detection rate to drop significantly.
The best completion criterion is probably a combination of the three
types just discussed. For the module test, particularly because most
projects do not formally track detected errors during this phase, the
best completion criterion is probably the first. You should request that
a particular set of test-case design methodologies be used. For the function
and system test phases, the completion rule might be to stop when
a predefined number of errors are detected or when the scheduled
time has elapsed, whichever comes later, but provided that an analysis
of the errors-versus-time graph indicates that the test has become
unproductive.
Higher-Order Testing 139
60
50
40
30
Errors Found
20
10
1 2 3 4
Week
5 6 7
0
60
50
40
30
Errors Found
20
10
1 2 3 4
Week
5 6 7
0
FIGURE 6.5 Estimating Completion by Plotting Errors Detected by
Unit Time.
140 The Art of Software Testing
The Independent Test Agency
Earlier in this chapter and in Chapter 2, we emphasized that an organization
should avoid attempting to test its own programs. Our reasoning is
that the organization responsible for developing a program has difficulty
in objectively testing the same program. The test organization should be as
far removed as possible, in terms of the structure of the company, from the
development organization. In fact, it is desirable that the test organization
not be part of the same company, for if it is, it is still influenced by the same
management pressures influencing the development organization.
One way to avoid this conflict is to hire a separate company for software
testing. This is a good idea, whether the company that designed the system
and will use it developed the system, or whether a third-party developer
produced the system. The advantages usually noted are increased motivation
in the testing process, a healthy competition with the development
organization, removal of the testing process from under the management
900
800
700
500
Errors Found per period
600
400
300
200
100
1 2 3 4 5 6 7 8 9 10 11 12 13
Two-week periods
0
FIGURE 6.6 Postmortem Study of the Testing Processes of a Large
Project.
Higher-Order Testing 141
control of the development organization, and the advantages of specialized
knowledge that the independent test agency brings to bear on the problem.
Summary
Higher-order testing could be considered the next step. We have discussed
and advocated the concept of module testing—using various techniques to
test software components, the building blocks that combine to form the
finished product. With individual components tested and debugged, it is
time to see how well they work together.
Higher-order testing is important for all software products, but it becomes
increasingly important as the size of the project increases. It stands
to reason that the more modules and the more lines of code a project contains,
the more opportunity exists for coding or even design errors.
Function testing attempts to uncover design errors, that is, discrepancies
between the finished program and its external specifications—a precise
description of the program’s behavior from the end-user’s perspective.
The system test, on the other hand, tests the relationship between the
software and its original objectives. System testing is designed to uncover
errors made during the process of translating program objectives into the
external specification and ultimately into lines of code. It is this translation
step where errors have the most far-reaching effects; likewise, it is the stage
in the development process that is most error prone. Perhaps the most difficult
part of system testing is designing the test cases. In general you want
to focus on main categories of testing, then get really creative in testing
these categories. Table 6.1 summarizes 15 categories we detailed in this
chapter that can guide your system testing efforts.
Make no mistake, higher-order testing certainly is an important part of
thorough software testing, but it also can become a daunting process, especially
for very large systems, such as an operating system. The key to success
is consistent and well-planned test planning. We introduce this topic
in this chapter, but if you are managing the testing of large systems, more
thought and planning will be required. One approach to handling this issue
is to hire an outside company for testing or for test management.
In Chapter 7 we expand on one important aspect of higher-order testing:
user or usability testing.
142 The Art of Software Testing
7 Usability (User)
Testing
An important category of system test cases is one that attempts to find
human-factor, or usability, problems. When the first edition of this
book was published, the computing industry mostly ignored the human
factors associated with computer software. Developers gave little attention
to how humans interacted with their software. That is not to say that there
were no developers testing applications at the user level. In the early
1980s, some—including developers at the Xerox Palo Alto Research Center
(PARC), for example—were conducting user-based software testing.
By 1987 or 1988, the three of us had become intimately involved in
usability testing of early personal computer hardware and software, when
we contracted with computer manufacturers to test and review their new
desktop computers prior to release to the public. Over perhaps two years,
this prerelease testing prevented potential usability problems with new
hardware and software designs. These early computer manufacturers obviously
were convinced that the time and expense required for this level of
user testing resulted in real marketing and financial advantages.
Usability Testing Basics
Today’s software systems—particularly those designed for a mass, commercial
market—generally have undergone extensive human-factor studies,
and modern programs, of course, benefit from the thousands of programs
and systems that have gone before. Nevertheless, an analysis of human
143
factors is still a highly subjective matter. Here’s our list of questions you
might ask to derive testing considerations:
1. Has each user interface been tailored to the intelligence, educational
background, and environmental pressures of the end user?
2. Are the outputs of the program meaningful, noninsulting to the user,
and devoid of computer gibberish?
3. Are the error diagnostics, such as error messages, straightforward, or
does the user need a PhD in computer science to comprehend them?
For instance, does the program produce such messages as IEK022A
OPEN ERROR ON FILE 'SYSIN' ABEND CODE¼102? Messages such as
these weren’t all that uncommon in software systems of the 1970s
and 1980s. Mass-market systems do better today in this regard, but
users still will encounter unhelpful messages such as, ‘‘An unknown
error has occurred,’’ or ‘‘This program has encountered an error and
must be restarted.’’
Programs you design yourself are under your control and should
not be plagued with such useless messages. Even if you didn’t design
the program, if you are on the testing team, you can push for improvements
in this area of the human interface.
4. Does the total set of user interfaces exhibit considerable conceptual
integrity, an underlying consistency, and uniformity of syntax, conventions,
semantics, format, style, and abbreviations?
5. Where accuracy is vital, such as in an online banking system, is sufficient
redundancy present in the input? For example, such a system
should ask for an account number, a customer name, and a personal
identification number (PIN) to verify that the proper person is
accessing account information.
6. Does the system contain an excessive number of options, or options
that are unlikely to be used? One trend in modern software is to present
to users only those menu choices they are most likely to use,
based on software testing and design considerations. Then a welldesigned
program can learn from individual users and begin to
present those menu items that they frequently access. Even with
such an intelligent menu system, successful programs still must be
designed so that accessing the various options is logical and intuitive.
7. Does the system return some type of immediate acknowledgment to
all inputs? Where a mouse click is the input, for example, the chosen
144 The Art of Software Testing
item can change color, or a button object can depress or be presented
in a raised format. If the user is expected to choose from a list, the
selected number should be presented on the screen when the choice
is made. Moreover, if the selected action requires some processing
time—which is frequently the case when the software is accessing a
remote system—then a message should be displayed informing the
user of what is going on. This level of testing sometimes is referred to
as component testing, whereby interactive software components are
tested for reasonable selection and user feedback.
8. Is the program easy to use? For example, is the input case-sensitive
without making this fact clear to the user? Also, if a program requires
navigation through a series of menus or options, is it clear how to
return to the main menu? Can the user easily move up or down one
level?
9. Is the design conducive to user accuracy? One test would be an analysis
of how many errors each user makes during data entry or when
choosing program options. Were these errors merely an inconvenience—
errors the user was able to correct—or did an incorrect
choice or action cause some kind of application failure?
10. Are the user actions easily repeated in later sessions? In other words,
is the software design conducive to the user learning how to be more
efficient in using the system?
11. Did the user feel confident while navigating the various paths or
menu choices? A subjective evaluation might be the user response to
using the application. At the end of the session did the user feel
stressed by or satisfied with the outcome? Would the user be likely to
choose this system for his or her own use, or recommend it to someone
else?
12. Did the software live up to its design promise? Finally, usability testing
should include an evaluation of the software specifications versus
the actual operation. From the user perspective—real people using
the software in a real-world environment—did the software perform
according to its specifications?
Usability or user-based testing basically is a black-box testing technique.
Recall from our discussion in Chapter 2 that black-box testing concentrates
on finding situations in which the program does not behave
according to specifications. In a black-box scenario you are not concerned
Usability (User) Testing 145
with the internal workings of the software, or even with understanding
program structure. Presented this way, usability testing obviously is an important
part of any development process. If users perceive, because of improper
design, a cumbersome user interface, or specifications missed or
ignored, that a given application does not perform according to its specifications,
the development process has failed. User testing should uncover
problems from design flaws to software ergonomics mistakes.
Usability Testing Process
It should be obvious from our list of items to test that usability testing is
more than simply seeking user opinions or high-level reactions to a software
application. When the errors have been found and corrected, and an application
is ready for release or for sale, focus groups can be used to elicit opinions
from users or potential purchasers. This is marketing and focusing.
Usability testing occurs earlier in the process and is much more involved.
Any usability test should begin with a plan. (Review our vital software
testing guidelines in Chapter 2, Table 2.1.) You should establish practical,
real-world, repeatable exercises for each user to conduct. Design these testing
scenarios to present the user with all aspects of the software, perhaps in
various or random order. For example, among the processes you might test
in a customer tracking application are:
  Locate an individual customer record and modify it.
  Locate a company record and modify it.
  Create a new company record.
  Delete a company record.
  Generate a list of all companies of a certain type.
  Print this list.
  Export a selected list of contacts to a text file or spreadsheet format.
  Import a text file or spreadsheet file of contacts from another
application.
  Add a photograph to one or more records.
  Create and save a custom report.
  Customize the menu structure.
During each phase of the test, have observers document the user experience
as they perform each task. When the test is complete, conduct an
146 The Art of Software Testing
interview with the user or provide a written questionnaire to document
other aspects of the user’s experience, such as his or her perception of usage
versus specification.
In addition, write down detailed instructions for user tests, to ensure
that each user starts with the same information, presented in the same
way. Otherwise, you risk coloring some of the tests if some users receive
different instructions.
Test User Selection
A complete usability testing protocol usually involves multiple tests from
the same users, as well as tests from multiple users. Why multiple tests
from the same users? One area we want to test is user recall, that is, how
much of what a user learns about software operation is retained from session
to session. Any new system presented to users for the first time will
require some time to learn, but if the design for a particular application is
consistent with the industry or technology with which the target user community
is familiar, the learning process should be fairly quick.
A user already familiar with computer-based engineering design, for
example, would expect any new software in this same industry to follow
certain conventions of terminology, menu design, and perhaps even color,
shading, and font usage. Certainly, a developer may stray from these conventions
purposefully to achieve perceived operational improvements, but
if the design goes too far afield from industry standards and expectations,
the software will take longer for new users to learn; in fact, user acceptance
may be so slow as to cause the application to be a commercial failure. If the
application is developed for a single client, such differences may result in
the client rejecting the design or requiring a complete user interface redesign.
Either result is a costly developer mistake.
Therefore, software targeted for a specific end-user type or industry
should be tested by what could be described as expert users, people already
familiar with this class of application in a real-world environment.
In contrast, software with a more general target market—mobile device
software, for example, or general-purpose Web pages—might better be
tested by users selected randomly. (Such test user selection sometimes is
referred to as hallway testing or hallway intercept testing, meaning that users
chosen for software testing are selected at random from folk passing by
in the hallway.)
Usability (User) Testing 147
How Many Users Do You Need?
When designing a usability test plan, the question ‘‘How many testers do I
need?’’ will come to the forefront. Hiring usability testers is often overlooked
in the development process, and can add an unexpected and expensive cost
to the project. You need to find the right number of testers who can identify
the most errors for the least amount of capital investment.
Intuitively, you may think that the more testers you use the better. After
all, if you have enough evaluators testing your product, then all the errors
should be found. First, as mentioned, this is expensive. Second, it can become
a logistics nightmare. Finally, it is unlikely that you can ever detect
100 percent of your application’s usability problems.
Fortunately, significant research on usability has been conducted during
the last 15 years. Based on the work of Jakob Nielsen, a usability testing
expert, you may need fewer testers than you think. Nielsen’s research
found that the number of usability problems found in testing is:
E ¼ 100   ð1   ð1   LÞ^ nÞ
where: E ¼percent of errors found
n ¼number of testers
L ¼percent of usability problems found by a tester
Using the equation with L ¼ 31 percent, a reasonable value Nielsen also
gleaned from his research, produces the graph shown in Figure 7.1.
Examining the graph reveals a few interesting points. First, as we intuitively
know, it will never be possible to detect all of the usability errors in
the application. It’s not theoretically possible, because the curve only converges
on 100 percent; it never actually reaches it. Second, you only need a
small number of testers. The graph shows that approximately 83 percent of
the errors are detected by only 5 testers.
From a project manager’s point of view, this is refreshing news. No longer
do you need to incur the cost and complexity of working with a large
group of testers to check your application. Instead, you can focus on designing,
executing, and analyzing your tests—putting your effort and
money into what will make the most difference.
Also with fewer testers, you have less analysis to do, so you can quickly
implement changes to the application and the testing methodology; then
148 The Art of Software Testing
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
# of Users
% of Errors Found
FIGURE 7.1 Percent Errors Found Versus Number of Users.
test again with a new group of testers. In this iterative fashion you can
ensure that you catch most problems at minimal cost and time.
Nielsen’s research was conducted in the early 1990s while he was a systems
analyst at Sun Microsystems. On the one hand, his data and approach
to usability testing provides concrete guidance to those of us involved in
software design. On the other hand, since usability testing has become
more important and commonplace, and more evidence has been gathered
from practical testing and better formulaic analysis, some researchers have
come to question Nielsen’s firm statements that three to five users should
be enough.
Nielsen himself cautions that the precise number of testers depends on
economic considerations (how many testers will your budget support) and
on the type of system you are testing. Critical systems such as navigation
applications, banking or other financial software, or security related programs
will, per force, require closer user scrutiny than less-critical software.
Among the considerations important to developers who are designing a
usability testing program are whether the number of users and their individual
orientations represent sufficiently the total population of potential
users. In addition, as Nielsen notes, some programs are more complex
than others, meaning that detecting a significantly large percentage of
errors will be more difficult. And, since different users, because of their
backgrounds and experiences, are likely to detect different types of errors,
an individual testing situation may dictate a larger number of testers.
As with any testing methodology, it is up to the developers and project
administrators to design the tests, present a reasonable budget, evaluate
interim results, and conduct regressive tests as appropriate to the software
system, the overall project, and the client.
Data-Gathering Methods
Test administrators or observers can gather test results in several ways.
Videotaping a user test and using a think-aloud protocol can provide excellent
data on software usability and user perceptions about the application.
A think-aloud protocol involves users speaking aloud their thoughts and
observations while they are performing the assigned software testing tasks.
Using this process, the test participants describe out loud their task, what
they are thinking about the task, and/or whatever else comes to their mind
as they move through the testing scenario. Even when using think-aloud
150 The Art of Software Testing
protocol testing, developers may want to follow up with participants after
the test to get posttest comments, feelings, and observations. Taken together,
these two levels of user thoughts and comments can provide valuable
feedback to developers for software corrections or improvements.
A disadvantage to the think-aloud process, where videotaping or observers
are involved, is the possibility that the user experience will be
clouded or modified by the unnatural user environment. Developers also
may wish to conduct remote user testing, whereby the application is installed
at the testing user’s business where the software may ultimately be
applied. Remote testing has the advantage of placing the user in a familiar
environment, one in which the final application likely would be used, thus
removing the potential for external influences modifying test results. Of
course, the disadvantage is that developers may not receive feedback as
detailed as would be possible with a think-aloud protocol.
Nevertheless, in a remote testing environment, accurate user data still
can be gathered. Additional software can be installed with the application
to be tested to gather user keystrokes and to capture time required for the
user to complete each assigned task. This requires additional development
time (and more software), but the results of such tests can be enlightening
and very detailed.
In the absence of timing or keystroke capture software, testing users can
be tasked with writing down the start and end times of each assigned task,
along with brief one-word or short-phrase comments during the process.
Posttest questionnaires or interviews can help users recall their thoughts
and opinions about the software.
A sophisticated but potentially useful data-gathering protocol is eye
tracking. When we read a printed page, view a graphical presentation, or
interact with a computer screen, our eyes move over the scanned material
in particular patterns. Research data gathered on eye movement over more
than 100 years shows that eye movement—particularly how long an observer
pauses on certain visual elements—reflects at least to some degree
the thought processes of the observer. Tracking this eye movement, which
can be done with video systems and other technologies, shows researchers
which visual elements attract the observers attention, in what order, and
for how long. Such data is potentially useful in determining the efficiency
of software screens presented to users.
Despite extensive research during the last half of the twentieth century,
however, some controversy remains over the ultimate value of eye
Usability (User) Testing 151
movement research in specific applications. Still, coupled with other user
testing techniques, where developers need the deepest possible user input
data to ensure the highest level of software efficiency (weapons guidance
systems, robotic control systems, vehicle controls or other system that require
fast and accurate responses), eye tracking can be a useful tool.
Usability Questionnaire
As with the software testing procedure itself, a usability questionnaire
should be carefully planned to return the information required from the
associated test procedure. Although you may want to include some questions
that elicit free-form comments from the user, in general you want to
develop questionnaires that generate responses that can be counted and
analyzed across the spectrum of testers. These fall into three general types:
  Yes/no answers
  True/false answers
  Agree/disagree on a scale
For example, instead of asking ‘‘What is your opinion of the main menu
system,’’ you might ask a series of questions that require an answer from
1 to 5, where 5 is totally agree and 1 is totally disagree:
1. The main menu was easy to navigate.
2. It was easy to find the proper software operation from the main
menu.
3. The screen design led me quickly to the correct software operational
choices.
4. Once I had operated the system, it was easy to remember how to repeat
my actions.
5. The menu operations did not provide enough feedback to verify my
choices.
6. The main menu was more difficult to navigate than other similar programs
I use.
7. I had difficulty repeating previously accomplished operations.
Notice that it may be good practice to ask the same question more
than once, but present it from the opposite perspective so that one elicits
152 The Art of Software Testing
a negative response and the other a positive one. Such practice can
ensure that the user understood the question and that perceptions remain
constant. In addition, you want to separate the user questionnaire
into sections that correspond to software areas tested or to the testing
tasks assigned.
Experience will teach you quickly which types of questions are conducive
to data analysis and which ones aren’t very useful. Statistical analysis
software is available to help capture and interpret data. With a small number
of testing users, the usability test results may be obvious; or you might
develop an ad hoc analysis routine within a spreadsheet application to better
document results. For large software systems that undergo extensive
testing with a large user base, statistical software may help uncover trends
that aren’t obvious with manual interpretation methods.
When Is Enough, Enough?
How do you plan usability testing so that all aspects of the software are
reasonably tested while staying within an acceptable budget? The answer
to that question, of course, depends in part on the complexity of the system
or unit being tested. If budget and time allow, it is advisable to test
software in stages, as each segment is completed. If individual components
have been tested throughout the development process, then the final series
of tests need only test the integrated operation of the parts.
Additionally, you may design component tests, which are intended to test
the usability of an interactive component, something that requires user input
and that responds to this input in a user-perceivable way. This kind of
feedback testing can help improve the user experience, reduce operational
errors, and improve software consistency. Again, if you have tested a software
system at this level as the user interface was being designed, you will
have collected a significant body of important testing and operational
knowledge before total system testing begins.
How many individual users should test your software? Again, system
complexity and initial test results should dictate the number of individual
testers. For example, if three or five (or some reasonable number) of users
have difficulty navigating from the opening screen to screens that support
the assigned tasks, and if these users are sufficiently representative of the
target market, then you likely have enough information to tell you that the
user interface needs more design work.
Usability (User) Testing 153
A reasonable corollary to this might be that if none of the initial testers
have a problem navigating through their assigned tasks, and none uncover
any mistakes or malfunctions, then perhaps the testing pool is too small.
After all, is it reasonable to assume that usability tests of a reasonably complex
software system will uncover no errors or required changes? Recall
principle 6, from Table 2.1: Examining a program to see if it does not do
what it is supposed to do is only half the battle; the other half is seeing whether
the program does what it is not supposed to do. There’s a subtle difference in
this comparison. You might find that a series of users determine that a program
does, in fact, seem to do what it is supposed to do. They find no
errors or problems in working through the software. But have they also
proven that the program isn’t doing anything it is not supposed to do? If
things appear to be running too smoothly during initial testing, it probably
is time for more tests.
We don’t believe there is a formula that tells you how many tests each
user should conduct, or how many iterations of each test should be required.
We do believe, however, that careful analysis and understanding of
the results you gather from some reasonable number of testers and tests
can guide you to the answer of when enough testing is enough.
Summary
Modern software, coupled with the pressure of intense competition and
tight deadlines, make user testing of any software product crucial to successful
development. It stands to reason that the targeted software user can
be a valuable asset during testing. The knowledgeable user can determine
whether the product meets the goal of its design, and by conducting realworld
tasks can find errors of commission and omission.
Depending on the software target market, developers also may benefit
from selecting random users—persons who are not familiar with the program’s
specification, or perhaps even the industry or market for which it is
intended—who can uncover errors or user interface problems. For the
same reason that the developers don’t make good error testers, expert users
may avoid operational areas that might produce problems because they
know how the software is supposed to work. Over many years of software
development we have discovered one unavoidable testing truth: The software
the developer has tested for many hours can be broken easily, and in a
154 The Art of Software Testing
short time, by an unsophisticated user who attempts a task for which the
user interface or the software was not designed.
Remember, too, that a key to successful user (or usability) testing is accurate
and detailed data gathering and analysis. The data-gathering process
actually begins with the development of detailed user instructions
and a task list. It ends by compiling results from user observation or posttest
questionnaires.
Finally, the testing results must be interpreted, and then developers
must effect software changes identified from the data. This may be an iterative
process wherein the same testing users are asked to complete similar
tasks after identified software changes have been completed.
Usability (User) Testing 155

8 Debugging
In brief, debugging is what you do after you have executed a successful
test case. Remember that a successful test case is one that shows that a
program does not do what it was designed to do. Debugging is a two-step
process that begins when you find an error as a result of a successful test
case. Step 1 is the determination of the exact nature and location of the
suspected error within the program. Step 2 consists of fixing the error.
As necessary and integral as debugging is to program testing, it seems to
be the one aspect of the software production process that programmers
enjoy the least, for these reasons primarily:
  Your ego may get in the way. Like it or not, debugging confirms that
programmers are not perfect; they commit errors in either the design
or the coding of the program.
  You may run out of steam. Of all the software development activities,
debugging is the most mentally taxing activity. Moreover, debugging
usually is performed under a tremendous amount of organizational or
self-induced pressure to fix the problem as quickly as possible.
  You may lose your way. Debugging is mentally taxing because the
error you’ve found could occur in virtually any statement within the
program. Without examining the program first, you can’t be absolutely
sure, for example, that the origin of a numerical error in a paycheck
produced by a payroll program is not a subroutine that asks the
operator to load a particular form into the printer. Contrast this with
157
the debugging of a physical system, such as an automobile. If a car
stalls when moving up an incline (the symptom), you can immediately
and validly eliminate as the cause of the problem certain parts
of the system—the AM/FM radio, for example, or the speedometer or
the trunk lock. The problem must be in the engine; and, based on
our overall knowledge of automotive engines, we can even rule out
certain engine components such as the water pump and the oil filter.
  You may be on your own. Compared to other software development
activities, comparatively little research, literature, and formal instruction
exist on the process of debugging.
Although this is a book about software testing, not debugging, the two
processes are obviously related. Of the two aspects of debugging, locating
the error and correcting it, locating the error represents perhaps 95 percent
of the problem. Hence, this chapter concentrates on the process of finding
the location of an error, given that a successful test case has found one.
Debugging by Brute Force
The most common scheme for debugging a program is the so-called bruteforce
method. It is popular because it requires little thought and is the least
mentally taxing of the methods; unfortunately, it is inefficient and generally
unsuccessful.
Brute-force methods can be partitioned into at least three categories:
  Debugging with a storage dump.
  Debugging according to the common suggestion to ‘‘scatter print
statements throughout your program.’’
  Debugging with automated debugging tools.
The first, debugging with a storage dump (usually a crude display of all
storage locations in hexadecimal or octal format) is the most inefficient of
the brute-force methods. Here’s why:
  It is difficult to establish a correspondence between memory locations
and the variables in a source program.
  With any program of reasonable complexity, such a memory dump
will produce a massive amount of data, most of which is irrelevant.
158 The Art of Software Testing
  A memory dump is a static picture of the program, showing the state
of the program at only one instant in time; to find errors, you have to
study the dynamics of a program (state changes over time).
  A memory dump is rarely produced at the exact point of the error, so
it doesn’t show the program’s state at the point of the error. Program
actions between the time of the dump and the time of the error can
mask the clues you need to find the error.
  Adequate methodologies don’t exist for finding errors by analyzing
a memory dump (so many programmers stare, with glazed eyes,
wistfully expecting the error to expose itself magically from the
program dump).
Scattering statements throughout a failing program to display variable
values isn’t much better. It may be better than a memory dump because
it shows the dynamics of a program and lets you examine information
that is easier to relate to the source program, but this method, too, has
many shortcomings:
  Rather than encouraging you to think about the problem, it is largely
a hit-or-miss method.
  It produces a massive amount of data to be analyzed.
  It requires you to change the program; such changes can mask the
error, alter critical timing relationships, or introduce new errors.
  It may work on small programs, but the cost of using it in large
programs is quite high. Furthermore, it often is not even feasible on
certain types of programs such as operating systems or process
control programs.
Automated debugging tools work similarly to inserting print statements
within the program, but rather than making changes to the program, you
analyze the dynamics of the program with the debugging features of the
programming language or special interactive debugging tools. Typical language
features that might be used are facilities that produce printed traces
of statement executions, subroutine calls, and/or alterations of specified
variables. A common capability and function of debugging tools is to set
breakpoints that cause the program to be suspended when a particular
statement is executed or when a particular variable is altered, enabling the
programmer to examine the current state of the program. This method,
Debugging 159
too, is largely hit or miss, however, and often results in an excessive
amount of irrelevant data.
The general problem with these brute-force methods is that they ignore
the process of thinking. You can draw an analogy between program debugging
and solving a homicide. In virtually all murder mystery novels, the
crime is solved by careful analysis of the clues and by piecing together
seemingly insignificant details. This is not a brute-force method; setting
up roadblocks or conducting property searches would be.
There also is some evidence to indicate that whether the debugging
teams are composed of experienced programmers or students, people who
use their brains rather than a set of aids work faster and more accurately in
finding program errors. Therefore, we could recommend brute-force
methods only: (1) when all other methods fail, or (2) as a supplement to,
not a substitute for, the thought processes we’ll describe next.
Debugging by Induction
It should be obvious that careful thought will find most errors without the
debugger even going near the computer. One particular thought process is
induction, where you move from the particulars of a situation to the whole.
That is, start with the clues (the symptoms of the error and possibly the
results of one or more test cases) and look for relationships among the
clues. The induction process is illustrated in Figure 8.1.
FIGURE 8.1 The Inductive Debugging Process.
160 The Art of Software Testing
The steps are as follows:
1. Locate the pertinent data. A major mistake debuggers make is failing
to take account of all available data or symptoms about the problem.
Therefore, the first step is the enumeration of all you know about
what the program did correctly and what it did incorrectly—the
symptoms that led you to believe there was an error. Additional
valuable clues are provided by similar, but different, test cases that
do not cause the symptoms to appear.
2. Organize the data. Remember that induction implies that you’re
processing from the particulars to the general, so the second step is
to structure the pertinent data to let you observe the patterns. Of
particular importance is the search for contradictions, events such as
the error occurs only when the customer has no outstanding balance
in his or her margin account.
You can use a form such as the one shown in Figure 8.2 to structure
the available data. In the ‘‘what’’ boxes list the general symptoms;
in the ‘‘where’’ boxes describe where the symptoms were observed; in
the ‘‘when’’ boxes list anything you know about the times when the
symptoms occurred; and in the ‘‘to what extent’’ boxes describe the
scope and magnitude of the symptoms. Notice the ‘‘is’’ and ‘‘is not’’
FIGURE 8.2 A Method for Structuring the Clues.
Debugging 161
columns: In them describe the contradictions that may eventually
lead to a hypothesis about the error.
3. Devise a hypothesis. Next, study the relationships among the clues, and
devise, using the patterns that might be visible in the structure of the
clues, one or more hypotheses about the cause of the error. If you can’t
devise a theory, more data are needed, perhaps from new test cases. If
multiple theories seem possible, select the more probable one first.
4. Prove the hypothesis. A major mistake at this point, given the pressures
under which debugging usually is performed, is to skip this
step and jump to conclusions to fix the problem. Resist this urge, for
it is vital to prove the reasonableness of the hypothesis before you
proceed. If you skip this step, you’ll probably succeed in correcting
only the problem symptom, not the problem itself. Prove the hypothesis
by comparing it to the original clues or data, making sure that
this hypothesis completely explains the existence of the clues. If it
does not, the hypothesis is invalid, the hypothesis is incomplete, or
multiple errors are present.
5. Fix the problem. You can proceed with fixing the problem once you
complete the previous steps. By taking the time to fully work through
each step, you can feel confident that your fix will correct the bug.
Remember though, that you still need to perform some type of regression
testing to ensure your bug fix didn’t create problems in other
program areas. As the application grows larger, so does the likelihood
that your fix will cause problems elsewhere.
As a simple example, assume that an apparent error has been reported
in the examination grading program described in Chapter 4. The apparent
error is that the median grade seems incorrect in some, but not all,
instances. In a particular test case, 51 students were graded. The mean
score was correctly printed as 73.2, but the median printed was 26 instead
of the expected value of 82. By examining the results of this test case and a
few other test cases, the clues are organized as shown in Figure 8.3.
The next step is to derive a hypothesis about the error by looking for
patterns and contradictions. One contradiction we see is that the error
seems to occur only in test cases that use an odd number of students. This
might be a coincidence, but it seems significant, since you compute a
median differently for sets of odd and even numbers. There’s another
strange pattern: In some test cases, the calculated median always is less
162 The Art of Software Testing
than or equal to the number of students (26   51 and 1   1). One possible
avenue at this point is to run the 51-student test case again, giving the students
different grades from before to see how this affects the median calculation.
If we do so, the median is still 26, so the ‘‘to what extent! is not’’
box could be filled in with, ‘‘The median seems to be independent of the
actual grades.’’ Although this result provides a valuable clue, we might
have been able to surmise the error without it. From available data, the
calculated median appears to equal half of the number of students,
rounded up to the next integer. In other words, if you think of the grades
as being stored in a sorted table, the program is printing the entry number
of the middle student rather than his or her grade. Hence, we have a firm
hypothesis about the precise nature of the error. Next, we prove the
hypothesis by examining the code or by running a few extra test cases.
Debugging by Deduction
The process of deduction proceeds from some general theories or premises,
using the processes of elimination and refinement, to arrive at a conclusion
(the location of the error), as shown in Figure 8.4.
As opposed to the process of induction in a murder case, for example,
where you induce a suspect from the clues, using deduction, you start
with a set of suspects and, by the process of elimination (the gardener has
FIGURE 8.3 An Example of Clue Structuring.
Debugging 163
a valid alibi) and refinement (it must be someone with red hair), decide
that the butler must have done it. The steps are as follows:
1. Enumerate the possible causes or hypotheses. The first step is to develop
a list of all conceivable causes of the error. They don’t have to
be complete explanations; they are merely theories to help you structure
and analyze the available data.
2. Use the data to eliminate possible causes. Carefully examine all of
the data, particularly by looking for contradictions (you could use
Figure 8.2 here), and try to eliminate all but one of the possible causes.
If all are eliminated, you need more data gained from additional test
cases to devise new theories. If more than one possible cause remains,
select the most probable cause—the prime hypothesis—first.
3. Refine the remaining hypothesis. The possible cause at this point might
be correct, but it is unlikely to be specific enough to pinpoint the
error. Hence, the next step is to use the available clues to refine the
theory. For example, you might start with the idea that ‘‘there is an
error in handling the last transaction in the file’’ and refine it to ‘‘the
last transaction in the buffer is overlaid with the end-of-file indicator.’’
4. Prove the remaining hypothesis. This vital step is identical to step 4 in
the induction method.
5. Fix the error. Again this step is identical to step 5 in the induction
method. To re-emphasize though, you should thoroughly test your
fix to ensure it does not create problems elsewhere in the application.
As an example, assume that we are commencing the function testing
of the DISPLAY command discussed in Chapter 4. Of the 38 test cases
FIGURE 8.4 The Deductive Debugging Process.
164 The Art of Software Testing
identified by the process of cause-effect graphing, we start by running
four test cases. As part of the process of establishing input conditions,
we will initialize memory that the first, fifth, ninth, . . . , words have the
value 000; the second, sixth, . . . , words have the value 4444;
the third, seventh, . . . , words have the value 8888; and the fourth,
eighth, . . . , words have the value CCCC. That is, each memory word is
initialized to the low-order hexadecimal digit in the address of the first
byte of the word (the values of locations 23FC, 23FD, 23FE, and 23FF
are C).
The test cases, their expected output, and the actual output after the test
are shown in Figure 8.5.
Obviously, we have some problems, since apparently none of the test
cases produced the expected results (all were successful). But let’s start by
debugging the error associated with the first test case. The command indicates
that, starting at location 0 (the default), E locations (14 in decimal)
are to be displayed. (Recall that the specification stated that all output will
contain four words, or 16 bytes per line.)
Enumerating the possible causes for the unexpected error message, we
might get:
1. The program does not accept the word DISPLAY.
2. The program does not accept the period.
3. The program does not allow a default as a first operand; it expects a
storage address to precede the period.
4. The program does not allow an E as a valid byte count.
FIGURE 8.5 Test Case Results from the DISPLAY Command.
Debugging 165
The next step is to try to eliminate the causes. If all are eliminated, we
must retreat and expand the list. If more than one remain, we might want
to examine additional test cases to arrive at a single error hypothesis, or
proceed with the most probable cause. Since we have other test cases at
hand, we see that the second test case in Figure 8.5 seems to eliminate the
first hypothesis; and the third test case, although it produced an incorrect
result, seems to eliminate the second and third hypotheses.
The next step is to refine the fourth hypothesis. It seems specific
enough, but intuition might tell us that there is more to it than meets the
eye—it sounds like an instance of a more general error.We might contend,
then, that the program does not recognize the special hexadecimal characters
A–F. This absence of such characters in the other test cases makes this
sound like a viable explanation.
Rather than jumping to a conclusion, however, we should first consider
all of the available information. The fourth test case might represent a
totally different error, or it might provide a clue about the current error.
Given that the highest valid address in our system is 7FFF, how could the
fourth test case display an area that appears to be nonexistent? The fact
that the displayed values are our initialized values, and not garbage, might
lead to the supposition that this command is somehow displaying something
in the range 0–7FFF. One idea that may arise is that this could occur
if the program were treating the operands in the command as decimal values
rather than hexadecimal, as stated in the specification. This is borne
out by the third test case: Rather than displaying 32 bytes of memory, the
next increment above 11 in hexadecimal (17 in base 10), it displays 16
bytes of memory, which is consistent with our hypothesis that the 11 is
being treated as a base-10 value. Hence, the refined hypothesis is that the
program is treating the byte count as storage address operands, and the
storage addresses on the output listing as decimal values.
The last step is to prove this hypothesis. Looking at the fourth test case,
if 8000 is interpreted as a decimal number, the corresponding base-16
value is 1F40, which would lead to the output shown. As further proof,
examine the second test case. The output is incorrect, but if 21 and 29 are
treated as decimal numbers, the locations of storage addresses 15–1D
would be displayed; this is consistent with the erroneous result of the test
case. Hence, we have almost certainly located the error: The program is
assuming that the operands are decimal values and is printing the memory
addresses as decimal values, which is inconsistent with the specification.
166 The Art of Software Testing
Moreover, this error seems to be the cause of the erroneous results of all
four test cases. A little thought has led to the error, and it also solved three
other problems that, at first glance, appear to be unrelated.
Note that the error probably manifests itself at two locations in the program:
the part that interprets the input command and the part that prints
memory addresses on the output listing.
As an aside, this error, likely caused by a misunderstanding of the specification,
reinforces the suggestion that a programmer should not attempt
to test his or her own program. If the programmer who created this error
is also designing the test cases, he or she likely will make the same mistake
while writing the test cases. In other words, the programmer’s expected
outputs would not be those of Figure 8.5; they would be the outputs calculated
under the assumption that the operands are decimal values. Therefore,
this fundamental error probably would go unnoticed.
Debugging by Backtracking
An effective method for locating errors in small programs is to backtrack
the incorrect results through the logic of the program until you find
the point where the logic went astray. In other words, start at the point
where the program gives the incorrect result—such as where incorrect
data were printed. Here, you deduce from the observed output what the
values of the program’s variables must have been. By performing a mental
reverse execution of the program from this point and repeatedly applying
the if-then logic that states ‘‘if this was the state of the program at this
point, then this must have been the state of the program up here,’’ you can
quickly pinpoint the error. You’re looking for the location in the program
between the point where the state of the program was what it was expected
to be and the first point where the state of the program was not what it was
expected to be.
Debugging by Testing
The last ‘‘thinking type’’ debugging method is the use of test cases. This
probably sounds a bit peculiar since, at the beginning of this chapter, we
distinguished debugging from testing. However, consider two types of test
cases: test cases for testing, whose purpose is to expose a previously
undetected error, and test cases for debugging, whose purpose is to provide
Debugging 167
information useful in locating a suspected error. The difference between
the two is that test cases for testing tend to be ‘‘fat,’’ in that you are trying
to cover many conditions in a small number of test cases. Test cases for
debugging, on the other hand, are ‘‘slim,’’ because you want to cover only a
single condition or a few conditions in each test case.
In other words, after you have discovered a symptom of a suspected
error, you write variants of the original test case to attempt to pinpoint the
error. Actually, this is not an entirely separate method; it often is used in
conjunction with the induction method to obtain information needed to
generate a hypothesis and/or to prove a hypothesis. It also is used with
the deduction method to eliminate suspected causes, refine the remaining
hypothesis, and/or prove a hypothesis.
Debugging Principles
In this section, we want to discuss a set of debugging principles that are
psychological in nature. As with the testing principles in Chapter 2, many
of these debugging principles are intuitively obvious, yet they are often
forgotten or overlooked.
Since debugging is a two-part process—locating an error and then
repairing it—we discuss two sets of principles here.
Error-Locating Principles
Think As implied in the previous section, debugging is a problem-solving
process. The most effective method of debugging involves a mental
analysis of the information associated with the error’s symptoms. An
efficient program debugger should be able to pinpoint most errors without
going near a computer. Here’s how:
1. Position yourself in a quiet place, where outside stimuli—voices of
coworkers, telephones, radio or other potential interruptions—won’t
interfere with your concentration.
2. Without looking at the program code, review in your mind how the
program is designed, how the software should be performing within
the area that is performing incorrectly.
3. Concentrate on the process for correct performance, and then
imagine ways in which the code may be incorrectly designed.
168 The Art of Software Testing
This sort of prethinking the physical debugging process will, in many
cases, lead you directly to the area of the program that is causing problems
and help you achieve a fix, quickly.
If You Reach an Impasse, Sleep on It The human subconscious is a
potent problem solver. What we often refer to as inspiration is simply the
subconscious mind working on a problem when the conscious mind is
focused on something else, such as eating, walking, or watching a movie.
If you cannot locate an error in a reasonable amount of time (perhaps
30 minutes for a small program, several hours for a larger one), drop it
and turn your attention to something else, since your thinking efficiency is
about to collapse anyway. After putting aside the problem for a while, your
subconscious mind will have solved the problem, or your conscious mind
will be clear for a fresh examination of its symptoms.
We have used this technique regularly over the years, both as a development
process as well as a debugging process. It may take some practice to
accept this extraordinary functioning of the human brain, and make efficient
use of it, but it does work. We have actually awakened in the night to
realize we have solved a software problem while asleep. For this reason,
we recommend that you keep by your bedside a small tape recorder, a
telephone capable of voice recording, a PDA, or a notepad to capture the
solution you found while sleeping. Resist the temptation to return to sleep
believing you will be able to regenerate the solution in the morning. You
probably won’t—at least not in our experience.
If You Reach an Impasse, Describe the Problem to Someone Else
Talking about the problem with someone else may help you discover
something new. In fact, often, simply by describing the problem to a
good listener, you will suddenly see the solution without any assistance
from the person.
Use Debugging Tools Only as a Second Resort Turn to debugging tools
only after you’ve tried other methods, and then only as an adjunct to, not a
substitute for, thinking. As noted earlier in this chapter, debugging tools,
such as dumps and traces, represent a haphazard approach to debugging.
Experiments show that people who shun such tools, even when they are
debugging programs that are unfamiliar to them, are more successful than
people who use the tools.
Debugging 169
Why should this be so? Depending on a tool to solve a problem can
short-circuit the diagnostic process. If you believe that the tool can solve
the problem, you are likely to be less attentive to the clues you already
have picked up, information that could help you solve the problem directly,
without the help of a generic diagnostic tool.
Avoid Experimentation—Use It Only as a Last Resort The most common
mistake novice debuggers make is to try to solve a problem by making
experimental changes to the program. You might think, ‘‘I know what
is wrong, so I’ll change this DO statement and see what happens.’’ This
totally haphazard approach cannot even be considered debugging; it
represents an act of blind hope. Not only does it have a minuscule chance
of success, but you will often compound the problem by adding new errors
to the program.
Error-Repairing Techniques
Where There Is One Bug, There Is Likely to Be Another This is a
restatement of principle 9 in Chapter 2, which states that when you
find an error in a section of a program, the probability of the existence
of another error in that same section is higher than if you hadn’t already
found one error. In other words, errors tend to cluster. When
repairing an error, examine its immediate vicinity for anything else
that looks suspicious.
Fix the Error, Not Just a Symptom of It Another common failing is repairing
the symptoms of the error, or just one instance of the error, rather
than the error itself. If the proposed correction does not match all the clues
about the error, you may be fixing only a part of the error.
The Probability of the Fix Being Correct Is Not 100 Percent Tell this to
someone in general conversation and of course he or she would agree; but
tell it to someone in the process of correcting an error and you may get a
different answer—‘‘Yes, in most cases, but this correction is so minor that it
just has to work.’’ Never assume that code added to a program to fix an
error is correct. Statement for statement, corrections are much more error
prone than the original code in the program. One implication is that error
corrections must be tested, perhaps more rigorously than the original
170 The Art of Software Testing
program. A solid regression testing plan can help ensure that correcting an
error does not introduce another error somewhere else in the application.
The Probability of the Fix Being Correct Drops as the Size of the Program
Increases Stating it differently, in our experience, the ratio of
errors caused by incorrect fixes, versus original errors, increases in large
programs. In one widely used large program, one of every six new errors
discovered is an error in a prior correction to the program.
If you accept this as fact, how can you avoid causing problems by trying
to fix them? Read the first three techniques in this section, for starters. One
error found does not mean all errors have been found, and you must be
sure you are correcting the actual error, not just its symptom.
Beware of the Possibility That an Error Correction Creates a New
Error Not only do you have to worry about incorrect corrections, you
also have to worry about a seemingly valid correction having an undesirable
side effect, thus introducing a new error. Not only is there a probability
that a fix will be invalid, but there also is a probability that a fix will
introduce a new error. One implication is that not only do you have to test
the error situation after the correction is made, but you must also perform
regression testing to determine whether a new error has been introduced.
The Process of Error Repair Should Put You Temporarily Back into the
Design Phase Realize that error correction is a form of program design.
Given the error-prone nature of corrections, common sense says that whatever
procedures, methodologies, and formalism were used in the design
process should also apply to the error-correction process. For instance, if
the project rationalized that code inspections were desirable, then it must
be doubly important that they be implemented after correcting an error.
Change the Source Code, Not the Object Code When debugging large
systems, particularly those written in an assembly language, occasionally
there is the tendency to correct an error by making an immediate change
to the object code, with the intention of changing the source program later.
Two problems are associated with this approach: (1) It usually is a sign that
‘‘debugging by experimentation’’ is being practiced; and (2) the object code
and source program are now out of synchronization, meaning that
the error could easily resurface when the program is recompiled or
Debugging 171
reassembled. This practice is an indication of a sloppy, unprofessional approach
to debugging.
Error Analysis
The last point to realize about program debugging is that in addition to its
value in removing an error from the program, it can have another valuable
effect: It can tell us something about the nature of software errors, something
we still know too little about. Information about the nature of
software errors can provide valuable feedback in terms of improving future
design, coding, and testing processes.
Every programmer and programming organization could improve immensely
by performing a detailed analysis of the detected errors, or at least
a subset of them. Admittedly, it is a difficult and time-consuming task, for
it implies much more than a superficial grouping such as ‘‘x percent of
the errors are logic design errors,’’ or ‘‘x percent of the errors occur in IF
statements.’’ A careful analysis might include the following studies:
  Where was the error made? This question is the most difficult one to
answer, because it requires a backward search through the documentation
and history of the project; at the same time, it also is the most
valuable question. It requires that you pinpoint the original source
and time of the error. For example, the original source of the error
might be an ambiguous statement in a specification, a correction to a
prior error, or a misunderstanding of an end-user requirement.
  Who made the error? Wouldn’t it be useful to discover that 60 percent
of the design errors were created by one of the 10 analysts, or that
programmer X makes three times as many mistakes as the other programmers?
(Not for the purposes of punishment but for the purposes
of education.)
  What was done incorrectly? It is not sufficient to determine when and
by whom each error was made; the missing link is a determination of
exactly why the error occurred. Was it caused by someone’s inability
to write clearly? Someone’s lack of education in the programming language?
A typing mistake? An invalid assumption? A failure to consider
valid input?
  How could the error have been prevented? What can be done differently
in the next project to prevent this type of error? The answer to this
172 The Art of Software Testing
question constitutes much of the valuable feedback or learning for
which we are searching.
  Why wasn’t the error detected earlier? If the error was detected during
a test phase, you should study why the error was not unearthed
during earlier testing phases, code inspections, and design reviews.
  How could the error have been detected earlier? The answer to this
offers another piece of valuable feedback. How can the review and
testing processes be improved to find this type of error earlier in future
projects? Providing that we are not analyzing an error found by
an end user (that is, the error was found by a test case), we should
realize that something valuable has happened: We have written a successful
test case. Why was this test case successful? Can we learn
something from it that will result in additional successful test cases,
either for this program or for future programs?
We repeat, this analysis process is difficult, and costly, but the answers
you may discover by going through it can be invaluable in improving
subsequent programming efforts. The quality of future products will
increase while the capital investment will decrease. It is alarming that the
vast majority of programmers and programming organizations do not
employ it.
Summary
The main focus of this book is on software testing: How do you go about
uncovering as many software errors as possible? Therefore, we don’t want
to spend too much time on the next step—debugging—but the simple fact
is, errors found by successful test cases lead directly to it.
In this chapter we touched on some of the more important aspects of
software debugging. The least desirable method, debugging by brute force,
involves such techniques as dumping memory locations, placing print
statements throughout the program, or using automated tools. Brute-force
techniques may point you to the solution for some errors uncovered during
testing, but they are not an efficient way to go about debugging.
We demonstrated that you can begin debugging by studying the error
symptoms, or clues, and moving from them to the larger picture (inductive
debugging). Another technique begins the debugging process by considering
general theories, then, through the process of elimination, identifies
Debugging 173
the error locations (deductive debugging). We also covered program
backtracking—starting with the error and moving backwards through the
program to determine where incorrect information originated. Finally, we
discussed debugging by testing.
If, however, we were to offer a single directive to those tasked with
debugging a software system, we would say, ‘‘Think!’’ Review the numerous
debugging principles described in this chapter. We believe they can
lead you in the right direction, toward accurate and efficient debugging.
But the bottom line is, depend on your expertise and knowledge of the
program itself. Open your mind to creative solutions, review what you
know, and let your knowledge and subconscious lead you to the error
locations.
In the next chapter we take on the subject of extreme testing, techniques
well suited to help uncover errors in extreme programming environments
such as agile development.
174 The Art of Software Testing
9 Testing in the Agile
Environment
Increased competition and interconnectedness in all markets have forced
businesses to shorten their time-to-market while continuing to provide
high-quality products to their customers. This is particularly true in the software
development industry where the Internet makes possible near-instant
delivery of software applications and services. Whether creating a product
for the masses or for the human resources department, one fact remains immutable:
The twenty-first century customer demands a quality application
delivered almost immediately. Unfortunately, traditional software development
processes cannot keep up in this competitive environment.
In the early 2000s, a group of developers met to discuss the state of lightweight
and rapid development methodologies. At the gathering they compared
notes to identify what successful software projects look like; what
made some projects succeed while others limped along. In the end, they
created the ‘‘Manifesto for Agile Software Development,’’ a document that
became the cornerstone of the Agile movement. Less a discrete methodology,
the Agile Manifesto (Figure 9.1) is a unique philosophy that focuses on
customers and employees, in lieu of rigid approaches and hierarchies.
Features of Agile Development
Agile development promotes iterative and incremental development, with
significant testing, that is customer-centric and welcomes change during
the process. All attributes of traditional software development approaches
175
neglect or minimize the importance of the customer. Although Agile methodologies
incorporate flexibility into their processes, the main emphasis is
on customer satisfaction. The customer is a key component of the process;
simply put, without customer involvement, the Agile method fails. And
knowing their interaction is welcomed helps customers build satisfaction
and confidence in the end product and development team. If the customer
is not committed, then more traditional processes may be a better development
choice.
Ironically, Agile development has no single development methodology
or process; many rapid development approaches may be considered Agile.
These approaches do, however, share three common threads: They rely on
customer involvement, mandate significant testing, and have short, iterative
development cycles. It is beyond the scope of this book to cover each
methodology in detail, but in Table 9.1 we identify the methodologies considered
Agile and give a brief description of each. (We urge you to learn
We are uncovering better ways of developing
software by doing it and helping others do it.
Through this work we have come to value:
Individuals and interactions over processes and tools
Working software over comprehensive documentation
Customer collaboration over contract negotiation
Responding to change over following a plan
That is, while there is value in the items on
the right, we value the items on the left more.
Kent Beck Mike Beedle Arie van Bennekum
Alistair Cockburn Ward Cunningham Martin Fowler
James Grenning Jim Highsmith Andrew Hunt
Ron Jeffries Jon Kern Brian Marick
Robert C. Martin Steve Mellor Ken Schwaber
Jeff Sutherland Dave Thomas
#2001, the above authors
this declaration may be freely copied in any form,
but only in its entirety through this notice.
FIGURE 9.1 Manifesto of Agile Software Development.
176 The Art of Software Testing
more about them because they represent the essence of the Agile philosophy.)
In addition, we cover Extreme Programming, one of the more popular
Agile methodologies, in greater detail later in this chapter, and offer a
practical example.
TABLE 9.1 Agile Development Methodologies
Methodology Description
Agile Modeling Not so much a single modeling methodology, but a
collection of principles and practices for modeling and
documenting software systems. Used to support
other methods such as Extreme Programming
and Scrum.
Agile Unified Process Simplified version of the Rational Unified Process
(RUP) tailored for Agile development.
Dynamic Systems
Development Method
Based on rapid application development approaches,
this methodology relies on continuous customer
involvement and uses an iterative and incremental
approach, with the goal of delivering software on time
and within budget.
Essential Unified Process
(EssUP)
An adaptation of RUP in which you choose the
practices, (e.g. use cases or team programming) that
fit your project. RUP generally uses all practices,
whether needed or not.
Extreme Programming Another iterative and incremental approach that relies
heavily on unit and acceptance testing. Probably the
best known of the Agile methodologies.
Feature Driven
Development
A methodology that uses industry best practices, such
as regular builds, domain object modeling, and
feature teams, that are driven by the customer’s
feature set.
Open Unified Process An Agile approach to implementing standard Unified
practices that allows a software team to rapidly
develop their product.
Scrum An iterative and incremental project management
approach that supports many Agile methodologies.
Velocity Tracking Applies to all Agile development methodologies. It
attempts to measure the rate, or ‘‘velocity,’’ at which
the development process is moving.
Testing in the Agile Environment 177
It’s worth noting that some Agile methodologies are collections, or
adaptations, of traditional software development processes. The Essential
Unified Process (EssUP) is an example. EssUP takes processes from the
Rational Unified Process (RUP) and other well-known software development
process models that support the Agile development philosophy.
Make no mistake, adopting an Agile development methodology is challenging.
It takes the right combination of developers, managers, and customers
to make it work. But in the end, the product will benefit from
constant testing and heavy customer involvement.
Agile Testing
In essence, Agile testing is a form of collaborative testing, in that everyone
is involved in the process through design, implementation, and execution
of the test plan. Customers are involved in defining acceptance tests by
defining use cases and program attributes. Developers collaborate with
testers to build test harnesses that can test functionality automatically.
Agile testing requires that everyone be engaged in the test process, which
requires a lot of communication and collaboration.
As with most aspects of Agile development, Agile testing necessitates
engaging the customer as early as possible and throughout the development
cycle. For example, once developers produce a stable code base,
customers should begin acceptance testing and provide feedback to the
development team. It also means that testing is not a phase; rather, it is
integrated with development efforts to compel continuous progress.
To ensure that the customer receives a stable product with which to perform
acceptance testing, developers generally begin by writing unit tests
first, then move to coding software units. The unit tests are failure tests, in
that developers design them to cause their software to fail some requirement.
Paradoxically, developers must write failing software to, in effect,
test the test. Once test harnesses are in place, developers proceed to write
software that passes the unit tests.
To facilitate the timely feedback needed for rapid development, Agile
testing relies on automated testing. Development cycles are short, so time
is valuable, and automated testing is more reliable than manual testing
approaches. Not only is manual testing time-consuming, it may itself introduce
bugs. Numerous open-source and commercial testing suites exist. It
really does not matter which of these available testing suites is used, only
178 The Art of Software Testing
that developers and testers use one. Although some problems may require
exploratory manual testing, automated testing is preferred.
Agile development environments often comprise only small teams of
developers, who also act as testers. Larger projects with more resources
may include an individual tester or a testing group. In either case, testers
should not be considered finger-pointers. Their job is to move the project
forward by providing feedback about the quality of the software so that
developers can implement bug fixes and make requirement changes and
general improvements.
Agile testing fits well into the Extreme Programming methodology
whereby developers create unit tests first, then the software. In the remainder
of this chapter we cover Extreme Programming and Extreme Testing in
more detail.
Extreme Programming and Testing
In the 1990s an innovative software development methodology termed
Extreme Programming (XP) was born. A project manager named Kent
Beck is credited with conceiving this lightweight, Agile development process,
first testing it while working on a project at Daimler-Chrysler in
1996. Although several other Agile software development processes have
since been created, XP is still the most popular. In fact, numerous opensource
tools exist to support it, which is testimony to XP’s popularity
among developers and project managers.
XP likely was developed to support the adoption of programming
languages such as Java, Visual Basic, and C#.
These object-based languages allow developers to create large, complex
applications much more quickly than with traditional languages such as C,
Fortran, or COBOL. Developing with these languages often requires building
general-purpose libraries to support the application’s coding efforts.
Methods for common tasks such as printing, sorting, networking, and statistical
analysis are not standard components. Languages such as C# and
Java ship with full-featured application programming interfaces (APIs)
that eliminate or reduce the need for creating custom libraries.
However, along with the benefits of rapid application development languages
came liabilities. Although developers were creating applications
much more quickly, their quality was not guaranteed. If an application
compiled, it often failed to meet the customer’s specifications or
Testing in the Agile Environment 179
expectations. The XP development methodology facilitates the creation of
quality programs in short time frames. Although classical software processes
still work, they often take too much time, which equates to lost income
in the highly competitive arena of software development.
Besides customer involvement, the XP model relies heavily on unit and
acceptance testing. In general, developers run unit tests for every incremental
code change, no matter how small, to ensure that the code base still
meets its specification. In fact, testing is of such importance in XP that the
process requires you to create the unit (module) and acceptance tests first,
then your code base. This form of testing is called, appropriately, Extreme
Testing (XT).
Extreme Programming Basics
As mentioned, XP is a software process that helps developers create highquality
code, rapidly. Here, we define ‘‘quality’’ as a code base that meets
the design specification and customer expectation.
XP focuses on:
  Implementing simple designs.
  Communicating between developers and customers.
  Continually testing the code base.
  Refactoring, to accommodate specification changes.
  Seeking customer feedback.
XP tends to work well for small to medium-size development efforts in
environments that have frequent specification changes, and where nearinstant
communication is possible.
XP differs from traditional development processes in several ways. First,
it avoids the large-scale project syndrome in which the customer and the
programming team meet to design every detail of the application before
coding begins. Project managers know this approach has its drawbacks,
not the least of which is that customer specifications and requirements
constantly change to reflect new business rules or marketplace conditions.
For example, the finance department may want payroll reports sorted by
processed date instead of check numbers; or the marketing department
may determine that consumers will not buy product XYZ if it doesn’t
send an e-mail after website registration. In contrast, XP planning sessions
180 The Art of Software Testing
focus on collecting general application requirements, not narrowing in on
every detail.
Another difference with the XP methodology is that it avoids coding unneeded
functionality. If your customer thinks the feature is needed but not
required, it generally is left out of the release. Thus, you can focus on the
task at hand, adding value to a software product. Concentrating only on
the required functionality helps you produce quality software in short
time frames.
But the primary difference of XP compared to traditional methodologies
is its approach to testing. After an all-inclusive design phase, traditional
software development models suggest you code first and create testing
interfaces later. In XP, you must create the unit tests first, and then write
the code to pass the tests. You design unit tests in an XP environment by
following the concepts discussed in Chapter 5.
The XP development model has 12 core practices that drive the process,
summarized in Table 9.2. In a nutshell, you can group the 12 core XP practices
into four concepts:
1. Listening to the customer and other programmers.
2. Collaborating with the customer to develop the application’s specification
and test cases.
3. Coding with a programming partner.
4. Testing, and retesting, the code base.
Most of the comments for each practice listed in Table 9.2 are selfexplanatory.
However, a couple of the more important principles, namely
planning and testing, warrant further discussion.
XP Planning A successful planning phase lays the foundation of the XP
process. The planning phase in XP differs from that in traditional development
models, which often combine requirements gathering and application
design. Planning in XP focuses on identifying your customer’s application
requirements and designing user stories (or case stories) that meet them.
You gain significant insight into the application’s purpose and requirements
by creating user stories. In addition, the customer employs the user stories
when performing acceptance tests at the end of a release cycle. Finally, an
intangible benefit of the planning phase is that the customer gains ownership
and confidence in the application by participating intimately in it.
Testing in the Agile Environment 181
TABLE 9.2 The 12 Practices of Extreme Programming
Practice Comment
1. Planning and
requirements
Marketing and business development personnel work
together to identify the maximum business value of
each software feature.
Each major software feature is written as a user story.
Programmers provide time estimates to complete each
user story.
The customer chooses the software features based on
time estimates and business value.
2. Small, incremental
releases
Strive to add small, tangible, value-added features and
release a new code base often.
3. System metaphors Your programming team identifies an organizing
metaphor to help with naming conventions and
program flow.
4. Simple designs Implement the simplest design that allows your code to
pass its unit tests. Assume change will come, so don’t
spend a lot of time designing; just implement.
5. Continuous testing Write unit tests before writing the code module. Each
unit is not complete until it passes its unit test. Further,
the program is not complete until it passes all unit tests,
and acceptance tests are complete.
6. Refactoring Clean up and streamline your code base. Unit tests help
ensure that you do not destroy the functionality in the
process. You must rerun all unit tests after any
refactoring.
7. Pair programming You and another programmer work together, at the
same machine, to create the code base. This allows for
real-time code review, which dramatically facilitates bug
detection and resolution.
8. Collective
ownership of the
code
All code is owned by all programmers.
No single programmer is dedicated to a specific code
base.
9. Continuous
integration
Every day, integrate all changes; after the code passes
the unit tests, add it back into the code base.
10. Forty-hour
workweek
No overtime is allowed. If you work with dedication for
40 hours per week, overtime will not be needed. The
exception is the week before a major release.
182 The Art of Software Testing
XP Testing Continuous testing is central to the success of a XP-based
effort. Although acceptance testing falls under this principle, unit testing
occupies the bulk of the effort. Unit tests are devised to make the software
fail. Only by ensuring that your tests detect errors can you begin correcting
the code so it passes the tests. Assuring that your unit tests catch failures is
key to the testing process—and to a developer’s confidence. At this point,
the developer can experiment with different implementations, knowing
that the unit tests will catch any mistakes.
You want to ensure that any code changes improve the application and
do not introduce bugs. The continuous testing principle also supports
refactoring efforts used to optimize and streamline the code base. Constant
testing also leads to that intangible benefit already mentioned: confidence.
The programming team gains confidence in the code base because you
constantly validate it with unit tests. In addition, your customers’ confidence
in their investment soars because they know the code base passes
unit tests every day.
Example XP Project Flow Now that we’ve presented the 12 practices of
the XP process, you may be wondering, how does a typical XP project
flow? Here is a quick example of what you might experience if you worked
on an XP-based project:
1. Programmers meet with the customer to determine the product requirements
and build user stories.
2. Programmers meet without the customer to divide the requirements
into independent tasks and estimate the time to complete
each task.
Table 9.2 (continued)
Practice Comment
11. On-site customer
presence
You and your programming team have unlimited access
to the customer, to enable you to resolve questions
quickly and decisively, which keeps the development
process from stalling.
12. Coding standards All code should look the same. Developing a system
metaphor helps meet this principle.
Testing in the Agile Environment 183
3. Programmers present the customer with the task list and with time
estimates, and ask them to generate a priority list of features.
4. The programming team assigns tasks to pairs of programmers, based
on their skill sets.
5. Each pair creates unit tests for their programming task using the
application’s specification.
6. Each pair works on their task with the goal of creating a code base
that passes the unit tests.
7. Each pair fixes, then retests their code until all unit tests have passed.
8. All pairs gather every day to integrate their code bases.
9. The team releases a preproduction version of the application.
10. Customers run acceptance tests and either approve the application or
produce a report identifying the bugs/deficiencies.
11. Upon successful acceptance tests, programmers release a version into
production.
12. Programmers update time estimates based on latest experience.
Although compelling, XP is not for every project or every organization.
Proponents of XP conclude that if a programming team fully implements
the 12 practices, then the chances of successful application development increase
dramatically. Detractors say that because XP is a process, you must do
all or nothing; if you skip a practice, then you are not properly implementing
XP, and your program quality may suffer. Detractors also claim that the
cost of changing a program in the future to add more features is higher than
the cost of initially anticipating and coding the requirement. Finally, some
programmers find working in pairs very cumbersome and invasive; therefore,
they do not embrace the XP philosophy.
Whatever your views, we recommend that you consider XP as a software
methodology for your project. Carefully weigh its pros and cons
against the attributes of your project and make the best decision based on
that assessment.
Extreme Testing: The Concepts
To meet the pace and philosophy of XP, developers use Extreme Testing,
which focuses on constant testing. As mentioned earlier, two forms of testing
make up the bulk of XT: unit testing and acceptance testing. The theory
used when writing the tests does not vary significantly from the theory
presented in Chapter 5; however, the stage in the development process in
184 The Art of Software Testing
which you create the tests does differ. XT mandates creating tests before
coding begins, not after. Nonetheless, XT and traditional testing share the
same goal: to identify errors in a program.
In the rest of this section we provide more information on unit and acceptance
testing, from an Extreme Programming perspective.
Extreme Unit Testing Unit testing, the primary testing approach used in
Extreme Testing, and has two simple rules: All code modules must have
unit tests before coding begins, and all code modules must pass unit tests
before being released into acceptance testing. At first glance this may not
seem so extreme. Closer inspection reveals the big difference between unit
testing, as previously described, and XTunit testing: The unit tests must be
defined and created before coding the module.
Initially, you may wonder why you should, or how you can, create test
drivers for code you haven’t yet written. You may also think that you do
not have time to create the tests and still meet the project deadline. These
are valid concerns, but concerns we can address easily by listing a number
of important benefits associated with writing unit tests before you start
coding the application:
  You gain confidence that your code will meet its specification and
requirements.
  You express the end result before you start coding.
  You better understand the application’s specification and requirements.
  You may implement simple designs initially and confidently refactor
the code later to improve performance, without worrying about
breaking the specification.
Of these benefits, the insight and understanding you gain of the application’s
specification and requirements cannot be underestimated. For example,
if you start coding first, you may not fully understand the acceptable
data types and boundaries for the input values of an application. How can
you write a unit test to perform boundary analysis without understanding
the acceptable inputs? Can the application accept only numbers, only
characters, or both? If you create the unit tests first, you must understand
the specification. The practice of creating unit tests first is the shining star
in the XP methodology, as it forces you to understand the specification to
resolve ambiguities before you begin coding.
Testing in the Agile Environment 185
As mentioned in Chapter 5, you determine the unit’s scope. Given that
today’s popular programming languages such as Java, C#, and Visual Basic
are mostly object-oriented, modules are often classes, or even individual
class methods. You may sometimes define a module as a group of classes
or methods that represent some functionality. Only you, as the programmer,
know the architecture of the application and how best to build the
unit tests for it.
Manually running unit tests, even for the smallest application, can be a
daunting task. As the application grows, you may generate hundreds or
thousands of unit tests. Therefore, you typically use an automated software
testing suite to ease the burden of running these unit tests. With these
suites you script the tests and then run all or part of them. In addition,
testing suites typically allow you to generate reports and classify the bugs
that frequently occur in your application. This information may help you
proactively eliminate bugs in the future.
Interestingly enough, once you create and validate your unit tests, the
‘‘testing’’ code base becomes as valuable as the software application you are
trying to create. As a result, you should keep the tests in a code repository,
for protection. Likewise, you should institute adequate backups of the test
code, and ensure that needed security is in place.
Extreme Acceptance Testing Acceptance testing represents the second,
and equally important, type of XT that occurs in the XP methodology. Acceptance
testing determines whether the application meets other requirements,
such as functionality and usability. You and the customer create the
acceptance tests during the design/planning phases.
Unlike the other forms of testing discussed thus far, customers, not
you or your programming partners, conduct the acceptance tests. In this
manner, customers provide the unbiased verification that the application
meets their needs. Customers create the acceptance tests from user stories.
The ratio of user stories to acceptance tests is usually one too many;
that is, more than one acceptance test may be needed for each user story.
Acceptance tests in XT may or may not be automated. For example, an
unautomated test is required when the customer must validate that a user
input screen meets its specification with respect to color and screen layout.
An example of an automated test is when the application must calculate
payroll values using data input via some data source such as a flat file to
simulate production values.
186 The Art of Software Testing
Through acceptance tests, the customer validates an expected result
from the application. A deviation from the expected result is considered a
bug and is reported to the development team. If the customer discovers
several bugs, then he or she must prioritize them before passing the list to
your development group. After you correct the bugs, or after any change,
the customer reruns the acceptance tests. In this manner, the acceptance
tests also become a form of regression testing.
An important note is that a program may pass all unit tests but fail the
acceptance tests. How is this possible? Because a unit test validates
whether a program unit meets some specification, such as calculating payroll
deductions, correctly, not some defined functionality or aesthetics. For
a commercial application, the look and feel is a very important component.
Understanding the specification, but not the functionality, generally
results in this scenario.
Extreme Testing Applied
In this section we create a small Java application and employ JUnit, a Javabased
open-source unit testing suite, to illustrate the concepts of Extreme
Testing (see Figure 9.2). The example itself is trivial; the concepts, however,
apply to most programming situations.
Our example is a command-line application that simply determines
whether an input value is a prime number. For brevity, the source code,
JUnit is a freely available open-source tool used to automate unit tests of Java applications
in Extreme Programming environments. The creators, Kent Beck and Erich
Gamma, developed JUnit to support the significant unit testing that occurs in the
Extreme Programming environment. JUnit is very small, but very flexible and feature
rich. You can create individual tests or a suite of tests. You can automatically generate
reports detailing the errors.
Before using JUnit, or any testing suite, you must fully under- stand how to use it.
JUnit is powerful but only after you master its API. However, whether or not you
adopt an XP methodology, JUnit is a useful tool to provide sanity checks for your
own code.
Visit www.junit.org for more information and to download the test suite. In addition,
there is a wealth of information on XP and XT at this website.
FIGURE 9.2 JUnit Description and Background.
Testing in the Agile Environment 187
check4Prime.java, and its test harness, check4PrimeTest.java, are
listed in Appendix. In this section we provide snippets from the application
to illustrate the main points.
The specification of this program is as follows:
Develop a command-line application that accepts any positive integer,
n, where 0<¼n<¼1,000, and determine whether it is a prime
number. If n is a prime number, then the application should return a
message stating it is a prime number. If n is not a prime number, then
the application should return a message stating it is not a prime number.
If n is not a valid input, then the application should display a
help message.
Following the XP methodology and the principles listed in Chapter 5,
we begin the application by designing unit tests. With this application, we
can identify two discrete tasks: validating inputs and determining prime
numbers. We could use black-box and white-box testing approaches,
boundary value analysis, and the decision coverage criterion, respectively.
However, the XT practice mandates a hands-off black-box approach, to
eliminate any bias.
Test-Case Design We begin designing test cases by identifying a testing
approach. In this instance, we will use boundary analysis to validate the
inputs because this application can only accept positive integers within a
certain range. All other input values, including character datatypes and
negative numbers, should raise an error and not be used. Of course, you
could certainly make the case that input validation could fall under the
decision coverage criterion, as the application must decide whether the input
is valid. The important concept is to identify, and commit to, a testing
approach when designing your tests.
With the testing approach identified, the next step is to develop a list of
test cases based on possible inputs and expected outcome. Table 9.3 shows
the eight test cases we identified for this example. (Note: As stated, we are
using a very simple example here to illustrate the basics of Extreme Testing.
In practice, you would have a much more detailed program specification,
which might include items such as user interface requirements and
output verbiage. As a result, the list of test cases would increase
substantially.)
188 The Art of Software Testing
Test case 1 from Table 9.3 combines two test scenarios. It checks
whether the input is a valid prime and how the application behaves with a
valid input value. You may use any valid prime in this test.
We also test two scenarios with test case 2: What happens when the input
value is equal to the upper bounds and when the input is not a prime
number? This case could have been broken out into two unit tests, but one
goal of software testing in general is to minimize the number of test cases
while still adequately checking for error conditions.
Test case 3 checks the lower boundary of valid inputs, as well as testing
for invalid primes. The second part of the check is not needed because test
case 2 handles this scenario. However, it is included by default because 0 is
not a prime number. Test cases 4 and 5 ensure that the inputs are within
the defined range, which is greater than or equal to 0 and less than or equal
to 1,000.
Case 6 tests whether the application properly handles character input
values. Because we are doing a calculation, it is obvious that the
TABLE 9.3 Test Case Descriptions for check4Prime.java
Case
Number Input
Expected
Output Comments
1 n ¼ 3 Affirm n is a
prime number.
Tests for a valid prime number.
Tests input within boundaries.
2 n ¼ 1,000 Affirm n is not a
prime number.
Tests input equal to upper bounds.
Tests whether n is an invalid
prime.
3 n ¼ 0 Affirm n is not a
prime number.
Tests input equal to lower bounds.
4 n ¼ - 1 Print help
message.
Tests input below lower bounds.
5 n ¼ 1,001 Print help
message.
Tests input greater than the upper
bounds.
6 n ¼ ‘‘a’’ Print help
message.
Tests input is an integer and not a
character datatype.
7 Two or more
inputs
Print help
message.
Tests for correct number of input
values.
8 n is empty
(blank)
Print help
message.
Tests whether an input value is
supplied.
Testing in the Agile Environment 189
application should reject character datatypes. The assumption with this
test case is that Java will handle the datatype check. This application
must handle the exception raised when an invalid datatype is supplied.
This test will ensure that the exception is thrown. Last, tests 7 and 8
check for the correct number of input values; any number of inputs other
than 1 should fail.
Test Driver and Application Now that we have designed both test cases,
we can create the test driver class, check4PrimeTest. Table 9.4 maps the
JUnit methods in check4PrimeTest to the test cases covered.
Note that the testCheckPrime_false() method tests two conditions,
because the boundary values are not prime numbers. Therefore, we can
check for boundary value errors and for invalid primes with one test
method. Examining this method in detail reveals that the two tests actually
do occur within it. Here is the complete JUnit method from the
check4JavaTest class listed in the Appendix.
public void testCheckPrime_false(){
assertFalse(check4prime.primeCheck(0));
assertFalse(check4prime.primeCheck(10000));
}
Notice that the JUnit method, assertFalse(), checks to see whether
the parameter supplied causes the method to return a false Boolean value.
If false is returned, the test is considered a success.
The snippet also demonstrates one of the benefits of creating test cases
and test harnesses first. You may notice that the parameter in the
TABLE 9.4 Test Driver Methods
Methods
Test Case(s)
Examined
testCheckPrime_true() 1
testCheckPrime_false() 2, 3
testCheck4Prime_checkArgs_char_input() 6
testCheck4Prime_checkArgs_above_upper_bound() 5
testCheck4Prime_checkArgs_neg_input() 4
testCheck4Prime_checkArgs_2_inputs() 7
testCheck4Prime_checkArgs_0_inputs() 8
190 The Art of Software Testing
assertFalse() method is another method, check4prime.
primeCheck(n). This method will reside in a class of the application.
Creating the test harness first forced us to think about the structure of the
application. In some respects, the application is designed to support the
test harness. Here we need a method to check whether the input is a prime
number, so we included it in the application.
With the test harness complete, application coding can begin. Based on
the program specification, test cases, and the test harness, the resultant
Java application will consist of a single class, check4Prime, with the following
definition:
public class check4Prime {
public static void main (String [] args)
public void checkArgs(String [] args) throws
Exception
public boolean primeCheck (int num)
}
Briefly, per Java requirements, the main() procedure provides the entry
point into the application. The checkArgs() method asserts that the input
value is a positive integer, n, where 0<¼n<¼1,000. The primeCheck()
procedure checks the input value against a calculated list of prime numbers.
We implemented the sieve of Eratosthenes to quickly calculate the
prime numbers. This approach is acceptable because of the small number
of prime numbers involved.
Summary
With the heightened competitiveness of software development today, there
is a growing need to introduce products very quickly into the marketplace.
The Agile development process, when strictly adopted, provides a way for
developers to create quality software for their customers at a faster rate
than using traditional software development models. The end result is a
satisfied customer, whether an internal or commercial consumer.
The Extreme Programming model is one of more popular Agile methodologies.
This lightweight development process focuses on communication,
planning, and testing. The testing aspect of Extreme Programming, termed
Extreme Testing, focuses on unit and acceptance tests. You run unit tests
during development and whenever a change to the code base occurs. The
customer runs the acceptance tests at major release points.
Testing in the Agile Environment 191
Extreme Testing also requires you to create the test harness, based on
the program specification, before you start coding your application. In this
manner, you design your application to pass the unit tests, thus increasing
the probability that it will meet the specification.
192 The Art of Software Testing
10 Testing Internet
Applications
Just a few years ago, Internet-based applications seemed to be the wave
of the future; today, the wave has arrived onshore, and customers,
employees, and business partners expect companies to have a Web presence.
This expectation is not limited only to business. Most churches, civic
groups, schools, and governments all have Internet presences to serve
their patrons.
Generally, small to medium-size businesses have simple Web pages they
use to tout their products and services. Larger enterprises often build fullfledged
e-commerce applications to sell their wares, from cookies to cars
and from consulting services to entire virtual companies that exist only on
the Internet.
Internet applications are essentially client-server applications in which
the client is a Web browser, and the server is a Web or application server.
Although conceptually simple, the complexity of these applications varies
wildly. Some companies have applications built for business-to-consumer
uses such as banking services and retail stores, while others have businessto-
business applications such as supply chain or sales force management.
Development and user presentation/user interface strategies vary for these
different types of websites, and, as you might imagine, the testing approach
varies as well.
The goal of testing Internet-based applications is no different from that
of traditional applications. You need to uncover errors in the application
before deploying it to the Internet and the end user. And, given the
193
complexity of these applications and the interdependency of the components,
you likely will succeed in finding plenty of errors.
The importance of rooting out the errors in an Internet application
cannot be overstated. As a result of the openness and accessibility of
the Internet, competition in the business-to-consumer and business-tobusiness
arena is intense. Thus, the Internet has created a buyer’s
market for goods and services. Consumers have developed high
expectations, and if your site does not load quickly, respond immediately,
and provide intuitive navigation features, chances are that the
user will find another company with which to conduct business. This
issue is not confined to strictly e-commerce or product promotion
sites. Websites that are developed as research or information resources
frequently are maintained by advertising or user donations. Either way,
ample competition exists to lure users away, thereby reducing activity
and concomitant revenue.
It would seem that consumers have higher-quality expectations for
Internet applications than they do for those that come shrink-wrapped.
When people buy boxed software from a store or an online retailer, as long
as the quality is ‘‘average,’’ they will continue to use them. One reason for
this behavior is that they have paid for the application, so it must be a
product they perceived as useful or desirable. And even a less-thansatisfactory
program can’t be corrected easily, so if it at least satisfies the
users’ basic needs, they likely will retain the program. In contrast, a poor,
or even average, quality application on the Internet, will likely cause your
customer to switch to a competitor’s site. Not only will the customer leave
your site if it exhibits poor quality, your corporate image will become tarnished
as well. After all, who feels comfortable buying a car from a company
that cannot build a suitable website? Like it or not, websites have
become the new first impression for business. In general, consumers don’t
pay to access most websites, so there is little incentive to remain loyal in
the face of mediocre design or performance.
This chapter covers some of the basics of testing Internet applications.
This subject is large and complex, and many references exist that explore
its details. However, you will find that the techniques explained in the
early chapters apply to Internet testing as well. Nevertheless, because there
are, indeed, functional and design differences between Web and conventional
applications, we want to point out some of the particulars of Webbased
application testing.
194 The Art of Software Testing
Basic E-Commerce Architecture
Before diving into testing Internet-based applications, we will provide an
overview of the three-tier client-server (C/S) architecture used in a typical
Internet-based e-commerce application. Conceptually, each tier is treated
as a black box with well-defined interfaces. This model allows you to
change the internals of each tier without worrying about breaking another
tier. Figure 10.1 illustrates each tier and the associated components used
by most e-commerce sites.
Although not an official tier in the architecture, the client side and its
relevance are worth explaining. Most of the access to your applications
occurs from a Web browser running on a computer, although many devices,
such as cell phones, PDAs, game consoles, music players, pagers,
and even refrigerators and automobiles, increasingly are being developed
with Internet connectivity in mind. Browsers vary dramatically in how
they render content from a website. As we discuss later in this chapter,
testing for browser compatibility is one challenge associated with testing
Internet applications. Vendors loosely follow published standards to help
make browsers behave consistently, but they also build in proprietary
enhancements that cause inconsistent behavior. The remainder of the
clients employ custom applications that use the Internet as a pipeline to a
particular site. In this scenario, the application mimics a standard clientserver
application you might find on a company’s local area network.
The Web server represents the first tier in the three-tier architecture
and houses the website. The look and feel of an Internet application
Clients
Internet
Firewall
Tier 1
Web
Server
Tier 2
Business
Logic
XYZ, Inc.
Tier 3
Data
Stores
Laptop computer
IBM Compatible
FIGURE 10.1 Typical Architecture of an E-Commerce Site.
Testing Internet Applications 195
comes from the first tier. Thus, another term for this tier is the presentation
tier or layer, so dubbed because it provides the visual content to
the end user. The Web server can use static HyperText Markup Language
(HTML) pages or Common Gateway Interface (CGI) scripts to create
dynamic HTML, but most likely it uses a combination of static and
dynamic pages.
Tier 2, or the business layer, houses the application server. Here, you run
the software that models your business processes. The following lists some
of the functionality associated with the Business layer:
  Transaction processing
  User authentication
  Data validation
  Application logging
The third tier focuses on storing and retrieving data from a data source,
typically a relational database management system (RDBMS). Another
term for tier 3 is the data layer. This tier consists of a database infrastructure
to communicate with the second tier. The interface into the data
layer is defined by the data model, which describes how you want to store
data. Sometimes, several database servers make up this tier. You typically
tune database systems into this layer to handle the high transaction rates
encountered in an e-commerce site. In addition to a database server, some
e-commerce sites may place an authentication server in this layer. Most
often, you use a Lightweight Directory Application Protocol (LDAP) server
for this function.
Testing Challenges
You will face many challenges when designing and testing Internet-based
applications due to the large number of elements you cannot control and
the number of interdependent components. Adequately testing your application
requires that you make some assumptions about your customers
and how they use the site.
An Internet-based application has many failure points that you should
consider when designing a testing approach. The following list provides
some examples of the challenges associated with testing Internet-based
applications:
196 The Art of Software Testing
  Large and varied user base. The users of your website possess different
skill sets, employ a variety of browsers, and use different operating
systems or devices. You can also expect your customers to access
your website using a wide range of connection speeds. Ten years
ago not everyone had broadband Internet access. Today, most do.
However, you still need to consider bandwidth as Internet content
becomes ‘‘richer’’ and more interactive.
  Business environment. If you operate an e-commerce site, then you
must consider issues such as calculating taxes, determining shipping
costs, completing financial transactions, and tracking customer profiles.
These requirements may necessitate a number of external links
to third-party servers or databases to manage these billing and shipping
tasks, for example. The developer must thoroughly understand
the structure of the remote system, and work closely with its owners
and developers to ensure security and accuracy.
  Locales. Users may reside in other countries, in which case you will
have internationalization issues such as language translation, time
zone differences, and currency conversion.
  Security. Because your site is open to the world, you must protect it
from hackers. They can bring your website to a grinding halt with
denial-of-service (DoS) attacks, or rip off your customers’ credit
card information.
  Testing environments. To properly test your application, you will need
to duplicate the production environment. This means you should use
Web servers, application servers, and database servers that are identical
to the production equipment. For the most accurate testing
results, the network infrastructure will have to be duplicated as well,
which includes routers, switches, and firewalls.
Even from this list, which could be expanded considerably by including
viewpoints from a wide variety of developers and businesses, you can see
that configuring a testing environment is one of the most challenging
aspects of e-commerce development. Testing applications that process
financial transactions requires the most effort and expense. You must replicate
all the components, both hardware and software, used for the application
to produce valid test results. Configuring such an environment is a
costly endeavor. You will incur not only equipment costs, but labor costs
as well. Most companies fail to factor in these expenses when creating a
Testing Internet Applications 197
budget for their applications, and those that do generally underestimate
the time and monetary requirements. In addition, the testing environment
needs a maintenance plan to support application upgrade efforts.
Another significant testing challenge you face is testing browser
compatibility. There are several different browsers on the market today,
and each behaves differently. Although standards exist for browser operation,
most vendors enhance their products in an effort to attract a
loyal user base. Unfortunately, this causes the browsers to operate in a
nonstandard way. We cover this topic in greater detail later in this
chapter.
As noted, you will face many challenges when testing Internet-based
applications; therefore, the best way to proceed is to narrow your testing
efforts to specific areas. Table 10.1 identifies some of the most important
areas to test, to help ensure that users have a positive experience on your
website.
TABLE 10.1 Examples of Presentation, Business, and Data Tier
Testing
Presentation Tier Business Tier Data Tier
Ensure fonts are the
same across browsers.
Verify proper calculation of
sales tax and shipping
charges.
Ensure database
operations meet
Confirm that all links performance goals.
point to valid files or
websites.
Ensure documented
performance rates are met
for response times and
throughput rates.
Verify that data are
stored correctly and
Verify that graphics are accurately.
the correct resolution
and size. Verify that transactions
complete properly.
Verify that you can
recover using current
Spell-check each page. backups.
Confirm that failed
transactions roll back
correctly.
Test failover or
redundancy operations.
Have a copy editor check
grammar and style.
Ensure data are collected
correctly.
Test for proper data
encryption and security
(credit card and user’s
personal information, in
particular).
Check cursor positioning
when page loads to
ensure it is in the correct
text box.
198 The Art of Software Testing
Because the first impression is the most important impression, some of
your testing will focus on usability and human-factor concerns. This area
concentrates on the look and feel of your application. Items such as fonts,
colors, and graphics play a major role in whether users accept or reject
your application. Keep in mind, the developer has little or no control over
who will access a given application, how much computer knowledge they
have, whether or not they are motivated to stay with an application in the
face of navigation issues, or what users might ultimately expect in terms of
information or performance.
System performance greatly influences a customer’s first impression. As
mentioned earlier, Internet users want instant gratification. They will not
wait long for pages to load or transactions to complete. Literally, a few seconds’
delay can cause a customer to try another site. Poor performance
may also lead customers to doubt the reliability of your site. Therefore,
you should set performance goals then design tests that reveal problems
that cause your site to miss the goals.
Users also demand that their transactions complete rapidly and accurately
when purchasing products or services from your site. They do not,
and should not, tolerate inaccurate billings or shipping errors. Probably
worse than losing a customer is finding yourself liable for more than
the transaction amount if your application does not process financial
transactions correctly.
Your application likely will collect data to complete tasks such as purchases
or e-mail registrations. Therefore, you should ensure that the data
you collect are valid. For example, make sure that phone numbers, ID
Table 10.1 (continued)
Presentation Tier Business Tier Data Tier
Test backend data entry
and management
routines for usability and
accuracy.
Confirm that default
button is selected when
the page loads.
Check for consistent and
user-friendly feedback on
interactive operations.
Check for business- or
industry-specific terms
and style.
Testing Internet Applications 199
numbers, currencies, e-mail addresses, and credit card numbers are the
correct length and are properly formatted. In addition, verify the integrity
of your data. Localization issues can easily cause data corruption via truncation
due to character-set issues.
In the Internet environment, it is critical to keep the website available
for customer use. This requires that you develop and implement maintenance
guidelines for all the supporting applications and servers. A Web
server and RDBMS require a high level of management. You must monitor
logs, system resources, and backups, and respond to any anomalies immediately.
As described in Chapter 6, you want to maximize the mean time
between failures (MTBF) and minimize the mean time to recovery
(MTTR) for these systems.
Finally, network connectivity is another area where it is important to
focus your testing efforts. At some point, you can count on losing network
connectivity. The source of the failure might be the Internet itself, your
service provider, or your internal network. Therefore, you need to create
contingency plans for your application and infrastructure so your systems
respond gracefully when an outage occurs. Keeping with the theme of testing,
design your tests to break your contingency plans.
Testing Strategies
Developing a testing strategy for Internet-based applications requires a
solid understanding of the hardware and software components that make
up the application. As is critical to successful testing of standard applications,
you will need a specification document to describe the expected
functionality and performance of your website. Without this document,
you will not be able to design the appropriate tests.
You need to test components developed internally and those purchased
from a third party. For the components developed in-house
you should employ the tactics presented in earlier chapters. This includes
creating unit/module tests and performing code reviews. Integrate
the components into your system only after verifying that they
meet the design specifications and functionality outlined in the specification
document.
If you purchase components, then you need to develop a series of
system tests to validate that they perform correctly, independently of your
application. Do not rely on the vendor’s quality-control program to detect
200 The Art of Software Testing
errors in its components. Ideally, you should complete this task independently
of your application testing. Integrate these components only once
you have determined that they perform acceptably. Including a nonfunctional
third-party component in your architecture makes it difficult to
interpret test results and identify the source of errors. Generally, you will
use black-box approaches for third-party components because you rarely
will have access to the component internals.
Testing Internet-based applications is best tackled with a divide- andconquer
approach. Fortunately, the architecture of Internet applications
allows you to identify discrete areas to target testing. Figure 10.1 presented
the basic architecture of Internet applications. Figure 10.2 provides a more
detailed view of each tier.
As mentioned earlier in this chapter, Internet applications are considered
three-tier client-server applications. Each tier, or layer, from Figure
10.2 is defined as follows:
  Presentation layer. The layer of an Internet application that provides
the user interface (UI; or GUI, graphical user interface).
  Business layer. The layer that models your business processes, such as
user authentication and transactions.
  Data layer. The layer that houses data used by the application or that
is collected from the end user.
Each tier has its own characteristics that encourage test segmentation.
Testing each tier independently allows you to more easily identify bugs
Clients Internet
Credit
Card
Processing
Shipping
Companies
Bank
Account
Services
Hosted Services
Firewall
LDAP
Stores
Tier 1
Presentation
Layer
Tier 2
Business
Logic
XYZ, Inc.
Tier 3
Data Layer
Laptop computer
IBM Compatible
FIGURE 10.2 Detailed View of Internet Application Architecture.
Testing Internet Applications 201
and errors before complete system testing begins. If you rely only on
system testing, then you may have a difficult time locating the specific
components that are creating the problem.
Table 10.2 lists items that you should test in each tier. The list is not
comprehensive, but provides you with a starting point to develop your
own testing criteria. In the remainder of this chapter we provide more
details on how to test each tier.
TABLE 10.2 Items to Test in Each Tier
Test Area Comments
Usability/human
factors
Review overall look and feel.
Fonts, colors, and graphics play a major role in the application
aesthetics.
Ensure that all user input is acknowledged so that it is clear to
the user that input has been accepted.
Performance Check for fast-loading pages.
Check for quick transactions.
Poor performance often creates a bad impression.
Business rules Check for accurate representation of business process.
Consider business environment for target user groups.
Ensure that business or industry conventions of terminology
and style are followed.
Transaction
accuracy
Verify that transactions complete accurately.
Confirm that cancelled transactions roll back correctly.
Is input verification sufficiently strong to support security and
accuracy requirements?
Data validity and
integrity
Check for valid formats of phone number, e-mail addresses,
and currency amounts.
Ensure proper character sets.
System reliability Test the failover capabilities of your Web, application, and
database servers.
Maximize MTBF and minimize MTTR.
Network
architecture
Test connectivity redundancy.
Test application behavior during network outages.
202 The Art of Software Testing
Presentation Layer Testing
Testing the presentation layer consists of finding errors in the GUI, or front
end, of your application. This important layer serves as the ‘‘curb appeal’’
of your site, so detecting and correcting errors here are critical to presenting
a quality, robust website. If your customers encounter errors in this
layer, they may not return. They may conclude, for example, that if your
company posts Web pages with misspelled words, it cannot be trusted to
successfully execute a credit card transaction.
In a nutshell, presentation layer testing is very labor intensive. However,
just as you can segment the testing of an Internet application into discrete
entities, you can do the same when testing the presentation layer. Here are
the three major areas of presentation layer testing:
1. Content testing. Overall aesthetics, fonts, colors, spelling, content accuracy,
default values.
2. Website architecture. Broken links or graphics.
3. User environment. Web browser versions and operating system
configuration.
Content testing involves checking the human-interface element of a
website. You need to search for errors in font type, screen layout, colors,
graphic resolutions, and other features that directly affect the end-user
experience. In addition, you should verify the accuracy of the information
on your website. Providing grammatically correct, but inaccurate, information
harms your company’s credibility as much as any other GUI bug.
Inaccurate information may also cause legal problems for your company.
Test the website architecture by trying to find navigational and structural
errors. Search for broken links, missing pages, wrong files, or anything
that sends the user to the wrong area of the site. These errors can
occur very easily, especially for dynamic websites and during development
or upgrade phases. All a project team member needs to do is rename a file,
and its hyperlink becomes invalid. Similarly, if a graphic element is
renamed or moved, then a hole will exist in your Web page because
the file cannot be found. You can validate your website’s architecture by
creating a unit test that checks each page for architectural problems.
As a best practice, you should migrate architecture testing into the
Testing Internet Applications 203
regression-testing process as well. Numerous tools exist that can automate
the process of verifying links and checking for missing files.
White-box testing techniques are useful when testing website architecture.
Just as program units have decision points and execution paths, so do
Web pages. Users may click on links and buttons in any order, which will
navigate to another page. For large sites, there exist many combinations of
navigation events that can occur. Review Chapter 4 for more information
on white-box testing and logic coverage theory.
As mentioned earlier, testing the end-user environment—also known as
browser-compatibility testing—is often the most challenging aspect of
testing Internet-based applications. The combination of browsers and an
operating system (OS) is very large. Not only should you test each browser
configuration, but different versions of the same browser as well. Vendors
often improve some feature of their browsers with each release, which may
or may not be compatible with older versions. It is interesting (and frustrating)
to see that even in this era of advanced Internet development and
functionality, you can still encounter Web pages that display a message
saying the site is not compatible with the Web browser you are using. It
should not be the user’s responsibility to choose the right browser to access
your site. To ensure a successful user visit, spend extra time in application
design, development, and testing with a wide variety of browsers and
operating systems.
User environment testing becomes more convoluted when your application
relies heavily on client-side script processing. Every browser has a
different scripting engine or virtual machine to run scripts and code on
the client’s computer. Pay particular attention to browser-compatibility
issues if you use any of the following:
  ActiveX controls
  JavaScript
  VBScript
  Java applets
  HTML 5
  Adobe Flash
  PHP
You can overcome most of the challenges associated with browser compatibility
testing by generating well-defined functional requirements. For
204 The Art of Software Testing
example, during the requirements-gathering phase, your marketing department
may decide that the application should be certified to work only
with certain browsers. On the one hand, this requirement eliminates a
significant amount of testing because you will have a well-defined
target platform to test against. On the other hand, while this might be a
cost- and time-saving decision, it may not be a smart business decision.
The days when a single (or even a few) Web browser applications dominated
the user community are long past. Good business practice would be
to design and test against a wide range of possible user Web browser
applications.
Business Layer Testing
Business layer testing focuses on finding errors in the business logic of
your Internet application. You will find testing this layer very similar to
that of stand-alone applications, in that you can employ both white- and
black-box techniques. You will want to create test plans and procedures
that detect errors in the application’s performance specification, data
acquisition, and transaction processing.
You should employ white-box approaches for components developed
in-house, because you have access to the program logic. For third-party
components, however, black-box testing techniques should comprise your
primary testing approach. You will start by developing test drivers to unittest
the individual components. Next, you can perform a system test to
determine whether all the components work together correctly.
When conducting a system test for this layer, you need to mimic the
steps a user performs when purchasing a product or service. For example,
for an e-commerce site you may need to build a test driver that searches
inventory, fills a shopping cart, and checks out. Pragmatically modeling
these steps can prove challenging.
The technologies that you use to build the business logic dictate
how you build and conduct your tests. There are numerous technologies
and techniques you may use to build this layer, which makes it
impossible to suggest a cookie-cutter testing method. For instance,
you might architect your solution using a dedicated application server
such as JBoss. Or you could have stand-alone CGI modules written in
C, Python, or Perl.
Testing Internet Applications 205
Regardless of your approach, there exist certain characteristics of
your application that you should always test. These areas include the
following:
  Performance. Test to see whether the application meets documented
performance specifications (generally specified in response times and
throughput rates).
  Data validity. Test to detect errors in data collected from customers.
  Transactions. Test to uncover errors in transaction processing, which
may include credit card processing, e-mailing verifications, and
calculating sales tax.
Performance Testing A poorly performing Internet application raises
doubt in your user’s mind about its robustness, and often turns the person
away. Lengthy page loads and slow transactions are typical examples. To
help achieve adequate performance levels, you need to ensure that operational
specifications are written during the requirements-gathering phase.
Without written specifications or goals, you cannot know whether your
application performs acceptably. Operational specifications are often stated
in terms of response times or throughput rates. For instance, a page should
load in x seconds, or the application server will complete y credit card
transactions per minute.
A common approach you may use when evaluating performance is
stress testing. Often, performance degrades to the point of being unusable
when the system becomes overloaded with requests. This might cause
time-sensitive transactional components to fail. If you perform financial
transactions, then component failures could cause you or your customer
to lose money. The concepts on stress testing presented in Chapter 6 apply
to testing business layer performance.
As a quick review, stress testing involves blasting the application with
multiple logins, and simulating transactions to the point of failure so you
can determine whether your application meets its performance objectives.
Of course, you need to model a typical user visit for valid results. Just loading
the homepage does not equate to the overhead of filling a shopping cart
and processing a transaction. You must fully tax the system to uncover
processing errors.
Stress-testing the application also allows you to investigate the robustness
and scalability of your network infrastructure. You may think that
206 The Art of Software Testing
your application has bottlenecks that allow only x transactions per second.
But further investigation shows that a misconfigured router, server, or firewall
is throttling bandwidth. Therefore, you should ensure that your supporting
infrastructure components are in order before beginning stress
testing. Not doing so may lead to erroneous results.
Data Validation An important function of the business layer is to ensure
that data collected from users are valid. If your system operates with invalid
information, such as erroneous credit card numbers or malformed
addresses, then egregious errors may occur. If you are unlucky, the
errors could have financial implications for you and your customers. You
should test for data collection errors much like you search for user-input
or parameter errors when testing stand-alone applications. Refer to
Chapter 5 for more information on designing tests of this nature.
Transactional Testing Your e-commerce site must process transactions
correctly 100 percent of the time. No exceptions. Customers will not
tolerate failed transactions. Besides a tarnished reputation and lost
customers, you may also incur legal liabilities associated with failed
transactions.
You can consider transactional testing as system testing of the business
layer. In other words, you test the business layer from start to finish, trying
to uncover errors. Once again, you should have a document specifying
exactly what constitutes a transaction. Does it include a user searching a
site and filling a shopping cart, or does it consist only of processing the
purchase?
For a typical Internet application, the transaction component is more
than completing a financial transaction (such as processing credit cards).
Typical events related to customer transactions include:
  Searching inventory.
  Collecting items the user wants to purchase.
  Presenting the user with related items that might be of interest.
  Presenting users with product or company reviews from other users.
  Soliciting and capturing product or company reviews from the
current user.
  Creating or accessing a user account.
Testing Internet Applications 207
  Purchasing items, which may involve calculating sales tax and
shipping costs, as well as processing financial transactions.
  Notifying the user of the completed transaction, usually via e-mail.
In addition to testing internal transaction processes, you must test the
external services, such as credit card validation, banking, and address verification.
You typically will use third-party components and well-defined
interfaces to communicate with financial institutions when conducting
financial transactions. Don’t assume these items work correctly. You must
test and validate that you can communicate with the external services and
that you receive correct data back from them.
Data Layer Testing
Once your site is up and running, the data you collect become very valuable.
Credit card numbers, payment information, and user profiles are
examples of the types of data you may collect while running your
e-commerce site. Losing this information could prove disastrous and crippling
to your business. Therefore, you should develop a set of procedures
to protect your data storage systems.
Testing the data layer consists primarily of testing the database management
system that your application uses to store and retrieve information.
Smaller sites may store data in text files or open-source databases. Larger,
more complex sites, use full-featured enterprise-level databases. Depending
upon your needs, you may use both approaches.
One of the biggest challenges associated with testing this layer is
duplicating the production environment. You must use equivalent
hardware platforms and software versions to conduct valid tests. In
addition, once you obtain the resources, both financial and labor,
you must develop a methodology for keeping production and test environments
synchronized.
As with the other tiers, you should search for errors in certain areas
when testing the data layer. These include the following:
  Response time. Quantifying completion times for Structured Query
Language (SQL) operations.
  Data integrity. Verifying that the data are stored correctly and
accurately.
208 The Art of Software Testing
  Fault tolerance and recoverability. Maximizing the MTBF and minimizing
the MTTR.
Response Time Testing Slow e-commerce applications cause unhappy
and untrusting customers. Thus, it is in your interest to ensure that your
website responds in a timely manner to user requests and actions. Response
time testing in this layer does not include timing page loads; rather,
it focuses on identifying database operations that do not meet performance
objectives. When testing the data-tier response time, you want to ensure
that individual database operations occur quickly so as not to bottleneck
other operations.
That said, before you can measure database operations, you must understand
what constitutes one. For this discussion, a database operation involves
inserting, deleting, updating, or querying data from the RDBMS.
Measuring the response time simply consists of determining how long
each operation takes. You are not interested in measuring transactional
times, as that may involve multiple database operations. Profiling transaction
speeds occurs while testing the business layer.
Because you want to isolate problem database operations, you do not
want to measure the speed of a complete transaction when testing data
layer response times. Too many factors may skew the test results if you test
the whole transaction. For example, if it takes a long time for users to retrieve
their profiles, you need to determine where the bottleneck for that
operation resides. Is it the SQL statement, Web server, or firewall? Testing
the database operation independently allows you to identify the problem.
In this example, if the SQL statement is poorly written, it will reveal itself
when you test response time.
Data layer response-time testing is plagued with challenges. You
must have a test environment that matches what you use in production;
otherwise, you may get invalid test results. Also, you must have
a thorough understanding of your database system to make certain
that it is set up correctly and operating efficiently. You may find that a
database operation is performing poorly because the RDBMS is configured
incorrectly.
Generally speaking, though, you perform most response-time testing
using black-box methods. All you are interested in is the elapsed time for
database transactions. Many tools exist to help with these efforts, or you
may write your own.
Testing Internet Applications 209
Data Integrity Testing Data integrity testing is the process of finding inaccurate
data in your data stores. This test differs from data validation,
which you conduct while testing the business layer. Data validation testing
tries to find errors in data collection. Data integrity testing strives to find
errors in how you store data.
Many factors can affect how the database stores data. The datatype and
length can cause data truncation or loss of precision. For date and time
fields, time zone issues come into play. For instance, do you store time
based on the location of the client, the Web server, the application server,
or the RDBMS? Internationalization and character sets can also affect data
integrity. For example, multibyte character sets can double the amount of
storage required, plus they can cause queries to return padded data.
You should also investigate the accuracy of the reference tables used by
your application, such as sales tax, zip codes, and time zone information.
Not only must you ensure that this information is accurate, you must keep
it up to date.
Fault Tolerance and Recoverability Testing If your e-commerce site
relies on an RDBMS, then the system must stay up and running. There is
very little, if any, downtime availability in this scenario. Thus, you must
test the fault tolerance and recoverability of your database system.
One goal of database operations, in general, is to maximize MTBF and
minimize MTTR. You should find these values specified in the system
requirements documentation for your e-commerce site. Your goal when
testing the database system robustness is to try to exceed these numbers.
Maximizing MTBF depends on the fault-tolerance level of your database
system. You might have a failover architecture that allows active transactions
to switch to a new database when the primary system fails. In this
case, your customers might experience a short service disruption, but the
system should remain usable. Another scenario is that you build fault tolerance
into your application so that a downed database affects the system
very little. The types of tests you run depend on the architecture.
You should consider database recovery as equally important. The objective
of recoverability testing is to create a scenario in which you cannot
recover that database. At some point, your database will crash, so you
need to have procedures in place to recover it very quickly. The planning
for recovery begins in obtaining valid backups. If you cannot recover the
database during recoverability testing, then you need to modify your
210 The Art of Software Testing
backup plan. A fault-tolerant database system may reside in multiple locations
connected over a private or shared network. This aspect of database
management must be tested as well. If the local server fails, are the remote
systems current, and can your software connect to a remote system
quickly? What happens if one or more network connections fail? What
happens if a system failure occurs while data is being written?
In general, strive to test all aspects of the system, everything required to
support all levels of activity and data integrity for which your application
is designed.
Summary
The public Internet did not exist when the first edition of this book was
written. Indeed, remotely accessed systems, and applications in general,
were infantile compared to those of today’s Internet. Users in those early
days mostly were sophisticated, computer-savvy folk who could tolerate a
fairly high level of difficulty in accessing and using remote applications.
Today, Internet users may know very little about the actual operation of
computers and computer software, yet they have a virtually infinite choice
of commercial sites from which to choose. Consequently, they have little or
no patience for a Web-based application that is unattractive, difficult to
use, or dysfunctional. Therefore, in-depth testing of any Internet application
is extremely important.
Testing software in the Internet environment presents many challenges,
particularly the large and varied user base and the need for extreme accuracy
and security for electronic commerce applications. In general, we
want to test three main Internet application areas: presentation (or user
interface), business logic, and data management. As might be expected,
large user-base applications require extensive user testing (see Chapter 7
for more information on this process) to ensure that the software meets
design specifications and user acceptance criteria. It is important for any
software application to be attractive and easy to use, but applications for
the Internet are judged more harshly. In this environment, software success
often equates to business success, and this factor alone should drive
developers toward aggressive and thorough testing.
Testing Internet Applications 211

11 Mobile Application
Testing
Computer technology changes rapidly. In a blink of an eye the computer
went from the desktop to the laptop and now to the handheld
mobile device. This migration has changed the way we conduct our lives,
businesses, and governments. It has also significantly affected the way software
developers and testers do their jobs.
Most software testing professionals find testing mobile applications very
challenging—more so than almost any other software types or platforms.
Actually, it’s the devices and mobile environment more than the ‘‘application’’
that impose the challenge. These two components add many
variables and complexities that may skew or mask problems in your
application, which makes designing a robust test plan difficult. Briefly, you
need to consider network performance and reliability, consistent user
interfaces, transcoder influences, device diversity, and limited resource
platforms.
In this chapter, we introduce a relatively new area of software testing:
testing mobile and smartphone applications. We begin by describing the
mobile application environment, which differs from that of a stand-alone
application on desktops, laptops, and servers. Next, we enumerate the
challenges of testing mobile applications—some of which we touched on
earlier in this book. Finally, we cover some testing approaches and testcase
considerations to help lower your learning curve in this new territory.
After reading this chapter you should better understand the challenges and
hurdles of testing mobile application.
213
Mobile Environment
With the widespread rollout of wireless hotspots, the line between mobile
computing and ‘‘traditional’’ wireless network-based activities has blurred.
Thus, to begin here we need to define the terms mobile device and mobile
applications, with respect to the content of this chapter. In that light, we
refer to a mobile device as one that has the capability to run network-based
applications over a cellular or satellite data link. This encompasses most
smartphones, tablets, and PDAs. That said, don’t make the mistake of identifying
mobile devices only by their appearance. Modern laptops can accept
plug-in cellular or satellite cards, and some laptop devices have this
access built in. Based on this definition of a mobile device then, a mobile
application is a network-based program that runs on a mobile device.
This distinction is important. Yes, it is true that most mobile devices can
use hotspots and wireless access points without a problem. However, those
connections provide greater reliability and higher speeds than cellular networks,
even with the adoption of 3G and 4G technologies. Thus, you design
your mobile application with the expectation that it will use relatively slow,
comparatively unreliable data links. You can also develop stand-alone applications,
such as games, that run on a mobile device without the need to use
the carrier’s network. But for the purposes of this chapter, we do not consider
stand-alone applications as mobile applications. Our focus is on the
challenges associated with applications running on cellular data networks.
The key to creating successful test plans for your mobile applications is
to understand the mobile computing environment. Table 11.1 identifies a
number of crucial areas you should investigate while designing test plans.
First, you must understand device connectivity issues and network speeds,
regional availability, and latency. Keep in mind the underlying philosophy
of this book: Your tests should not prove that your application works, but
that your application does not work for the use cases. For example, if you
have a location-based service or e-mail application, then your tests should
identify software problems when the carriers network is slow or
unavailable.
Next are three areas regarding devices—diversity, constraints, and input
methods—which we cover in great detail later in the chapter. To create
successful test plans, you and your testing staff must consider the numerous
devices in the marketplace, the varying capabilities of each, and how
the user interacts with the devices.
214 The Art of Software Testing
Last, you need to determine how you will install and maintain your
application. Some vendors, such as Apple, maintain online stores where
the user purchases the application, but only after Apple certifies your
application for its platform. This makes installation and maintenance a
little easier, as you have a single, certified distribution system.
Testing Challenges
As stated, mobile application testing is fraught with challenges. To help
meet them, we can categorize most into four categories: device diversity,
carrier network infrastructure, scripting, and usability. You need to think
through each carefully when designing test cases. The shear combination
of device types, operating systems, user input methods, and network concerns
mean that trade-offs must be balanced with time, financial, and labor
TABLE 11.1 Mobile Environment Test Design Considerations
Area Comment
Connectivity Device provisioning
Network speed
Network latency
Network availability in remote areas
Service reliability
Diversity Devices Numerous web browsers to test
Multiple versions of runtimes for Java or other
languages
Device Donstraints Limited memory or processor
Small screen size
Multiple operating systems
Multitasking capabilities
Data cache sizes
Input Devices Touch screens
Stylus
Mouse
Buttons
Rollers
Installation and
Maintenance
Installing and uninstalling
Patching
Upgrading
Mobile Application Testing 215
resources to arrive at an economical test plan that detects most bugs in a
reasonable time frame. Building a testing strategy that combines the methods
discussed in earlier chapters will help.
In the rest of this section we discuss these categories and offer advice on
how to tackle each one.
Mobile Device Diversity
The ever-expanding diversity of devices presents an often-underestimated
and significant testing challenge to someone new to mobile application
testing. It sometimes seems that manufacturers introduce new devices
daily, making it almost impossible to keep up with the release cycles.
Worse, more devices means more items to consider in your testing. Here’s
a simple example to illustrate only a few items you need to evaluate when a
new device is released:
Suppose Motorola develops a new method of text input via the touch
screen for its Android-based phones. Can you design a test to determine
whether the device’s new input method breaks your application?
If it does, can you fix your application without breaking
support for other Android-based devices such as tablets? Can you
even obtain a device to test? Do you have access to a supported carrier
network?
Almost by definition, along with diversity of the devices comes diversity
of operating systems, browsers, application runtime environments, screen
resolutions, user interfaces, ergonomics, screen size, and more. You must
be aware of all of these factors when creating tests. Device diversity also
forces usability testing front and center, which at some point requires testers
to evaluate your application on target devices. Using emulators is great
way to start, but ultimately you will need to test real devices on real carrier
networks.
This raises another facet of mobile application testing: testing on real
devices versus emulators. From an economic standpoint you should do as
much testing as you can with emulators. It may be financially unfeasible,
even if you can obtain a device and access the wireless network, to test on
the real platform. That said, emulators only emulate; they are not the real
devices. So it is likely that you will observe differences between testing
216 The Art of Software Testing
with an emulator and the actual device. For example, the colors and
shapes of buttons and input boxes may pass acceptance tests on an emulator
but fail on the target device because of screen resolution and color
depth differences between the device and a PC-based emulator.
In short, you need to realize there may be hundreds of mobile devices
with the potential to access your application. Therefore, during the
requirements-gathering and specification-writing phases, you will be
called upon to make some tough decisions and choose a reasonable subset
of devices to support and test. Be mindful that every device you do not test
may not work with your application; hence, you may lose not only a customer
but a customer base.
Carrier Network Infrastructure
Testing your application on a carrier network sets up another challenge.
This is especially true if you want to support multiple carriers. Two of the
highest hurdles to jump are: understanding and adapting to the carrier’s
infrastructure, and overcoming location-based obstacles.
Understanding a carrier’s infrastructure is fundamental to developing a
good test plan. Initially, you would think that your mobile application uses
a carrier’s network like an IP wireless hotspot. Not so. Figure 11.1 illustrates
the ‘‘typical’’ infrastructure of most wireless carriers. The first difference
is that the protocol is not IP-based; it is usually an RF-based protocol
Internet
Firewall
Mobile Proxy
Web Proxy
or transcodes
Tablets
Devices
Smartphones
PDAs
Carrier telco
EQUIP
FIGURE 11.1 Generic Wireless Carrier Data Network.
Mobile Application Testing 217
such as code division multiple access (CDMA), time division multiple access
(TDMA), or global system for mobile (GSM). The RF-based protocols
treat the IP-based protocols as a ‘‘payload’’ and delivers them to the mobile
device, which then decodes the payload and presents it to the application.
Also most carriers use some form of transcoder or Web proxy between
the Internet and the device. These devices may perform a variety of functions.
And, it is sometimes hard to determine exactly what occurs unless
you work directly with the carriers. Often, they do not reveal this information
for competitive purposes. The following is a short list of what may
occur at a carrier’sWeb proxy or transcoder:
  Transform or transcode content intoWAP or HTTP.
  Compress data for better throughput.
  Encrypt traffic for privacy and security.
  Block access to certain high-bandwidth sites.
  Strip HTML headers and other metadata from Web pages that your
application may use.
Transcoding may cause UI inconsistencies across multiple devices.
Some devices support Wireless Application Protocol (WAP) while others
support HTTP. WAP uses Wireless Markup Language (WML) for content
delivery. WAP and WML were intended to be the ‘‘standard’’ for wireless
content delivery, but never gained a strong foothold. Nonetheless, numerous
devices implement it, so you may encounter it during your tests. However,
most smartphones and tablets support HTML and therefore rely on
HTTP to deliver content. If you have UI problems across devices and carriers,
check with each to determine whether WAP/WML or HTTP/HTML is
being used.
Although data compression is intended to improve throughput, often
during periods of high activity throughput may slow due to the overhead
of compression. The same holds with security: Firewalls and similar layers
may slow throughput during high-volume hours.
Finally, you must overcome location-based hurdles. Obviously, to test
on a carrier’s network, you need access to it. For instance, what if you have
a travel application for a smartphone: How do you test carrier networks in
other parts of the country or in other countries? Answer: You must travel
there or hire someone there to test it for you. Both add to the cost of
testing.
218 The Art of Software Testing
Scripting
An often-overlooked area of mobile applications testing is creating and
running test scripts. Real devices do not allow you to load automated, repeatable
scripts onto the device; test personnel manually execute all
scripts. That is, someone walks through a written test script designed
to find errors in a test case on the target device. Notice we said ‘‘target’’
device. There exist many targets in the mobile environment.
As we pointed out in previous chapters, manual testing is error prone.
Unfortunately, it is unavoidable when testing mobile applications on real
devices. As mentioned, most emulators have rich scripting functionality
and can perform the bulk of regression testing and system tests. However,
in the end you still need to have someone work with the device. (Later in
the chapter, we explain how to create a generic manual test script to support
multiple devices.)
The refreshing news is that mobile devices are becoming much more
sophisticated and powerful. Given the competitiveness of the marketplace,
it is reasonable to expect an automated scripting product to appear. Apple’s
iOS,Windows Mobile OS, and the Android OS are maturing rapidly, so it is
likely that this problem may be a non-issue in future versions.
Usability
Usability testing presents challenges similar to those of test scripts. Recall
from previous chapters that usability testing is mostly a white-box approach.
Just like testing stand-alone desktop applications, a testing staff
must manually try to find bugs in the user interface and user interaction
layers of your application.
Unlike testing stand-alone desktop applications, mobile device testing
involves more than one platform to test. For instance, you will want to
search for UI consistency issues between Apple’s products and the Android-
based platforms. Although you are testing mobile applications,
much of Chapter 7’s discussions apply.
Testing Approaches
Some areas of testing mobile devices are similar to testing Internet applications,
especially when evaluating the back-end infrastructures. The major
Mobile Application Testing 219
difference lies in how you approach testing the device itself. With Internet
testing, you have only a handful of browsers to evaluate; with mobile devices,
you have exponentially more.
Naturally, when testing back-end components, you should employ similar
techniques and evaluate similar considerations as those discussed in
Chapter 10, ‘‘Testing Internet Applications.’’ Referring back to Figure
10.1, tiers 2 and 3 should have approximately the same configuration as a
normal Internet application. As a quick review, you should test the performance
specifications, data validation routines, and transaction processing
components of tier 2. Testing tier 3 also is the same as with Internet
applications; test response times, data integrity, fault tolerance, and recoverability
on this tier. If possible test the tier 2 and 3 components separately
from the device to ensure they meet your design specifications
using function testing.
Testing tier 1, the user environment, differs from traditional Internet
testing. The concepts presented on testing your content and website
architecture still apply. However, user environment testing equates to
device testing.
We should note the importance of use cases when developing test plans
for your devices. Knowing who will use your application, and how and
when, is imperative, as mobile applications have numerous points of failure.
Table 11.2 lists items you might not generally consider when designing
test cases for standard applications, whether stand-alone orWeb-based. For
example, testing your application on the carrier’s network is extremely important.
You want to find problems related to spotty coverage or sudden
loss of connectivity. If your application involves data transfers, look for
problems with data caching and incomplete synchronization with back-end
data stores. What happens when coverage is suddenly restored after an interruption
during an application download? Does a purchase occur twice?
Check for bugs related to handling session reinitialization and data corruption.
Some of these issues apply to Web-based applications running in a
PC-based browser. However, LANs/WANs are much more stable. When
dealing with cellular networks, you should expect to lose connectivity.
A test case specific to mobile testing is how your application handles
incoming voice calls and text messages. Chances are end users will want
to suspend your application, or run it in the background, while they answer
the phone or read the text message. Try to build test cases where incoming
calls and messages cause problems in your application.
220 The Art of Software Testing
TABLE 11.2 Test Categories for Mobile Application Testing
Test Category Description
Install/Uninstall Ensure the user can correctly install your application.
Ensure the user can completely uninstall your application.
Network
Infrastructure
Verify the application responds appropriately to loss of
network.
Verify the application responds appropriately to network
restoration.
Verify the application responds appropriately to weak signals.
Incoming Call/
Message
Handling
Test whether user can accept calls/text messages while
application is running.
Test whether user can resume application when finishing
calls/text messages.
Test whether user can reject calls/text messages without
disrupting application.
Test whether user can initiate a call/text message without
disrupting the application.
Low Memory Ensure application remains stable when device encounters a
low memory situation.
Key Mappings Test that all key mapping works as specified.
Feedback Ensure user feedback to keypress occurs within application
design specifications.
Exiting Verify that the application exits gracefully when initiated through
pressing keys, closing the cover, or using the slider.
Confirm the application meets design specifications when the
user initiates a shutdown of device.
Charging Ensure that application works as designed when entering
charge mode.
Ensure that application works as designed while in charge mode.
Ensure that application works as designed when exiting charge
mode.
Battery
Conditions
Test how the application behaves on a low battery.
Measure how quickly application drains the battery.
Ensure the application responds per specification when the
battery is removed while the device is powered on.
Device Interaction Ensure the application does not overload the CPU.
Ensure the application does not consume too much memory.
Mobile Application Testing 221
In the rest of the chapter we will cover some approaches to device testing
in which you basically have two choices: test on real devices or use
device emulators. Table 11.3 offers some advantages and disadvantages of
each approach.
Testing with Real Devices
Manual testing with real devices is inevitable. Although costly, it has some
advantages. Only by testing with the device can you experience its nuances
and get a true feel for the user’s experience. In addition, you can only test
certain cases with real devices. Testing the reliability of a carrier’s network
and determining the effect of an incoming call or text message are obvious
examples. On a real device you also can evaluate how your application behaves.
Does it load fast and run at an acceptable speed? Does it look okay?
Is the UI consistent across your target devices? Last but not least, you can
determine device-specific bugs. This is almost impossible with an emulator.
If you do find a device-specific bug, the challenge is to fix it without
breaking compatibility with other devices.
Despite the advantages, testing with real devices also has some serious
drawbacks. For example, it is costly because you must purchase the device,
TABLE 11.3 Devices versus Emulators
Testing
Approach Disadvantages Advantages
Real
Devices
Expensive, especially if you target a
broad base of mobile devices
Ability to test responsiveness
of the application
Inability to install metering or
diagnostic development tools
Visual inspection of application
on real device to verify UI
Unable to install on run test consistency
scripts Test carriers’ network
Network availability responsiveness
Identify device-specific bugs
Emulators Inability to identify device- related
bugs
Cost-effective
Underlying hardware may skew
performance on real device
Easy to manage; multiple
device support with single
emulator
222 The Art of Software Testing
as well as pay for carrier airtime. Neither is inexpensive, and if you are testing
multiple devices from multiple carriers in multiple regions, the
expenses grow accordingly. Some device manufacturers and service providers
have devices that you can rent or access remotely, which may mitigate
some costs. If you target an individual platform, such as the Apple
iPhone family, you may be spared much of this expense. Still, you will
need enough of each type (iPad, iPhone, iTouch) to test.
In addition, testing with real devices is a manual, white-box process.
Someone must push the buttons, tap the screens, and enter data. As you
know, manual testing is error prone, even with the best instructions and
trained testers. Plus, it adds another expense to the process. You should
keep exacting notes about each well-documented test script and its results.
Then evaluate the effectiveness of the scripts and eliminate those with little
or no value (i.e., fail to find bugs).
As we noted earlier, using real devices eliminates one important weapon
in the software tester’s arsenal: automated test scripts. Therefore, you
should use written, manual scripts that specify generic actions, not details
on how to perform the action on a device. Detailed test scripts for every
device would be a challenge to create and maintain. In short order you
would have a library of scripts, which may be obsolete when the device is
updated. Generic scripts allow you to test system specifications across
multiple devices.
For example, iPhones, iPads, and Android-based devices rely heavily on
touch screens for user input. Other devices, such as BlackBerries or ‘‘standard’’
phones, have keyboards or keypads to allow for user input. Table 11.4
provides an example script to check whether your application, an e-reader,
aborts if you receive a text message while reading an e-book. Notice the
script does not specify exactly how to do any one step, only to perform the
step using the user-input facilities of the device. These may be buttons,
touch screens, or voice commands. At no point do you specify ‘‘Press OK’’
or ‘‘Press Send.’’ This generic approach will allow you to evaluate test cases
across multiple devices.
Last, manufacturers often ‘‘lock down’’ real devices, meaning you cannot
load tools to monitor or debug your application. So when you hit a bug it is
more challenging to isolate the problem. For instance, if your application
is running slowly, you do not know whether it is the carrier’s network,
transcoding issues, your application, or a combination. Only by trial and
error can you identify problems.
Mobile Application Testing 223
Testing with Emulators
Testing with emulators may not be the preferred approach, but it is usually
the most practical and cost-effective, and it even has some advantages.
First, emulators allow for inexpensive and quick functional testing of your
application. You can step through the application to find events and circumstances
that do not meet the program requirements. Identify these
bugs using emulators before you get into the expense of device testing.
Second, emulators are easy to manage, and because they run on PCs,
every tester or developer can have an emulator. Developers can manage
the software themselves, precluding the need of system administrators.
Third, most emulator packages support multiple devices. To test a different
device, just load a different device profile. Best of all, you incur no expensive
carrier airtime costs. Fourth, emulators run on computers with more
resources, such as faster CPUs and more memory. Fast response times
during testing enables you to complete tests more quickly.
The last and probably most significant advantage is that most emulators
employ high-level scripting languages, so you can create consistent, automated
tests, which are less error prone and quicker than manual testing.
Automated scripting also allows for easier and faster regression testing,
which is especially important when verifying that changes made to your
TABLE 11.4 Generic Device Test Script
1. Start e-reader application.
2. Open e-book.
3. Initiate SMS message to device from another device.
4. Verify SMS message alert is displayed.
5. Open SMS message.
6. Choose Reply to SMS message.
7. Compose SMS message.
8. Send SMS message.
9. Verify SMS message sent notification.
10. Return to e-book.
11. Verify e-book application is running.
12. Verify return to same page or bookmark.
13. Exit e-reader application.
224 The Art of Software Testing
application to support one device don’t break support for another. The
scripting languages in emulators generally are device-agnostic. Referring
to Table 11.4, when you script Step 8, ‘‘Send SMS Message,’’ the emulator
will perform that function regardless of the device. This allows scripts to
be used across devices.
The disadvantage of using emulators for testing is that you cannot identify
the nuances and bugs of each device. As we’ve said before, at some
point, you must test your application on the target devices.Without testing
on real devices, you never can be 100 percent sure that you meet compatibility
and performance specifications. Nonetheless, do not rule out using
emulators for the bulk of your testing. It is a cost-effective and efficient
way to eliminate most of your bugs.
Summary
Mobile application testing represents a new frontier in software testing.
The mobile environment adds greater complexity and more interactions
not experienced when testing standard stand-alone applications. That
said, with an understanding of the challenges, you can greatly improve
your chances of successfully testing your application.
Begin by trying to gain a handle on the device universe you want to
support. Do you want to support only Android-based smartphones and
tablets, or go for broke and support most major tablet and smartphone
vendors? Next, understand the carrier’s network infrastructure. Does it
transcode, encrypt, compress, or in any way modify the data before sending
it to the device?
You also need to find a balance between emulator and real device testing.
Both have their pros and cons. Due to costs, you will likely use emulators
more, and save device testing for the final phases. Use the test categories in
Table 11.2 as a starting point to developing your own. Refer to the categories
often when defining your test cases. Also, treat any written test script and
result like source code; ensure you have adequate backups and some form
of change control on the test documents. To save time and money, review
the effectiveness of each script and eliminate ones that fail to add value.
Once you understand the fundamentals of mobile application testing,
you should have no problems creating test plans and use cases. One thing
is certain, mobile applications are here, and sooner or later you will need
to learn how to test these unique applications. Why not start now?
Mobile Application Testing 225

Appendix
Sample Extreme Testing
Application
1. check4Prime.java
To compile:
&> javac check4Prime.java
To run:
$> java -cp check4Prime 5
Right . . . 5 is a prime number!
$> java -cp check4Prime 10
Sorry . . . 10 is NOT a prime number!
$> java -cp check4Prime A
Usage: check4Prime x
– where 0<=x<=1000
Source code:
//check4Prime.java
//Imports
import java.lang.*;
public class check4Prime {
static final int max = 1000; // Set upper bounds.
static final int min = 0; // Set lower bounds
static int input =0; // Initialize input variable
public static void main (String [] args) {
//Initialize class object to work with
check4Prime check = new check4Prime();
try{
//Check arguments and assign value to input variable
check.checkArgs(args);
227
//Check for Exception and display help
}catch (Exception e){
System.out.println("Usage: check4Prime x");
System.out.println(" – where 0<=x<=1000");
System.exit(1);
}
//Check if input is a prime number
if (check.primeCheck(input))
System.out.println("Right... " + input + " is a prime number!");
else
System.out.println("Sorry... " + input + " is NOT a prime number!");
} //End main
//Calculates prime numbers and compares it to the input
public boolean primeCheck (int num){
double sqroot = Math.sqrt(max); // Find square root of n
//Initialize array to hold prime numbers
boolean primeBucket [] = new boolean [max+1];
//Initialize all elements to true, then set non-primes to false
for (int i=2; i<=max; i++){
primeBucket[i]=true;
}
//Do all multiples of 2 first
int j=2;
for (int i=j+j; i<=max; i=i+j){ //start with 2j as 2 is prime
primeBucket[i]=false; //set all multiples to false
}
for (j=3; j<=sqroot; j=j+2){ // do up to sqrt of n
if (primeBucket[j]==true){ // only do if j is a prime
for (int i=j+j; i<=max; i=i+j){ // start with 2j as j is prime
primeBucket[i]=false; // set all multiples to false
}
}
}
//Check input against prime array
if (primeBucket[num] == true) {
return true;
}else{
return false;
}
}//end primeCheck()
228 Appendix
//Method to validate input
public void checkArgs(String [] args) throws Exception{
//Check arguments for correct number of parameters
if (args.length != 1) {
throw new Exception();
}else{
//Get integer from character
Integer num = Integer.valueOf(args[0]);
input = num.intValue();
//If less than zero
if (input < 0) //If less than lower bounds
throw new Exception();
else if (input > max) //If greater than upper bounds
throw new Exception();
}
}
}//End check4Prime
2. check4PrimeTest.java
Requires the JUnit api, junit.jar
To compile:
$> javac -classpath .:junit.jar check4PrimeTest.java
To run:
$> java -cp .:junit.jar check4PrimeTest
Examples:
Starting test . . .
. . . . . .
Time: 0.01
OK (7 tests)
Test finished . . .
Source code:
//check4PrimeTest.java
//Imports
import junit.framework.*;
public class check4PrimeTest extends TestCase{
//Initialize a class to work with.
private check4Prime check4prime = new check4Prime();
//constructor
public check4PrimeTest (String name){
super(name);
}
Appendix 229
//Main entry point
public static void main(String[] args) {
System.out.println("Starting test...");
junit.textui.TestRunner.run(suite());
System.out.println("Test finished...");
} // end main()
//Test case 1
public void testCheckPrime_true(){
assertTrue(check4prime.primeCheck(3));
}
//Test cases 2,3
public void testCheckPrime_false(){
assertFalse(check4prime.primeCheck(0));
assertFalse(check4prime.primeCheck(1000));
}
//Test case 7
public void testCheck4Prime_checkArgs_char_input(){
try {
String [] args= new String[1];
args[0]="r";
check4prime.checkArgs(args);
fail("Should raise an Exception.");
} catch (Exception success){
//successfull test
}
} //end testCheck4Prime_checkArgs_char_input()
//Test case 5
public void testCheck4Prime_checkArgs_above_upper_bound(){
try {
String [] args= new String[1];
args[0]="10001";
check4prime.checkArgs(args);
fail("Should raise an Exception.");
} catch (Exception success){
//successfull test
}
} // end testCheck4Prime_checkArgs_upper_bound()
//Test case 4
public void testCheck4Prime_checkArgs_neg_input(){
try {
String [] args= new String[1];
args[0]="-1";
check4prime.checkArgs(args);
fail("Should raise an Exception.");
} catch (Exception success){
//successfull test
}
}// end testCheck4Prime_checkArgs_neg_input()
//Test case 6
public void testCheck4Prime_checkArgs_2_inputs(){
try {
String [] args= new String[2];
args[0]="5";
args[1]="99";
230 Appendix
check4prime.checkArgs(args);
fail("Should raise an Exception.");
} catch (Exception success){
//successfull test
}
} // end testCheck4Prime_checkArgs_2_inputs
//Test case 8
public void testCheck4Prime_checkArgs_0_inputs(){
try {
String [] args= new String[0];
check4prime.checkArgs(args);
fail("Should raise an Exception.");
} catch (Exception success){
//successfull test
}
} // end testCheck4Prime_checkArgs_0_inputs
//JUnit required method.
public static Test suite() {
TestSuite suite = new TestSuite(check4PrimeTest.class);
return suite;
}//end suite()
} //end check4PrimeTest
Appendix 231

Index
A
Acceptance testing, 131
extreme, 184, 186
Agile development, 175
manifesto, 176
table-methodologies, 177
Agile testing, 175, 178
Application server, 205
Automated debugging tools, 159
B
Backtract debugging, 167
Beck, Kent, 176
Beedle, Mike, 176
Big-bang testing, 98
Black-box testing, 8
equivalence partitioning, 49
usability testing, 145
Black box–white box comparison,
42
Bottom-up testing, 107
compared with top-down testing,
108
Boundary value analysis, 55
guidelines for, 56
MTEST program for, 57
Branch coverage testing, 44
Browser compatibility testing, 204
Brute force debugging, 158
Business layer, 196, 201
Business layer testing, 205
Business tier
table-testing criteria, 198
C
C/S architecture, 195
C++
black-box testing of, 9
Carrier network infrastructure, 217
Cause effect graphing
constraint symbols for, 65
logic diagram for, 64
sample, 64
sample-without constraints, 71
symbols for, 63
with exclusive constraint, 66
Cause–effect graphing, 61
test cases for, 62
CDMA, 218
CGI, 196, 205
Client server architecture, 195
COBOL
history of, 26
Cockburn, Alistair, 176
Code division multiple access, 218
Code inspections, 22
Common Business Oriented
Language. See COBOL
233
Common gateway interface, 196
Comparison errors, 29
Compatibility/conversion testing,
127
Component tests, 153
Computation errors, 28
Computer
definition, 1
Condition coverage testing, 45
Condition masking, 46
Configuration testing, 126
Control-flow graph, 11
Control-flow errors, 31
Cunningham,Ward, 176
D
Data-declaration errors, 28
Data-gathering methods
usability testing, 150
Data-integrity testing, 210
Data layer, 201
Data-layer testing, 208
Data-reference errors, 25
Data tier
table-testing criteria, 198
Data validation, 207
Data-driven testing. See Black-box
testing
Debugging, 157
automated tools for, 159
by backtracking, 167
by brute force, 158
by deduction, 163
by induction, 160
clue structuring example, 163
error analysis, 172
inductive flowchart, 160
principals of, 168
programmer resistance, 157
with test cases, 167
Debugging principals
error locating, 168
error repairing, 170
Decision coverage testing, 44
Decision/condition coverage
testing, 46
Deductive debugging, 163
the process flowchart, 164
the steps, 164
Desk checking, 21, 37
DISPLAY command
cause-effect graph for, 72
graph for, 70
Diversity
mobile devices, 216
Documentation
software flowchart, 116
Driver module, 98
E
E-Commerce
basic architecture of, 195
Economics of testing, 8
Equivalence class form, 51
Equivalence classes
identifying, 51
table-classes list, 54
test cases for, 52
Equivalence partitioning, 49, 50
Error analysis
with debugging, 172
Error checklist, 25
Error guessing, 80
Errors
comparison, 29
computation, 28
234 Index
control flow, 31
data declaration, 28
estimating by plotting, 140
estimating number, 136
input/output, 33
interface, 32
rounding, 29
table-when errors found, 138
Essential unified process, 178
EssUP. See Essential unified process
Exhaustive input testing, 9
Extreme acceptance testing, 186
Extreme programming, 179
table-12 practices, 182
Extreme programming basics, 180
Extreme testing, 179, 180
acceptance testing with, 184
applied, 187
concepts of, 184
JUnit test driver, 190
test case design, 188
unit testing with, 184
Extreme unit testing, 185
Eye tracking, 151
F
Facility testing, 123
Fault tolerant testing, 210
Form
equivalence class, 51
Formula Translating System. See
Fortran
Fortran
history of, 26
Fowler, martin, 176
Function test
purpose of, 116
Function testing, 119
G
Global system for mobile, 218
Graphical User Interface, 2
Graphing
cause effect, 61
Grenning, James, 176
GSM, 218
GUI. See Graphical User Interface
H
Hallway testing, 147
Higher order testing, 113
performing the test, 130
test plan components, 133
test planning and control, 132
Highsmith, Jim, 176
HTML, 196
Human testing, 19
Hunt, Andrew, 176
Hypertext markup language, 196
I
Incremental testing, 96
Independent test agency, 141
Induction debugging, 160
Inductive debugging
steps for, 161
structuring the clues, 161
Inductive flowchart
for program debugging, 160
Input/Output errors, 33
Input/output testing. See Black-box
testing
Inspection error
checklist summary table, 35
Inspections
agenda for, 23
effectiveness of, 21
Index 235
Inspections (continued)
error checklist, 25
side benefits of, 24
team description, 22
time required, 24
Inspections and walkthroughs, 20
Installation testing, 127, 132
Interface errors, 32
Internet applications
data integrity testing in, 210
data layer testing in, 208
data validation in, 207
fault tolerant testing in, 210
illustration-architecture, 201
performance testing, 206
recoverability testing in, 210
response time testing in, 209
table-test criteria, 202
testing of, 193
testing strategies, 200
transactional testing in, 207
Internet testing
challenges of, 196
J
JBoss, 205
Jeffries, Ron, 176
JUnit, 187
test driver, 190
K
Kern, Jon, 176
L
LDAP, 196
Lightweight directory application
protocol, 196
Logic coverage testing, 43
Logic-driven testing. See White-box
testing
M
Marick, Brian, 176
Martin, Robert C., 176
Mean Time Between Failures, 128,
200
Mean Time to Repair, 129, 200
Mellor, Steve, 176
Mobile application
definition, 214
Mobile application testing, 213
approaches to, 219
challenges, 215
scripting in, 219
table-categories, 221
table-devices versus emulators,
222
table-generic test script, 224
usibility testing in, 219
with emulators, 224
with real devices, 222
Mobile device
definition, 214
Mobile device diversity, 216
Mobile environment, 214
table-test design considerations,
215
Module
driver, 98
input tables for, 87
stub, 98
Module test
purpose of, 116
Module testing, 85
performing the test, 109
test case design, 86
236 Index
MTBF, 200, 210, See Mean Time
Between Failures
MTEST
program input chart, 58
program specifications, 57
MTTR, 200, 210, See Mean Time To
Repair
Multiple-condition coverage
testing, 47, 48
N
Nielsen, Jakob, 148
Nonincremental testing, 98
P
Palo Alto Research Center,
143
PARC. See Palo Alto Research
Center
Path sensitizing, 73
Peer ratings, 38
Performance testing, 126, 206
Performing the test
higher order testing, 130
PL/1
background, 88
Presentation layer, 196, 201
Presentation layer testing, 203
Presentation tier
table-testing criteria, 198
Principals of debugging
error locating, 168
error repairing, 170
Procedure testing, 130
Program
12 module sample, 102
Agile development, 175
breakpoints in, 159
control flow graph, 11
error checklist, 25
inspections, walkthroughs and
reviews, 19
Java sample, 43
module input tables, 87
regression testing, 16
sample flowchart, 43
six module diagram, 98
testing principals, 13
Program testing
definition, 17
successful criteria, 18
Program testing guidelines, 13
Psychology of testing, 5
Q
Questionnaire
usability testing, 152
R
Random input testing, 41
Rapid application development,
179
Rational unified process, 178
RDBMS, 196, 209, 210
Recoverability testing, 210
Recovery testing, 129
Regression testing, 16
Relational database management
system, 196
Reliability testing, 127
Remote user testing, 151
Response time testing, 209
Resultant decision table, 76
Rounding error
Java code sample, 29
RUP. See Rational unified process
Index 237
S
Schwaber, Ken, 176
Scripting
in mobile application testing,
219
Security testing, 125
Serviceability/maintenance testing,
129
Software
documentation flowchart,
116
documentation of, 115
external specification, 114
testing versus development,
117
Software development
process flowchart, 114
Software reliability engineering
(SRE), 128
Software testing
correct definition, 6
economics of, 8
wrong definition, 5
Software testing principals, 12
SQL, 209
SRE. See Software Reliability
Engineering
Storage testing, 126
Stress testing, 123, 206
Stub module, 98
Sutherland, Jeff, 176
System test
flowchart for, 121
purpose of, 116
System testing, 119
facility, 123
stress, 123
volume, 123
T
TDMA, 218
Test-case
for extreme testing, 188
Test-case debugging, 167
Test-case design, 41
module testing, 86
unit testing, 86
Test-case exam, 2
Test-case strategy, 82
Test cases
table-categories of, 122
types of, 167
Test completion criteria, 135
Test planning and control, 132
Test user selection
usability testing, 147
Testing, 13, 44
acceptance, 131
agile, 178
agile environment, 175
big-bang, 98
branch coverage, 44
browser compatibility, 204
business layer, 205
code-oriented, 20
compatibility/conversion, 127
completion-criteria, 135
condition-coverage, 45
condition-masking, 46
configuration, 126
debugging, 157
decision-coverage, 44
decision/condition-coverage, 46
desk-testing, 21
estimating-errors, 136
human, 19
installation, 127, 132
238 Index
Internet applications, 193
mobile applications, 213
multiple-condition coverage, 47
nonincremental, 98
performance, 126
presentation layer, 203
procedure-testing, 130
recovery, 129
reliability, 127
security, 125
serviceability/maintenance, 129
storage, 126
top-down, 101
usability, 125, 143
usability questionnaire, 152
Web applications, 194
Testing approaches
mobile applications, 219
Testing principals, 13
Testing strategies
Internet applications, 200
Think aloud protocol, 150
Thomas, Dave, 176
Time-division multiple access, 218
Top-down design, 101
Top-down development, 101
Top-down testing, 101
compared with bottom up
testing, 108
Transactional testing, 207
Triangle
Definition, 2
U
UI, 218
Unit testing, 85
extreme, 185
test case design, 86
with extreme testing, 184
Uptime requirements
table-hours per year, 129
Usability
in mobile application testing, 219
Usability testing, 125, 143
component tests, 153
conducting sufficient tests, 153
data gathering methods, 150
determining number of testers,
148
eye tracking, 151
graph-errors versus testers, 149
hallway testing, 147
questionnaire, 152
remote user testing, 151
test user selection, 147
testing considerations, 144
the process, 146
think aloud protocol, 150
User interface, 218
User testing, 143
V
Van Bennekum, Arie, 176
Volume testing, 123
W
Walkthroughs, 34
effectiveness of, 21
WAP, 218
Web applications
browser compatibility, 195
testing of, 194
testing strategies, 200
White-box testing, 10, 42
White box–black box comparison, 42
Wide Area Network. See
Index 239
Wireless application protocol, 218
Wireless markup language, 218
WML, 218
X
Xerox, 143
XP, 179, 180
planning, 181
project flow example, 183
testing, 183
XP Planning, 181
XP Project flow, 183
XP Testing, 183
XT, 180
240 Index

·
·
·- 0
Software Architecture in Practice
Third Edition
Len Bass
Paul Clements
Rick Kazman
T•.. Addison-Wesley
Upper Saddle River, NJ • Boston • Indianapolis • San Francisco
New York • Toronto • Montreal • London • Munich • Paris • Madrid
Capetown • Sydney • Tokyo • Singapore • Mexico City
A a
·
·
·- 0
Software Engineering Institute I CarnegieMellon
The SEI Series in Software Engineering
A a
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as
trademarks. Where those designations appear in this book, and the publisher was aware of a trademark
claim, the designations have been printed with initial capital letters or in all capitals.
CMM, CMMI, Capability Maturity Model, Capability Maturity Modeling, Carnegie Mellon, CERT,
and CERT Coordination Center are registered in the U.S. Patent and Trademark Office by Carnegie
Mellon University.
ATAM; Architecture Tradeoff Analysis Method; CMM Integration; COTS Usage-Risk Evaluation;
CURE; EPIC; Evolutionary Process for Integrating COTS Based Systems; Framework for Software
Product Line Practice; IDEAL; Interim Profile; OAR; OCTAVE; Operationally Critical Threat, Asset,
and Vulnerability Evaluation; Options Analysis for Reengineering; Personal Software Process; PLTP;
Product Line Technical Probe; PSP; SCAMPI; SCAMPI Lead Appraiser; SCAMPI Lead Assessor;
SCE; SEI; SEPG; Team Software Process; and TSP are service marks of Carnegie Mellon University.
Special permission to reproduce portions of works copyright by Carnegie Mellon University, as listed
on page 588, is granted by the Software Engineering Institute.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as
trademarks. Where those designations appear in this book, and the publisher was aware of a trademark
claim, the designations have been printed with initial capital letters or in all capitals.
The authors and publisher have taken care in the preparation of this book, but make no expressed or
implied warranty of any kind and assume no responsibility for errors or omissions. No liability is
assumed for incidental or consequential damages in connection with or arising out of the use of the
information or programs contained herein.
The publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or
special sales, which may include electronic versions and/or custom covers and content particular to your
business, training goals, marketing focus, and branding interests. For more information, please contact:
U.S. Corporate and Government Sales
(800) 382-34 1 9
corpsales@pearsontechgroup.com
For sales outside the United States, please contact:
International Sales
international@pearson. com
Visit us on the Web: informit.com/aw
Library of Congress Cataloging-in-Publication Data
Bass, Len.
·
·
·- 0
Software architecture in practice I Len Bass, Paul Clements, Rick Kazman. 3rd ed.
p. em. (SEI series in software engineering)
Includes bibliographical references and index.
ISBN 978-0-3 2 1 -8 1 573-6 (hardcover : all<. paper) 1 . Software architecture. 2. System
design. I. Clements, Paul, 1 955- II. Kazman, Rick. III. Title.
QA76.754.B37 2 0 1 2
005 . 1 dc23
Copyright © 2013 Pearson Education, Inc.
201 2023744
A a
All rights reserved. Printed in the United States of America. This publication is protected by copyright,
and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a
retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying,
recording, or likewise. To obtain permission to use material from this work, please submit a written
request to Pearson Education, Inc., Permissions Department, One Lake Street, Upper Saddle River,
New Jersey 07458, or you may fax your request to (20 1 ) 236-3290.
ISBN- 1 3 : 978-0-32 1 - 8 1 573-6
ISBN- 10: 0-32 1 - 8 1 573-4
Text printed in the United States on recycled paper at Courier in Westford, Massachusetts.
First printing, September 2 0 1 2
·
·
·-
The SEI Series in
Software Engineering
Software Engineering Institute I CamegieMellon
Rdle-=tions
0
on M.anagemcnt
􀌚·􀌚 Addison-Wesley
Wuu S. llu.ti l' 'rc:'
...... 􀂁 .. "'--
Visit informit.com/sei for a complete list of available products.
The SEI Series in Software Engineering represents is a collaborative
undertaking of the Carnegie Mellon Software Engineering lnstrtute (SEI) and
Addison-Wesley to develop and publish books on software engineering and
related topics. The common goal of the SEI and Addison-Wesley is to provide
the most current information on these topics in a form that is easily usable by
practitioners and students.
Books in the series describe frameworks, tools, methods, and technologies
designed to help organizations, teams, and individuals improve their technical
or management capabilities. Some books describe processes and practices for
developing higher-quality software, acquiring programs for complex systems, or
delivering services more effectively. Other books focus on software and system
architecture and product·line development. Still others, from the SEI's CERT
Program, describe technologies and practices needed to manage software
and network security risk. These and all books in the series address critical
problems in software engineering for which practical solutions are available.
PEARSON
----
··•Addison-Wesley Cisco Press EXAM/CRAM 􀞞 our: H ::;'.!,E􀛭TICE sAM.s 1 Safarr>
--
A a
Contents
Preface v
Reader's Guide
Acknowledgments
Part One Introduction
Chapter 1 What Is Software Architecture?
1 . 1 What Software Architecture Is and What It Isn't
1 .2 Architectural Structures and Views
1 .3 Architectural Patterns
1 .4 What Makes a "Good" Architecture?
1 . 5 Sutnmary
1 .6 For Further Reading
1 .7 Discussion Questions
Chapter 2 Why Is Software Architecture Important?
2. 1 Inhibitingo r Enabling a System'sQ ualityA ttributes
2.2 Reasoning About and Managing Change
2.3 Predicting System Qualities
2.4 Enhancing Communication among Stakeholders
2.5 Carrying Early Design Decisions
2.6 Defining Constraints on an Implementation
2.7 Influencing the Organizational Structure
2.8 Enabling Evolutionary Prototyping
2.9 Improving Cost and Schedule Estimates
2 . 1 0 Supplying a Transferable, Reusable Model
·
·
·- 0
2. 1 1 Allowing Incorporation of Independently Developed Components
2 . 1 2 Restricting the Vocabulary of Design Alternatives
2 . 1 3 Providing a Basis for Training
2 . 1 4 Summary
2 . 1 5 For Further Reading
2 . 1 6 Discussion Questions
A a
Chapter 3 The Many Contexts of Software Architecture
311 Archi tecture in a Technical Context
312 Archi tecture in a Proje ct Life-Cycle Context
3.3 Architecture in a Business Context
3.4 Archi tecture in a Professional Context
3 I 5 Sta keholders
316 How Is Architecture In fluenced?
3�7 What Do Architectures In fluence?
3.8 Summary
3.9 For Further Reading
3.10 Discuss ion Questions
Part Two Quality Attributes
Chapter 4 Understanding Quality Attributes
4.1 Archi tecture and Requ irements
4.2 Function ality
4.3 Quality Attribut e Consideration s
4.4 Specifyi ng Quality Attribute Requirements
4.5 Achieving Quality Attributes through Tactics
4.6 Guiding Quality Design Decisions
4.7 Sum mary
4 I 8 For Fu rther Reading
4.9 Discuss ion Questions
Chapter 5 A vail ability
5 I 1 Availabi lity General Sc enari o
5.2 Tactics for Availabil ity
5. 3 A Design Ch ecklist fo r Availability
5.4 Summary
515 For Further Reading
5.6 Discussion Questions
Chapter 6 lnteroperability
611 In teroperability General Scenario
6.2 Tactics for Interope rabil ity
·
·
·- 0 A a
6.3 A Design Checklist fo r Interoperability
6.4 Summary
6.5 For Further Reading
6.6 Discussion Questions
Chapter 7 Modifiability
7.1 Modifiabi lity General Sce nario
7.2 Tactics for Modifiability
7.3 A Design Checklist for Modifiability
7.4 Summary
7. 5 For Further Reading
7.6 Discussion Questions
Chapter 8 Performance
8.1 Performance General Sc enario
8.2 Tactics for Performance
8.3 A Design Ch ecklist fo r Per formance
8.4 Summary
8.5 For Further Reading
8.6 Discussion Questions
Chapter 9 Security
9.1 Security General Scenario
9.2 Tactics for Security
9.3 A Design Checklist for Security
9.4 Summary
9.5 For Further Reading
9.6 Discussion Questions
Chapter 10 Testability
10.1 Testabi lity General Sc enario
10.2 Tactics for Testability
10.3 A Design Ch ecklist for Testability
10 .4S ummary
10.5 For Further Reading
10.6 Discussion Questions
·
·
·- 0 A a
Chapter 11 Usability
1 1 . 1 Usability General Scenario
1 1 .2 Tactics for Usability
1 1 .3 A Design Checklist for Usability
1 1 .4 Summary
1 1 .5 For Further Reading
1 1 .6 Discussion Questions
Chapter 12 Other Quality Attributes
12. 1 Other Important Quality Attributes
12.2 Other Categories of Quality Attributes
·
·
·-
1 2 .3 Software Quality Attributes and System Quality Attributes
12.4 Using Standard Lists of Quality Attributes or Not
0
12.5 Dealing with "X-ability": Bringing a New Quality Attribute into the Fold
12.6 For Further Reading
12.7 Discussion Questions
Chapter 13 Architectural Tactics and Patterns
1 3 . 1 Architectural Patterns
1 3 .2 Overview of the Patterns Catalog
1 3 .3 Relationships between Tactics and Patterns
13.4 Using Tactics Together
1 3 . 5 Summary
13.6 For Further Reading
13.7 Discussion Questions
Chapter 14 Quality Attribute Modeling and Analysis
1 4 . 1 Modeling Architectures to Enable Quality Attribute Analysis
14.2 QualityA ttribute Checklists
14.3 Thought Experiments and Back-of-the-Envelope Analysis
14.4 Experiments, Simulations, and Prototypes
14.5 Analysis at Different Stages of the Life Cycle
14.6 Summary
14.7 For Further Reading
14.8 Discussion Questions
A a
Part Three Architecture in the Life Cycle
Chapter 15 Architecture in Agile Projects
15.1 How Much Architecture?
15.2 Agility and Architecture Methods
15.3 A Brief Example of Agile Architecting
15.4 Guidelines for the Agile Architect
15.5 Summary
15.6 For Further Reading
15.7 Discussion Questions
Ch apter 16 Architecture and Requirements
16.1 Gathering ASRs from Requirements Documents
16.2 Gathering ASRs by Interviewing Stakeholders
·
·
·-
16.3 Gathering ASRs by Understanding the Business Goals
16.4 Capturing ASRs in a Utility Tree
16.5 Tying the Methods Together
16.6 Summary
16.7 For Further Reading
16.8 Discussion Questions
Chapter 17 Designing an Architecture
17.1 Design Strategy
17.2 The Attribute-Driven Design Method
17.3 The Steps of ADD
17.4 Summary
17.5 For Further Reading
17.6 Discussion Questions
Chapter 18 Documenting Software Architectures
18. 1 Uses and Audiences for Architecture Documentation
18.2 Notations for Architecture Documentation
18.3 Views
18.4 Choosing the Views
18.5 Combining Views
18.6 Building the Documentation Package
0 A a
1 8.7 Documenting Behavior
1 8.8 Architecture Documentation and Quality Attributes
·
·
·- 0 A a
1 8.9 Documenting Architectures That Change Faster Than You Can Document Them
1 8 . 1 0 Documenting Architecture in an Agile Development Project
1 8. 1 1 Sutnmary
1 8 . 1 2 For Further Reading
1 8 . 1 3 Discussion Questions
Chapter 19 Architecture, Implementation, and Testing
19. 1 Architecture and Implementation
1 9.2 Architecture and Testing
1 9.3 Summary
19.4 For Further Reading
19.5 Discussion Questions
Chapter 20 Architecture Reconstruction and Conformance
20. 1 Architecture Reconstruction Process
20.2 Raw View Extraction
20.3 Database Construction
20.4 View Fusion
20.5 Architecture Analysis: Finding Violations
20.6 Guidelines
20.7 Summary
20.8 For Further Reading
20.9 Discussion Questions
Chapter 21 Architecture Evaluation
2 1 . 1 Evaluation Factors
2 1 .2 The Architecture Tradeoff Analysis Method
2 1 .3 Lightweight Architecture Evaluation
2 1 .4 Summary
2 1 .5 For Further Reading
2 1 .6 Discussion Questions
Chapter 22 Management and Governance
22. 1 Planning
22.2 Organizing
22.3 Implementing
22.4 Measuring
22.5 Governance
22.6 Sutnmary
22.7 For Further Reading
22.8 Discussion Questions
Part Four Architecture and Business
Chapter 23 Economic Analysis of Architectures
23.1 Decision-Making Context
23.2 The Basis for the Economic Analyses
23.3 Putting Theory into Practice: The CBAM
23.4 Case Study: The NASA ECS Project
23.5 Summary
23.6 For Further Reading
23.7 Discussion Questions
Chapter 24 Architecture Competence
·
·
·- 0
24. 1 Competence of Individuals: Duties, Skills, and Knowledge of Architects
24.2 Competence of a Software Architecture Organization
24.3 Summary
24.4 For Further Reading
24.5 Discussion Questions
Chapter 25 Architecture and Software Product Lines
25.1 An Example of Product Line Variability
25.2 What Makes a Software Product Line Work?
25.3 Product Line Scope
25.4 The Quality Attribute of Variability
25.5 The Role of a Product Line Architecture
25.6 Variation Mechanisms
25.7 Evaluating a Product Line Architecture
25.8 Key Software Product Line Issues
25.9 Summary
A a
25.10 For Further Reading
25111 Discussion Questions
Part Five The Brave New World
Chapter 26 Architecture in the Cloud
261 1 Bas ic Cloud Definitions
2612 Service Models and Deployment Options
26.3 Economic Justification
26.4 Base Mechanisms
2615 Sample Technologies
26�6 Architecting in a Cloud Environment
2617 Summary
2618 For Further Reading
2619 Discussion Questions
Chapter 27 Architectures for the Edge
2711 The Ecosystem of Edge-Dominant Systems
27�2 Changes to the Software Development Life Cycle
2713 Implications for Architecture
27.4 Impl ica tion s of the Metropolis Model
27 I 5 Summary
27 I 6 For Further Reading
27�7 Discussion Questions
Chapter 28 Epilogue
References
About the Authors
Index
·
·
·- 0 A a
Preface
·
·
·- 0
I should have no objection to go over the same
life fi"om its beginning to the end: requesting only
the advantage authors have, of correcting in a
[third} edition the faults ofthefirst [two}.
A a
-Benjamin Franklin
It has been a decade since the publication of the second edition of this book. During that time, the field
of software architecture has broadened its focus from being primarily internally oriented How does
one design, evaluate, and document software? to including external impacts as well a deeper
understanding of the influences on architectures and a deeper understanding of the impact architectures
have on the life cycle, organizations, and management.
The past ten years have also seen dramatic changes in the types of systems being constructed. Large
data, social media, and the cloud are all areas that, at most, were embryonic ten years ago and now are
not only mature but extremely influential.
We listened to some of the criticisms of the previous editions and have included much more
material on patterns, reorganized the material on quality attributes, and made interoperability a quality
attribute worthy of its own chapter. We also provide guidance about how you can generate scenarios
and tactics for your own favorite quality attributes.
To accommodate this plethora of new material, we had to make difficult choices. In particular, this
edition of the book does not include extended case studies as the prior editions did. This decision also
reflects the maturing of the field, in the sense that case studies about the choices made in software
architectures are more prevalent than they were ten years ago, and they are less necessary to convince
readers of the importance of software architecture. The case studies from the frrst two editions are
available, however, on the book's website, at www.informit.com/title/978032 1 8 1 5736. In addition, on
the same website, we have slides that will assist instructors in presenting this material.
We have thoroughly reworked many of the topics covered in this edition. In particular, we realize
that the methods we present for architecture design, analysis, and documentation are one version of
how to achieve a particular goal, but there are others. This led us to separate the methods that we
present in detail from their underlying theory. We now present the theory first with specific methods
given as illustrations of possible realizations of the theories. The new topics in this edition include
architecture-centric project management; architecture competence; requirements modeling and analysis;
Agile methods; implementation and testing; the cloud; and the edge.
As with the prior editions, we firmly believe that the topics are best discussed in either reading
groups or in classroom settings, and to that end we have included a collection of discussion questions at
the end of each chapter. Most of these questions are open-ended, with no absolute right or wrong
answers, so you, as a reader, should emphasize how you justify your answer rather than just answer the
question itself.
Reader's Guide
·
·
·- 0 A a
We have structured this book into five distinct portions. Part One introduces architecture and the
various contextual lenses through which it could be viewed. These are the following:
• Technical. What technical role does the software architecture play in the system or systems of
which it's a part?
• Project. How does a software architecture relate to the other phases of a software development
life cycle?
• Business. How does the presence of a software architecture affect an organization's business
environment?
• Professional. What is the role of a software architect in an organization or a development
project?
Part Two is focused on technical background. Part Two describes how decisions are made.
Decisions are based on the desired quality attributes for a system, and Chapters 5-1 1 describe seven
different quality attributes and the techniques used to achieve them. The seven are availability,
interoperability, maintainability, performance, security, testability, and usability. Chapter 1 2 tells you
how to add other quality attributes to our seven, Chapter 1 3 discusses patterns and tactics, and Chapter
1 4 discusses the various types of modeling and analysis that are possible.
Part Three is devoted to how a software architecture is related to the other portions of the life cycle.
Of special note is how architecture can be used in Agile projects. We discuss individually other aspects
of the life cycle: requirements, design, implementation and testing, recovery and conformance, and
evaluation.
Part Four deals with the business of architecting from an economic perspective, from an
organizational perspective, and from the perspective of constructing a series of similar systems.
Part Five discusses several important emerging technologies and how architecture relates to these
technologies.
Acknowledgments
·
·
·- 0 A a
We had a fantastic collection of reviewers for this edition, and their assistance helped make this a better
book. Our reviewers were Muhammad Ali Babar, Felix Bachmann, Joe Batman, Phil Bianco, Jeromy
Carriere, Roger Champagne, Steve Chenoweth, Viktor Clerc, Andres Diaz Pace, George Fairbanks, Rik
Farenhorst, Ian Gorton, Greg Hartman, Rich Hilliard, James Ivers, John Klein, Philippe Kruchten, Phil
Laplante, George Leih, Grace Lewis, John McGregor, Tommi Mikkonen, Linda Northrop, Ipek
Ozkaya, Eltjo Poort, Eelco Rommes, Nick Rozanski, Jungwoo Ryoo, James Scott, Antony Tang, Arjen
Uittenbogaard, Hans van Vliet, Hiroshi Wada, Rob Wojcik, Eo in Woods, and Liming Zhu.
In addition, we had significant contributions from Liming Zhu, Hong-Mei Chen, Jungwoo Ryoo,
Phil Laplante, James Scott, Grace Lewis, and Nick Rozanski that helped give the book a richer flavor
than one written by just the three of us.
The issue of build efficiency in Chapter 1 2 came from Rolf Siegers and John McDonald of
Raytheon. John Klein and Eltjo Poort contributed the "abstract system clock" and "sandbox mode"
tactics, respectively, for testability. The list of stakeholders in Chapter 3 is from Documenting Software
Architectures: Views and Beyond, Second Edition. Some of the material in Chapter 28 was inspired by a
talk given by Anthony Lattanze called "Organizational Design Thinking" in 20 1 1 .
Joe Batman was instrumental in the creation of the seven categories of design decisions we describe
in Chapter 4. In addition, the descriptions of the security view, communications view, and exception
view in Chapter 1 8 are based on material that Joe wrote while planning the documentation for a real
system's architecture. Much of the new material on modifiability tactics was based on the work of Felix
Bachmann and Rod Nord. James Ivers helped us with the security tactics.
Both Paul Clements and Len Bass have taken new positions since the last edition was published,
and we thank their new respective managements (BigLever Software for Paul and NICT A for Len) for
their willingness to support our work on this edition. We would also like to thank our (former)
colleagues at the Software Engineering Institute for multiple contributions to the evolution of the ideas
expressed in this edition.
Finally, as always, we thank our editor at Addison-Wesley, Peter Gordon, for providing guidance
and support during the writing and production processes.
·
·
·-
Part One. Introduction
0 A a
What is a software architecture? What is it good for? How does it come to be? What effect does its
existence have? These are the questions we answer in Part I.
Chapter 1 deals with a technical perspective on software architecture. We define it and relate it to
system and enterprise architectures. We discuss how the architecture can be represented in different
views to emphasize different perspectives on the architecture. We define patterns and discuss what
makes a "good" architecture.
In Chapter 2, we discuss the uses of an architecture. You may be surprised that we can find so many
-ranging from a vehicle for communication among stakeholders to a blueprint for implementation, to
the carrier of the system's quality attributes. We also discuss how the architecture provides a reasoned
basis for schedules and how it provides the foundation for training new members on a team.
Finally, in Chapter 3, we discuss the various contexts in which a software architecture exists. It
exists in a technical context, in a project life-cycle context, in a business context, and in a professional
context. Each of these contexts defines a role for the software architecture to play, or an influence on it.
These impacts and influences define the Architecture Influence Cycle.
1 . What Is Software Architecture?
·
·
·- 0
Good judgment is usually the result of experience.
And experience is frequently the result of bad
judgment. But to learn from the experience of
others requires those who have the experience to
share the knowledge with those who follow.
-Barry LePatner
A a
Writing (on our part) and reading (on your part) a book about software architecture, which distills the
experience of many people, presupposes that
1. having a software architecture is important to the successful development of a software
system and
2. there is a sufficient, and sufficiently generalizable, body of knowledge about software
architecture to fill up a book.
One purpose of this book is to convince you that both of these assumptions are true, and once you are
convinced, give you a basic knowledge so that you can apply it yourself.
Software systetns are constructed to satisfy organizations' business goals. The architecture is a
bridge between those (often abstract) business goals and the final (concrete) resulting system. While the
path from abstract goals to concrete systems can be complex, the good news is that software
architectures can be designed, analyzed, documented, and implemented using known techniques that
will support the achievement of these business and mission goals. The complexity can be tamed, made
tractable.
These, then, are the topics for this book: the design, analysis, documentation, and implementation of
architectures. We will also examine the influences, principally in the form of business goals and quality
attributes, which inform these activities.
In this chapter we will focus on architecture strictly from a software engineering point of view. That is,
we will explore the value that a software architecture brings to a development project. (Later chapters
will take a business and organizational perspective.)
1.1. What Software Architecture Is and What It Isn't
·
·
·- 0 A a
There are many definitions of software architecture, easily discoverable with a web search, but the
one we like is this one:
The software architecture of a system is the set of structures needed to reason about the
system, which comprise software elements, relations among them, and properties of both.
This definition stands in contrast to other definitions that talk about the system's "early" or "major"
design decisions. While it is true that many architectural decisions are made early, not all are:especially
in Agile or spiral-development projects. It's also true that very many decisions are made
early that are not architectural. Also, it's hard to look at a decision and tell whether or not it's "major."
Sometimes only time will tell. And since writing down an architecture is one of the architect's most
important obligations, we need to know now which decisions an architecture comprises.
Structures, on the other hand, are fairly easy to identify in software, and they form a powerful tool
for system design.
Let us look at some of the implications of our definition.
Architecture Is a Set of Software Structures
This is the first and most obvious implication of our definition. A structure is simply a set of elements
held together by a relation. Software systems are composed of many structures, and no single structure
holds claim to being the architecture. There are three categories of architectural structures, which will
play an important role in the design, documentation, and analysis of architectures:
1. First, some structures partition systems into implementation units, which in this book we call
modules. Modules are assigned specific computational responsibilities, and are the basis of
work assignments for progratruning teams (Team A works on the database, Team B works on
the business rules, Team C works on the user interface, etc.). In large projects, these elements
(modules) are subdivided for assignment to subteams. For example, the database for a large
enterprise resource planning (ERP) implementation might be so complex that its
implementation is split into many parts. The structure that captures that decomposition is a
kind of module structure, the module decomposition structure in fact. Another kind of
module structure emerges as an output of object-oriented analysis and design class
diagratns. If you aggregate your modules into layers, you've created another (and very useful)
module structure. Module structures are static structures, in that they focus on the way the
system' s functionality is divided up and assigned to implementation teams.
2. Other structures are dynamic, meaning that they focus on the way the elements interact with
each other at runtime to carry out the system's functions. Suppose the system is to be built as
a set of services. The services, the infrastructure they interact with, and the synchronization
and interaction relations among them form another kind of structure often used to describe a
system. These services are made up of (compiled from) the programs in the various
implementation units that we just described. In this book we will call runtime structures
component-and-connector (C&C) structures. The term component is overloaded in software
·
·
·-
engineering. In our use, a component is always a runtime entity.
0 A a
3. A third kind of structure describes the mapping from software structures to the system's
organizational, developmental, installation, and execution environments. For example,
modules are assigned to teams to develop, and assigned to places in a file structure for
implementation, integration, and testing. Components are deployed onto hardware in order to
execute. These mappings are called allocation structures.
Although software comprises an endless supply of structures, not all of them are architectural. For
example, the set of lines of source code that contain the letter "z," ordered by increasing length from
shortest to longest, is a software structure. But it's not a very interesting one, nor is it architectural. A
structure is architectural if it supports reasoning about the system and the system's properties. The
reasoning should be about an attribute of the system that is important to some stakeholder. These
include functionality achieved by the system, the availability of the system in the face of faults, the
difficulty of making specific changes to the system, the responsiveness of the system to user requests,
and many others. We will spend a great deal of time in this book on the relationship between
architecture and quality attributes like these.
Thus, the set of architectural structures is not fixed or limited. What is architectural is what is useful
in your context for your system.
Architecture Is an Abstraction
Because architecture consists of structures and structures consist of elementsl and relations, it follows
that an architecture comprises software elements and how the elements relate to each other. This means
that architecture specifically omits certain information about elements that is not useful for reasoning
about the system in particular, it omits information that has no ramifications outside of a single
element. Thus, an architecture is foremost an abstraction of a system that selects certain details and
suppresses others. In all modem systems, elements interact with each other by means of interfaces that
partition details about an element into public and private parts. Architecture is concerned with the
public side of this division; private details of elements details having to do solely with internal
implementation are not architectural. Beyond just interfaces, though, the architectural abstraction lets
us look at the system in terms of its elements, how they are arranged, how they interact, how they are
composed, what their properties are that support our system reasoning, and so forth. This abstraction is
essential to taming the complexity of a system we simply cannot, and do not want to, deal with all of
the complexity all of the time.
1. In this book we use the term "element" when we mean either a module or a component, and don't want to
distinguish.
Every Software System Has a Software Architecture
Every system can be shown to comprise elements and relations among them to support some type of
reasoning. In the most trivial case, a system is itself a single element an uninteresting and probably
non-useful architecture, but an architecture nevertheless.
Even though every system has an architecture, it does not necessarily follow that the architecture is
known to anyone. Perhaps all of the people who designed the system are long gone, the documentation
·
·
·- 0 A a
has vanished (or was never produced), the source code has been lost (or was never delivered), and all
we have is the executing binary code. This reveals the difference between the architecture of a system
and the representation of that architecture. Because an architecture can exist independently of its
description or specification, this raises the importance of architecture documentation, which is
described in Chapter 18, and architecture reconstruction, discussed in Chapter 20.
Architecture Includes Behavior
The behavior of each element is part of the architecture insofar as that behavior can be used to reason
about the system. This behavior embodies how elements interact with each other, which is clearly part
of our definition of architecture.
This tells us that box-and-line drawings that are passed off as architectures are in fact not
architectures at all. When looking at the names of the boxes (database, graphical user interface,
executive, etc.), a reader may well imagine the functionality and behavior of the corresponding
elements. This mental image approaches an architecture, but it springs from the imagination of the
observer's mind and relies on information that is not present. This does not mean that the exact
behavior and performance of every element must be documented in all circumstances some aspects of
behavior are fine-grained and below the architect's level of concern. But to the extent that an element's
behavior influences another element or influences the acceptability of the system as a whole, this
behavior must be considered, and should be documented, as part of the software architecture.
Not All Architectures Are Good Architectures
The definition is indifferent as to whether the architecture for a system is a good one or a bad one. An
architecture may permit or preclude a system's achievement of its behavioral, quality attribute, and lifecycle
requirements. Assuming that we do not accept trial and error as the best way to choose an
architecture for a system that is, picking an architecture at random, building the system from it, and
then hacking away and hoping for the best this raises the importance of architecture design, which is
treated in Chapter 17, and architecture evaluation, which we deal with in Chapter 2 1 .
System and Enterprise Architectures
Two disciplines related to software architecture are system architecture and enterprise
architecture. Both of these disciplines have broader concerns than software and affect
software architecture through the establishment of constraints within which a software
system must live. In both cases, the software architect for a system should be on the
team that provides input into the decisions made about the system or the enterprise.
System architecture
A system's architecture is a representation of a system in which there is a mapping of
functionality onto hardware and software components, a mapping of the software
architecture onto the hardware architecture, and a concern for the human interaction
with these components. That is, system architecture is concerned with a total system,
including hardware, software, and humans.
A system architecture will determine, for example, the functionality that is assigned
·
·
·- 0 A a
to different processors and the type of network that connects those processors. The
software architecture on each of those processors will determine how this functionality
is implemented and how the various processors interact through the exchange of
messages on the network.
A description of the software architecture, as it is mapped to hardware and
networking components, allows reasoning about qualities such as performance and
reliability. A description of the system architecture will allow reasoning about
additional qualities such as power consumption, weight, and physical footprint.
When a particular system is designed, there is frequently negotiation between the
system architect and the software architect as to the distribution of functionality and,
consequently, the constraints placed on the software architecture.
Enterprise architecture
Enterprise architecture is a description of the structure and behavior of an
organization's processes, information flow, personnel, and organizational subunits,
aligned with the organization's core goals and strategic direction. An enterprise
architecture need not include information systems clearly organizations had
architectures that fit the preceding definition prior to the advent of computers but
these days, enterprise architectures for all but the smallest businesses are unthinkable
without information system support. Thus, a modem enterprise architecture is
concerned with how an enterprise's software systems support the business processes
and goals of the enterprise. Typically included in this set of concerns is a process for
deciding which systems with which functionality should be supported by an enterprise.
An enterprise architecture will specify the data model that various systems use to
interact, for example. It will specify rules for how the enterprise's systems interact with
external systems.
Software is only one concern of enterprise architecture. Two other common
concerns addressed by enterprise architecture are how the software is used by humans
to perform business processes, and the standards that determine the computational
environment.
Sometimes the software infrastructure that supports communication among systems
and with the external world is considered a portion of the enterprise architecture; other
times, this infrastructure is considered one of the systems within an enterprise. (In
either case, the architecture of that infrastructure is a software architecture!) These two
views will result in different management structures and spheres of influence for the
individuals concerned with the infrastructure.
The system and the enterprise provide environments for, and constraints on, the
software architecture. The software architecture must live within the system and
enterprise, and increasingly it is the focus for achieving the organization's business
goals. But all three forms of architecture share important commonalities: They are
concerned with major elements taken as abstractions, the relationships among the
·
·
·- 0
elements, and how the elements together meet the behavioral and quality goals of the
thing being built.
Are these in scope for this book? Yes! (Well, no.)
A a
System and enterprise architectures share a great deal with software architectures. All
can be designed, evaluated, and documented; all answer to requirements; all are
intended to satisfy stakeholders; all consist of structures, which in tum consist of
elements and relationships; all have a repertoire of patterns and styles at their respective
architects' disposal; and the list goes on. So to the extent that these architectures share
commonalities with software architecture, they are in the scope of this book. But like
all technical disciplines, each has its own specialized vocabulary and techniques, and
we won't cover those. Copious other sources do.
1.2. Architectural Structures and Views
·
·
·- 0 A a
The neurologist, the orthopedist, the hematologist, and the dermatologist all have different views of the
structure of a human body. Ophthalmologists, cardiologists, and podiatrists concentrate on specific
subsystems. And the kinesiologist and psychiatrist are concerned with different aspects of the entire
arrangement's behavior. Although these views are pictured differently and have very different
properties, all are inherently related, interconnected: together they describe the architecture of the
human body. Figure 1 . 1 shows several different views of the human body: the skeletal, the vascular,
and the X-ray.
,
Figure 1.1. Physiological structures (Getty images: Brand X Pictures [skeleton], Don FarraH
[woman], Mads Abildgaard [man])
So it is with software. Modern systems are frequently too complex to grasp all at once. Instead, we
restrict our attention at any one moment to one (or a small number) of the software system's structures.
To communicate meaningfully about an architecture, we must make clear which structure or structures
we are discussing at the moment which view we are taking of the architecture.
Structures and Views
We will be using the related terms structure and view when discussing architecture representation.
• A view is a representation of a coherent set of architectural elements, as written by and read by
system stakeholders. It consists of a representation of a set of elements and the relations among
them.
• A structure is the set of elements itself, as they exist in software or hardware.
·
·
·- 0 A a
In short, a view is a representation of a structure. For example, a module structure is the set of the
system's modules and their organization. A module view is the representation of that structure,
documented according to a template in a chosen notation, and used by some system stakeholders.
So: Architects design structures. They document views of those structures.
Three Kinds of Structures
As we saw in the previous section, architectural structures can be divided into three major categories,
depending on the broad nature of the elements they show. These correspond to the three broad kinds of
decisions that architectural design involves:
1. Module structures embody decisions as to how the system is to be structured as a set of code
or data units that have to be constructed or procured. In any module structure, the elements
are modules of some kind (perhaps classes, or layers, or merely divisions of functionality, all
of which are units of implementation). Modules represent a static way of considering the
system. Modules are assigned areas of functional responsibility; there is less emphasis in
these structures on how the resulting software manifests itself at runtime. Module structures
allow us to answer questions such as these:
• What is the primary functional responsibility assigned to each module?
• What other software elements is a module allowed to use?
• What other software does it actually use and depend on?
• What modules are related to other modules by generalization or specialization (i.e.,
inheritance) relationships?
Module structures convey this information directly, but they can also be used by extension to
ask questions about the impact on the system when the responsibilities assigned to each module
change. In other words, examining a system's module structures that is, looking at its module
views is an excellent way to reason about a system's modifiability.
2. Component-and-connector structures embody decisions as to how the system is to be
structured as a set of elements that have runtime behavior (components) and interactions
(connectors). In these structures, the elements are runtime components (which are the
principal units of computation and could be services, peers, clients, servers, filters, or many
other types of runtime elements) and connectors (which are the communication vehicles
among components, such as call-return, process synchronization operators, pipes, or others).
Component-and-connector views help us answer questions such as these:
• What are the major executing components and how do they interact at runtime?
• What are the major shared data stores?
• Which parts of the system are replicated?
• How does data progress through the system?
• What parts of the system can run in parallel?
• Can the system's structure change as it executes and, if so, how?
·
·
·- 0 A a
By extension, component-and-connector views are crucially important for asking questions
about the system's runtime properties such as performance, security, availability, and more.
3. Allocation structures embody decisions as to how the system will relate to nonsoftware
structures in its environment (such as CPUs, file systems, networks, development teams,
etc.). These structures show the relationship between the software elements and elements in
one or more external environments in which the software is created and executed. Allocation
views help us answer questions such as these:
• What processor does each software element execute on?
• In what directories or files is each element stored during development, testing, and
system building?
• What is the assignment of each software element to development teams?
Structures Provide Insight
Structures play such an important role in our perspective on software architecture because of the
analytical and engineering power they hold. Each structure provides a perspective for reasoning about
some of the relevant quality attributes. For example:
• The module "uses" structure, which embodies what modules use what other modules, is
strongly tied to the ease with which a system can be extended or contracted.
• The concurrency structure, which embodies parallelism within the system, is strongly tied to
the ease with which a system can be made free of deadlock and performance bottlenecks.
• The deployment structure is strongly tied to the achievement of performance, availability, and
security goals.
And so forth. Each structure provides the architect with a different insight into the design (that is,
each structure can be analyzed for its ability to deliver a quality attribute). But perhaps more important,
each structure presents the architect with an engineering leverage point: By designing the structures
appropriately, the desired quality attributes emerge.
Scenarios, described in Chapter 4, are useful for exercising a given structure as well as its
connections to other structures. For example, a software engineer wanting to make a change to the
concurrency structure of a system would need to consult the concurrency and deployment views,
because the affected mechanisms typically involve processes and threads, and physical distribution
might involve different control mechanisms than would be used if the processes were co-located on a
single machine. If control mechanisms need to be changed, the module decomposition would need to be
consulted to determine the extent of the changes.
Some Useful Module Structures
Useful module structures include the following:
• Decomposition structure. The units are modules that are related to each other by the is-asubmodule-
of relation, showing how modules are decomposed into smaller modules recursively
until the modules are small enough to be easily understood. Modules in this structure represent
a common starting point for design, as the architect enumerates what the units of software will
·
·
·- 0 A a
have to do and assigns each item to a module for subsequent (more detailed) design and
eventual implementation. Modules often have products (such as interface specifications, code,
test plans, etc.) associated with them. The decomposition structure determines, to a large
degree, the system's modifiability, by assuring that likely changes are localized. That is,
changes fall within the purview of at most a few (preferably small) modules. This structure is
often used as the basis for the development project's organization, including the structure of the
documentation, and the project's integration and test plans. The units in this structure tend to
have names that are organization-specific such as "segment" or "subsystem."
• Uses structure. In this important but overlooked structure, the units here are also modules,
perhaps classes. The units are related by the uses relation, a specialized form of dependency. A
unit of software uses another if the correctness of the first requires the presence of a correctly
functioning version (as opposed to a stub) of the second. The uses structure is used to engineer
systems that can be extended to add functionality, or from which useful functional subsets can
be extracted. The ability to easily create a subset of a syste1n allows for incremental
development.
• Layer structure. The modules in this structure are called layers. A layer is an abstract "virtual
machine" that provides a cohesive set of services through a managed interface. Layers are
allowed to use other layers in a strictly managed fashion; in strictly layered systems, a layer is
only allowed to use the layer immediately below. This structure is used to imbue a system with
portability, the ability to change the underlying computing platform.
• Class (or generalization) structure. The module units in this structure are called classes. The
relation is inherits from or is an instance of This view supports reasoning about collections of
similar behavior or capability (e.g., the classes that other classes inherit from) and
parameterized differences. The class structure allows one to reason about reuse and the
incremental addition of functionality. If any documentation exists for a project that has
followed an object-oriented analysis and design process, it is typically this structure.
• Data model. The data model describes the static information structure in tenns of data entities
and their relationships. For example, in a banking system, entities will typically include
Account, Customer, and Loan. Account has several attributes, such as account number, type
(savings or checking), status, and current balance. A relationship may dictate that one custo·mer
can have one or 1nore accounts, and one account is associated to one or two customers.
Some Useful C&C Structures
Component-and-connector structures show a runtime view of the system. In these structures the
modules described above have all been compiled into executable forms. All component-and-connector
structures are thus orthogonal to the module-based structures and deal with the dynamic aspects of a
running syste1n. The relation in all component-and-connector structures is attachment, showing how the
components and the connectors are hooked together. (The connectors themselves can be familiar
constructs such as "invokes.") Useful C&C structures include the following:
• Service structure. The units here are services that interoperate with each other by service
coordination mechanisms such as SOAP (see Chapter 6). The service structure is an important
·
·
·- 0 A a
structure to help engineer a system composed of components that may have been developed
anonymously and independently of each other.
• Concurrency structure. This component-and-connector structure allows the architect to
determine opportunities for parallelism and the locations where resource contention may occur.
The units are components and the connectors are their communication mechanisms. The
components are arranged into logical threads; a logical thread is a sequence of computations
that could be allocated to a separate physical thread later in the design process. The
concurrency structure is used early in the design process to identify the requirements to manage
the issues associated with concurrent execution.
Some Useful Allocation Structures
Allocation structures define how the elements from C&C or module structures map onto things that are
not software: typically hardware, teams, and file systems. Useful allocation structures include these:
• Deployment structure. The deployment structure shows how software is assigned to hardware
processing and communication elements. The elements are software elements (usually a
process from a C&C view), hardware entities (processors), and communication pathways.
Relations are allocated-to, showing on which physical units the software elements reside, and
migrates-to if the allocation is dynamic. This structure can be used to reason about
performance, data integrity, security, and availability. It is of particular interest in distributed
and parallel systems.
• Implementation structure. This structure shows how software elements (usually modules) are
mapped to the file structure( s) in the system's development, integration, or configuration
control environments. This is critical for the management of development activities and build
processes. (In practice, a screenshot of your development environment tool, which manages the
implementation environment, often makes a very useful and sufficient diagram of your
implementation view.)
• Work assignment structure. This structure assigns responsibility for implementing and
integrating the 1nodules to the teams who will carry it out. Having a work assignment structure
be part of the architecture makes it clear that the decision about who does the work has
architectural as well as management implications. The architect will know the expertise
required on each team. Also, on large multi-sourced distributed development projects, the work
assignment structure is the means for calling out units of functional commonality and assigning
those to a single team, rather than having them implemented by everyone who needs them. This
structure will also determine the major communication pathways among the teams: regular
teleconferences, wikis, email lists, and so forth.
Table 1 . 1 summarizes these structures. The table lists the meaning of the elements and relations in
each structure and tells what each might be used for.
Table 1.1. Useful Architectural Structures
Software Element
Structure Types Relations
Module Decomposilion Module Is a submodufe of
Structures
Uses Module Uses (i.e .• requires the corree1
presence of)
Layers Layer Requires the correct presence
of, uses the services of,
provides abstraction to
Useful For
·
·
·-
Resource allocalion and pro1ect structuring and
planning; information hiding, encapsu lation;
configuration control
Engineering subsets. engineering extensions
Incremental development, implementing systems
on top of "virtual machines"
0
Quality Attributes
Affected
Modifiability
"Subsetability;
extensibility
Portability
Class Class. object Is an instance of. shares access In object-oriented design systems. factoring out Modifiability.
extensibility
C&C
Structures
Allocation
Structures
Data model
Service
Concurrency
Deployment
methods of
Data entity {one, many}-to-{one, many].
generalizes, specializes
Service, ESB, registry, Runs concurrently with, may
others run concurrently with. excludes.
precedes, etc.
Processes, threads Can run In parallel
Components, hardware Allocated to, migrates to
elements
Implementation Modules. me structure Slored in
Work assignment Modules, organizational Assigned to
units
Relating Structures to Each Other
commonality; planning extensions of functionality
Engineering global dala structures for consistency
and performance
Scheduling analysis, performance analysis
Identifying locations where resource contention
exists. or where threads may fork, join, be created.
or be killed
Performance, availability. security analysis
Modifiability.
performance
lnleroperability,
modifiability
Pero lnna nee,
availabifijy
Perfonnance.
security, availabifijy
Configuration control, integration, test activities Development
efficiency
Projecl managemenl, best use of expertise and Development
available resources. management of commonality efficiency
A a
Each of these structures provides a different perspective and design handle on a system, and each is
valid and useful in its own right. Although the structures give different system perspectives, they are not
independent. Elements of one structure will be related to elements of other structures, and we need to
reason about these relations. For example, a module in a decomposition structure may be manifested as
one, part of one, or several components in one of the component-and-connector structures, reflecting its
runtime alter ego. In general, mappings between structures are many to many.
Figure 1 .2 shows a very simple example of how two structures might relate to each other. The figure
on the left shows a module decomposition view of a tiny client-server system. In this system, two
modules must be implemented: The client software and the server software. The figure on the right
shows a component-and-connector view of the same system. At runtime there are ten clients running
and accessing the server. Thus, this little system has two modules and eleven components (and ten
connectors).
System
Client
Server
Key: J Module I
Decomposttion View
C5
Key: Q· Component
- Request-Reply
Client-Server Vi,ew
Figure 1.2. Two views of a client-server system
Whereas the correspondence between the elements in the decomposition structure and the clientserver
structure is obvious, these two views are used for very different things. For example, the view on
·
·
·- 0 A a
the right could be used for performance analysis, bottleneck prediction, and network traffic
managetnent, which would be extremely difficult or impossible to do with the view on the left.
(In Chapter 1 3 we'll learn about the map-reduce pattern, in which copies of simple, identical
functionality are distributed across hundreds or thousands of processing nodes one module for the
whole system, but one component per node.)
Individual projects sometimes consider one structure dominant and cast other structures, when
possible, in tenns of the dominant structure. Often the dominant structure is the module decomposition
structure. This is for a good reason: it tends to spawn the project structure, because it mirrors the team
structure of development. In other projects, the dominant structure might be a C&C structure that shows
how the system's functionality and/or critical quality attributes are achieved.
Fewer Is Better
Not all systems warrant consideration of many architectural structures. The larger the systetn, the more
dramatic the difference between these structures tends to be; but for small systems we can often get by
with fewer. Instead of working with each of several component-and-connector structures, usually a
single one will do. If there is only one process, then the process structure collapses to a single node and
need not be explicitly represented in the design. If there is to be no distribution (that is, if the system is
implemented on a single processor), then the deployment structure is trivial and need not be considered
further. In general, design and document a structure only if doing so brings a positive return on the
investment, usually in tenns of decreased development or maintenance costs.
Which Structures to Choose?
We have briefly described a number of useful architectural structures, and there are many more. Which
ones shall an architect choose to work on? Which ones shall the architect choose to document? Surely
not all of them. Chapter 1 8 will treat this topic in more depth, but for now a good answer is that you
should think about how the various structures available to you provide insight and leverage into the
system's most important quality attributes, and then choose the ones that will play the best role in
delivering those attributes.
Ask Cal
More than a decade ago I went to a customer site to do an architecture evaluation one
of the first instances of the Architecture Tradeoff Analysis Method (AT AM) that I had
ever performed (you can read about the AT AM, and other architecture evaluation
topics, in Chapter 2 1). In those early days, we were still figuring out how to make
architecture evaluations repeatable and predictable, and how to guarantee useful
outcomes from them. One of the ways that we ensured useful outco1nes was to enforce
certain preconditions on the evaluation. A precondition that we figured out rather
quickly was this: if the architecture has not been documented, we will not proceed with
the evaluation. The reason for this precondition was simple: we could not evaluate the
architecture by reading the code we didn't have the time for that and we couldn't
just ask the architect to sketch the architecture in real time, since that would produce
vague and very likely erroneous representations.
·
·
·- 0
Okay, it's not completely true to say that they had no architecture documentation.
They did produce a single-page diagram, with a few boxes and lines. Some of those
boxes were, however, clouds. Yes, they actually used a cloud as one of their icons.
When I pressed them on the meaning of this icon Was it a process? A class? A
thread? they waffled. This was not, in fact, architecture documentation. It was, at
best, "marketecture."
A a
But in those early days we had no preconditions and so we didn't stop the evaluation
there. We just blithely waded in to whatever swamp we found, and we enforced
nothing. As I began this evaluation, I interviewed some of the key project stakeholders:
the project manager and several of the architects (this was a large project with one lead
architect and several subordinates). As it happens, the lead architect was away, and so I
spent my time with the subordinate architects. Every time I asked the subordinates a
tough question "How do you ensure that you will meet your latency goal along this
critical execution path?" or "What are your rules for layering?" they would answer:
"Ask Cal. Cal knows that." Cal was the lead architect. Immediately I noted a risk for
this system: What if Cal gets hit by a bus? What then?
In the end, because of my pestering, the architecture team did in fact produce
respectable architecture documentation. About halfway through the evaluation, the
project manager came up to me and shook my hand and thanked me for the great job I
had done. I was dumbstruck. In my mind I hadn't done anything, at that point; the
evaluation was only partially complete and I hadn't produced a single report or finding.
I said that to the manager and he said: "You got those guys to document the
architecture. I've never been able to get them to do that. So . . . thanks ! "
If Cal had been hit by a bus or just left the company, they would have had a serious
problem on their hands: all of that architectural knowledge located in one guy's head
and he is no longer with the organization. In can happen. It does happen.
The moral of this story? An architecture that is not documented, and not
communicated, may still be a good architecture, but the risks surrounding it are
enormous.
-RK
1.3. Architectural Patterns
·
·
·- 0 A a
In some cases, architectural elements are composed in ways that solve particular problems. The
compositions have been found useful over time, and over many different domains, and so they have
been documented and disseminated. These compositions of architectural elements, called architectural
patterns, provide packaged strategies for solving some of the problems facing a system.
An architectural pattern delineates the element types and their forms of interaction used in solving
the problem. Patterns can be characterized according to the type of architectural elements they use. For
example, a common module type pattern is this:
• Layered pattern. When the uses relation among software elements is strictly unidirectional, a
system of layers emerges. A layer is a coherent set of related functionality. In a strictly layered
structure, a layer can only use the services of the layer immediately below it. Many variations
of this patten1, lessening the structural restriction, occur in practice. Layers are often designed
as abstractions (virtual tnachines) that hide implementation specifics below from the layers
above, engendering portability.
Common component-and-connector type patterns are these:
• Shared-data (or repository) pattern. This pattern comprises components and connectors that
create, store, and access persistent data. The repository usually takes the form of a
(commercial) database. The connectors are protocols for managing the data, such as SQL.
• Client-server pattern. The components are the clients and the servers, and the connectors are
protocols and messages they share among each other to carry out the system's work.
Common allocation patterns include the following:
• Multi-tier pattern, which describes how to distribute and allocate the components of a system
in distinct subsets of hardware and software, connected by some communication medium. This
pattern specializes the generic deployment (software-to-hardware allocation) structure.
• Competence center and platform, which are patterns that specialize a software system's work
assignment structure. In competence center, work is allocated to sites depending on the
technical or domain expertise located at a site. For example, user-interface design is done at a
site where usability engineering experts are located. In platform, one site is tasked with
developing reusable core assets of a software product line (see Chapter 25), and other sites
develop applications that use the core assets.
Architectural patterns will be investigated much further in Chapter 13.
1.4. What Makes a "Good" Architecture?
·
·
·- 0 A a
There is no such thing as an inherently good or bad architecture. Architectures are either more or less fit
for some purpose. A three-tier layered service-oriented architecture may be just the ticket for a large
enterprise's web-based B2B system but completely wrong for an avionics application. An architecture
carefully crafted to achieve high modifiability does not make sense for a throwaway prototype (and vice
versa!). One of the messages of this book is that architectures can in fact be evaluated one of the great
benefits of paying attention to them but only in the context of specific stated goals.
Nevertheless, there are rules of thumb that should be followed when designing most architectures.
Failure to apply any of these does not automatically mean that the architecture will be fatally flawed,
but it should at least serve as a warning sign that should be investigated.
We divide our observations into two clusters: process recommendations and product (or structural)
recommendations. Our process recommendations are the following:
1 . The architecture should be the product of a single architect or a small group of architects with
an identified technical leader. This approach gives the architecture its conceptual integrity
and technical consistency. This recommendation holds for Agile and open source projects as
well as "traditional" ones. There should be a strong connection between the architect(s) and
the development team, to avoid ivory tower designs that are impractical.
2. The architect (or architecture team) should, on an ongoing basis, base the architecture on a
prioritized list of well-specified quality attribute requirements. These will inform the
tradeoffs that always occur. Functionality matters less.
3. The architecture should be documented using views. The views should address the concerns
of the most important stakeholders in support of the project timeline. This might mean
minimal documentation at first, elaborated later. Concerns usually are related to construction,
analysis, and maintenance of the system, as well as education of new stakeholders about the
system.
4. The architecture should be evaluated for its ability to deliver the system's important quality
attributes. This should occur early in the life cycle, when it returns the most benefit, and
repeated as appropriate, to ensure that changes to the architecture (or the environment for
which it is intended) have not rendered the design obsolete.
5. The architecture should lend itself to incremental implementation, to avoid having to
integrate everything at once (which almost never works) as well as to discover problems
early. One way to do this is to create a "skeletal" system in which the communication paths
are exercised but which at first has minimal functionality. This skeletal system can be used to
"grow" the system incrementally, refactoring as necessary.
Our structural rules of thumb are as follows:
1. The architecture should feature well-defmed modules whose functional responsibilities are
assigned on the principles of information hiding and separation of concerns. The informationhiding
modules should encapsulate things likely to change, thus insulating the software from
the effects of those changes. Each module should have a well-defined interface that
·
·
·- 0 A a
encapsulates or "hides" the changeable aspects from other software that uses its facilities.
These interfaces should allow their respective development teams to work largely
independently of each other.
2. Unless your requirements are unprecedented possible, but unlikely your quality attributes
should be achieved using well-known architectural patterns and tactics (described in Chapter
13) specific to each attribute.
3. The architecture should never depend on a particular version of a commercial product or tool.
If it must, it should be structured so that changing to a different version is straightforward and
• • Inexpensive.
4. Modules that produce data should be separate from modules that consume data. This tends to
increase modifiability because changes are frequently confined to either the production or the
consumption side of data. If new data is added, both sides will have to change, but the
separation allows for a staged (incremental) upgrade.
5. Don't expect a one-to-one correspondence between modules and components. For example,
in systems with concurrency, there may be multiple instances of a component running in
parallel, where each component is built from the same module. For systems with multiple
threads of concurrency, each thread Inay use services from several c01nponents, each of
which was built fr01n a different module.
6. Every process should be written so that its assignment to a specific processor can be easily
changed, perhaps even at runtime.
7. The architecture should feature a small number of ways for components to interact. That is,
the system should do the same things in the same way throughout. This will aid in
understandability, reduce development time, increase reliability, and enhance modifiability.
8. The architecture should contain a specific (and small) set of resource contention areas, the
resolution of which is clearly specified and maintained. For example, if network utilization is
an area of concern, the architect should produce (and enforce) for each development team
guidelines that will result in a minimum of network traffic. If performance is a concern, the
architect should produce (and enforce) time budgets for the major threads.
1.5. Summary
·
·
·- 0 A a
The software architecture of a system is the set of structures needed to reason about the system, which
comprise software elements, relations among them, and properties of both.
A structure is a set of elements and the relations among them.
A view is a representation of a coherent set of architectural elements, as written by and read by
system stakeholders. A view is a representation of one or more structures.
There are three categories of structures:
• Module structures show how a system is to be structured as a set of code or data units that have
to be constructed or procured.
• Component-and-connector structures show how the system is to be structured as a set of
elements that have runtime behavior (components) and interactions (connectors).
• Allocation structures show how the system will relate to nonsoftware structures in its
environment (such as CPUs, file systems, networks, development teams, etc.).
Structures represent the primary engineering leverage points of an architecture. Each structure
brings with it the power to manipulate one or more quality attributes. They represent a powerful
approach for creating the architecture (and later, for analyzing it and explaining it to its stakeholders).
And as we will see in Chapter 18, the structures that the architect has chosen as engineering leverage
points are also the primary candidates to choose as the basis for architecture documentation.
Every system has a software architecture, but this architecture may be documented and
disseminated, or it may not be.
There is no such thing as an inherently good or bad architecture. Architectures are either more or
less fit for some purpose.
1.6. For Further Reading
·
·
·- 0 A a
The early work of David Parnas laid much of the conceptual foundation for what became the study of
software architecture. A quintessential Parnas reader would include his foundational article on
information hiding [Parnas 72] as well as his works on program families [Parnas 76], the structures
inherent in software systems [Parnas 74], and introduction of the uses structure to build subsets and
supersets of systems [Parnas 79]. All of these papers can be found in the more easily accessible
collection of his important papers [Hoffman 00].
An early paper by Perry and Wolf [Perry 92] drew an analogy between software architecture views
and structures and the structures one finds in a house (plumbing, electrical, and so forth).
Software architectural patterns have been extensively catalogued in the series Pattern-Oriented
Software Architecture [Buschmann 96] and others. Chapter 1 3 of this book also deals with architectural
patterns.
Early papers on architectural views as used in industrial development projects are [Soni 95] and
[Kruchten 95]. The former grew into a book [Hofmeister 00] that presents a comprehensive picture of
using views in development and analysis. The latter grew into the Rational Unified Process, about
which there is no shortage of references, both paper and online. A good one is [Kruchten 03].
Cristina Gacek and her colleagues discuss the process issues surrounding software architecture in
[Gacek 95].
Garlan and Shaw's seminal work on software architecture [Garlan 93] provides many excellent
examples of architectural styles (a concept similar to patterns).
In [Clements 1 Oa] you can find an extended discussion on the difference between an architectural
pattern and an architectural style. (It argues that a pattern is a context-problem-solution triple; a style is
simply a condensation that focuses most heavily on the solution part.)
See [Taylor 09] for a definition of software architecture based on decisions rather than on structure.
1.7. Discussion Questions
·
·
·- 0 A a
1. Software architecture is often compared to the architecture of buildings as a conceptual analogy.
What are the strong points of that analogy? What is the correspondence in buildings to software
architecture structures and views? To patterns? What are the weaknesses of the analogy? When
does it break down?
2. Do the architectures you've been exposed to document different structures and relations like
those described in this chapter? If so, which ones? If not, why not?
3. Is there a different definition of software architecture that you are familiar with? If so, compare
and contrast it with the definition given in this chapter. Many definitions include considerations
like "rationale" (stating the reasons why the architecture is what it is) or how the architecture will
evolve over time. Do you agree or disagree that these considerations should be part of the
definition of software architecture?
4. Discuss how an architecture serves as a basis for analysis. What about decision-making? What
kinds of decision-making does an architecture empower?
5. What is architecture 's role in project risk reduction?
6. Find a commonly accepted definition of system architecture and discuss what it has in common
with software architecture. Do the same for enterprise architecture.
7. Find a published example of an architecture. What structure or structures are shown? Given its
purpose, what structure or structures should have been shown? What analysis does the
architecture support? Critique it: What questions do you have that the representation does not
answer?
8. Sailing ships have architectures, which means they have "structures" that lend themselves to
reasoning about the ship ' s performance and other quality attributes. Look up the technical
definitions for barque, brig, cutter,ji􀆒igate, ketch, schooner, and sloop. Propose a useful set of
"structures" for distinguishing and reasoning about ship architectures.
·
·
·-
2. Why Is Software Architecture Important?
0
Software architecture is the set of design
decisions which, if made incorrectly, may cause
your project to be cancelled.
-Eoin Woods
If architecture is the answer, what was the question?
A a
While Chapter 3 will cover the business importance of architecture to an enterprise, this chapter
focuses on why architecture matters from a technical perspective. We will examine a baker's dozen of
the most important reasons.
1 . An architecture will inhibit or enable a system's driving quality attributes.
2. The decisions made in an architecture allow you to reason about and manage change as the
system evolves.
3. The analysis of an architecture enables early prediction of a system's qualities.
4. A documented architecture enhances communication among stakeholders.
5. The architecture is a carrier of the earliest and hence most fundamental, hardest-to-change
design decisions.
6. An architecture defines a set of constraints on subsequent implementation.
7. The architecture dictates the structure of an organization, or vice versa.
8. An architecture can provide the basis for evolutionary prototyping.
9. An architecture is the key artifact that allows the architect and project manager to reason
about cost and schedule.
10. An architecture can be created as a transferable, reusable model that forms the heart of a
product line.
1 1 . Architecture-based development focuses attention on the assembly of components, rather
than simply on their creation.
12. By restricting design alternatives, architecture channels the creativity of developers, reducing
design and system complexity.
13. An architecture can be the foundation for training a new team member.
Even if you already believe us that architecture is important and don't need the point hammered thirteen
more titnes, think of these thirteen points (which form the outline for this chapter) as thirteen useful
ways to use architecture in a project.
·
·
·-
2.1. Inhibiting or Enabling a System's Quality Attributes
0 A a
Whether a system will be able to exhibit its desired (or required) quality attributes is substantially
determined by its architecture.
This is such an important message that we've devoted all of Part 2 of this book to expounding that
message in detail. Until then, keep these examples in mind as a starting point:
• If your system requires high performance, then you need to pay attention to managing the timebased
behavior of elements, their use of shared resources, and the frequency and volume of
inter -element comtnunication.
• If modifiability is important, then you need to pay careful attention to assigning responsibilities
to elements so that the majority of changes to the system will affect a small number of those
elements. (Ideally each change will affect just a single element.)
• If your system must be highly secure, then you need to manage and protect inter-element
communication and control which elements are allowed to access which information; you may
also need to introduce specialized elements (such as an authorization mechanism) into the
architecture.
• If you believe that scalability will be important to the success of your system, then you need to
carefully localize the use of resources to facilitate introduction of higher-capacity replacements,
and you must avoid hard-coding in resource assumptions or limits.
• If your projects need the ability to deliver incremental subsets of the system, then you must
carefully tnanage intercomponent usage.
• If you want the elements from your system to be reusable in other systems, then you need to
restrict inter-element coupling, so that when you extract an element, it does not come out with
too many attachments to its current environment to be useful.
The strategies for these and other quality attributes are supremely architectural. But an architecture
alone cannot guarantee the functionality or quality required of a system. Poor downstream design or
implementation decisions can always undermine an adequate architectural design. As we like to say
(mostly in jest): The architecture giveth and the implementation taketh away. Decisions at all stages of
the life cycle from architectural design to coding and implementation affect system quality.
Therefore, quality is not completely a function of an architectural design.
A good architecture is necessary, but not sufficient, to ensure quality. Achieving quality attributes
must be considered throughout design, implementation, and deployment. No quality attribute is entirely
dependent on design, nor is it entirely dependent on implementation or deployment. Satisfactory results
are a matter of getting the big picture (architecture) as well as the details (implementation) correct.
For example, modifiability is determined by how functionality is divided and coupled (architectural)
and by coding techniques within a module (nonarchitectural). Thus, a system is typically modifiable if
changes involve the fewest possible number of distinct elements. In spite of having the ideal
architecture, however, it is always possible to make a system difficult to modify by writing obscure,
tangled code.
2.2. Reasoning About and Managing Change
This point is a corollary to the previous point.
·
·
·- 0 A a
Modifiability the ease with which changes can be made to a system is a quality attribute (and
hence covered by the arguments in the previous section), but it is such an important quality that we have
awarded it its own spot in the List of Thirteen. The software development community is coming to grips
with the fact that roughly 80 percent of a typical software system's total cost occurs after initial
deployment. A corollary of this statistic is that most systems that people work on are in this phase.
Many programmers and software designers never get to work on new development; they work under the
constraints of the existing architecture and the existing body of code. Virtually all software systems
change over their lifetime, to accommodate new features, to adapt to new environments, to fix bugs,
and so forth. But these changes are often fraught with difficulty.
Every architecture partitions possible changes into three categories: local, nonlocal, and
architectural.
• A local change can be accomplished by modifying a single element. For example, adding a new
business rule to a pricing logic module.
• A nonlocal change requires multiple element modifications but leaves the underlying
architectural approach intact. For example, adding a new business rule to a pricing logic
module, then adding new fields to the database that this new business rule requires, and then
revealing the results of the rule in the user interface.
• An architectural change affects the fundamental ways in which the elements interact with each
other and will probably require changes all over the system. For example, changing a system
from client-server to peer-to-peer.
Obviously, local changes are the most desirable, and so an effective architecture is one in which the
most common changes are local, and hence easy to make.
Deciding when changes are essential, determining which change paths have the least risk, assessing
the consequences of proposed changes, and arbitrating sequences and priorities for requested changes
all require broad insight into relationships, performance, and behaviors of system software elements.
These activities are in the job description for an architect. Reasoning about the architecture and
analyzing the architecture can provide the insight necessary to make decisions about anticipated
changes.
2.3. Predicting System Qualities
·
·
·- 0 A a
This point follows from the previous two. Architecture not only imbues systems with qualities, but it
does so in a predictable way.
Were it not possible to tell that the appropriate architectural decisions have been made (i.e., if the
system will exhibit its required quality attributes) without waiting until the system is developed and
deployed, then choosing an architecture would be a hopeless task randomly making architecture
selections would perform as well as any other method. Fortunately, it is possible to make quality
predictions about a system based solely on an evaluation of its architecture. If we know that certain
kinds of architectural decisions lead to certain quality attributes in a system, then we can make those
decisions and rightly expect to be rewarded with the associated quality attributes. After the fact, when
we examine an architecture, we can look to see if those decisions have been made, and confidently
predict that the architecture will exhibit the associated qualities.
This is no different from any mature engineering discipline, where design analysis is a standard part
of the development process. The earlier you can find a problem in your design, the cheaper, easier, and
less disruptive it will be to fix.
Even if you don't do the quantitative analytic modeling sometimes necessary to ensure that an
architecture will deliver its prescribed benefits, this principle of evaluating decisions based on their
quality attribute implications is invaluable for at least spotting potential trouble spots early.
The architecture modeling and analysis techniques described in Chapter 14, as well as the
architecture evaluation teclmiques covered in Chapter 2 1 , allow early insight into the software product
qualities made possible by software architectures.
2.4. Enhancing Communication among Stakeholders
·
·
·- 0 A a
Software architecture represents a common abstraction of a system that most, if not all, of the
system's stakeholders can use as a basis for creating mutual understanding, negotiating, forming
consensus, and communicating with each other. The architecture or at least parts of it is sufficiently
abstract that most nontechnical people can understand it adequately, particularly with some coaching
from the architect, and yet that abstraction can be refined into sufficiently rich technical specifications
to guide implementation, integration, test, and deployment.
Each stakeholder of a software system customer, user, project manager, coder, tester, and so onis
concerned with different characteristics of the system that are affected by its architecture. For
example:
• The user is concerned that the system is fast, reliable, and available when needed.
• The customer is concerned that the architecture can be implemented on schedule and according
to budget.
• The manager is worried (in addition to concerns about cost and schedule) that the architecture
will allow teams to work largely independently, interacting in disciplined and controlled ways.
• The architect is worried about strategies to achieve all of those goals.
Architecture provides a common language in which different concerns can be expressed, negotiated,
and resolved at a level that is intellectually manageable even for large, complex systems. Without such
a language, it is difficult to understand large systems sufficiently to make the early decisions that
influence both quality and usefulness. Architectural analysis, as we will see in Chapter 2 1 , both depends
on this level of communication and enhances it.
Section 3.5 covers stakeholders and their concerns in greater depth.
"What Happens When I Push This Button?" Architecture as a Vehicle for
Stakeholder Communication
The project review droned on and on. The government-sponsored development was
behind schedule and over budget and was large enough that these lapses were attracting
congressional attention. And now the government was making up for past neglect by
holding a marathon come-one-come-all review session. The contractor had recently
undergone a buyout, which hadn't helped matters. It was the afternoon of the second
day, and the agenda called for the software architecture to be presented. The young
architect an apprentice to the chief architect for the system was bravely explaining
how the software architecture for the massive system would enable it to meet its very
demanding real-time, distributed, high-reliability requirements. He had a solid
presentation and a solid architecture to present. It was sound and sensible. But the
audience about 30 government representatives who had varying roles in the
management and oversight of this sticky project was tired. Some of them were even
thinking that perhaps they should have gone into real estate instead of enduring another
one of these marathon let's-finally-get-it-right-this-time reviews.
·
·
·- 0 A a
The viewgraph showed, in semiformal box-and-line notation, what the major
software elements were in a runtime view of the system. The names were all acronyms,
suggesting no setnantic meaning without explanation, which the young architect gave.
The lines showed data flow, message passing, and process synchronization. The
elements were internally redundant, the architect was explaining. "In the event of a
failure," he began, using a laser pointer to denote one of the lines, "a restart mechanism
triggers along this path when "
"What happens when the mode select button is pushed?" interrupted one of the
audience members. He was a government attendee representing the user community for
this system.
"Beg your pardon?" asked the architect.
"The mode select button," he said. "What happens when you push it?"
"Urn, that triggers an event in the device driver, up here," began the architect, laserpointing.
"It then reads the register and interprets the event code. If it's mode select,
well, then, it signals the blackboard, which in turns signals the objects that have
subscribed to that event. . . ."
"No, I mean what does the system do, " interrupted the questioner. "Does it reset the
displays? And what happens if this occurs during a system reconfiguration?"
The architect looked a little surprised and flicked off the laser pointer. This was not
an architectural question, but since he was an architect and therefore fluent in the
requirements, he knew the answer. "If the command line is in setup mode, the displays
will reset," he said. "Otherwise an error message will be put on the control console, but
the signal will be ignored." He put the laser pointer back on. "Now, the restart
mechanism that I was talking about "
"Well, I was just wondering," said the users' delegate. "Because I see from your
chart that the display console is sending signal traffic to the target location module."
"What should happen?" asked another member of the audience, addressing the first
questioner. "Do you really want the user to get mode data during its reconfiguring?"
And for the next 45 minutes, the architect watched as the audience consumed his time
slot by debating what the correct behavior of the system was supposed to be in various
esoteric states.
The debate was not architectural, but the architecture (and the graphical rendition of
it) had sparked debate. It is natural to think of architecture as the basis for
communication among some of the stakeholders besides the architects and developers:
Managers, for example, use the architecture to create teams and allocate resources
among them. But users? The architecture is invisible to users, after all; why should they
latch on to it as a tool for understanding the system?
The fact is that they do. In this case, the questioner had sat through two days of
viewgraphs all about function, operation, user interface, and testing. But it was the first
·
·
·- 0 A a
slide on architecture that even though he was tired and wanted to go home made
him realize he didn't understand something. Attendance at many architecture reviews
has convinced me that seeing the system in a new way prods the mind and brings new
questions to the surface. For users, architecture often serves as that new way, and the
questions that a user poses will be behavioral in nature. In a memorable architecture
evaluation exercise a few years ago, the user representatives were much more interested
in what the system was going to do than in how it was going to do it, and naturally so.
Up until that point, their only contact with the vendor had been through its marketers.
The architect was the first legitimate expert on the system to whom they had access,
and they didn't hesitate to seize the moment.
Of course, careful and thorough requirements specifications would ameliorate this
situation, but for a variety of reasons they are not always created or available. In their
absence, a specification of the architecture often serves to trigger questions and
improve clarity. It is probably more prudent to recognize this reality than to resist it.
Sometimes such an exercise will reveal unreasonable requirements, whose utility
can then be revisited. A review of this type that emphasizes synergy between
requirements and architecture would have let the young architect in our story off the
hook by giving him a place in the overall review session to address that kind of
information. And the user representative wouldn't have felt like a fish out of water,
asking his question at a clearly inappropriate moment.
-PCC
2.5. Carrying Early Design Decisions
·
·
·- 0 A a
Software architecture is a manifestation of the earliest design decisions about a system, and these early
bindings carry enormous weight with respect to the system's remaining development, its deployment,
and its maintenance life. It is also the earliest point at which these important design decisions affecting
the system can be scrutinized.
Any design, in any discipline, can be viewed as a set of decisions. When painting a picture, an artist
decides on the material for the canvas, on the media for recording oil paint, watercolor, crayon even
before the picture is begun. Once the picture is begun, other decisions are immediately made: Where is
the first line? What is its thickness? What is its shape? All of these early design decisions have a strong
influence on the final appearance of the picture. Each decision constrains the many decisions that
follow. Each decision, in isolation, might appear innocent enough, but the early ones in particular have
disproportionate weight simply because they influence and constrain so much of what follows.
So it is with architecture design. An architecture design can also be viewed as a set of decisions.
The early design decisions constrain the decisions that follow, and changing these decisions has
enormous ramifications. Changing these early decisions will cause a ripple effect, in terms of the
additional decisions that must now be changed. Yes, sometimes the architecture must be refactored or
redesigned, but this is not a task we undertake lightly (because the "ripple" might turn into a tsunami).
What are these early design decisions embodied by software architecture? Consider:
• Will the system run on one processor or be distributed across multiple processors?
• Will the software be layered? If so, how many layers will there be? What will each one do?
• Will components communicate synchronously or asynchronously? Will they interact by
transferring control or data or both?
• Will the system depend on specific features of the operating system or hardware?
• Will the information that flows through the syste1n be encrypted or not?
• What operating system will we use?
• What communication protocol will we choose?
Imagine the nightmare of having to change any of these or a myriad other related decisions.
Decisions like these begin to flesh out some of the structures of the architecture and their interactions.
In Chapter 4, we describe seven categories of these early design decisions. In Chapters 5-1 1 we show
the implications of these design decision categories on achieving quality attributes.
2.6. Defining Constraints on an Implementation
·
·
·- 0 A a
An implementation exhibits an architecture if it conforms to the design decisions prescribed by the
architecture. This means that the implementation must be implemented as the set of prescribed
elements, these elements must interact with each other in the prescribed fashion, and each element must
fulfill its responsibility to the other elements as dictated by the architecture. Each of these prescriptions
is a constraint on the implementer.
Element builders must be fluent in the specifications of their individual elements, but they may not
be aware of the architectural tradeoffs the architecture (or architect) simply constrains them in such a
way as to meet the tradeoffs. A classic example of this phenomenon is when an architect assigns
performance budget to the pieces of software involved in some larger piece of functionality. If each
software unit stays within its budget, the overall transaction will meet its performance requirement.
Implementers of each of the constituent pieces may not know the overall budget, only their own.
Conversely, the architects need not be experts in all aspects of algorithm design or the intricacies of
the programtning language although they should certainly know enough not to design something that
is difficult to build but they are the ones responsible for establishing, analyzing, and enforcing the
architectural tradeoffs.
2. 7. Influencing the Organizational Structure
·
·
·- 0 A a
Not only does architecture prescribe the structure of the system being developed, but that structure
becomes engraved in the structure of the development project (and sometimes the structure of the entire
organization). The normal method for dividing up the labor in a large project is to assign different
groups different portions of the system to construct. This is called the work-breakdown structure of a
system. Because the architecture includes the broadest decomposition of the system, it is typically used
as the basis for the work-breakdown structure. The work-breakdown structure in turn dictates units of
planning, scheduling, and budget; interteam communication channels; configuration control and filesystem
organization; integration and test plans and procedures; and even project minutiae such as how
the project intranet is organized and who sits with whom at the company picnic. Teams communicate
with each other in terms of the interface specifications for the major elements. The maintenance
activity, when launched, will also reflect the software structure, with teams fonned to maintain specific
structural elements from the architecture: the database, the business rules, the user interface, the device
drivers, and so forth.
A side effect of establishing the work-breakdown structure is to freeze some aspects of the software
architecture. A group that is responsible for one of the subsystems will resist having its responsibilities
distributed across other groups. If these responsibilities have been formalized in a contractual
relationship, changing responsibilities could become expensive or even litigious.
Thus, once the architecture has been agreed on, it becomes very costly for managerial and
business reasons to significantly modify it. This is one argument (among many) for carrying out
extensive analysis before settling on the software architecture for a large system because so much
depends on it.
2.8. Enabling Evolutionary Prototyping
·
·
·- 0 A a
Once an architecture has been defined, it can be analyzed and prototyped as a skeletal system. A
skeletal system is one in which at least some of the infrastructure how the elements initialize,
communicate, share data, access resources, report errors, log activity, and so forth is built before
much of the system's functionality has been created. (The two can go hand in hand: build a little
infrastructure to support a little end-to-end functionality; repeat until done.)
For example, systems built as plug-in architectures are skeletal systems: the plug-ins provide the
actual functionality. This approach aids the development process because the system is executable early
in the product's life cycle. The fidelity of the system increases as stubs are instantiated, or prototype
parts are replaced with complete versions of these parts of the software. In some cases the prototype
parts can be low-fidelity versions of the final functionality, or they can be surrogates that consume and
produce data at the appropriate rates but do little else. Among other things, this approach allows
potential performance problems to be identified early in the product's life cycle.
These benefits reduce the potential risk in the project. Furthermore, if the architecture is part of a
family of related systems, the cost of creating a framework for prototyping can be distributed over the
development of many systems.
2.9. Improving Cost and Schedule Estimates
·
·
·- 0 A a
Cost and schedule estimates are important tools for the project manager both to acquire the necessary
resources and to monitor progress on the project, to know if and when a project is in trouble. One of the
duties of an architect is to help the project manager create cost and schedule estimates early in the
project life cycle. Although top-down estimates are useful for setting goals and apportioning budgets,
cost estimations that are based on a bottom-up understanding of the system's pieces are typically more
accurate than those that are based purely on top-down system knowledge.
As we have said, the organizational and work-breakdown structure of a project is almost always
based on its architecture. Each team or individual responsible for a work item will be able to make
more-accurate estimates for their piece than a project manager and will feel more ownership in making
the estimates come true. But the best cost and schedule estimates will typically emerge from a
consensus between the top-down estimates (created by the architect and project manager) and the
bottom-up estimates (created by the developers). The discussion and negotiation that results from this
process creates a far more accurate estimate than either approach by itself.
It helps if the requirements for a system have been reviewed and validated. The more up-front
knowledge you have about the scope, the more accurate the cost and schedule estimates will be.
Chapter 22 delves into the use of architecture in project management.
2.1 0. Supplying a Transferable, Reusable Model
·
·
·- 0 A a
The earlier in the life cycle that reuse is applied, the greater the benefit that can be achieved. While code
reuse provides a benefit, reuse of architectures provides tremendous leverage for systems with similar
requirements. Not only can code be reused, but so can the requirements that led to the architecture in
the first place, as well as the experience and infrastructure gained in building the reused architecture.
When architectural decisions can be reused across multiple systems, all of the early-decision
consequences we just described are also transferred.
A software product line or family is a set of software systems that are all built using the same set of
reusable assets. Chief among these assets is the architecture that was designed to handle the needs of the
entire family. Product-line architects choose an architecture (or a family of closely related architectures)
that will serve all envisioned members of the product line. The architecture defines what is fixed for all
members of the product line and what is variable. Software product lines represent a powerful approach
to multi-system development that is showing order-of-magnitude payoffs in time to market, cost,
productivity, and product quality. The power of architecture lies at the heart of the paradigm. Similar to
other capital investments, the architecture for a product line becomes a developing organization's core
asset. Software product lines are explained in Chapter 25.
·
·
·- 0
2.11. Allowing Incorporation of Independently Developed Components
A a
Whereas earlier software paradigms have focused on programming as the prime activity, with progress
measured in lines of code, architecture-based development often focuses on composing or assembling
elements that are likely to have been developed separately, even independently, from each other. This
composition is possible because the architecture defines the elements that can be incorporated into the
system. The architecture constrains possible replacements (or additions) according to how they interact
with their environment, how they receive and relinquish control, what data they consume and produce,
how they access data, and what protocols they use for communication and resource sharing.
In 1 793, Eli Whitney's mass production of muskets, based on the principle of interchangeable parts,
signaled the dawn of the industrial age. In the days before physical measurements were reliable,
manufacturing interchangeable parts was a daunting notion. Today in software, until abstractions can be
reliably delimited, the notion of structural interchangeability is just as daunting and just as significant.
Commercial off-the-shelf components, open source software, publicly available apps, and
networked services are all modem-day software instantiations of Whitney's basic idea. Whitney's
musket parts had "interfaces" (having to do with fit and durability) and so do today's interchangeable
software components.
For software, the payoff can be
• Decreased time to market (it should be easier to use someone else's ready solution than build
your own)
• Increased reliability (widely used software should have its bugs ironed out already)
• Lower cost (the software supplier can amortize development cost across their customer base)
• Flexibility (if the component you want to buy is not terribly special-purpose, it's likely to be
available from several sources, thus increasing your buying leverage)
2.12. Restricting the Vocabulary of Design Alternatives
·
·
·- 0 A a
As useful architectural patterns are collected, it becomes clear that although software elements can be
combined in more or less infinite ways, there is something to be gained by voluntarily restricting
ourselves to a relatively small number of choices of elements and their interactions. By doing so we
minimize the design complexity of the system we are building.
A software engineer is not an artiste, whose creativity and freedom are paramount. Engineering is
about discipline, and discipline comes in part by restricting the vocabulary of alternatives to proven
solutions. Advantages of this approach include enhanced reuse, more regular and simpler designs that
are more easily understood and communicated, more capable analysis, shorter selection time, and
greater interoperability. Architectural patterns guide the architect and focus the architect on the quality
attributes of interest in large part by restricting the vocabulary of design alternatives to a relatively
small number.
Properties of software design follow from the choice of an architectural pattern. Those patterns that
are more desirable for a particular problem should improve the implementation of the resulting design
solution, perhaps by making it easier to arbitrate conflicting design constraints, by increasing insight
into poorly understood design contexts, or by helping to surface inconsistencies in requirements. We
will discuss architectural patterns in more detail in Chapter 1 3 .
2.13. Providing a Basis for Training
·
·
·- 0 A a
The architecture, including a description of how the elements interact with each other to carry out the
required behavior, can serve as the first introduction to the system for new project members. This
reinforces our point that one of the important uses of software architecture is to support and encourage
communication among the various stakeholders. The architecture is a common reference point.
Module views are excellent for showing someone the structure of a project: Who does what, which
teams are assigned to which parts of the system, and so forth. Component-and-connector views are
excellent for explaining how the system is expected to work and accomplish its job.
We will discuss these views in more detail in Chapter 1 8.
2.14. Summary
·
·
·- 0 A a
Software architecture is important for a wide variety of technical and nontechnical reasons. Our list
includes the following:
1 . An architecture will inhibit or enable a system's driving quality attributes.
2. The decisions made in an architecture allow you to reason about and manage change as the
system evolves.
3. The analysis of an architecture enables early prediction of a system's qualities.
4. A documented architecture enhances communication among stakeholders.
5. The architecture is a carrier of the earliest and hence most fundamental, hardest-to-change
design decisions.
6. An architecture defines a set of constraints on subsequent implementation.
7. The architecture dictates the structure of an organization, or vice versa.
8. An architecture can provide the basis for evolutionary prototyping.
9. An architecture is the key artifact that allows the architect and project manager to reason
about cost and schedule.
10. An architecture can be created as a transferable, reusable model that forms the heart of a
product line.
1 1 . Architecture-based development focuses attention on the assembly of components, rather
than simply on their creation.
12. An architecture channels the creativity of developers, reducing design and system
complexity.
13. An architecture can be the foundation for training of a new team member.
2.15. For Further Reading
·
·
·- 0 A a
Rebecca Grinter has observed architects from a sociological standpoint. In [Grinter 99] she argues
eloquently that the architect's primary role is to facilitate stakeholder communication. The way she puts
it is that architects enable communication among parties who would otherwise not be able to talk to
each other.
The granddaddy of papers about architecture and organization is [Conway 68]. Conway's law states
that "organizations which design systems . . . are constrained to produce designs which are copies of the
communication structures of these organizations."
There is much about software development through composition that remains unresolved. When the
components that are candidates for importation and reuse are distinct subsystems that have been built
with conflicting architectural assumptions, unanticipated complications can increase the effort required
to integrate their functions. David Garlan and his colleagues coined the term architectural mismatch to
describe this situation, and their paper on it is worth reading [Garlan 95].
Paulish [Paulish 02] discusses architecture-based project management, and in particular the ways in
which an architecture can help in the estimation of project cost and schedule.
2.16. Discussion Questions
·
·
·- 0 A a
1 . For each of the thirteen reasons articulated in this chapter why architecture is important, take the
contrarian position: Propose a set of circumstances under which architecture is not necessary to
achieve the result indicated. Justify your position. (Try to come up with different circumstances
for each of the thirteen.)
2. This chapter argues that architecture brings a number of tangible benefits. How would you
measure the benefits, on a particular project, of each of the thirteen points?
3. Suppose you want to introduce architecture-centric practices to your organization. Your
management is open to the idea, but wants to know the ROI for doing so. How would you
respond?
4. Prioritize the list of thirteen points in this chapter according to some criteria meaningful to you.
Justify your answer. Or, if you could choose only two or three of the reasons to promote the use
of architecture in a project, which would you choose and why?
·
·
·-
3. The Many Contexts of Software Architecture
0
People in London think of London as the center of
the world, whereas New Yorkers think the world
ends three miles outside of Manhattan.
-Toby Young
A a
In 1 976, a New Yorker magazine cover featured a cartoon by Saul Steinberg showing a New Yorker's
view of the world. You've probably seen it; if not, you can easily find it online. Looking to the west
from 9th Avenue in Manhattan, the illustration shows lOth Avenue, then the wide Hudson River, then a
thin strip of cotnpletely nondescript land called "Jersey," followed by a somewhat thicker strip of land
representing the entire rest of the United States. The mostly empty United States has a cartoon mountain
or two here and there and a few cities haphazardly placed "out there," and is flanked by featureless
"Canada" on the right and "Mexico" on the left. Beyond is the Pacific Ocean, only slightly wider than
the Hudson, and beyond that lie tiny amorphous shapes for Japan and China and Russia, and that' s
pretty much the world from a New Yorker' s perspective.
In a book about architecture, it is tempting to view architecture in the same way, as the most
important piece of the software universe. And in some chapters, we unapologetically will do exactly
that. But in this chapter we put software architecture in its place, showing how it supports and is
informed by other critical forces and activities in the various contexts in which it plays a role.
These contexts, around which we structured this book, are as follows:
• Technical. What technical role does the software architecture play in the system or systems of
which it's a part?
• Project life cycle. How does a software architecture relate to the other phases of a software
development life cycle?
• Business. How does the presence of a software architecture affect an organization's business
environment?
• Professional. What is the role of a software architect in an organization or a development
project?
These contexts all play out throughout the book, but this chapter introduces each one. Although the
contexts are unchanging, the specifics for your system may change over time. One challenge for the
architect is to envision what in their context might change and to adopt mechanisms to protect the
system and its development if the envisioned changes come to pass.
3.1. Architecture in a Technical Context
·
·
·- 0 A a
Architectures inhibit or enable the achievement of quality attributes, and one use of an architecture is to
support reasoning about the consequences of change in the particular quality attributes important for a
system at its inception.
Architectures Inhibit or Enable the Achievement of Quality Attributes
Chapter 2 listed thirteen reasons why software architecture is important and merits study. Several of
those reasons deal with exigencies that go beyond the bounds of a particular development project (such
as communication among stakeholders, many of whom may reside outside the project's organization).
Others deal with nontechnical aspects of a project (such as the architecture's influence on a project's
team structure, or its contribution to accurate budget and schedule estimation). The first three reasons in
that List of Thirteen deal specifically with an architecture 's technical impact on every system that uses
it:
1 . An architecture will inhibit or enable the achievement of a system's quality attributes.
2. You can predict many aspects of a system's qualities by studying its architecture.
3. An architecture makes it easier for you to reason about and manage change.
These are all about the architecture 's effect on a system's quality attributes, although the first one
states it the most explicitly. While all of the reasons enumerated in Chapter 2 are valid statements of the
contribution of architecture, probably the most important reason that it warrants attention is its critical
effect on quality attributes.
This is such a critical point that, with your indulgence, we'll add a few more points to the bullet list
that we gave in Section 2. 1 . Remember? The one that started like this:
• If your system requires high performance, then you need to pay attention to managing the timebased
behavior of elements, their use of shared resources, and the frequency and volume of
interelement communication.
To that list, we'll add the following:
• If you care about a system's availability, you have to be concerned with how components take
over for each other in the event of a failure, and how the system responds to a fault.
• If you care about usability, you have to be concen1ed about isolating the details of the user
interface and those elements responsible for the user experience from the rest of the system, so
that those things can be tailored and improved over titne.
• If you care about the testability of your system, you have to be concerned about the testability
of individual elements, which means making their state observable and controllable, plus
understanding the emergent behavior of the elements working together.
• If you care about the safety of your system, you have to be concerned about the behavioral
envelope of the elements and the emergent behavior of the elements working in concert.
• If you care about interoperability between your system and another, you have to be concerned
about which elements are responsible for external interactions so that you can control those
interactions.
·
·
·- 0 A a
These and other representations are all saying the same thing in different ways: If you care about
this quality attribute, you have to be concerned with these decisions, all of which are thoroughly
architectural in nature. An architecture inhibits or enables a system's quality attributes. And conversely,
nothing else influences an architecture more than the quality attribute requirements it must satisfy.
If you care about architecture for no other reason, you should care about it for this one. We feel so
strongly about architecture' s importance with respect to achieving system quality attributes that all of
Part II of this book is devoted to the topic.
Why is functionality missing frmn the preceding list? It is missing because the architecture mainly
provides containers into which the architect places functionality. Functionality is not so much a driver
for the architecture as it is a consequence of it. We return to this point in more detail in Part II.
Architectures and the Technical Environment
The technical environment that is current when an architecture is designed will influence that
architecture. It might include standard industry practices or software engineering techniques prevalent
in the architect's professional co1nmunity. It is a brave architect who, in today's environment, does not
at least consider a web-based, object-oriented, service-oriented, mobility-aware, cloud-based, socialnetworking-
friendly design for an information system. It wasn't always so, and it won't be so ten years
from now when another crop of technological trends has come to the fore.
The Swedish Ship Vasa
In the 1 620s, Sweden and Poland were at war. The king of Sweden, Gustavus
Adolphus, was determined to put a swift and favorable end to it and commissioned a
new warship the likes of which had never been seen before. The Vasa, shown in Figure
3 . 1 , was to be the world's most formidable instrument of war: 70 meters long, able to
carry 300 soldiers, and with an astonishing 64 heavy guns mounted on two gun decks.
Seeking to add overwhelming firepower to his navy to strike a decisive blow, the king
insisted on stretching the Vasa's armaments to the limits. Her architect, Henrik
Hybertsson, was a seasoned Dutch shipbuilder with an impeccable reputation, but the
Vasa was beyond even his broad experience. Two-gun-deck ships were rare, and none
had been built of the Vasa's size and armament.
·
·
·- 0 A a
Figure 3.1. The warship. Used with permission of The Vasa Museum, Stockholm,
Sweden.
Like all architects of systems that push the envelope of experience, Hybertsson had
to balance many concerns. Swift time to deployment was critical, but so were
performance, functionality, safety, reliability, and cost. He was also responsible to a
variety of stakeholders. In this case, the primary customer was the king, but Hybertsson
also was responsible to the crew that would sail his creation. Also like all architects,
Hybertsson brought his experience with him to the task. In this case, his experience told
him to design the Vasa as though it were a single-gun-deck ship and then extrapolate,
which was in accordance with the technical environment of the day. Faced with an
impossible task, Hybertsson had the good sense to die about a year before the ship was
finished.
The project was completed to his specifications, however, and on Sunday morning,
August 1 0, 1 628, the mighty ship was ready. She set her sails, waddled out into
Stockholm's deep-water harbor, fired her guns in salute, and promptly rolled over.
Water poured in through the open gun ports, and the Vasa plummeted. A few minutes
later her first and only voyage ended 30 meters beneath the surface. Dozens among her
1 5 0-man crew drowned.
Inquiries followed, which concluded that the ship was well built but "badly
proportioned." In other words, its architecture was flawed. Today we know that
Hybertsson did a poor job of balancing all of the conflicting constraints levied on him.
In particular, he did a poor job of risk management and a poor job of customer
managetnent (not that anyone could have fared better). He simply acquiesced in the
face of impossible requirements.
The story of the Vasa, although more than 375 years old, well illustrates the
Architecture Influence Cycle: organization goals beget requirements, which beget an
architecture, which begets a system. The architecture flows from the architect's
experience and the technical environment of the day. Hybertsson suffered from the fact
that neither of those were up to the task before him.
·
·
·- 0
In this book, we provide three things that Hybertsson could have used:
1 . Examples of successful architectural practices that work under demanding
requirements, so as to help set the technical playing field of the day.
2. Methods to assess an architecture before any system is built from it, so as to
mitigate the risks associated with launching unprecedented designs.
3. Techniques for incremental architecture-based development, so as to uncover
design flaws before it is too late to correct them.
A a
Our goal is to give architects another way out of their design dilemmas than the one
that befell the ill-fated Dutch ship designer. Death before deployment is not nearly so
admired these days.
-PCC
3.2. Architecture in a Project Life-Cycle Context
·
·
·- 0 A a
Software development processes are standard approaches for developing software systems. They
impose a discipline on software engineers and, more important, teams of software engineers. They tell
the members of the team what to do next. There are four dominant software development processes,
which we describe in roughly the order in which they came to prominence:
1 . Waterfall. For many years the Waterfall model dominated the field of software development.
The Waterfall model organized the life cycle into a series of connected sequential activities,
each with entry and exit conditions and a formalized relationship with its upstream and
downstream neighbors. The process began with requirements specification, followed by
design, then implementation, then integration, then testing, then installation, all followed by
maintenance. Feedback paths from later to earlier steps allowed for the revision of artifacts
(requirements documents, design documents, etc.) on an as-needed basis, based on the
knowledge acquired in the later stage. For example, designers might push back against overly
stringent requirements, which would then be reworked and flow back down. Testing that
uncovered defects would trigger reimplementation (and maybe even redesign). And then the
cycle continued.
2. Iterative. Over time the feedback paths of the Waterfall model became so pronounced that it
became clear that it was better to think of software development as a series of short cycles
through the steps some requirements lead to some design, which can be implemented and
tested while the next cycle's worth of requirements are being captured and designed. These
cycles are called iterations, in the sense of iterating toward the ultimate software solution for
the given problem. Each iteration should deliver something working and useful. The trick
here is to uncover early those requirements that have the most far-reaching effect on the
design; the corresponding danger is to overlook requirements that, when discovered later, will
capsize the design decisions made so far. An especially well-known iterative process is called
the Unified Process (originally named the Rational Unified Process, after Rational Software,
which originated it). It defines four phases of each iteration: inception, elaboration,
construction, and transition. A set of chosen use cases defines the goals for each iteration, and
the iterations are ordered to address the greatest risks first.
3. Agile. The term "Agile software development" refers to a group of software developtnent
methodologies, the best known of which include Scrmn, Extreme Programming, and Crystal
Clear. These methodologies are all incremental and iterative. As such, one can consider some
iterative methodologies as Agile. What distinguishes Agile practices is early and frequent
delivery of working software, close collaboration between developers and customers, selforganizing
teams, and a focus on adaptation to changing circumstances (such as late-arriving
requirements). All Agile methodologies focus on teamwork, adaptability, and close
collaboration (both within the team and between team members and customers/end users).
These methodologies typically eschew substantial up-front work, on the assumption that
requirements always change, and they continue to change throughout the project's life cycle.
As such, it might seem that Agile methodologies and architecture cannot happily coexist. As
we will show in Chapter 1 5, this is not so.
·
·
·- 0 A a
4. Model-driven development. Model-driven development is based on the idea that humans
should not be writing code in programming languages, but they should be creating models of
the domain, from which code is automatically generated. Humans create a platformindependent
model (PIM), which is combined with a platform-definition model (PDM) to
generate running code. In this way the PIM is a pure realization of the functional
requirements while the PDM addresses platform specifics and quality attributes.
All of these processes include design among their obligations, and because architecture is a special
kind of design, architecture finds a home in each one. Changing from one development process to
another in the middle of a project requires the architect to save useful information from the old process
and determine how to integrate it into the new process.
No matter what software development process or life-cycle model you're using, there are a nutnber
of activities that are involved in creating a software architecture, using that architecture to realize a
complete design, and then implementing or managing the evolution of a target system or application.
The process you use will determine how often and when you revisit and elaborate each of these
activities. These activities include:
1 . Making a business case for the system
2. Understanding the architecturally significant requirements
3. Creating or selecting the architecture
4. Documenting and communicating the architecture
5. Analyzing or evaluating the architecture
6. Implementing and testing the system based on the architecture
7. Ensuring that the implementation conforms to the architecture
Each of these activities is covered in a chapter in Part III of this book, and described briefly below.
Making a Business Case for the System
A business case is, briefly, a justification of an organizational investment. It is a tool that helps you
make business decisions by predicting how they will affect your organization. Initially, the decision will
be a go/no-go for pursuing a new business opportunity or approach. After initiation, the business case is
reviewed to assess the accuracy of initial estimates and then updated to examine new or alternative
angles on the opportunity. By documenting the expected costs, benefits, and risks, the business case
serves as a repository of the business and marketing data. In this role, management uses the business
case to determine possible courses of action.
Knowing the business goals for the system Chapter 1 6 will show you how to elicit and capture
them in a systematic way is also critical in the creation of a business case for a system.
Creating a business case is broader than simply assessing the market need for a system. It is an
important step in shaping and constraining any future requirements. How much should the product cost?
What is its targeted market? What is its targeted time to market and lifetime? Will it need to interface
with other systems? Are there system limitations that it must work within?
·
·
·- 0 A a
These are all questions about which the system's architects have specialized knowledge; they must
contribute to the answers. These questions cannot be decided solely by an architect, but if an architect is
not consulted in the creation of the business case, the organization may be unable to achieve its business
goals. Typically, a business case is created prior to the initiation of a project, but it also may be revisited
during the course of the project for the organization to determine whether to continue making
investments in the project. If the circumstances assumed in the initial version of the business case
change, the architect may be called upon to establish how the system will change to reflect the new set
of circumstances.
Understanding the Architecturally Significant Requirements
There are a variety of techniques for eliciting requirements from the stakeholders. For example, objectoriented
analysis uses use cases and scenarios to embody requirements. Safety-critical systems
sometimes use tnore rigorous approaches, such as finite-state-machine models or formal specification
languages. In Part II of this book, which covers quality attributes, we introduce a collection of quality
attribute scenarios that aid in the brainstorming, discussion, and capture of quality attribute
requirements for a system.
One fundamental decision with respect to the system being built is the extent to which it is a
variation on other systems that have been constructed. Because it is a rare system these days that is not
similar to other systems, requirements elicitation techniques involve understanding these prior systems'
characteristics. We discuss the architectural implications of software product lines in Chapter 25.
Another technique that helps us understand requirements is the creation of prototypes. Prototypes
may help to model and explore desired behavior, design the user interface, or analyze resource
utilization. This helps to make the system "real" in the eyes of its stakeholders and can quickly build
support for the project and catalyze decisions on the system's design and the design of its user interface.
Creating or Selecting the Architecture
In the landmark book The Mythical Man-Month, Fred Brooks argues forcefully and eloquently that
conceptual integrity is the key to sound system design and that conceptual integrity can only be had by
a small number of minds coming together to design the system's architecture. We firmly believe this as
well. Good architecture almost never results as an emergent phenomenon.
Chapters 5- 1 2 and 11 will provide practical techniques that will aid you in creating an architecture
to achieve its behavioral and quality requirements.
Documenting and Communicating the Architecture
For the architecture to be effective as the backbone of the project's design, it must be communicated
clearly and unambiguously to all of the stakeholders. Developers must understand the work assignments
that the architecture requires of them, testers must understand the task structure that the architecture
imposes on them, management must understand the scheduling implications it contains, and so forth.
Toward this end, the architecture's documentation should be informative, unambiguous, and
readable by many people with varied backgrounds. Architectural documentation should also be minimal
and aimed at the stakeholders who will use it; we are no fans of documentation for documentation's
·
·
·- 0 A a
sake. We discuss the documentation of architectures and provide examples of good documentation
practices in Chapter 1 8. We will also discuss keeping the architecture up to date when there is a change
in something on which the architecture documentation depends.
Analyzing or Evaluating the Architecture
In any design process there will be multiple candidate designs considered. Some will be rejected
immediately. Others will contend for primacy. Choosing among these competing designs in a rational
way is one of the architect's greatest challenges.
Evaluating an architecture for the qualities that it supports is essential to ensuring that the system
constructed from that architecture satisfies its stakeholders' needs. Analysis techniques to evaluate the
quality attributes that an architecture imparts to a system have become much more widespread in the
past decade. Scenario-based techniques provide one of the most general and effective approaches for
evaluating an architecture. The most mature methodological approach is found in the Architecture
Tradeoff Analysis Method (ATAM) of Chapter 2 1 , while the economic implications of architectural
decisions are explored in Chapter 23.
Implementing and Testing the System Based on the Architecture
If the architect designs and analyzes a beautiful, conceptually sound architecture which the
implementers then ignore, what was the point? If architecture is important enough to devote the time
and effort of your best minds to, then it is just as important to keep the developers faithful to the
structures and interaction protocols constrained by the architecture. Having an explicit and wellcommunicated
architecture is the first step toward ensuring architectural conformance. Having an
environment or infrastructure that actively assists developers in creating and maintaining the
architecture (as opposed to just the code) is better.
There are many reasons why developers might not be faithful to the architecture: It might not have
been properly documented and disseminated. It might be too confusing. It might be that the architect
has not built ground-level support for the architecture (particularly if it presents a different way of
"doing business" than the developers are used to), and so the developers resist it. Or the developers may
sincerely want to implement the architecture but, being human, they occasionally slip up. This is not to
say that the architecture should not change, but it should not change purely on the basis of the whims of
the developers, because they may not have the overall picture.
Ensuring That the Implementation Conforms to the Architecture
Finally, when an architecture is created and used, it goes into a maintenance phase. Vigilance is
required to ensure that the actual architecture and its representation remain faithful to each other during
this phase. And when they do get significantly out of sync, effort must be expended to either fix the
implementation or update the architectural documentation.
Although work in this area is still relatively immature, it has been an area of intense activity in
recent years. Chapter 20 will present the current state of recovering an architecture from an existing
system and ensuring that it conforms to the specified architecture.
3.3. Architecture in a Business Context
·
·
·- 0 A a
Architectures and systems are not constructed frivolously. They serve some business purposes, although
as mentioned before, these purposes may change over time.
Architectures and Business Goals
Systems are created to satisfy the business goals of one or more organizations. Development
organizations want to make a profit, or capture market, or stay in business, or help their customers do
their jobs better, or keep their staff gainfully employed, or make their stockholders happy, or a little bit
of each. Customers have their own goals for acquiring a system, usually involving some aspect of
making their lives easier or more productive. Other organizations involved in a project's life cycle, such
as subcontractors or government regulatory agencies, have their own goals dealing with the system.
Architects need to understand who the vested organizations are and what their goals are. Many of
these goals will have a profound influence on the architecture.
Many business goals will be manifested as quality attribute requirements. In fact, every quality
attribute such as a user-visible response time or platform flexibility or ironclad security or any of a
dozen other needs should originate from some higher purpose that can be described in terms of added
value. If we ask, for example, " Why do you want this system to have a really fast response time?" we
might hear that this will differentiate the product from its competition and let the developing
organization capture market share.
Some business goals, however, will not show up in the form of requirements. We know of one
software architect who was informed by his manager that the architecture should include a database.
The architect was perplexed, because the requirements for the system really didn't warrant a database
and the architect's design had nicely avoided putting one in, thereby simplifying the design and
lowering the cost of the product. The architect was perplexed, that is, until the manager reminded the
architect that the company's database department was currently overstaffed and underworked. They
needed something to do! The architect put in the database, and all was well. That kind of business goal
-keeping staff gainfully employed is not likely to show up in any requirements document, but if the
architect had failed to meet it, the manager would have considered the architecture as unacceptable, just
as the customer would have if it failed to provide a key piece of functionality.
Still other business goals have no effect on the architecture whatsoever. A business goal to lower
costs might be realized by asking employees to work from home, or tum the office thermostats down in
the winter, or using less paper in the printers. Chapter 1 6 will deal with uncovering business goals and
the requirements they lead to.
Figure 3.2 illustrates the major points from the preceding discussion. In the figure, the arrows mean
"leads to." The solid arrows highlight the relationships of most interest to us.
J
I
I
I
I
v
Non architectural Solutions
·
·
·- 0
Quality Attributes
Architecture
A a
Figure 3.2. Some business goals may lead to quality attribute requirements (which lead to
architectures), or lead directly to architectural decisions, or lead to nonarchitectural solutions.
Architectures and the Development Organization
A development organization contributes many of the business goals that influence an architecture. For
example, if the organization has an abundance of experienced and idle programmers skilled in peer-topeer
communications, then a peer-to-peer architecture might be the approach supported by
management. If not, it may well be rejected. This would support the business goal, perhaps left implicit,
of not wanting to hire new staff or lay off existing staff, or not wanting to invest significantly in the
retraining of existing staff.
More generally, an organization often has an investment in assets, such as existing architectures and
the products based on them. The foundation of a development project may be that the proposed system
is the next in a sequence of similar systems, and the cost estimates assume a high degree of asset reuse
and a high degree of skill and productivity from the programmers.
Additionally, an organization may wish to make a long-term business investment in an
infrastructure to pursue strategic goals and may view the proposed system as one means of financing
and extending that infrastructure. For example, an organization may decide that it wants to develop a
reputation for supporting solutions based on cloud computing or service-oriented architecture or highperformance
real-time computing. This long-term goal would be supported, in part, by infrastructural
investments that will affect the developing organization: a cloud-computing group needs to be hired or
grown, infrastructure needs to be purchased, or perhaps training needs to be planned.
Finally, the organizational structure can shape the software architecture, and vice versa.
Organizations are often organized around technology and application concepts: a database group, a
networking group, a business rules team, a user-interface group. So the explicit identification of a
distinct subsystem in the architecture will frequently lead to the creation of a group with the name of the
subsystem. Furthermore, if the user-interface team frequently needs to communicate with the business
rules team, these teams will need to either be co-located or they will need some regular means of
communicating and coordinating.
3.4. Architecture in a Professional Context
·
·
·- 0 A a
What do architects do? How do you become an architect? In this section we talk about the many facets
of being an architect that go beyond what you learned in a programming or software engineering
course.
You probably know by now that architects need more than just technical skills. Architects need to
explain to one stakeholder or another the chosen priorities of different properties, and why particular
stakeholders are not having all of their expectations fulfilled. To be an effective architect, then, you will
need diplomatic, negotiation, and communication skills.
You will perform many activities beyond directly producing an architecture. These activities, which
we call duties, form the backbone of individual architecture competence. We surveyed the broad body
of information aimed at architects (such as websites, courses, books, and position descriptions for
architects), as well as practicing architects, and duties are but one aspect. Writers about architects also
speak of skills and knowledge. For example, architects need the ability to communicate ideas clearly
and need to have up-to-date knowledge about (for example) patterns, or database platforms, or web
services standards.
Duties, skills, and knowledge form a triad on which architecture competence rests. You will need to
be involved in supporting management and dealing with customers. You will need to manage a diverse
workload and be able to switch contexts frequently. You will need to know business considerations.
You will need to be a leader in the eyes of developers and management. In Chapter 24 we examine at
length the architectural competence of organizations and people.
Architects' Background and Experience
We are all products of our experiences, architects included. If you have had good results using a
particular architectural approach, such as three-tier client-server or publish-subscribe, chances are that
you will try that same approach on a new development effort. Conversely, if your experience with an
approach was disastrous, you may be reluctant to try it again.
Architectural choices may also come from your education and training, exposure to successful
architectural patterns, or exposure to systems that have worked particularly poorly or particularly well.
You may also wish to experiment with an architectural pattern or technique learned from a book (such
as this one) or a training course.
Why do we mention this? Because you (and your organization) must be aware of this influence, so
that you can manage it to the best of your abilities. This may mean that you will critically examine
proposed architectural solutions, to ensure that they are not simply the path of least resistance. It may
mean that you will take training courses in interesting new technologies. It may mean that you will
invest in exploratory projects, to "test the water" of a new technology. Each of these steps is a way to
proactively manage your background and experience.
3.5. Stakeholders
·
·
·- 0 A a
Many people and organizations are interested in a software system. We call these entities stakeholders.
A stakeholder is anyone who has a stake in the success of the system: the customer, the end users, the
developers, the project manager, the maintainers, and even those who market the system, for example.
But stakeholders, despite all having a shared stake in the success of the system, typically have different
specific concerns that they wish the system to guarantee or optimize. These concerns are as diverse as
providing a certain behavior at runtime, performing well on a particular piece of hardware, being easy
to customize, achieving short time to market or low cost of development, gainfully employing
programmers who have a particular specialty, or providing a broad range of functions. Figure 3.3 shows
the architect receiving a few helpful stakeholder "suggestions."
Neat features,
short time to market,
low co.st, parity with
competing products!
Low cost,
keeping people
emptoyed!
Behavior,
performance,
security,
rel.iability,
usability!
Modifiability!
Low cost, timely
delivery, not changed
very often!
Figure 3.3. Influence of stakeholders on the architect
You will need to know and understand the nature, source, and priority of constraints on the project
as early as possible. Therefore, you must identify and actively engage the stakeholders to solicit their
needs and expectations. Early engagement of stakeholders allows you to understand the constraints of
·
·
·- 0 A a
the task, manage expectations, negotiate priorities, and make tradeoffs. Architecture evaluation
(covered in Part III of this book) and iterative prototyping are two means for you to achieve stakeholder
engagement.
Having an acceptable system involves appropriate performance, reliability, availability, platform
compatibility, memory utilization, network usage, security, modifiability, usability, and interoperability
with other systems as well as behavior. All of these qualities, and others, affect how the delivered
system is viewed by its eventual recipients, and so such quality attributes will be demanded by one or
more of the system's stakeholders.
The underlying problem, of course, is that each stakeholder has different concerns and goals, some
of which may be contradictory. It is a rare requirements document that does a good job of capturing all
of a system's quality requirements in testable detail (a property is testable if it is falsifiable; "make the
system easy to use" is not falsifiable but "deliver audio packets with no more than 1 0 ms. jitter" is
falsifiable). The architect often has to fill in the blanks the quality attribute requirements that have not
been explicitly stated and mediate the conflicts that frequently emerge.
Therefore, one of the best pieces of advice we can give to architects is this: Know your
stakeholders. Talk to them, engage them, listen to them, and put yourself in their shoes. Table 3 . 1
enumerates a set of stakeholders. Notice the remarkable variety and length of this set, but remember
that not every stakeholder named in this list may play a role in every system, and one person may play
many roles.
Name
Analyst
Architect
Business
Manager
Conformance
Checker
Customer
Database
Administrator
Oeptoyer
Designer
Table 3.1. Stakeholders for a System and Their Interests
Description
Responsible for analyzu'lg the architecture to make sure it meets certain
critical quality attribute requirements. Analysts are often specialized; for
instance, performance analysts. safety analysts, and security analysts
may have well·defined positions in a project.
Responsible for the development of the architecture and its
documentation. Focus and responsibility is on the system.
Responsible for the functioning of the business/organizational en1ily
thai owns the system. Includes managerial/executive responsibility,
responsibility for defining business processes, etc.
Aesponstble for assunng conformance to standards and processes to
provide confidence in a product's suitability.
Pays for the system and ensures its delivery. The customer often speaks
for or represents the end user. especially In a government acquisition
context.
Involved fn many aspects of the data stores, Including database design,
data analysis. data modeling and optimization, installation of database
software, and monitoring and administration of database security.
Responsible for accepting the completed system from the development
effort and deploying it, making it operational. and fulfilling its allocated
business function.
Responsible for systems and/or software design downstream or the
archilecture, applying the archilecture to meet specific requirements of the
parts for which they are responsible.
Interest in Architecture
Analyzing satisfaction of quality attlibute requirements of the system
based on its architecture.
Negotiating and making tradeofls among competing requirements
and design approaches. A vessel for recording design decisions.
Providing evidence that the architecture satisfies its requirements.
Understanding the ability of the architecture to meet business goals.
Basis for conformance checking, for assurance that Implementations
have been fa􀃻hful to the architectural prescriptions.
Assuring required functionality and quality will be delivered; gauging
progress; estimating cost; and setting expectations for What will be
delivered, when, and for how much.
Understanding how data is created, used, and updated by other
architectural elements, and what properties the data and database
must have for the overall system to meet its quality goals.
Understanding the architectural elements that are delivered and
to be installed at the customer or end user's site, and their overall
responsibility toward system function.
Resolving resource contention and establishing performance and
other kinds of runtime resource consumption budgets. Understanding
how their part will communicate and interact with other parts of
the system.
Evaluator
Implementer
Integrator
Maintainer
Network
Administrator
Product-line
Manager
Project Manager
Representative of
External Systems
System Engineer
Tester
User
Responsible for conducting a formal evaluation of the architecture (and hs
documentation) against some clearly defined criteria.
Responsible for the development of specific elements according to
designs, requirements. and the architecture.
Responsible for taking individual components and integrating them,
according to the architecture and system designs.
Responsible lor fixing bugs and providing enhancements to the system
throughout its life (Including adaptation of the system lor uses not originally
envisioned).
Responsible lor the maintenance and oversight of computer hardware
and software in a computer network. This may include the deployment.
configuration. maintenance, and monitoring ol network components.
Responsible tor development of an entire family of products, all built using
the same core assets (including the architecture).
Responsible for planning. sequencing, scheduling, and allocating
resources to develop software components and deliver components to
integration and test activities.
Responsible for managing a system with which this one must interoperate,
and its interface with our system.
Responsible for design and development of systems or system
components in which software plays a role.
Responsible for the (independent) test and verification of the system or its
elements against the formal requirements and the arcMecture.
The actual end users of the system. There may be distinguished kinds of
users. such as administrators. superusers, etc.
·
·
·- 0
Evaluating the architecture's abilhy to deliver required behavior and
quality attributes.
Understanding inviolable constraints and exploitable freedoms on
development activities.
Producing integration plans and procedures, and locating the source
of integration !allures.
Understanding the ramifications of a change.
Determining network loads during various use profiles, understanding
uses of the network.
Determining whether a potential new member of a product family is in
or out of scope and, if out. by how much.
Helping to set budget and schedule. gauging progress against
established budget and schedule, identifying and resolving
development-time resource contention.
Defining the set of agreement between the systems.
Assuring that the system environment provided for the software is
sufficient.
Creating tests based on the behavior and interaction of the software
elements.
Users, in the role of reviewers, might use architecture documentation
to check whether desired functionality is being delivered. Users might
also use the documentation to understand what the major system
elements are, which can aid them In emergency field ma1ntenance.
A a
3.6. How Is Architecture Influenced?
·
·
·- 0 A a
For decades, software designers have been taught to build systems based on the software 's technical
requirements. In the older Waterfall model, the requirements document is "tossed over the wall" into
the designer's cubicle, and the designer must come forth with a satisfactory design. Requirements beget
design, which begets system. In an iterative or Agile approach to development, an increment of
requirements begets an increment of design, and so forth.
This vision of software development is short-sighted. In any development effort, the requirements
make explicit some but only some of the desired properties of the final system. Not all requirements
are focused directly on desired system properties; some requirements might mandate a development
process or the use of a particular tool. Furthermore, the requirements specification only begins to tell
the story. Failure to satisfy other constraints may render the system just as problematic as if it
functioned poorly.
What do you suppose would happen if two different architects, working in two different
organizations, were given the same requirements specification for a system? Do you think they would
produce the same architecture or different ones?
The answer is that they would very likely produce different ones, which immediately belies the
notion that requirements determine architecture. Other factors are at work.
A software architecture is a result of business and social influences, as well as technical ones. The
existence of an architecture in tum affects the technical, business, and social environments that
subsequently influence future architectures. In particular, each of the contexts for architecture that we
just covered technical, project, business, and professional plays a role in influencing an architect and
the architecture, as shown in Figure 3.4.
Architecfs Influences
Busjness
Technical ... Stakeholders ...
.... ,
Project
Professional
Architect
Figure 3.4. Influences on the architect
An architect designing a syste1n for which the real-time deadlines are tight will make one set of
design choices; the same architect, designing a similar system in which the deadlines can be easily
satisfied, will make different choices. And the same architect, designing a non-real-time system, is
likely to make quite different choices still. Even with the same requirements, hardware, support
·
·
·- 0 A a
software, and human resources available, an architect designing a system today is likely to design a
different system than might have been designed five years ago.
3.7. What Do Architectures Influence?
·
·
·- 0 A a
The story about contexts influencing architectures has a flip side. It turns out that architectures have an
influence on the very factors that influence them. Specifically, the existence of an architecture affects
the technical, project, business, and professional contexts that subsequently influence future
architectures.
Here is how the cycle works:
• Technical context. The architecture can affect stakeholder requirements for the next system by
giving the customer the opportunity to receive a system (based on the same architecture) in a
more reliable, timely, and economical manner than if the subsequent system were to be built
from scratch, and typically with fewer defects. A customer may in fact be willing to relax some
of their requirements to gain these economies. Shrink-wrapped software has clearly affected
people's requirements by providing solutions that are not tailored to any individual's precise
needs but are instead inexpensive and (in the best of all possible worlds) of high quality.
Software product lines have the same effect on customers who cannot be so flexible with their
requirements.
• Project context. The architecture affects the structure of the developing organization. An
architecture prescribes a structure for a system; as we will see, it particularly prescribes the
units of software that must be implemented (or otherwise obtained) and integrated to form the
system. These units are the basis for the development project's structure. Teams are formed for
individual software units; and the development, test, and integration activities all revolve
around the units. Likewise, schedules and budgets allocate resources in chunks corresponding
to the units. If a company becomes adept at building families of similar systems, it will tend to
invest in each team by nurturing each area of expertise. Teams become embedded in the
organization' s structure. This is feedback from the architecture to the developing organization.
In any design undertaken by the organization at large, these groups have a strong voice in the
system's decomposition, pressuring for the continued existence of the portions they control.
• Business context. The architecture can affect the business goals of the developing organization.
A successful system built from an architecture can enable a company to establish a foothold in
a particular market segment think of the iPhone or Android app platforms as examples. The
architecture can provide opportunities for the efficient production and deployment of similar
systems, and the organization may adjust its goals to take advantage of its newfound expertise
to plumb the market. This is feedback from the system to the developing organization and the
systems it builds.
• Professional context. The process of system building will affect the architect's experience with
subsequent systems by adding to the corporate experience base. A system that was successfully
built around a particular technical approach will make the architect more inclined to build
systems using the same approach in the future. On the other hand, architectures that fail are less
likely to be chosen for future projects.
These and other feedback mechanisms form what we call the Architecture Influence Cycle, or AIC,
·
·
·- 0 A a
illustrated in Figure 3.5, which depicts the influences of the culture and business of the development
organization on the software architecture. That architecture is, in turn, a primary determinant of the
properties of the developed system or systems. But the AIC is also based on a recognition that shrewd
organizations can take advantage of the organizational and experiential effects of developing an
architecture and can use those effects to position their business strategically for future projects.
Architect's Influences
Business------.
Tech nica�l---+-� Stakeholders 1---:t--
P roject-----.J
Professional ---------�
Architect
Figure 3.5. Architecture Influence Cycle
3.8. Summary
Architectures exist in four different contexts.
·
·
·- 0 A a
1 . Technical. The technical context includes the achievement of quality attribute requirements.
We spend Part II discussing how to do this. The technical context also includes the current
technology. The cloud (discussed in Chapter 26) and mobile computing (discussed in Chapter
27) are important current technologies.
2. Project life cycle. Regardless of the software development methodology you use, you must
make a business case for the system, understand the architecturally significant requirements,
create or select the architecture, document and communicate the architecture, analyze or
evaluate the architecture, implement and test the system based on the architecture, and ensure
that the implementation conforms to the architecture.
3. Business. The system created from the architecture must satisfy the business goals of a wide
variety of stakeholders, each of whmn has different expectations for the system. The
architecture is also influenced by and influences the structure of the development
organization.
4. Professional. You must have certain skills and knowledge to be an architect, and there are
certain duties that you must perform as an architect. These are influenced not only by
coursework and reading but also by your experiences.
An architecture has some influences that lead to its creation, and its existence has an impact on the
architect, the organization, and, potentially, the industry. We call this cycle the Architecture Influence
Cycle.
3.9. For Further Reading
·
·
·- 0 A a
The product line framework produced by the Software Engineering Institute includes a discussion of
business cases from which we drew [SEI 1 2].
The SEI has also published a case study of Celsius Tech that includes an example of how
organizations and customers change over time (Brownsword 96].
Several other SEI reports discuss how to find business goals and the business goals that have been
articulated by certain organizations [Kazman 05, Clements 1 Ob].
Ruth Malan and Dana Bredemeyer provide a description of how an architect can build buy-in within
an organization [Malan 00].
3.1 0. Discussion Questions
·
·
·- 0 A a
1 . Enumerate six different software systems used by your organization. For each of these systems:
a. What are the contextual influences?
b. Who are the stakeholders?
c. How do these systems reflect or impact the organizational structure?
2. What kinds of business goals have driven the construction of the following:
a. The World Wide Web
b. Amazon's EC2 cloud infrastructure
c. Google's Android platform
3. What mechanisms are available to improve your skills and knowledge? What skills are you
lacking?
4. Describe a system you are familiar with and place it into the AIC. Specifically, identify the
forward and reverse influences on contextual factors.
·
·
·- 0
Part Two. Quality Attributes
A a
In Part II, we provide the technical foundations for you to design or analyze an architecture to achieve
particular quality attributes. We do not discuss design or analysis processes here; we cover those topics
in Part III. It is impossible, however, to understand how to improve the performance of a design, for
example, without understanding something about performance.
In Chapter 4 we describe how to specify a quality attribute requirement and motivate design
techniques called tactics to enable you to achieve a particular quality attribute requirement. We also
enumerate seven categories of design decisions. These are categories of decisions that are universally
important, and so we provide material to help an architect focus on these decisions. In Chapter 4, we
describe these categories, and in each of the following chapters devoted to a particular quality attribute
Chapters 5 1 1 we use those categories to develop a checklist that tells you how to focus your
attention on the important aspects associated with that quality attribute. Many of the items in our
checklists may seem obvious, but the purpose of a checklist is to help ensure the completeness of your
design and analysis process.
In addition to providing a treatment of seven specific quality attributes (availability, interoperability,
modifiability, performance, security, testability, and usability), we also describe how you can generate
the material provided in Chapters 5-1 1 for other quality attributes that we have not covered.
Architectural patterns provide known solutions to a number of common problems in design. In
Chapter 1 3 , we present some of the most important patterns and discuss the relationship between
patterns and tactics.
Being able to analyze a design for a particular quality attribute is a key skill that you as an architect
will need to acquire. In Chapter 14, we discuss modeling techniques for some of the quality attributes.
4. Understanding Quality Attributes
·
·
·- 0
Between stimulus and response, there is a space.
In that space is our power to choose our
response. In our response lies our growth and our
freedom.
-Viktor E. Frankl, Man 's Search for Meaning
A a
As we have seen in the Architecture Influence Cycle (in Chapter 3), many factors determine the
qualities that must be provided for in a system's architecture. These qualities go beyond functionality,
which is the basic statement of the system's capabilities, services, and behavior. Although functionality
and other qualities are closely related, as you will see, functionality often takes the front seat in the
development scheme. This preference is shortsighted, however. Systems are frequently redesigned not
because they are functionally deficient the replacements are often functionally identical but because
they are difficult to maintain, port, or scale; or they are too slow; or they have been compromised by
hackers. In Chapter 2, we said that architecture was the first place in software creation in which quality
requirements could be addressed. It is the mapping of a system's functionality onto software structures
that determines the architecture's support for qualities. In Chapters 5-1 1 we discuss how various
qualities are supported by architectural design decisions. In Chapter 1 7 we show how to integrate all of
the quality attribute decisions into a single design.
We have been using the term "quality attribute" loosely, but now it is time to define it more
carefully. A quality attribute (QA) is a measurable or testable property of a system that is used to
indicate how well the system satisfies the needs of its stakeholders. You can think of a quality attribute
as measuring the "goodness" of a product along some dimension of interest to a stakeholder.
In this chapter our focus is on understanding the following:
• How to express the qualities we want our architecture to provide to the system or systems we
are building from it
• How to achieve those qualities
• How to determine the design decisions we might make with respect to those qualities
This chapter provides the context for the discussion of specific quality attributes in Chapters 5-1 1 .
4.1. Architecture and Requirements
·
·
·- 0 A a
Requirements for a system come in a variety of forms: textual requirements, mockups, existing systems,
use cases, user stories, and more. Chapter 1 6 discusses the concept of an architecturally significant
requirement, the role such requirements play in architecture, and how to identify them. No matter the
source, all requirements encompass the following categories:
1 . Functional requirements. These requirements state what the system must do, and how it must
behave or react to runtime stimuli.
2. Quality attribute requirements. These requirements are qualifications of the functional
requirements or of the overall product. A qualification of a functional requirement is an item
such as how fast the function must be performed, or how resilient it must be to erroneous
input. A qualification of the overall product is an item such as the time to deploy the product
or a limitation on operational costs.
3. Constraints. A constraint is a design decision with zero degrees of freedom. That is, it's a
design decision that's already been 1nade. Examples include the requirement to use a certain
programming language or to reuse a certain existing module, or a managetnent fiat to make
your system service oriented. These choices are arguably in the purview of the architect, but
exten1al factors (such as not being able to train the staff in a new language, or having a
business agreement with a software supplier, or pushing business goals of service
interoperability) have led those in power to dictate these design outcomes.
What is the "response" of architecture to each of these kinds of requirements?
1 . Functional requirements are satisfied by assigning an appropriate sequence of responsibilities
throughout the design. As we will see later in this chapter, assigning responsibilities to
architectural elements is a fundamental architectural design decision.
2. Quality attribute requirements are satisfied by the various structures designed into the
architecture, and the behaviors and interactions of the elements that populate those structures.
Chapter 1 7 will show this approach in more detail.
3. Constraints are satisfied by accepting the design decision and reconciling it with other
affected design decisions.
4.2. Functionality
·
·
·- 0 A a
Functionality is the ability of the system to do the work for which it was intended. Of all of the
requirements, functionality has the strangest relationship to architecture.
First of all, functionality does not determine architecture. That is, given a set of required
functionality, there is no end to the architectures you could create to satisfy that functionality. At the
very least, you could divide up the functionality in any number of ways and assign the subpieces to
different architectural elements.
In fact, if functionality were the only thing that mattered, you wouldn't have to divide the system
into pieces at all; a single monolithic blob with no internal structure would do just fine. Instead, we
design our systems as structured sets of cooperating architectural elements modules, layers, classes,
services, databases, apps, threads, peers, tiers, and on and on to make them understandable and to
support a variety of other purposes. Those "other purposes" are the other quality attributes that we'll
turn our attention to in the remaining sections of this chapter, and the remaining chapters of Part II.
But although functionality is independent of any particular structure, functionality is achieved by
assigning responsibilities to architectural elements, resulting in one of the most basic of architectural
structures.
Although responsibilities can be allocated arbitrarily to any modules, software architecture
constrains this allocation when other quality attributes are important. For example, systems are
frequently divided so that several people can cooperatively build them. The architect's interest in
functionality is in how it interacts with and constrains other qualities.
4.3. Quality Attribute Considerations
·
·
·- 0 A a
Just as a system's functions do not stand on their own without due consideration of other quality
attributes, neither do quality attributes stand on their own; they pertain to the functions of the system. If
a functional requirement is "When the user presses the green button, the Options dialog appears," a
performance QA annotation might describe how quickly the dialog will appear; an availability QA
annotation might describe how often this function will fail, and how quickly it will be repaired; a
usability QA annotation might describe how easy it is to learn this function.
Functional Requirements
After more than 1 5 years of writing and discussing the distinction between functional
requirements and quality requirements, the definition of functional requirements still
eludes me. Quality attribute requirements are well defined: performance has to do with
the timing behavior of the system, modifiability has to do with the ability of the system
to support changes in its behavior or other qualities after initial deployment, availability
has to do with the ability of the system to survive failures, and so forth.
Function, however, is much more slippery. An international standard (ISO 250 1 0)
defines functional suitability as "the capability of the software product to provide
functions which meet stated and implied needs when the software is used under
specified conditions." That is, functionality is the ability to provide functions. One
interpretation of this definition is that functionality describes what the system does and
quality describes how well the system does its function. That is, qualities are attributes
of the system and function is the purpose of the system.
This distinction breaks down, however, when you consider the nature of some of the
"function." If the function of the software is to control engine behavior, how can the
function be correctly implemented without considering timing behavior? Is the ability
to control access through requiring a user name/password combination not a function
even though it is not the purpose of any system?
I like much better the use of the word "responsibility" to describe computations that
a system must perform. Questions such as "What are the timing constraints on that set
of responsibilities?", "What modifications are anticipated with respect to that set of
responsibilities?", and "What class of users is allowed to execute that set of
responsibilities?" make sense and are actionable.
The achievement of qualities induces responsibility; think of the user
name/password example just mentioned. Further, one can identify responsibilities as
being associated with a particular set of requirements.
So does this mean that the term "functional requirement" shouldn't be used? People
have an understanding of the term, but when precision is desired, we should talk about
sets of specific responsibilities instead.
Paul Clements has long ranted against the careless use of the term "nonfunctional,"
·
·
·- 0 A a
and now it's my turn to rant against the careless use of the term "functional" probably
equally ineffectually.
-LB
Quality attributes have been of interest to the software community at least since the 1 970s. There
are a variety of published taxonomies and definitions, and many of them have their own research and
practitioner communities. From an architect's perspective, there are three problems with previous
discussions of system quality attributes:
1 . The definitions provided for an attribute are not testable. It is meaningless to say that a
system will be "modifiable." Every system may be modifiable with respect to one set of
changes and not modifiable with respect to another. The other quality attributes are similar in
this regard: a system tnay be robust with respect to some faults and brittle with respect to
others. And so forth.
2. Discussion often focuses on which quality a particular concern belongs to. Is a system failure
due to a denial-of-service attack an aspect of availability, an aspect of performance, an aspect
of security, or an aspect of usability? All four attribute communities would claim ownership
of a system failure due to a denial-of-service attack. All are, to some extent, correct. But this
doesn' t help us, as architects, understand and create architectural solutions to manage the
attributes of concern.
3. Each attribute community has developed its own vocabulary. The performance community
has "events" arriving at a system, the security community has "attacks" arriving at a system,
the availability community has "failures" of a system, and the usability community has "user
input." All of these may actually refer to the same occurrence, but they are described using
different terms.
A solution to the first two of these problems (untestable definitions and overlapping concerns) is to
use quality attribute scenarios as a means of characterizing quality attributes (see the next section). A
solution to the third problem is to provide a discussion of each attribute concentrating on its
underlying concerns to illustrate the concepts that are fundamental to that attribute community.
There are two categories of quality attributes on which we focus. The first is those that describe
some property of the system at runtime, such as availability, performance, or usability. The second is
those that describe some property of the development of the system, such as modifiability or testability.
Within complex systems, quality attributes can never be achieved in isolation. The achievement of
any one will have an effect, sometimes positive and sometimes negative, on the achievement of others.
For example, almost every quality attribute negatively affects performance. Take portability. The main
technique for achieving portable software is to isolate system dependencies, which introduces overhead
into the system's execution, typically as process or procedure boundaries, and this hurts performance.
Determining the design that satisfies all of the quality attribute requirements is partially a matter of
making the appropriate tradeoffs; we discuss design in Chapter 1 7 . Our purpose here is to provide the
context for discussing each quality attribute. In particular, we focus on how quality attributes can be
·
·
·- 0 A a
specified, what architectural decisions will enable the achievement of particular quality attributes, and
what questions about quality attributes will enable the architect to make the correct design decisions.
4.4. Specifying Quality Attribute Requirements
·
·
·- 0 A a
A quality attribute requirement should be unambiguous and testable. We use a common form to specify
all quality attribute requirements. This has the advantage of emphasizing the commonalities among all
quality attributes. It has the disadvantage of occasionally being a force-fit for some aspects of quality
attributes.
Our common form for quality attribute expression has these parts:
• Stimulus. We use the term "stimulus" to describe an event arriving at the system. The stimulus
can be an event to the performance community, a user operation to the usability community, or
an attack to the security community. We use the same term to describe a motivating action for
developmental qualities. Thus, a stimulus for modifiability is a request for a modification; a
stimulus for testability is the completion of a phase of development.
• Stimulus source. A stimulus must have a source it must come from somewhere. The source of
the stimulus may affect how it is treated by the system. A request from a trusted user will not
undergo the same scrutiny as a request by an untrusted user.
• Response. How the system should respond to the stimulus must also be specified. The response
consists of the responsibilities that the system (for runtime qualities) or the developers (for
development-time qualities) should perform in response to the stimulus. For example, in a
performance scenario, an event arrives (the stimulus) and the system should process that event
and generate a response. In a modifiability scenario, a request for a modification arrives (the
stimulus) and the developers should implement the modification without side effects and
then test and deploy the modification.
• Response measure. Determining whether a response is satisfactory whether the requirement is
satisfied is enabled by providing a response measure. For performance this could be a
measure of latency or throughput; for modifiability it could be the labor or wall clock time
required to make, test, and deploy the modification.
These four characteristics of a scenario are the heart of our quality attribute specifications. But there
are two more characteristics that are important: environment and artifact.
• Environment. The environment of a requirement is the set of circumstances in which the
scenario takes place. The environment acts as a qualifier on the stimulus. For example, a
request for a modification that arrives after the code has been frozen for a release may be
treated differently than one that arrives before the freeze. A failure that is the fifth successive
failure of a component may be treated differently than the first failure of that component.
• Artifact. Finally, the artifact is the portion of the system to which the requirement applies.
Frequently this is the entire system, but occasionally specific portions of the system may be
called out. A failure in a data store may be treated differently than a failure in the metadata
store. Modifications to the user interface may have faster response times than modifications to
the middleware.
To summarize how we specify quality attribute requirements, we capture them formally as six-part
scenarios. While it is common to o·mit one or more of these six parts, particularly in the early stages of
·
·
·- 0 A a
thinking about quality attributes, knowing that all parts are there forces the architect to consider whether
each part is relevant.
In summary, here are the six parts:
1 . Source of stimulus. This is some entity (a hutnan, a computer system, or any other actuator)
that generated the stimulus.
2. Stimulus. The stimulus is a condition that requires a response when it arrives at a system.
3. Environment. The stimulus occurs under certain conditions. The system may be in an
overload condition or in normal operation, or some other relevant state. For many systems,
"normal" operation can refer to one of a number of modes. For these kinds of systems, the
environment should specify in which mode the system is executing.
4. Artifact. Some artifact is stimulated. This may be a collection of systems, the whole system,
or some piece or pieces of it.
5. Response. The response is the activity undertaken as the result of the arrival of the stimulus.
6. Response measure. When the response occurs, it should be measurable in some fashion so
that the requirement can be tested.
We distinguish general quality attribute scenarios (which we call "general scenarios" for short)those
that are system independent and can, potentially, pertain to any system from concrete quality
attribute scenarios (concrete scenarios) those that are specific to the particular system under
consideration.
We can characterize quality attributes as a collection of general scenarios. Of course, to translate
these generic attribute characterizations into requirements for a particular system, the general scenarios
need to be tnade system specific. Detailed examples of these scenarios will be given in Chapters 5-1 1 .
Figure 4 . 1 shows the parts of a quality attribute scenario that we have just discussed. Figure 4.2 shows
an example of a general scenario, in this case for availability.
Source
of Stimulus
Stimulus
Artifact
Environment
Response
Response
Measure
Figure 4.1. The parts of a quality attribute scenario
Source
of Stimulus
I nter nai/External:
people, hardware.
software, physical
infrastructure,
physical
environment
..
Stimulus
Fault
' . omtsston,
crash,
incorrect
timing,
incorrect
response
Artifact
Processors,
commumcation
channels, persistent
storage. processes
Environment
Normal operation,
startup, shutdown,
repair mode.
degraded
operation,
overloaded
operation
·
·
·- 0
Response
Prevent fault from
becommg failure
Detect fault log, notify
Recover from fault:
disable event source.
be unavailable,
ftx/mask. de9raded
mode
Response
Measure
Time or time intervar
system must be available
Availability percentage
Time in degraded mode
Time to detect fault
Repair time
Proportion ol faults
system handles
Figure 4.2. A general scenario for availability
A a
4.5. Achieving Quality Attributes through Tactics
·
·
·- 0 A a
The quality attribute requirements specify the responses of the system that, with a bit of luck and a dose
of good planning, realize the goals of the business. We now tum to the techniques an architect can use
to achieve the required quality attributes. We call these techniques architectural tactics. A tactic is a
design decision that influences the achievement of a quality attribute response tactics directly affect
the system's response to some stimulus. Tactics impart portability to one design, high performance to
another, and integrability to a third.
Not My Problem
One time I was doing an architecture analysis on a complex system created by and for
Lawrence Livermore National Laboratory. If you visit their website (www.llnl.gov) and
try to figure out what Livermore Labs does, you will see the word "security" mentioned
over and over. The lab focuses on nuclear security, international and domestic security,
and environmental and energy security. Serious stuff. . .
Keeping this emphasis in mind, I asked them to describe the quality attributes of
concern for the system that I was analyzing. I'm sure you can imagine my surprise
when security wasn't mentioned once! The system stakeholders mentioned
performance, modifiability, evolvability, interoperability, configurability, and
portability, and one or two more, but the word security never passed their lips.
Being a good analyst, I questioned this seemingly shocking and obvious omission.
Their answer was simple and, in retrospect, straightforward: "We don't care about it.
Our systems are not connected to any external network and we have barbed-wire fences
and guards with machine guns." Of course, someone at Livermore Labs was very
interested in security. But it was clearly not the software architects.
-RK
The focus of a tactic is on a single quality attribute response. Within a tactic, there is no
consideration of tradeoffs. Tradeoffs must be explicitly considered and controlled by the designer. In
this respect, tactics differ from architectural patterns, where tradeoffs are built into the pattern. (We visit
the relation between tactics and patterns in Chapter 14. Chapter 1 3 explains how sets of tactics for a
quality attribute can be constructed, which are the steps we used to produce the set in this book.)
A system design consists of a collection of decisions. Some of these decisions help control the
quality attribute responses; others ensure achievement of system functionality. We represent the
relationship between stimulus, tactics, and response in Figure 4.3. The tactics, like design patterns, are
design techniques that architects have been using for years. Our contribution is to isolate, catalog, and
describe them. We are not inventing tactics here, we are just capturing what architects do in practice.
r
Stimulus
Tactics
to Control
Response Response
•-
·
·
·- 0
Figure 4.3. Tactics are intended to control responses to stimuli.
Why do we do this? There are three reasons:
A a
1 . Design patterns are complex; they typically consist of a bundle of design decisions. But
patterns are often difficult to apply as is; architects need to modify and adapt them. By
understanding the role of tactics, an architect can more easily assess the options for
augmenting an existing pattern to achieve a quality attribute goal.
2. If no pattern exists to realize the architect's design goal, tactics allow the architect to
construct a design fragment from "first principles." Tactics give the architect insight into the
properties of the resulting design fragment.
3. By cataloging tactics, we provide a way of making design more systematic within some
limitations. Our list of tactics does not provide a taxonomy. We only provide a
categorization. The tactics will overlap, and you frequently will have a choice among
multiple tactics to improve a particular quality attribute. The choice of which tactic to use
depends on factors such as tradeoffs among other quality attributes and the cost to implement.
These considerations transcend the discussion of tactics for particular quality attributes.
Chapter 17 provides some techniques for choosing among competing tactics.
The tactics that we present can and should be refined. Consider performance: Schedule resources is
a common performance tactic. But this tactic needs to be refined into a specific scheduling strategy,
such as shortest-job-first, round-robin, and so forth, for specific purposes. Use an intermediary is a
modifiability tactic. But there are multiple types of intermediaries (layers, brokers, and proxies, to name
just a few). Thus there are refinements that a designer will employ to make each tactic concrete.
In addition, the application of a tactic depends on the context. Again considering performance:
Manage sampling rate is relevant in some real-time systems but not in all real-time systems and
certainly not in database systems.
4.6. Guiding Quality Design Decisions
·
·
·- 0 A a
Recall that one can view an architecture as the result of applying a collection of design decisions. What
we present here is a systematic categorization of these decisions so that an architect can focus attention
on those design dimensions likely to be most troublesome.
The seven categories of design decisions are
1 . Allocation of responsibilities
2. Coordination model
3. Data model
4. Management of resources
5. Mapping among architectural eletnents
6. Binding time decisions
7. Choice of technology
These categories are not the only way to classify architectural design decisions, but they do provide
a rational division of concerns. These categories might overlap, but it's all right if a particular decision
exists in two different categories, because the concern of the architect is to ensure that every important
decision is considered. Our categorization of decisions is partially based on our definition of software
architecture in that many of our categories relate to the definition of structures and the relations among
them.
Allocation of Responsibilities
Decisions involving allocation of responsibilities include the following:
• Identifying the important responsibilities, including basic system functions, architectural
infrastructure, and satisfaction of quality attributes.
• Determining how these responsibilities are allocated to non-runtime and runtime elements
(namely, modules, components, and connectors).
Strategies for making these decisions include functional decomposition, modeling real-world
objects, grouping based on the major modes of system operation, or grouping based on similar quality
requirements: processing frame rate, security level, or expected changes.
In Chapters 5-1 1 , where we apply these design decision categories to a number of important quality
attributes, the checklists we provide for the allocation of responsibilities category is derived
systematically from understanding the stimuli and responses listed in the general scenario for that QA.
Coordination Model
Software works by having elements interact with each other through designed mechanisms. These
mechanisms are collectively referred to as a coordination model. Decisions about the coordination
model include these:
• Identifying the elements of the system that must coordinate, or are prohibited from
coordinating.
·
·
·- 0 A a
• Determining the properties of the coordination, such as timeliness, currency, completeness,
correctness, and consistency.
• Choosing the communication mechanisms (between systems, between our system and external
entities, between elements of our system) that realize those properties. Important properties of
the communication mechanisms include stateful versus stateless, synchronous versus
asynchronous, guaranteed versus nonguaranteed delivery, and performance-related properties
such as throughput and latency.
Data Model
Every system must represent artifacts of system-wide interest data in some internal fashion. The
collection of those representations and how to interpret them is referred to as the data model. Decisions
about the data model include the following:
• Choosing the major data abstractions, their operations, and their properties. This includes
determining how the data items are created, initialized, accessed, persisted, manipulated,
translated, and destroyed.
• Compiling metadata needed for consistent interpretation of the data.
• Organizing the data. This includes determining whether the data is going to be kept in a
relational database, a collection of objects, or both. If both, then the mapping between the two
different locations of the data must be determined.
Management of Resources
An architect may need to arbitrate the use of shared resources in the architecture. These include hard
resources (e.g., CPU, memory, battery, hardware buffers, system clock, I/0 ports) and soft resources
(e.g., system locks, software buffers, thread pools, and non-thread-safe code).
Decisions for management of resources include the following:
• Identifying the resources that must be managed and determining the limits for each.
• Determining which system element(s) manage each resource.
• Determining how resources are shared and the arbitration strategies employed when there is
contention.
• Determining the impact of saturation on different resources. For example, as a CPU becomes
more heavily loaded, performance usually just degrades fairly steadily. On the other hand,
when you start to run out of memory, at some point you start paging/swapping intensively and
your performance suddenly crashes to a halt.
Mapping among Architectural Elements
An architecture must provide two types of mappings. First, there is mapping between elements in
different types of architecture structures for example, mapping from units of development (modules)
to units of execution (threads or processes). Next, there is mapping between software elements and
environment elements for example, mapping from processes to the specific CPUs where these
processes will execute.
Useful mappings include these:
·
·
·- 0 A a
• The mapping of modules and runtime elements to each other that is, the runtime elements that
are created from each module; the modules that contain the code for each runtime element.
• The assignment of runtime elements to processors.
• The assignment of items in the data model to data stores.
• The mapping of modules and runtime elements to units of delivery.
Binding Time Decisions
Binding time decisions introduce allowable ranges of variation. This variation can be bound at different
times in the software life cycle by different entities from design time by a developer to runtime by an
end user. A binding time decision establishes the scope, the point in the life cycle, and the mechanism
for achieving the variation.
The decisions in the other six categories have an associated binding time decision. Examples of
such binding time decisions include the following:
• For allocation of responsibilities, you can have build-time selection of modules via a
parameterized makefile.
• For choice of coordination model, you can design runtime negotiation of protocols.
• For resource management, you can design a system to accept new peripheral devices plugged
in at runtime, after which the system recognizes them and downloads and installs the right
drivers automatically.
• For choice of technology, you can build an app store for a smartphone that automatically
downloads the version of the app appropriate for the phone of the cust01ner buying the app.
When making binding time decisions, you should consider the costs to implement the decision and
the costs to make a modification after you have implemented the decision. For example, if you are
considering changing platforms at some time after code time, you can insulate yourself from the effects
caused by porting your system to another platform at some cost. Making this decision depends on the
costs incurred by having to modify an early binding compared to the costs incurred by implementing
the mechanisms involved in the late binding.
Choice of Technology
Every architecture decision must eventually be realized using a specific technology. Sometimes the
technology selection is made by others, before the intentional architecture design process begins. In this
case, the chosen technology becomes a constraint on decisions in each of our seven categories. In other
cases, the architect must choose a suitable technology to realize a decision in every one of the
categories.
Choice of technology decisions involve the following:
• Deciding which technologies are available to realize the decisions made in the other categories.
• Determining whether the available tools to support this technology choice (IDEs, simulators,
testing tools, etc.) are adequate for development to proceed.
·
·
·- 0 A a
• Determining the extent of internal familiarity as well as the degree of external support available
for the technology (such as courses, tutorials, examples, and availability of contractors who can
provide expertise in a crunch) and deciding whether this is adequate to proceed.
• Determining the side effects of choosing a technology, such as a required coordination model
or constrained resource management opportunities.
• Determining whether a new technology is compatible with the existing technology stack. For
example, can the new technology run on top of or alongside the existing technology stack? Can
it communicate with the existing technology stack? Can the new technology be monitored and
managed?
4.7. Summary
Requirements for a system come in three categories:
·
·
·- 0
1 . Functional. These requirements are satisfied by including an appropriate set of
responsibilities within the design.
A a
2. Quality attribute. These requirements are satisfied by the structures and behaviors of the
architecture.
3. Constraints. A constraint is a design decision that's already been made.
To express a quality attribute requirement, we use a quality attribute scenario. The parts of the
scenario are these:
1 . Source of stimulus
2. Stimulus
3. Environment
4. Artifact
5. Response
6. Response measure
An architectural tactic is a design decision that affects a quality attribute response. The focus of a
tactic is on a single quality attribute response. Architectural patterns can be seen as "packages" of
tactics.
The seven categories of architectural design decisions are these:
1 . Allocation of responsibilities
2. Coordination model
3. Data model
4. Management of resources
5. Mapping among architectural elements
6. Binding time decisions
7. Choice of technology
4.8. For Further Reading
·
·
·- 0
Philippe Kruchten [Kruchten 04] provides another categorization of design decisions.
A a
Perra [Perra 87] uses categories of Function/Form/Economy/Time as a way of categorizing design
decisions.
Binding time and mechanisms to achieve different types of binding times are discussed in
[Bachmann 05].
Taxonomies of quality attributes can be found in [Boehm 78], [McCall 77], and [ISO 1 1].
Arguments for viewing architecture as essentially independent from function can be found in [Shaw
25}.
4.9. Discussion Questions
·
·
·- 0 A a
1 . What is the relationship between a use case and a quality attribute scenario? If you wanted to add
quality attribute information to a use case, how would you do it?
2. Do you suppose that the set of tactics for a quality attribute is finite or infinite? Why?
3. Discuss the choice of programming language (an example of choice of technology) and its
relation to architecture in general, and the design decisions in the other six categories? For
instance, how can certain programming languages enable or inhibit the choice of particular
coordination models?
4. We will be using the automatic teller machine as an example throughout the chapters on quality
attributes. Enumerate the set of responsibilities that an automatic teller machine should support
and propose an initial design to accommodate that set of responsibilities. Justify your proposal.
5. Think about the screens that your favorite automatic teller machine uses. What do those screens
tell you about binding time decisions reflected in the architecture?
6. Consider the choice between synchronous and asynchronous communication (a choice in the
coordination mechanism category). What quality attribute requirements might lead you to choose
one over the other?
7. Consider the choice between stateful and stateless communication (a choice in the coordination
mechanism category). What quality attribute requirements might lead you to choose one over the
other?
8. Most peer-to-peer architecture employs late binding of the topology. What quality attributes does
this promote or inhibit?
5. Availability
With James Scott
·
·
·-
Ninety percent of life is just showing up.
-Woody Allen
0 A a
Availability refers to a property of software that it is there and ready to carry out its task when you need
it to be. This is a broad perspective and encompasses what is normally called reliability (although it
may encompass additional considerations such as downtime due to periodic maintenance). In fact,
availability builds upon the concept of reliability by adding the notion of recovery that is, when the
system breaks, it repairs itself. Repair may be accomplished by various means, which we'll see in this
chapter. More precisely, A vizienis and his colleagues have defined dependability:
Dependability is the ability to avoid failures that are more frequent and more severe than is
acceptable.
Our definition of availability as an aspect of dependability is this: "Availability refers to the ability
of a system to mask or repair faults such that the cumulative service outage period does not exceed a
required value over a specified time interval." These definitions make the concept of failure subject to
the judgment of an external agent, possibly a human. They also subsume concepts of reliability,
confidentiality, integrity, and any other quality attribute that involves a concept of unacceptable failure.
Availability is closely related to security. A denial-of-service attack is explicitly designed to make a
system fail that is, to make it unavailable. Availability is also closely related to performance, because
it may be difficult to tell when a system has failed and when it is simply being outrageously slow to
respond. Finally, availability is closely allied with safety, which is concerned with keeping the system
from entering a hazardous state and recovering or limiting the damage when it does.
Fundamentally, availability is about minimizing service outage time by mitigating faults. Failure
implies visibility to a system or human observer in the environment. That is, a failure is the deviation of
the system from its specification, where the deviation is externally visible. One of the most demanding
tasks in building a high-availability, fault-tolerant system is to understand the nature of the failures that
can arise during operation (see the sidebar "Planning for Failure"). Once those are understood,
mitigation strategies can be designed into the software.
A failure 's cause is called a fault. A fault can be either internal or external to the system under
consideration. Intermediate states between the occurrence of a fault and the occurrence of a failure are
called errors. Faults can be prevented, tolerated, removed, or forecast. In this way a system becomes
"resilient" to faults.
Among the areas with which we are concerned are how system faults are detected, how frequently
system faults may occur, what happens when a fault occurs, how long a system is allowed to be out of
operation, when faults or failures may occur safely, how faults or failures can be prevented, and what
kinds of notifications are required when a failure occurs.
·
·
·- 0 A a
Because a system failure is observable by users, the time to repair is the time until the failure is no
longer observable. This may be a brief delay in the response time or it may be the time it takes someone
to fly to a remote location in the Andes to repair a piece of mining machinery (as was recounted to us
by a person responsible for repairing the software in a mining machine engine). The notion of
"observability" can be a tricky one: the Stuxnet virus, as an example, went unobserved for a very long
time even though it was doing damage. In addition, we are often concerned with the level of capability
that remains when a failure has occurred a degraded operating mode.
The distinction between faults and failures allows discussion of automatic repair strategies. That is,
if code containing a fault is executed but the system is able to recover from the fault without any
deviation from specified behavior being observable, there is no failure.
The availability of a system can be calculated as the probability that it will provide the specified
services within required bounds over a specified time interval. When referring to hardware, there is a
well-known expression used to derive steady-state availability:
MTB,F
(MTBF + MTTR)
where MTBF refers to the mean time between failures and MTTR refers to the mean time to repair. In
the software world, this formula should be interpreted to mean that when thinking about availability,
you should think about what will make your system fail, how likely that is to occur, and that there will
be some time required to repair it.
From this formula it is possible to calculate probabilities and make claims like "99 .999 percent
availability," or a 0.001 percent probability that the system will not be operational when needed.
Scheduled downtimes (when the system is intentionally taken out of service) may not be considered
when calculating availability, because the system is deemed "not needed" then; of course, this depends
on the specific requirements for the system, often encoded in service-level agreements (SLAs). This
arrangement may lead to seemingly odd situations where the system is down and users are waiting for
it, but the downtime is scheduled and so is not counted against any availability requirements.
In operational systems, faults are detected and correlated prior to being reported and repaired. Fault
correlation logic will categorize a fault according to its severity (critical, major, or minor) and service
impact (service-affecting or non-service-affecting) in order to provide the system operator with timely
and accurate system status and allow for the appropriate repair strategy to be employed. The repair
strategy may be automated or may require manual intervention.
The availability provided by a computer system or hosting service is frequently expressed as a
service-level agreement. This SLA specifies the availability level that is guaranteed and, usually, the
penalties that the computer system or hosting service will suffer if the SLA is violated. The SLA that
Amazon provides for its EC2 cloud service is
A WS will use commercially reasonable efforts to make Amazon EC2 available with an
Annual Uptime Percentage [defined elsewhere] of at least 99.95o/o during the Service Year.
In the event Amazon EC2 does not meet the Annual Uptime Percentage commitment, you
will be eligible to receive a Service Credit as described below.
·
·
·- 0 A a
Table 5 . 1 provides examples of system availability requirements and associated threshold values for
acceptable system downtime, measured over observation periods of 90 days and one year. The term
high availability typically refers to designs targeting availability of 99.999 percent ("5 nines") or
greater. By definition or convention, only unscheduled outages contribute to system downtime.
Table 5.1. System Availability Requirements
Availability
99.0%
99·.9%
99.99%
99.999%
99.9999%
Downtlme/90 Days
2 1 hours, 36 mtnutes
2 hours, 10 minutes
12 minutes, 58 seconds
1 minute, 18 seconds
8 seconds
Planning for Failure
Downtime/Year
3 days, 15.6 hours
8 hours, 0 minutes, 46 seconds
52 minutes, 34 seconds
5 minutes, 1 5 seconds
32 seconds
When designing a high-availability or safety-critical system, it's tempting to say that
failure is not an option. It's a catchy phrase, but it's a lousy design philosophy. In fact,
failure is not only an option, it's almost inevitable. What will make your system safe
and available is planning for the occurrence of failure or (more likely) failures, and
handling them with aplomb. The first step is to understand what kinds of failures your
system is prone to, and what the consequences of each will be. Here are three wellknown
techniques for getting a handle on this.
Hazard analysis
Hazard analysis is a teclmique that attempts to catalog the hazards that can occur
during the operation of a system. It categorizes each hazard according to its severity.
For example, the D0- 1 78B standard used in the aeronautics industry defmes these
failure condition levels in terms of their effects on the aircraft, crew, and passengers:
• Catastrophic. This kind of failure may cause a crash. This failure represents the loss
of critical function required to safely fly and land aircraft.
• Hazardous. This kind of failure has a large negative impact on safety or performance,
or reduces the ability of the crew to operate the aircraft due to physical distress or a
higher workload, or causes serious or fatal injuries among the passengers.
• Major. This kind of failure is significant, but has a lesser impact than a Hazardous
failure (for example, leads to passenger discomfort rather than inj uries) or
significantly increases crew workload to the point where safety is affected.
• Minor. This kind of failure is noticeable, but has a lesser impact than a Major failure
(for example, causing passenger inconvenience or a routine flight plan change).
• No effect. This kind of failure has no impact on safety, aircraft operation, or crew
workload.
Other domains have their own categories and definitions. Hazard analysis also
·
·
·- 0 A a
assesses the probability of each hazard occurring. Hazards for which the product of cost
and probability exceed some threshold are then made the subject of mitigation
activities.
Fault tree analysis
Fault tree analysis is an analytical technique that specifies a state of the system that
negatively impacts safety or reliability, and then analyzes the system's context and
operation to find all the ways that the undesired state could occur. The technique uses a
graphic construct (the fault tree) that helps identify all sequential and parallel sequences
of contributing faults that will result in the occurrence of the undesired state, which is
listed at the top of the tree (the "top event"). The contributing faults might be hardware
failures, human errors, software errors, or any other pertinent events that can lead to the
undesired state.
Figure 5 . 1 , taken from a NASA handbook on fault tree analysis, shows a very
simple fault tree for which the top event is failure of component D. It shows that
component D can fail if A fails and either B or C fails.
0 Fails
G1
�------�====�����======�------�
A Fai·ls B or C Fail
A G2
�-----L======�����====�------�
B Fails c Fails
B c
u ( )
Figure 5. 1. A simple fault tree. D fails if A fails and either B or C fails.
The symbols that connect the events in a fault tree are called gate symbols, and are
taken from Boolean logic diagrams. Figure 5.2 illustrates the notation.
n
GATE SYMBOLS
AND Output fault occurs if all ·Of the input faults occur
·
·
·-
OR Output fault occurs if a least one of the input faults occurs
COMBINATION Output fault occurs if n of the input faults occur
EXCLUSIVE OR Outplut fault occurs if exactly one of the input
faults occurs
0
PRIORHY AND Output fault occurs if all of the input faults ¢ccur in a
specific sequence (the sequence is represented by a CONDJTIONING
EVENT drawn to the right of the gate)
INHIBIT OutplJt fault ocours if the (single) input 1ault occurs in the
presence of an enabling condition (the enabling condition is represented
by a GONDlTIONING EVENT drawn to the r􀁒ght of the gate}
Figure 5.2. Fault tree gate symbols
A a
A fault tree lends itself to static analysis in various ways. For example, a "minimal
cut set" is the smallest combination of events along the bottom of the tree that together
can cause the top event. The set of minimal cut sets shows all the ways the bottom
events can combine to cause the overarching failure. Any singleton minimal cut set
reveals a single point of failure, which should be carefully scrutinized. Also, the
probabilities of various contributing failures can be combined to come up with a
probability of the top event occurring. Dynamic analysis occurs when the order of
contributing failures matters. In this case, techniques such as Markov analysis can be
used to calculate probability of failure over different failure sequences.
Fault trees aid in system design, but they can also be used to diagnose failures at
runtime. If the top event has occurred, then (assuming the fault tree model is complete)
one or more of the contributing failures has occurred, and the fault tree can be used to
track it down and initiate repairs.
Failure Mode, Effects, and Criticality Analysis (FMECA) catalogs the kinds of
failures that syste1ns of a given type are prone to, along with how severe the effects of
each one can be. FMECA relies on the history of failure of similar systems in the past.
Table 5 .2, also taken from the NASA handbook, shows the data for a system of
redundant amplifiers. Historical data shows that amplifiers fail most often when there is
a short circuit or the circuit is left open, but there are several other failure modes as well
(lumped together as "Other").
Table 5.2. Failure Probabilities and Effects
Failure Failure
Component Probability Mode
Open
A 1 X 10-.. Short
Other
B 1 X 10-3 OpBn
Short
Other
o/o Failures
by Mode Critical
90
5 X (5 X 10-􀅖)
5 X (5 X 10-5)
90
5 X (5 X 10-6)
5 X (5 X 10-s)
·
·
·-
Effects
0
Noncritical
X
X
Adding up the critical column gives us the probability of a critical system failure: 5
x 1 o-5 + 5 x 1 o-5 + 5 x 1 o-5 + 5 x 1 o-5
= 2 x 1 o-4.
A a
These techniques, and others, are only as good as the knowledge and experience of
the people who populate their respective data structures. One of the worst mistakes you
can make, according to the NASA handbook, is to let form take priority over substance.
That is, don't let safety engineering become a matter of just filling out the tables.
Instead, keep pressing to find out what else can go wrong, and then plan for it.
5.1. Availability General Scenario
·
·
·- 0 A a
From these considerations we can now describe the individual portions of an availability general
scenario. These are summarized in Table 5.3:
• Source of stimulus. We differentiate between internal and external origins of faults or failure
because the desired system response may be different.
• Stimulus. A fault of one of the following classes occurs:
• Omission. A component fails to respond to an input.
• Crash. The component repeatedly suffers omission faults.
• Timing. A component responds but the response is early or late.
• Response. A component responds with an incorrect value.
• Artifact. This specifies the resource that is required to be highly available, such as a processor,
communication channel, process, or storage.
• Environment. The state of the system when the fault or failure occurs may also affect the
desired system response. For example, if the system has already seen some faults and is
operating in other than normal mode, it may be desirable to shut it down totally. However, if
this is the first fault observed, some degradation of response time or function may be preferred.
• Response. There are a number of possible reactions to a system fault. First, the fault must be
detected and isolated (correlated) before any other response is possible. (One exception to this
is when the fault is prevented before it occurs.) After the fault is detected, the system must
recover from it. Actions associated with these possibilities include logging the failure, notifying
selected users or other systems, taking actions to limit the damage caused by the fault,
switching to a degraded mode with either less capacity or less function, shutting down external
systems, or becoming unavailable during repair.
• Response measure. The response measure can specify an availability percentage, or it can
specify a time to detect the fault, time to repair the fault, times or time intervals during which
the system must be available, or the duration for which the system must be available.
Table 5.3. Availability General Scenario
P'ortion of Possible Values
Scenario
·
·
·- 0
Source Internal/external: people, hardware, software, physical infrastructure,
physical environment
Stimulus Fault: omission, crash, inc-orrect timing, incorrect response
Artifact Processors, communication channels, persistent storage, processes
Envjronment Normal operation, startup, shutdown, repair mode, degraded operation,
over1oaded operation
Response Prevent the fault from becoming a failure
Detect the fault:
• Log the fault
• Notify appropriate entities (people or systems)
Recover from the fault:
• Disable source of events causing the fault
• Be temporarily unavailable while repair is being effected
• Fix or mask the fault/failure or contain the damage it causes
• Operate in a degraded mode while repair Is being effected
Response Time or time interval when the system must be available
Measure Availability percentage (e.g., 99.999%)
Time to detect the fault
Time to repair the fault
Time or time interval in which system can be in degraded mode
Proportion {e.g., 99%) or rate (e.g., up to 100 per second) of a certain
class of faults that the system prevents, or handtes without failing
A a
Figure 5.3 shows a concrete scenario generated from the general scenario: The heartbeat monitor
determines that the server is nonresponsive during normal operations. The system informs the operator
and continues to operate with no downtime.
Source:
Heartbeat
Monitor
Stimulus·:
Server
Unresponsive
Artifact:
Process
Environment:
Normal
Operation
Response:
Inform
Operator
Continue
to Operate
Response
Measure:
No Downtime
Figure 5.3. Sample concrete availability scenario
5.2. Tactics for Availability
·
·
·- 0 A a
A failure occurs when the system no longer delivers a service that is consistent with its specification;
this failure is observable by the system's actors. A fault (or combination of faults) has the potential to
cause a failure. Availability tactics, therefore, are designed to enable a system to endure system faults so
that a service being delivered by the system remains compliant with its specification. The tactics we
discuss in this section will keep faults from becoming failures or at least bound the effects of the fault
and make repair possible. We illustrate this approach in Figure 5.4.
Fault
Tactjcs
to Control
Ava ilability Fault Masked
or Repair Made
Figure 5.4. Goal of availability tactics
Availability tactics may be categorized as addressing one of three categories: fault detection, fault
recovery, and fault prevention. The tactics categorization for availability is shown in Figure 5.5 (on the
next page). Note that it is often the case that these tactics will be provided for you by a software
infrastructure, such as a middleware package, so your job as an architect is often one of choosing and
assessing (rather than implementing) the right availability tactics and the right combination of tactics.
Detect Faults
'
Ping I Echo
Monitor
Heartbeat
Timestamp
Fault
Sanity
Checking
Condition
Monitoring
Voting
Exception
Detection
Self-Test
Detect Faults
Availability TacUcs
-----..
Recover from Fau.lls
Preparation
and Repair
Active
<Redundancy
Passive
Redundancy
Spare
Exoeplion
HandHng
Rollback
Software
Upgrade
Retry
Ignore Faulty
Behavior
Degradation
Rei ntroduction
Shadow
State
Resynchroniz.ation
EscaJating
Restart
Non􀃰Stop
FoMarding
Aecootiguration
Figure 5.5. Availability tactics
·
·
·- 0
Prevent Faults
y
Removal from
Service
Transactions
Predictive
Model
Exception
Prevention
Increase
Competence Set
A a
Fault
Masked
or
Repair
Made
Before any system can take action regarding a fault, the presence of the fault must be detected or
anticipated. Tactics in this category include the following:
• Ping/echo refers to an asynchronous request/response message pair exchanged between nodes,
used to determine reachability and the round-trip delay through the associated network path.
But the echo also determines that the pinged component is alive and responding correctly. The
ping is often sent by a system monitor. Ping/echo requires a time threshold to be set; this
threshold tells the pinging component how long to wait for the echo before considering the
pinged component to have failed ("timed out"). Standard implementations of ping/echo are
available for nodes interconnected via IP.
• Monitor. A monitor is a component that is used to monitor the state of health of various other
parts of the system: processors, processes, 1/0, memory, and so on. A system monitor can
detect failure or congestion in the network or other shared resources, such as from a denial-of·

·
·- 0 A a
service attack. It orchestrates software using other tactics in this category to detect
malfunctioning c01nponents. For example, the system monitor can initiate self-tests, or be the
component that detects faulty time stamps or missed heartbeats.l
1. When the detection mechanism is implemented using a counter or timer that is periodically reset, this
specialization of system monitor is referred to as a "watchdog." During nominal operation, the process being
monitored will periodically reset the watchdog counter/timer as part of its signal that it's working correctly; this is
sometimes referred to as "petting the watchdog."
• Heartbeat is a fault detection mechanism that employs a periodic message exchange between a
system monitor and a process being monitored. A special case of heartbeat is when the process
being monitored periodically resets the watchdog timer in its monitor to prevent it from
expiring and thus signaling a fault. For systems where scalability is a concern, transport and
processing overhead can be reduced by piggybacking heartbeat messages on to other control
messages being exchanged between the process being monitored and the distributed system
controller. The big difference between heartbeat and ping/echo is who holds the responsibility
for initiating the health check the monitor or the component itself.
• Time stamp. This tactic is used to detect incorrect sequences of events, primarily in distributed
message-passing systems. A time stamp of an event can be established by assigning the state of
a local clock to the event immediately after the event occurs. Simple sequence numbers can
also be used for this purpose, if time information is not important.
• Sanity checking checks the validity or reasonableness of specific operations or outputs of a
component. This tactic is typically based on a knowledge of the internal design, the state of the
system, or the nature of the information under scrutiny. It is most often employed at interfaces,
to examine a specific information flow.
• Condition monitoring involves checking conditions in a process or device, or validating
assumptions made during the design. By monitoring conditions, this tactic prevents a system
from producing faulty behavior. The computation of checksums is a common example of this
tactic. However, the 1nonitor must itself be simple (and, ideally, provable) to ensure that it does
not introduce new software errors.
• Voting. The most common realization of this tactic is referred to as triple modular redundancy
(TMR), which employs three components that do the same thing, each of which receives
identical inputs, and forwards their output to voting logic, used to detect any inconsistency
among the three output states. Faced with an inconsistency, the voter reports a fault. It must
also decide what output to use. It can let the majority rule, or choose some computed average of
the disparate outputs. This tactic depends critically on the voting logic, which is usually
realized as a simple, rigorously reviewed and tested singleton so that the probability of error is
low.
• Replication is the simplest form of voting; here, the components are exact clones of each
other. Having multiple copies of identical components can be effective in protecting against
random failures of hardware, but this cannot protect against design or implementation
errors, in hardware or software, because there is no form of diversity embedded in this
tactic.
·
·
·- 0 A a
• Functional redundancy is a form of voting intended to address the issue of common-mode
failures (design or itnpletnentation faults) in hardware or software components. Here, the
components must always give the same output given the same input, but they are diversely
designed and diversely implemented.
• Analytic redundancy permits not only diversity among components' private sides, but also
diversity among the components' inputs and outputs. This tactic is intended to tolerate
specification errors by using separate requirement specifications. In embedded systems,
analytic redundancy also helps when some input sources are likely to be unavailable at
times. For example, avionics programs have multiple ways to compute aircraft altitude,
such as using barometric pressure, the radar altimeter, and geometrically using the straightline
distance and look -down angle of a point ahead on the ground. The voter mechanism
used with analytic redundancy needs to be more sophisticated than just letting majority rule
or computing a simple average. It may have to understand which sensors are currently
reliable or not, and it may be asked to produce a higher-fidelity value than any individual
component can, by blending and smoothing individual values over time.
• Exception detection refers to the detection of a system condition that alters the normal flow of
execution. The exception detection tactic can be further refined:
• System exceptions will vary according to the processor hardware architecture employed and
include faults such as divide by zero, bus and address faults, illegal program instructions,
and so forth.
• The parameter fence tactic incorporates an a priori data pattern (such as OxDEADBEEF)
placed immediately after any variable-length parameters of an object. This allows for
runtime detection of overwriting the memory allocated for the object's variable-length
parameters.
• Parameter typing employs a base class that defines functions that add, find, and iterate over
type-length-value (TLV) formatted message parameters. Derived classes use the base class
functions to implement functions that provide parameter typing according to each
parameter's structure. Use of strong typing to build and parse messages results in higher
availability than implementations that simply treat messages as byte buckets. Of course, all
design involves tradeoffs. When you employ strong typing, you typically trade higher
availability against ease of evolution.
• Timeout is a tactic that raises an exception when a component detects that it or another
component has failed to meet its timing constraints. For example, a component awaiting a
response from another component can raise an exception if the wait time exceeds a certain
value.
• Self-test. Components (or, more likely, whole subsystems) can run procedures to test
themselves for correct operation. Self-test procedures can be initiated by the component itself,
or invoked from time to time by a system monitor. These may involve employing some of the
techniques found in condition monitoring, such as checksums.
Recover from Faults
·
·
·- 0 A a
Recover-from-faults tactics are refined into preparation-and-repair tactics and reintroduction tactics.
The latter are concerned with reintroducing a failed (but rehabilitated) component back into normal
operation.
Preparation-and-repair tactics are based on a variety of combinations of retrying a computation or
introducing redundancy. They include the following:
• Active redundancy (hot spare). This refers to a configuration where all of the nodes (active or
redundant spare) in a protection group2. receive and process identical inputs in parallel,
allowing the redundant spare(s) to maintain synchronous state with the active node(s). Because
the redundant spare possesses an identical state to the active processor, it can take over from a
failed component in a matter of milliseconds. The simple case of one active node and one
redundant spare node is commonly referred to as 1 + 1 ("one plus one") redundancy. Active
redundancy can also be used for facilities protection, where active and standby network links
are used to ensure highly available network connectivity.
2. A protection group is a group of processing nodes where one or more nodes are "active," with the remaining
nodes in the protection group serving as redundant spares.
• Passive redundancy (warm spare). This refers to a configuration where only the active
members of the protection group process input traffic; one of their duties is to provide the
redundant spare( s) with periodic state updates. Because the state maintained by the redundant
spares is only loosely coupled with that of the active node(s) in the protection group (with the
looseness of the coupling being a function of the checkpointing mechanism employed between
active and redundant nodes), the redundant nodes are referred to as warm spares. Depending on
a system's availability requirements, passive redundancy provides a solution that achieves a
balance between the more highly available but more compute-intensive (and expensive) active
redundancy tactic and the less available but significantly less complex cold spare tactic (which
is also significantly cheaper). (For an example of implementing passive redundancy, see the
section on code templates in Chapter 1 9.)
• Spare (cold spare). Cold sparing refers to a configuration where the redundant spares of a
protection group remain out of service until a fail-over occurs, at which point a power-on-reset
procedure is initiated on the redundant spare prior to its being placed in service. Due to its poor
recovery performance, cold sparing is better suited for systems having only high-reliability
(MTBF) requirements as opposed to those also having high-availability requirements.
• Exception handling. Once an exception has been detected, the system must handle it in some
fashion. The easiest thing it can do is simply to crash, but of course that's a terrible idea from
the point of availability, usability, testability, and plain good sense. There are much more
productive possibilities. The mechanism employed for exception handling depends largely on
the programming environment employed, ranging from simple function return codes (error
codes) to the use of exception classes that contain information helpful in fault correlation, such
as the name of the exception thrown, the origin of the exception, and the cause of the exception
thrown. Software can then use this information to mask the fault, usually by correcting the
cause of the exception and retrying the operation.
• Rollback. This tactic permits the system to revert to a previous known good state, referred to as
·
·
·- 0 A a
the "rollback line" rolling back time upon the detection of a failure. Once the good state is
reached, then execution can continue. This tactic is often combined with active or passive
redundancy tactics so that after a rollback has occurred, a standby version of the failed
component is promoted to active status. Rollback depends on a copy of a previous good state (a
checkpoint) being available to the components that are rolling back. Checkpoints can be stored
in a fixed location and updated at regular intervals, or at convenient or significant times in the
processing, such as at the completion of a complex operation.
• Software upgrade is another preparation-and-repair tactic whose goal is to achieve in-service
upgrades to executable code images in a non-service-affecting manner. This may be realized as
a function patch, a class patch, or a hitless in-service software upgrade (ISSU). A function
patch is used in procedural programming and employs an incremental linker/loader to store an
updated software function into a pre-allocated segment of target memory. The new version of
the software function will employ the entry and exit points of the deprecated function. Also,
upon loading the new software function, the symbol table must be updated and the instruction
cache invalidated. The class patch tactic is applicable for targets executing object-oriented
code, where the class definitions include a back-door mechanism that enables the runtime
addition of member data and functions. Hitless in-service software upgrade leverages the active
redundancy or passive redundancy tactics to achieve non-service-affecting upgrades to software
and associated schema. In practice, the function patch and class patch are used to deliver bug
fixes, while the hitless in-service software upgrade is used to deliver new features and
capabilities.
• Retry. The retry tactic assumes that the fault that caused a failure is transient and retrying the
operation may lead to success. This tactic is used in networks and in server farms where
failures are expected and common. There should be a limit on the number of retries that are
attempted before a permanent failure is declared.
• Ignore faulty behavior. This tactic calls for ignoring messages sent from a particular source
when we determine that those messages are spurious. For example, we would like to ignore the
messages of an exten1al component launching a denial-of-service attack by establishing Access
Control List filters, for example.
• The degradation tactic maintains the most critical system functions in the presence of
component failures, dropping less critical functions. This is done in circumstances where
individual component failures gracefully reduce system functionality rather than causing a
complete system failure.
• Reconjiguration attempts to recover from component failures by reassigning responsibilities to
the (potentially restricted) resources left functioning, while maintaining as much functionality
as possible.
Reintroduction is where a failed component is reintroduced after it has been corrected.
Reintroduction tactics include the following:
• The shadow tactic refers to operating a previously failed or in-service upgraded component in a
"shadow mode" for a predefmed duration of time prior to reverting the component back to an
·
·
·- 0 A a
active role. During this duration its behavior can be monitored for correctness and it can
repopulate its state incrementally.
• State resynchronization is a reintroduction partner to the active redundancy and passive
redundancy preparation-and-repair tactics. When used alongside the active redundancy tactic,
the state resynchronization occurs organically, because the active and standby components each
receive and process identical inputs in parallel. In practice, the states of the active and standby
components are periodically compared to ensure synchronization. This comparison may be
based on a cyclic redundancy check calculation (checksum) or, for systems providing safetycritical
services, a message digest calculation (a one-way hash function). When used alongside
the passive redundancy (warm spare) tactic, state resynchronization is based solely on periodic
state information transmitted from the active component(s) to the standby component(s),
typically via checkpointing. A special case of this tactic is found in stateless services, whereby
any resource can handle a request from another (failed) resource.
• Escalating restart is a reintroduction tactic that allows the system to recover from faults by
varying the granularity of the component( s) restarted and minimizing the level of service
affected. For example, consider a system that supports four levels of restart, as follows. The
lowest level of restart (call it Level 0), and hence having the least impact on services, employs
passive redundancy (warm spare), where all child threads of the faulty component are killed
and recreated. In this way, only data associated with the child threads is freed and reinitialized.
The next level of restart (Level 1) frees and reinitializes all unprotected memory (protected
memory would remain untouched). The next level of restart (Level 2) frees and reinitializes all
memory, both protected and unprotected, forcing all applications to reload and reinitialize. And
the final level of restart (Level 3) would involve completely reloading and reinitializing the
executable image and associated data segments. Support for the escalating restart tactic is
particularly useful for the concept of graceful degradation, where a system is able to degrade
the services it provides while maintaining support for mission-critical or safety-critical
applications.
• Non-stop forwarding (NSF) is a concept that originated in router design. In this design
functionality is split into two parts: supervisory, or control plane (which manages connectivity
and routing information), and data plane (which does the actual work of routing packets from
sender to receiver). If a router experiences the failure of an active supervisor, it can continue
forwarding packets along known routes with neighboring routers while the routing protocol
information is recovered and validated. When the control plane is restarted, it implements what
is sometimes called "graceful restart," incrementally rebuilding its routing protocol database
even as the data plane continues to operate.
Prevent Faults
Instead of detecting faults and then trying to recover from them, what if your system could prevent
them from occurring in the first place? Although this sounds like some measure of clairvoyance might
be required, it turns out that in many cases it is possible to do just that..3.
.3.. These tactics deal with runtime means to prevent faults from occmTing. Of course, an excellent way to prevent
·
·
·- 0 A a
faults-at least in the system you're building, if not in systems that your system must interact with-is to produce
high-quality code. This can be done by means of code inspections, pair programming, solid requirements reviews,
and a host of other good engineering practices.
• Removal from service. This tactic refers to temporarily placing a system component in an outof-
service state for the purpose of mitigating potential system failures. One example involves
taking a component of a system out of service and resetting the component in order to scrub
latent faults (such as memory leaks, fragmentation, or soft errors in an unprotected cache)
before the accumulation of faults affects service (resulting in system failure). Another term for
this tactic is software rejuvenation.
• Transactions. Systems targeting high-availability services leverage transactional setnantics to
ensure that asynchronous messages exchanged between distributed components are atomic,
consistent, isolated, and durable. These four properties are called the "ACID properties." The
most common realization of the transactions tactic is "two-phase commit" ( a.k.a. 2PC)
protocol. This tactic prevents race conditions caused by two processes attempting to update the
same data item.
• Predictive model. A predictive model, when combined with a monitor, is employed to monitor
the state of health of a system process to ensure that the system is operating within its nominal
operating parameters, and to take corrective action when conditions are detected that are
predictive of likely future faults. The operational performance metrics monitored are used to
predict the onset of faults; examples include session establishment rate (in an HTTP server),
threshold crossing (monitoring high and low water marks for some constrained, shared
resource), or maintaining statistics for process state (in service, out of service, under
maintenance, idle), message queue length statistics, and so on.
• Exception prevention. This tactic refers to techniques employed for the purpose of preventing
system exceptions from occurring. The use of exception classes, which allows a system to
transparently recover from system exceptions, was discussed previously. Other examples of
exception prevention include abstract data types, such as smart pointers, and the use of
wrappers to prevent faults, such as dangling pointers and semaphore access violations from
occurring. Smart pointers prevent exceptions by doing bounds checking on pointers, and by
ensuring that resources are automatically deallocated when no data refers to it. In this way
resource leaks are avoided.
• Increase competence set. A program's competence set is the set of states in which it is
"competent" to operate. For example, the state when the denominator is zero is outside the
competence set of most divide programs. When a component raises an exception, it is signaling
that it has discovered itself to be outside its competence set; in essence, it doesn't know what to
do and is throwing in the towel. Increasing a component's competence set means designing it to
handle more cases faults as part of its normal operation. For example, a component that
assumes it has access to a shared resource might throw an exception if it discovers that access is
blocked. Another component might simply wait for access, or return immediately with an
indication that it will complete its operation on its own the next time it does have access. In this
example, the second component has a larger competence set than the first.
·
·
·- 0 A a
·
·
·- 0 A a
5.3. A Design Checklist for Availability
Table 5.4 is a checklist to support the design and analysis process for availability.
Table 5.4. Checklist to Support the Design and Analysis Process for Availability
Category
Allocation of
Responsibilities
Coordination Model
Checklist
Determine the system responsibiiJttes that need to be highly
availabte. Within those responsibilit1es, ensure that additional
responsibilities have been allocated to detect an omission,
crash1 incorrect tlmfng, or incorrect response. Additionally.
ensure that there are responsibilities to do the following:
• Log the fault
• Notify appropriate entities (people or systems)
• Disable the source of events causing the fault
• Be temporarily unavailable
• Fix or mask the fault/failure
• Operate in a degraded mode
Determine the system responsibilities that need to be 'highly
availab:te. With respect to those responsib'ilities, do the
following:
• Ensure that coordination mechanisms can detect an
omission, crash, incorrect timing, or incorrect response.
Consider, for example, whether guaranteed delivery is
necessary. Will the coordination work under conditions of
degraded communication?
• Ensure, that ooordination mechanisms enable the logging
of the fault, notification of appropriate entities, disabling of
the source of the events causing the fautt, fixing or masking
the fault, or operating in a degraded mode.
• Ensure that the coordination mode'l supports the replacement
of the artifacts used (processors, communications
channels, persistent storage, and processes). For example,
does replacement of a server allow the system to
continue to operate?
Data Model
·
·
·- 0
• Determine If the coordination will work under conditions
of degraded communication, at startup/shutdown, in repair
mode, or under overloaded operation. For example,
how much lost information can the coordination model
withstand and with what consequences?
Determine which portions of the system need to be highly
avai1ab·�e. Within those portions, determine which data
abstractions, along wit.h their operations or their properties,
could caus.e a fault ot omission, a crash, incorrect tim􀈱ng
behavior, or an incorrect response.
For those data abstractions, operations, and properties,
ensure that they can be disab�led, be temporarily unavailable,
or be fixed or masked in the event of a fault
For example, ensure that write requests are cached if a
server is temporarily unavailable and performed' when the
server is retu rned to service.
Mapping among Determine which artifacts (processors, communication
ArchitecturaJ Elements channels, persistent storage, or processes) may produce
a fault: omission, crash, incorrect timing, or incorrect
response.
Ensure that the mapping (or remapping) of architectural
elements Is flexible enough to perma the recovery from the
fault. This may involve a consideration of the following:
• Which processes on failed processors need to be reassigned
at runtime
• Which processors. data stores, or communication channels
can be activated or reassigned at runtime
• How data on failed processors or storage can be served
by replacement unlts
A a
Resource
Management
·
·
·- 0
• How quickly the system can be reinstal·led based on the
units of delivery provided
• How to (re)assign runume elements to processors, communication
channels, and data stores
• When employing tactics that depend on redund·ancy of
functionality, the mapping from modules to redundant
components is important. For example, it i·s possible to
write one module that contains code appropriate for both
the acUve component and backup components in a pro􀈲
tectton group.
Determine what crUical resources are necessary to
continue operating in the presence of a fault: omission,
crash, incorrect timingr or incorrect response. Ensure
there are sufficient remaining resources in the event of a
fault to log the fault; notify appropriate entities (people or
systems); disable the source of ·events causing the fault;
be temporarily unavailable; fix or mas<k the fault/failure;
operate normally, in startup, shutdown, repair mode,
degraded operation, and overloaded operation ..
Determine the availability time for critfcal resources, what
critical resources must be available during specified time
intervals, time intervals during which the critical resources
may be In a degraded mode, and repair time for critical
resources. Ensure that the critical resources are available
during these time intervals.
For example, ensure that input queues are large enough
to buffer anticipated messages if a server fails so that the
messages are not permanently lost.
Binding Time Determine how and when architectural elements are bound.
If late binding· is used to alternate between components
that can themselves be sources of faul1s (e.g. . , processes,
processors, communication channels), ensure the chosen
availability strategy is sufficient to cover fautts introduced by
all sources. For example:
• a late binding is used to switch between artifacts such
as processors that will receive or be the subject of faults,
will the chosen fault detection and recovery mechanisms
work for all possible bindings?
• ·1.f late binding is used to change the definition or tolerance
of what constitutes a fault {e.g.,. how long a process
can go without responding before a fault Is assumed)r
is the recovery strategy chosen suffident to handle all
cases? For example, if a fault is flagged after 0.1 milliseconds,
but the recovery mechanism takes 1 .5 seconds to
work, that might be an unacceptable mismatch.
• What are the availabil.ity characteristics of the late bindIng
mechanism itself? Can it fal l ?
Choice of Technology Determine the available technologies that can (help) detect
faults, recover from faults, or reintroduce failed components.
Determine what technologies are available that help the
response to a fault (e.g., event loggers).
Determine the avaHabllity characteristics of chosen
technologies themselves: What faults can they recover
from? What faults mi·ght they introduce into the system?
A a
5.4. Summary
·
·
·- 0 A a
Availability refers to the ability of the system to be available for use, especially after a fault occurs. The
fault must be recognized (or prevented) and then the system must respond in some fashion. The
response desired will depend on the criticality of the application and the type of fault and can range
from "ignore it" to "keep on going as if it didn't occur."
Tactics for availability are categorized into detect faults, recover from faults and prevent faults.
Detection tactics depend, essentially, on detecting signs of life from various components. Recovery
tactics are some combination of retrying an operation or maintaining redundant data or computations.
Prevention tactics depend either on removing elements from service or utilizing mechanisms to limit the
scope of faults.
All of the availability tactics involve the coordination model because the coordination model must
be aware of faults that occur to generate an appropriate response.
5.5. For Further Reading
Patterns for availability:
• You can find patterns for fault tolerance in [Hanmer 07].
Tactics for availability, overall:
·
·
·- 0 A a
• A more detailed discussion of some of the availability tactics in this chapter is given in [Scott
Q2].. This is the source of much of the material in this chapter.
• The Internet Engineering Task Force has promulgated a number of standards supporting
availability tactics. These standards include non-stop forwarding [IETF 04], ping/echo ICMPv6
[IETF 06b], echo request/response), and MPLS (LSP Ping) networks [IETF 06a].
Tactics for availability, fault detection:
• The parameter fence tactic was first used (to our knowledge) in the Control Data Series
computers of the late 1960s.
• Triple modular redundancy (TMR), part of the voting tactic, was developed in the early 1 960s
by Lyons [Lyons 62].
• The fault detection tactic of voting is based on the fundamental contributions to automata
theory by Von Neumann, who demonstrated how systems having a prescribed reliability could
be built from unreliable components [Von Neumann 56].
Tactics for availability, fault recovery:
• Standards-based realizations of active redundancy exist for protecting network links (i.e.,
facilities) at both the physical layer [Bellcore 99, Telcordia 00] and the network/link layer
[IETF 05].
• Exception handlinghas been written about by [Powel Douglass 99]. Software can then use this
information to mask the fault, usually by correcting the cause of the exception and retrying the
operation.
• [Morelos-Zaragoza 06] and [Schneier 96] have written about the comparison of state during
resynchronizati on.
• Some examples of how a system can degrade through use (degradation) are given in [Nygard
Qll.
• [Utas 05] has written about escalating restart.
• Mountains of papers have been written about paratneter typing, but [Utas 05] writes about it in
the context of availability (as opposed to bug prevention, its usual context).
• Hardware engineers often use preparation-and-repair tactics. Examples include error detection
and correction (EDAC) coding, forward error correction (FEC), and te1nporal redundancy.
EDAC coding is typically used to protect control memory structures in high-availability
distributed real-time embedded systems [Hamming 80]. Conversely, FEC coding is typically
employed to recover from physical-layer errors occurring on external network links MorelosZaragoza
06]. Temporal redundancy involves sampling spatially redundant clock or data lines
·
·
·- 0 A a
at time intervals that exceed the pulse width of any transient pulse to be tolerated, and then
voting out any defects detected [Mavis 02].
Tactics for availability, fault prevention:
• Pamas and Madey have written about increasing an element's competence set [Pamas 95].
• The ACID properties, important in the transactions tactic, were introduced by Gray in the
1 970s and discussed in depth in [Gray 93].
Analysis:
• Fault tree analysis dates from the early 1960s, but the granddaddy of resources for it is the U.S.
Nuclear Regulatory Commission's "Fault Tree Handbook," published in 1 9 8 1 (Vesely 8 1].
NASA's 2002 "Fault Tree Handbook with Aerospace Applications" (Vesely 02] is an updated
comprehensive primer of the NRC handbook, and the source for the notation used in this
chapter. Both are available online as downloadable PDF files.
5.6. Discussion Questions
·
·
·- 0 A a
1. Write a set of concrete scenarios for availability using each of the possible responses in the
general scenario.
2. Write a concrete availability scenario for the software for a (hypothetical) pilotless passenger
aircraft.
3. Write a concrete availability scenario for a program like Microsoft Word.
4. Redundancy is often cited as a key strategy for achieving high availability. Look at the tactics
presented in this chapter and decide how many of them exploit some form of redundancy and
how many do not.
5. How does availability trade off against modifiability? How would you make a change to a system
that is required to have "24/7'' availability (no scheduled or unscheduled downtime, ever)?
6. Create a fault tree for an automatic teller machine. Include faults dealing with hardware
component failure, communications failure, software failure, running out of supplies, user errors,
and security attacks. How would you modify your automatic teller machine design to
accommodate these faults?
7. Consider the fault detection tactics (ping/echo, heartbeat, system monitor, voting, and exception
detection). What are the performance implications of using these tactics?
·
·
·- 0
6. Interoperability
With Liming Zhu
The early bird (A) arrives and catches worm (B),
pulling string (C) and shooting off pistol (D).
Bullet (E) bursts balloon (F), dropping brick (G)
on bulb (H) of atomizer (I) and shooting perfume
(J) on sponge (K). As sponge gains in weight, it
lowers itself and pulls string (L), raising end of
board (M). Cannon ball (N) drops on nose of
sleeping gentleman. String tied to cannon ball
releases cork (0) of vacuum bottle (P) and ice
water falls on sleeper 's face to assist the cannon
ball in its good work.
-Rube Goldberg, instructions for "a simple alarm
clock"
A a
Interoperability is about the degree to which two or more systems can usefully exchange meaningful
information via interfaces in a particular context. The definition includes not only having the ability to
exchange data (syntactic interoperability) but also having the ability to correctly interpret the data being
exchanged (semantic interoperability). A system cannot be interoperable in isolation. Any discussion of
a system's interoperability needs to identify with whom, with what, and under what circumstanceshence,
the need to include the context.
Interoperability is affected by the systems expected to interoperate. If we already know the
interfaces of external systems with which our system will interoperate, then we can design that
knowledge into the system. Or we can design our system to interoperate in a more generic fashion, so
that the identity and the services that another system provides can be bound later in the life cycle, at
build time or runtime.
Like all quality attributes, interoperability is not a yes-or-no proposition but has shades of meaning.
There are several characterizing frameworks for interoperability, all of which seem to define five levels
of interoperability "maturity" (see the "For Further Reading" section at the end of this chapter for a
pointer). The lowest level signifies systems that do not share data at all, or do not do so with any
success. The highest level signifies systems that work together seamlessly, never make any mistakes
interpreting each other's communications, and share the same underlying semantic model of the world
in which they work.
"Exchanging Information via Interfaces"
Interoperability, as we said, is about two or more systems exchanging information via
interfaces.
·
·
·- 0
At this point, we need to clarify two critical concepts central to this discussion and
emphasize that we are taking a broad view of each.
The first is what it means to "exchange information." This can mean something as
simple as program A calling program B with some parameters. However, two systems
(or parts of a system) can exchange information even if they never communicate
directly with each other. Did you ever have a conversation like the following in junior
high school? "Charlene said that Kim told her that Trevor heard that Heather wants to
come to your party." Of course, junior high school protocol would preclude the
possibility of responding directly to Heather. Instead, your response (if you like
Heather) might be, "Cool," which would make its way back through Charlene, Kim,
and Trevor. You and Heather exchanged information, but never talked to each other.
(We hope you got to talk to each other at the party.)
A a
Entities can exchange information in even less direct ways. If I have an idea of a
progratn's behavior, and I design my program to work assuming that behavior, the two
progratns have also exchanged information just not at runtime.
One of the more infamous software disasters in history occurred when an antimissile
system failed to intercept an incoming ballistic rocket in Operation Desert Storm in
1 99 1 , resulting in 28 fatalities. One of the missile's software components "expected" to
be shut down and restarted periodically, so it could recalibrate its orientation
framework from a known initial point. The software had been running for some 1 00
hours when the missile was launched, and calculation errors had accumulated to the
point where the software component's idea of its orientation had wandered hopelessly
away from truth.
Systems (or components within systems) often have or embody expectations about
the behaviors of its "information exchange" partners. The assumption of everything
interacting with the errant component in the preceding example was that its accuracy
did not degrade over time. The result was a system of parts that did not work together
correctly to solve the problem they were supposed to.
The second concept we need to stress is what we mean by "interface." Once again,
we mean something beyond the simple case a syntactic description of a component's
programs and the type and number of their parameters, most commonly realized as an
API. That's necessary for interoperability heck, it's necessary if you want your
software to compile successfully but it's not sufficient. To illustrate this concept,
we'll use another "conversation" analogy. Has your partner or spouse ever come home,
slammed the door, and when you ask what's wrong, replied "Nothing !"? If so, then you
should be able to appreciate the keen difference between syntax and semantics and the
role of expectations in understanding how an entity behaves. Because we want
interoperable systems and components, and not simply ones that compile together
nicely, we require a higher bar for interfaces than just a statement of syntax. By
"interface," we mean the set of assumptions that you can safely make about an entity.
For example, it's a safe assumption that whatever's wrong with your spouse/partner,
·
·
·- 0 A a
it's not "Nothing," and you know that because that "interface" extends way beyond just
the words they say. And it's also a safe assumption that nothing about our missile
component's accuracy degradation over time was in its API, and yet that was a critical
part of its interface.
-PCC
Here are some of the reasons you might want systems to interoperate:
• Your system provides a service to be used by a collection of unknown systems. These systems
need to interoperate with your system even though you may know nothing about them. An
example is a service such as Google Maps.
• You are constructing capabilities from existing systems. For example, one of the existing
systems is responsible for sensing its environtnent, another one is responsible for processing the
raw data, a third is responsible for interpreting the data, and a final one is responsible for
producing and distributing a representation of what was sensed. An example is a traffic sensing
system where the input comes from individual vehicles, the raw data is processed into common
units of measurement, is interpreted and fused, and traffic congestion information is broadcast.
These examples highlight two important aspects of interoperability:
1. Discovery. The consumer of a service must discover (possibly at runtime, possibly prior to
runtime) the location, identity, and the interface of the service.
2. Handling of the response. There are three distinct possibilities:
• The service reports back to the requester with the response.
• The service sends its response on to another system.
• The service broadcasts its response to any interested parties.
These elements, discovery and disposition of response, along with management of interfaces, govern
our discussion of scenarios and tactics for interoperability.
Systems of Systems
If you have a group of systems that are interoperating to achieve a joint purpose, you
have what is called a system of systems (SoS). An SoS is an arrangement of systems
that results when independent and useful systems are integrated into a larger system
that delivers unique capabilities. Table 6 . 1 shows a categorization of SoSs.
Table 6.1. Taxonomy of Systems of Systems::
Directed
Acknowledged
Collaborative
Virtual
·
·
·- 0
SoS objectlves, centralized management, funding, and
authority for the overall SoS are in place. Systems are
subordinated to the SoS.
SoS objectives, centralized management, funding, and
authority In place. However, systems retain their own
management, funding, and authority in parallel with the
SoS.
There are no overall objectives, centralized
management, authority, responsibility, o·r funding at the
SoS level. Systems voluntarily work tog.ether to address
shared or common interests.
Like collaborative, but systems don't know about each
other .
.: The taxonomy shown is an extension of work done by Mark i\1aier in 1998.
In directed and acknowledged SoSs, there is a deliberate attempt to create an SoS.
A a
The key difference is that in the former, there is 8oS-level management that exercises
control over the constituent systems, while in the latter, the constituent systems retain a
high degree of autonmny in their own evolution. Collaborative and virtual systems of
systems are more ad hoc, absent an overarching authority or source of funding and, in
the case of a virtual SoS, even absent the knowledge about the scope and membership
of the SoS.
The collaborative case is quite common. Consider the Google Maps example from
the introduction. Google is the manager and funding authority for the map service. Each
use of the maps in an application (an SoS) has its own management and funding
authority, and there is no overall management of all of the applications that use Google
Maps. The various organizations involved in the applications collaborate (either
explicitly or implicitly) to enable the applications to work correctly.
A virtual SoS involves large systems and is much more ad hoc. For example, there
are over 3,000 electric companies in the U.S. electric grid, each state has a public utility
commission that oversees the utility companies operating in its state, and the federal
Department of Energy provides some level of policy guidance. Many of the systems
within the electric grid must interoperate, but there is no management authority for the
overall system.
6.1. Interoperability General Scenario
The following are the portions of an interoperability general scenario:
·
·
·- 0
• Source of stimulus. A system initiates a request to interoperate with another system.
• Stimulus. A request to exchange information among system(s).
• Artifacts. The systems that wish to interoperate.
A a
• Environment. The systems that wish to interoperate are discovered at runtime or are known
prior to runtime.
• Response. The request to interoperate results in the exchange of information. The information
is understood by the receiving party both syntactically and semantically. Alternatively, the
request is rejected and appropriate entities are notified. In either case, the request may be
logged.
• Response measure. The percentage of information exchanges correctly processed or the
percentage of information exchanges correctly rejected.
Figure 6 . 1 gives an example: Our vehicle information system sends our current location to the
traffic monitoring system. The traffic monitoring system combines our location with other information,
overlays this information on a Google Map, and broadcasts it. Our location information is correctly
included with a probability of 99.9o/o.
Artifact:
Traffic Monitoring
Stimulus: System
Current
Location
Source Sent
of Stimulus:
Our Vetlrcle
Information
System
Environm,ent:
Systems known
prior to run-time
Response:
Traffic Monitor
Combirles Current
Location with Other
Information,
Overlays on Google
Maps, and
Broadcasts
Response
Measure:
Our Information
Included Correctly
99.9% of the Time
Figure 6.1. Sample concrete interoperability scenario
Table 6.2 presents the possible values for each portion of an interoperability scenario.
Table 6.2. General Interoperability Scenario
·
·
·- 0
Portion of Scenario Possible Values
Source A system initiates a request to interoperate wHh another
system.
S1imulus A request to exc·hange Information among system(s).
Artifact The systems that wish to intemperate.
Environment System (s) wtsf"ling to interoperate are discovered at runtime or
known prior to runtime.
Response One or more of the following:
• The request is (appropriately) rejected and appropriate
entities (people or systems) are notified.
• The request is (app􀈳oprlately) accepted and information Is
exchanged successfully.
• The request is logged by one or more of the invojved
system·s.
Response Measure One or more of the following:
SOAP vs. REST
• Percentage of information exchanges correctly processed
• Percentage of Information exchanges correctly rejected
A a
If you want to allow web-based applications to interoperate, you have two major offthe-
shelf technology options today: (I) WS* and SOAP (which once stood for "Simple
Object Access Protocol," but that acronym is no longer blessed) and (2) REST (which
stands for "Representation State Transfer," and therefore is sometimes spelled ReST).
How can we compare these technologies? What is each good for? What are the road
hazards you need to be aware of? This is a bit of an apples-and-oranges comparison,
but I will try to sketch the landscape.
SOAP is a protocol specification for XML-based information that distributed
applications can use to exchange information and hence interoperate. It is most often
accompanied by a set of SOA middleware interoperability standards and compliant
implementations, referred to (collectively) as WS*. SOAP and WS* together define
many standards, including the following:
• An infrastructure for service composition. SOAP can employ the Business Process
Execution Language (BPEL) as a way to let developers express business processes
that are implemented as WS * services.
• Transactions. There are several web-service standards for ensuring that transactions
are properly managed: WS-AT, WS-BA, WS-CAF, and WS-Transaction.
• Service discovery. The Universal Description, Discovery and Integration (UDDI)
language enables businesses to publish service listings and discover each other.
• Reliability. SOAP, by itself, does not ensure reliable message delivery. Applications
that require such guarantees must use services compliant with SOAP 's reliability
standard: WS-Reliability.
SOAP is quite general and has its roots in a remote procedure call (RPC) model of
interacting applications, although other models are certainly possible. SOAP has a
·
·
·- 0 A a
simple type system, c01nparable to that found in the major programming languages.
SOAP relies on HTTP and RPC for message transmission, but it could, in theory, be
implemented on top of any communication protocol. SOAP does not mandate a
service 's method names, addressing model, or procedural conventions. Thus, choosing
SOAP buys little actual interoperability between applications it is just an information
exchange standard. The interacting applications need to agree on how to interpret the
payload, which is where you get semantic interoperability.
REST, on the other hand, is a client-server-based architectural style that is structured
around a small set of create, read, update, delete (CRUD) operations (called POST,
GET, PUT, DELETE respectively in the REST world) and a single addressing scheme
(based on a URI, or uniform resource identifier). REST imposes few constraints on an
architecture: SOAP offers completeness; REST offers simplicity.
REST is about state and state transfer and views the web (and the services that
service-oriented systems can string together) as a huge network of information that is
accessible by a single URI-based addressing scheme. There is no notion of type and
hence no type checking in REST it is up to the applications to get the semantics of
interaction right.
Because REST interfaces are so simple and general, any HTTP client can talk to any
HTTP server, using the REST operations (POST, GET, PUT, DELETE) with no further
configuration. That buys you syntactic interoperability, but of course there must be
organization-level agreement about what these programs actually do and what
information they exchange. That is, semantic interoperability is not guaranteed between
services just because both have REST interfaces.
REST, on top of HTTP, is meant to be self-descriptive and in the best case is a
stateless protocol. Consider the following example, in REST, of a phone book service
that allows someone to look up a person, given some unique identifier for that person:
http : / / www . XYZdirectory . com/phoneb o o k / U s e r i n f o / 9 9 9 9 9
The same simple lookup, implemented in SOAP, would be specified as something
like the following:
< ? xml ver s i o n= " l . O " ? >
<s oap : Envelope xmln s : s oap=http : / /www . w 3 . o r g / 2 0 0 1 /
1 2 / s oap-envelope
s oap : encodingStyle="http : / /www . w 3 . org / 2 0 0 1 / 1 2 /
soap-encoding " >
< s oap : Body pb="http : / / www . XYZdirecto r y . com/
phonebo o k " >
<pb : Ge t U s e r i n f o >
<pb : U s e r i denti f i e r> 9 9 9 9 9 < /pb : U s e r identi f i e r>
</pb : GetUserinfo>
< / soap : Body>
< / soap : Envelope>
One aspect of the choice between SOAP and REST is whether you want to accept
·
·
·- 0
the complexity and restrictions of SOAP+WSDL (the Web Services Description
Language) to get more standardized interoperability or if you want to avoid the
overhead by using REST, but perhaps benefit from less standardization. What are the
other considerations?
A message exchange in REST has somewhat fewer characters than a message
exchange in SOAP. So one of the tradeoffs in the choice between REST and SOAP is
the size of the individual messages. For systems exchanging a large number of
messages, another tradeoff is between performance (favoring REST) and structured
messages (favoring SOAP).
A a
The decision to implement WS * or REST will depend on aspects such as the quality
of service (QoS) required WS* implementation has greater support for security,
availability, and so on and type of functionality. A RESTful implementation, because
of its simplicity, is more appropriate for read-only functionality, typical of mash ups,
where there are minimal QoS requirements and concerns.
OK, so if you are building a service-based system, how do you choose? The truth is,
you don't have to make a single choice, once and for all time; each technology is
reasonably easy to use, at least for simple applications. And each has its strengths and
weaknesses. Like everything else in architecture, it's all about the tradeoffs; your
decision will likely hinge on the way those tradeoffs affect your system in your context.
-RK
6.2. Tactics for Interoperability
Figure 6.2 shows the goal of the set of interoperability tactics.
-------1.. ...
Information
Exchange
Request
Tactics
to Control
lnteroperabillty
Request
Correctly
Handled
·
·
·-
Figure 6.2. Goal of interoperability tactics
0
We identify two categories of interoperability tactics: locate and manage interfaces.
Locate
A a
There is only one tactic in this category: discover service. It is used when the systems that interoperate
must be discovered at runtime.
• Discover service. Locate a service through searching a known directory service. (By "service,"
we simply mean a set of capabilities that is accessible via some kind of interface.) There may
be multiple levels of indirection in this location process that is, a known location points to
another location that in tum can be searched for the service. The service can be located by type
of service, by name, by location, or by some other attribute.
Manage Interfaces
Managing interfaces consists of two tactics: orchestrate and tailor interface.
• Orchestrate. Orchestrate is a tactic that uses a control mechanism to coordinate and manage
and sequence the invocation of particular services (which could be ignorant of each other).
Orchestration is used when the interoperating systems must interact in a complex fashion to
accomplish a complex task; orchestration "scripts" the interaction. Workflow engines are an
example of the use of the orchestrate tactic. The mediator design pattern can serve this function
for simple orchestration. Complex orchestration can be specified in a language such as BPEL.
• Tailor interface. Tailor interface is a tactic that adds or removes capabilities to an interface.
Capabilities such as translation, adding buffering, or smoothing data can be added. Capabilities
may be removed as well. An example of removing capabilities is to hide particular functions
from untrusted users. The decorator pattern is an example of the tailor interface tactic.
The enterprise service bus that underlies many service-oriented architectures combines both of the
manage interface tactics.
Figure 6.3 shows a summary of the tactics to achieve interoperability.
Information
Exchange
Request
·
·
·-
lnteroperability Tactics
locate
Discover
Service
Manage Interfaces
Orchestrate
Tailor Interface
Figure 6.3. Summary of interoperability tactics
0
Request
Correctly
Handled
Why Standards Are Not Enough to Guarantee Interoperability By Grace
Lewis
A a
Developer of System A needs to exchange product data with System B. Developer A
finds that there is an existing WS * web service interface for sending product data that
among other fields contains price expressed in XML Schema as a decimal with two
fraction digits. Developer A writes code to interact with the web service and the system
works perfectly. However, after two weeks of operation, there is a huge discrepancy
between the totals reported by System A and the totals reported by System B. After
conversations between the two developers, they discover that System B expected to
receive a price that included tax and System A was sending it without tax.
This is a simple example of why standards are not enough. The systems exchanged
data perfectly because they both agreed that the price was a decimal with two fractions
digits expressed in XML Schema and the message was sent via SOAP over HTTP
(syntax) standards used in the implementation ofWS* web services but they did not
agree on whether the price included tax or not (semantics).
Of course, the only realistic approach to getting diverse applications to share
information is by reaching agreetnents on the structure and function of the information
to be shared. These agreements are often reflected in standards that provide a common
interface that multiple vendors and application builders support. Standards have indeed
been instrumental in achieving a significant level of interoperability that we rely on in
almost every domain. However, while standards are useful and in many ways
indispensable, expectations of what can be achieved through standards are unrealistic.
Here are some of the challenges that organizations face related to standards and
interoperability:
1. Ideally, every implementation of a standard should be identical and thus completely
interoperable with any other implementation. However, this is far from reality.
·
·
·- 0
Standards, when incorporated into products, tools, and services, undergo
customizations and extensions because every vendor wants to create a unique
selling point as a competitive advantage.
2. Standards are often deliberately open-ended and provide extension points. The
actual implementation of these extension points is left to the discretion of
implementers, leading to proprietary implementations.
A a
3. Standards, like any technology, have a life cycle of their own and evolve over time
in compatible and noncompatible ways. Deciding when to adopt a new or revised
standard is a critical decision for organizations. Committing to a new standard that
is not ready or eventually not adopted by the community is a big risk for
organizations. On the other hand, waiting too long may also become a problem,
which can lead to unsupported products, incompatibilities, and workarounds,
because everyone else is using the standard.
4. Within the software community, there are as many bad standards as there are
engineers with opinions. Bad standards include underspecified, overspecified,
inconsistently specified, unstable, or irrelevant standards.
5. It is quite common for standards to be championed by competing organizations,
resulting in conflicting standards due to overlap or mutual exclusion.
6. For new and rapidly emerging domains, the argument often made is that
standardization will be destructive because it will hinder flexibility: premature
standardization will force the use of an inadequate approach and lead to abandoning
other presumably better approaches. So what do organizations do in the meantime?
What these challenges illustrate is that because of the way in which standards are
usually created and evolved, we cannot let standards drive our architectures. We need
to architect systems first and then decide which standards can support desired system
requirements and qualities. This approach allows standards to change and evolve
without affecting the overall architecture of the system.
I once heard someone in a keynote address say that "The nice thing about standards
is that there are so many to choose from."
6.3. A Design Checklist for Interoperability
·
·
·- 0
Table 6.3 is a checklist to support the design and analysis process for inter-operability.
A a
Table 6.3. Checklist to Support the Design and Analysis Process for Interoperability
Category
Allocation of
Responsibilities
Coordination Model
Checklist
Determine which of your system responsibilitres will need to
interoperate with other systems.
Ensure that responsibilities have been allocated to detect
a request to interoperate with known or unknown external'
systems.
Ensure that responsibilities have been allocated to carry out the
fol.lowing tasks:
• Accept the request
• Exchange information
• Reject the request
• Notify appropriate entities (people or systems)
• Log the request (for lnteroperablllty In an untrusted environment,
logging for nonrepudiation is essential)
Ensure that th·e coordination mechan.lsms can meet the crit.tcal
quality attribute requirements. Considerations for performance
include the fol!'owing:
• Volume of traffic on the network both created by the systems
under your control and generated by systems not
under your control
• Timeliness of the messages being sent by your systems
• Currency of the messages being sent by your systems
" Jitter of the messages' arrival times
• Ensure that all of the systems under your control make assumptions
about protocols and underlying networks that are
consistent with the systems not under your control.
Data Model
Mapping among
Architectural
Elements
Resource
Management
Bindfng Time
Choi:ce of
Technology
·
·
·- 0
Determine the syntax and semantics of the maj,or data
abstractions that may be exchanged among interoperating
systems.
Ensure that these major data abstractions are consistent with
data from the interoperating systems. (If your system's data
model is confidential and must not be made public, you may
have to apply transformations to and from the data abstractions
of systems with which yours interoperates.)
For interoperability, the critical mapping is that of components
to processors. Beyond the necessity of making sure that
components that communicate externally are hosted
on processors that can reach the network, the primary
considerations deal wi1h meeting the security, availability, and
performance requirements for the communication. These will
be dealt wtth in the:ir respective chapters.
Ensure that interoperation with another system (accepting a
request and/or rejecting a request) can never exhaust crftical
system resources (e.g., can a flood of such requests cause
servlce to be denied to legitimate users?).
Ensure that the resource load imposed by the communication
requirements of interoperation is acceptable.
Ensure that if interoperation requires that resources be shared
among the participating systems, an adequate arbitration policy
is In p􀈴ace.
Determine the systems that may intemperate, and when they
beoome known to each other. For each system over which you
have control:
• Ensure tha1 1t has a policy for dealing with binding to both
known and unknown external systems.
• Ensure that it has mechanisms in place to reject unacceptable
bindings and to log such requests.
• In the case O'f la1e binding, ensure that mechanisms will
support the discovery of relevant new setv􀈵ces or protocols,
or the sending of information using chosen protocols.
For any of your chosen technologies, are they "visfble" at the
interface boundary of a system? If so, what interoperability
effects do they have? Do they support, undercut, or have
no effect on the lnteroperablllty scenarios that appty to your
system? Ensure the effects they have are acc,eptable.
Cons•der technologies that are designed to support
interoperability, such as web services. Can they be used to
satisfy the interoperability requirements for the systems under
your control?
A a
6.4. Summary
·
·
·- 0 A a
Interoperability refers to the ability of systems to usefully exchange information. These systems may
have been constructed with the intention of exchanging information, they may be existing systems that
are desired to exchange information, or they may provide general services without knowing the details
of the systems that wish to utilize those services.
The general scenario for interoperability provides the details of these different cases. In any
interoperability case, the goal is to intentionally exchange information or reject the request to exchange
information.
Achieving interoperability involves the relevant systems locating each other and then managing the
interfaces so that they can exchange information.
6.5. For Further Reading
·
·
·- 0 A a
An SEI report gives a good overview of interoperability, and it highlights some of the "maturity
frameworks" for interoperability [Brownsword 04].
The various WS* services are being developed under the auspices of the World Wide Web
Consortium (W3C) and can be found at www.w3.org/2002/ws.
Systems of systems are of particular interest to the U.S. Department of Defense. An engineering
guide can be found at [ODUSD 08].
6.6. Discussion Questions
·
·
·- 0 A a
1. Find a web service mashup. Write several concrete interoperability scenarios for this system.
2. What is the relationship between interoperability and the other quality attributes highlighted in
this book? For example, if two systems fail to exchange information properly, could a security
flaw result? What other quality attributes seem strongly related (at least potentially) to
interoperability?
3. Is a service-oriented system a system of systems? If so, describe a service-oriented system that is
directed, one that is acknowledged, one that is collaborative, and one that is virtual.
4. Universal Description, Discovery, and Integration (UDDI) was touted as a discovery service, but
commercial support for UDDI is being withdrawn. Why do you suppose this is? Does it have
anything to do with the quality attributes delivered or not delivered by UDDI solutions?
5. Why has the importance of orchestration grown in recent years?
6. If you are a technology producer, what are the advantages and disadvantages of adhering to
interoperability standards? Why would a producer not adhere to a standard?
7. With what other systems will an automatic teller machine need to interoperate? How would you
change your automatic teller system design to accommodate these other systems?
7. Modifiability
Change happens.
·
·
·-
Adapt or perish, now as ever, is nature 's
inexorable imperative.
-H.G. Wells
0 A a
Study after study shows that most of the cost of the typical software system occurs after it has been
initially released. If change is the only constant in the universe, then software change is not only
constant but ubiquitous. Changes happen to add new features, to change or even retire old ones.
Changes happen to fix defects, tighten security, or improve performance. Changes happen to enhance
the user's experience. Changes happen to embrace new technology, new platforms, new protocols, new
standards. Changes happen to make systems work together, even if they were never designed to do so.
Modifiability is about change, and our interest in it centers on the cost and risk of making changes.
To plan for modifiability, an architect has to consider four questions:
• What can change? A change can occur to any aspect of a system: the functions that the system
computes, the platform (the hardware, operating system, middleware), the environment in
which the system operates (the systems with which it must interoperate, the protocols it uses to
communicate with the rest of the world), the qualities the system exhibits (its performance, its
reliability, and even its future modifications), and its capacity (number of users supported,
number of simultaneous operations).
• What is the likelihood of the change? One cannot plan a system for all potential changes the
system would never be done, or if it was done it would be far too expensive and would likely
suffer quality attribute problems in other dimensions. Although anything might change, the
architect has to make the tough decisions about which changes are likely, and hence which
changes are to be supported, and which are not.
• When is the change made and who makes it? Most commonly in the past, a change was made
to source code. That is, a developer had to make the change, which was tested and then
deployed in a new release. Now, however, the question of when a change is made is intertwined
with the question of who makes it. An end user changing the screen saver is clearly making a
change to one of the aspects of the system. Equally clear, it is not in the same category as
changing the system so that it can be used over the web rather than on a single machine.
Changes can be made to the implementation (by modifying the source code), during compile
(using compile-time switches), during build (by choice of libraries), during configuration setup
(by a range of techniques, including parameter setting), or during execution (by parameter
settings, plugins, etc.). A change can also be made by a developer, an end user, or a system
administrator.
• What is the cost of the change? Making a system more modifiable involves two types of cost:
• The cost of introducing the mechanism(s) to make the system more modifiable
·
·
·-
• The cost of making the modification using the mechanism( s)
0 A a
For example, the simplest mechanism for making a change is to wait for a change request to come
in, then change the source code to accommodate the request. The cost of introducing the mechanism is
zero; the cost of exercising it is the cost of changing the source code and revalidating the system. At the
other end of the spectrum is an application generator, such as a user interface builder. The builder takes
as input a description of the designer user interface produced through direct manipulation techniques
and produces (usually) source code. The cost of introducing the mechanism is the cost of constructing
the UI builder, which can be substantial. The cost of using the mechanism is the cost of producing the
input to feed the builder (cost can be substantial or negligible), the cost of running the builder
(approximately zero), and then the cost of whatever testing is performed on the result (usually much
less than usual).
For N similar modifications, a simplified justification for a change mechanism is that
N x Cost of making the change without the mechanism < Cost of installing the mechanism + (N x Cost
of making the change using the mechanism).
N is the anticipated number of modifications that will use the modifiability mechanism, but N is a
prediction. If fewer changes than expected come in, then an expensive modification mechanism may
not be warranted. In addition, the cost of creating the modifiability mechanism could be applied
elsewhere in adding functionality, in improving the performance, or even in nonsoftware investments
such as buying tech stocks. Also, the equation does not take time into account. It might be cheaper in
the long run to build a sophisticated change-handling mechanism, but you might not be able to wait for
that.
7.1. Modifiability General Scenario
·
·
·- 0
From these considerations, we can see the portions of the modifiability general scenario:
A a
• Source of stimulus. This portion specifies who makes the change: the developer, a system
administrator, or an end user.
• Stimulus. This portion specifies the change to be made. A change can be the addition of a
function, the modification of an existing function, or the deletion of a function. (For this
categorization, we regard fixing a defect as changing a function, which presumably wasn't
working correctly as a result of the defect.) A change can also be made to the qualities of the
system: making it more responsive, increasing its availability, and so forth. The capacity of the
system may also change. Accommodating an increasing number of simultaneous users is a
frequent requirement. Finally, changes may happen to accommodate new technology of some
sort, the most common of which is porting the system to a different type of computer or
communication network.
• Artifact. This portion specifies what is to be changed: specific components or modules, the
system's platform, its user interface, its environtnent, or another system with which it
interoperates.
• Environment. This portion specifies when the change can be made: design time, compile time,
build time, initiation time, or runtime.
• Response. Make the change, test it, and deploy it.
• Response measure. All of the possible responses take time and cost money; time and money are
the most common response measures. Although both sound simple to measure, they aren't. You
can measure calendar time or staff time. But do you measure the time it takes for the change to
wind its way through configuration control boards and approval authorities (some of whom
may be outside your organization), or merely the time it takes your engineers to make the
change? Cost usually means direct outlay, but it might also include opportunity cost of having
your staff work on changes instead of other tasks. Other measures include the extent of the
change (number of modules or other artifacts affected) or the number of new defects introduced
by the change, or the effect on other quality attributes. If the change is being made by a user,
you may wish to measure the efficacy of the change mechanisms provided, which somewhat
overlaps with measures of usability (see Chapter 1 1).
Figure 7. 1 illustrates a concrete modifiability scenario: The developer wishes to change the user
interface by modifying the code at design time. The modifications are made with no side effects within
three hours.
Source:
Developer
Stimulus:
Wishe,s
to Change
the Ul
Artifact:
Code
Environm:ent:
Design
Time
·
·
·-
Response:
Change Made
and Unit Tested
0
..
Response
Measure: In Three
Hours
Figure 7 . 1 . Sample concrete modifiability scenario
Table 7 . 1 enumerates the elements of the general scenario that characterize modifiability.
Table 7.1. Modifiability General Scenario
Portion of Scenario Possible Values
Source
Stimulus
Artifacts
End user, deve.toper, system administrator
A directive to add/delete/modify functionality, or change a
quality attribute, capac􀈶ty, or technology
Code, data, interfaces, components, resources. configurations,
• • •
Environment Runtime, compile time, build time, initiation time, design tlme
Response One or more of the following:
• Make modification
• Test modification
• Deploy modification
Response Measure Cost in terms of the followJng:
• Number, size, complexity of affected artifacts
• Effort
• Calendar time
• Money (direct outlay or opportunity cost)
• Extent to which this modification affects other functions or
quality attributes
• New defects introduced
A a
7 .2. Tactics for Modifiability
·
·
·- 0 A a
Tactics to control modifiability have as their goal controlling the complexity of making changes, as well
as the time and cost to make changes. Figure 7.2 shows this relationship.
Chang.e
Arrives
Tactics
to Control
Modifiability Change Made within
Time and Budget
Figure 7 .2. The goal of modifiability tactics
To understand modifiability, we begin with coupling and cohesion.
Modules have responsibilities. When a change causes a module to be modified, its responsibilities
are changed in some way. Generally, a change that affects one module is easier and less expensive than
if it changes more than one module. However, if two modules' responsibilities overlap in some way,
then a single change may well affect them both. We can measure this overlap by measuring the
probability that a modification to one module will propagate to the other. This is called coupling, and
high coupling is an enemy of modifiability.
Cohesion measures how strongly the responsibilities of a module are related. Informally, it
measures the module's "unity of purpose." Unity of purpose can be measured by the change scenarios
that affect a module. The cohesion of a module is the probability that a change scenario that affects a
responsibility will also affect other (different) responsibilities. The higher the cohesion, the lower the
probability that a given change will affect multiple responsibilities. High cohesion is good; low
cohesion is bad. The definition allows for two modules with similar purposes each to be cohesive.
Given this framework, we can now identify the parameters that we will use to motivate
modifiability tactics:
• Size of a module. Tactics that split modules will reduce the cost of making a modification to the
module that is being split as long as the split is chosen to reflect the type of change that is likely
to be made.
• Coupling. Reducing the strength of the coupling between two modules A and B will decrease
the expected cost of any modification that affects A. Tactics that reduce coupling are those that
place intermediaries of various sorts between modules A and B.
• Cohesion. If module A has a low cohesion, then cohesion can be improved by removing
responsibilities unaffected by anticipated changes.
Finally we need to be concerned with when in the software development life cycle a change occurs.
If we ignore the cost of preparing the architecture for the modification, we prefer that a change is bound
as late as possible. Changes can only be successfully made (that is, quickly and at lowest cost) late in
·
·
·- 0 A a
the life cycle if the architecture is suitably prepared to accommodate them. Thus the fourth and final
parameter in a model of modifiability is this:
• Binding time of modification. An architecture that is suitably equipped to accommodate
modifications late in the life cycle will, on average, cost less than an architecture that forces the
same modification to be made earlier. The preparedness of the system means that some costs
will be zero, or very low, for late life-cycle modifications. This, however, neglects the cost of
preparing the architecture for the late binding.
Now we may understand tactics and their consequences as affecting one or more of the previous
parameters: reducing the size of a module, increasing cohesion, reducing coupling, and deferring
binding time. These tactics are shown in Figure 7.3.
Change
Arrives
Modifiability Tactics
Reduce Size rncrease:
of a Module Coh sion
I '
Split Moduf.e
Increase
semantic
Coherence
Reduce
Coupling
Encapsulate
Use an
Intermediary
Restrict
D·ependencies
Ref actor
Abstract Common
Services
Defer
Binding
Figure 7 .3. Modifiability tactics
Reduce the Size of a Module
Change Made
wlthin Time
and Budget
• Split module. If the module being modified includes a great deal of capability, the modification
costs will likely be high. Refining the module into several smaller modules should reduce the
average cost of future changes.
Increase Cohesion
Several tactics involve moving responsibilities from one module to another. The purpose of moving a
responsibility from one module to another is to reduce the likelihood of side effects affecting other
responsibilities in the original module.
• Increase semantic coherence. If the responsibilities A and B in a module do not serve the same
purpose, they should be placed in different modules. This may involve creating a new module
or it may involve moving a responsibility to an existing module. One method for identifying
responsibilities to be moved is to hypothesize likely changes that affect a module. If some
·
·
·- 0 A a
responsibilities are not affected by these changes, then those responsibilities should probably be
removed.
Reduce Coupling
We now turn to tactics that reduce the coupling between modules.
• Encapsulate. Encapsulation introduces an explicit interface to a module. This interface includes
an application programming interface (API) and its associated responsibilities, such as
"perform a syntactic transformation on an input parameter to an internal representation."
Perhaps the most common modifiability tactic, encapsulation reduces the probability that a
change to one module propagates to other modules. The strengths of coupling that previously
went to the module now go to the interface for the module. These strengths are, however,
reduced because the interface limits the ways in which external responsibilities can interact
with the module (perhaps through a wrapper). The external responsibilities can now only
directly interact with the module through the exposed interface (indirect interactions, however,
such as dependence on quality of service, will likely remain unchanged). Interfaces designed to
increase modifiability should be abstract with respect to the details of the module that are likely
to change that is, they should hide those details.
• Use an intermediary breaks a dependency. Given a dependency between responsibility A and
responsibility B (for example, carrying out A first requires carrying out B), the dependency can
be broken by using an intermediary. The type of intermediary depends on the type of
dependency. For example, a publish-subscribe intermediary will remove the data producer' s
knowledge of its consumers. So will a shared data repository, which separates readers of a
piece of data from writers of that data. In a service-oriented architecture in which services
discover each other by dynamic lookup, the directory service is an intermediary.
• Restrict dependencies is a tactic that restricts the modules that a given module interacts with or
depends on. In practice this tactic is achieved by restricting a module's visibility (when
developers cannot see an interface, they cannot employ it) and by authorization (restricting
access to only authorized modules). This tactic is seen in layered architectures, in which a layer
is only allowed to use lower layers (sometimes only the next lower layer) and in the use of
wrappers, where external entities can only see (and hence depend on) the wrapper and not the
internal functionality that it wraps.
• Refactor is a tactic undertaken when two modules are affected by the same change because
they are (at least partial) duplicates of each other. Code refactoring is a mainstay practice of
Agile development projects, as a cleanup step to make sure that teams have not produced
duplicative or overly complex code; however, the concept applies to architectural elements as
well. Common responsibilities (and the code that implements them) are "factored out" of the
modules where they exist and assigned an appropriate home of their own. By co-locating
common responsibilities that is, making them submodules of the same parent module the
architect can reduce coupling.
• Abstract common services. In the case where two modules provide not-quite-the-same but
similar services, it may be cost-effective to implement the services just once in a more general
·
·
·- 0 A a
(abstract) form. Any modification to the (common) service would then need to occur just in one
place, reducing modification costs. A common way to introduce an abstraction is by
parameterizing the description (and implementation) of a module's activities. The parameters
can be as simple as values for key variables or as cotnplex as statements in a specialized
language that are subsequently interpreted.
Defer Binding
Because the work of people is almost always more expensive than the work of computers, letting
computers handle a change as much as possible will almost always reduce the cost of making that
change. If we design artifacts with built-in flexibility, then exercising that flexibility is usually cheaper
than hand-coding a specific change.
Parameters are perhaps the best-known mechanism for introducing flexibility, and that is
reminiscent of the abstract common services tactic. A parameterized functionj{a, b) is more general
than the similar functionf(a) that assumes b = 0. When we bind the value of some parameters at a
different phase in the life cycle than the one in which we defined the parameters, we are applying the
defer binding tactic.
In general, the later in the life cycle we can bind values, the better. However, putting the
mechanisms in place to facilitate that late binding tends to be more expensive yet another tradeoff.
And so the equation on page 1 1 8 comes into play. We want to bind as late as possible, as long as the
mechanism that allows it is cost-effective.
Tactics to bind values at compile time or build time include these:
• Component replacement (for example, in a build script or make file)
• Compile-time parameterization
• Aspects
Tactics to bind values at deployment time include this:
• Configuration-time binding
Tactics to bind values at startup or initialization time include this:
• Resource files
Tactics to bind values at runtime include these:
• Runtime registration
• Dynamic lookup (e. g., for services)
• Interpret parameters
• Startup time binding
• N arne servers
• Plug-ins
• Publish-subscribe
• Shared repositories
• Polymorphism
·
·
·- 0 A a
Separating building a mechanism for modifiability from using the mechanism to make a
modification admits the possibility of different stakeholders being involved one stakeholder (usually a
developer) to provide the mechanism and another stakeholder (an installer, for example, or a user) to
exercise it later, possibly in a completely different life-cycle phase. Installing a mechanism so that
someone else can make a change to the system without having to change any code is smnetimes called
externalizing the change.
7 .3. A Design Checklist for Modifiability
·
·
·- 0
Table 7.2 is a checklist to support the design and analysis process for modifiability.
A a
Table 7 .2. Checklist to Support the Design and Analysis Process for Modifiability
Category
Allocation of
Responsib.i lities
Coordination
Model
Data Model
Checklist
Determine which changes or categories of changes are likely to
occur through consideration of changes in technical, legal, social,
business, and customer forces. For each potential change or
category of changes:
• Determine the responsibilities that would need to be added,
modifted, or deleted to mak.e the change.
• Determine what responsibilities are impacted by tile change.
• Determine an allocation of responsibilities to modul·es that
places, as much as possible, responsibilities that will be
changed (or Impacted by the change) together in the same
module, and places responsibilities that will be changed at
different times in separate modules.
Determine which functionality or quality attribute can change at
runtime and how this affects coo.rdination; for example, will the
information being communicated change at runtime, or will the
communication protocol change at runtime? If so, ensure that such
chang.es affect a small number set of modules.
Determine which devices, protocols, and communication paths
used for coordination are likely to change. For those devices,
protocolsf and communication paths, ensure that the impact of
changes will be limited to a small set of modules.
For those elements for which modifiability is a concern, use
a coordination model that reduces coupling such as publishsubscribe,
defers bindings such as enterprise service bus, or
restricts dependencies such as broadcast.
Determine which changes (or categories of changes) to the data
abstractions, their operations, or their properties are likely to
occur. Also determine which changes or categories of changes
to these data abstractions will involve their creation, initialization,
persistence, manipulatjon, translation, or destruction.
Mapping among
Architectural
Elements
·
·
·- 0
For each change or category of change, determine if the
changes will be made by an end user, a system administrator, or
a developer. For those changes to be made by an end user m
system administrator, ensure that the necessary attributes are
visible to that user and that the user has the correct priviteges to
modify the data, 'l1s operations, or its properties.
For each potential change or category of change:
• Determine which data abstractions would need to be added,
modified, or deleted to make the change.
• Determine whether there would be any ohanges to the
creation, initialization, persistence, manipulation, translation, or
destruction of these data abstractions.
• Determine which other data abstractions are impacted
by the change. For these additional data abstractions,
determine whether the impact would be· on the operations,
their properties, their creation, fnitialization, persistence,
manipulation, translation, or destruction.
• Ensure an allocation of data abstractions that minimizes 􀇨he
number and severity of modifications to the abstractions by the
potential changes.
Design your data model so that items allocated to each element of
the data model are likely to change together.
Determine if it is desirable to change the way in which functionalrty
is mapped to computational elements (e.g., processes, threads,
processors) at runtime, compile time, design time, or build time.
Determine the extent of modifications necessary to accommodate
the addition, deletion, or modification of a function or a quality
attribute. This might involve a de1ermination of the following, for
example:
" Execution dependencies
• Assignment of data to databases
" Assignment of runtime elements to processes, threads, or
processors
A a
Resource
Manag·ement
Binding Time
Choice of
Technology
·
·
·- 0
Ensure that such changes are performed with mechanlsms that
uttlize d.eferred blndtng of mapping decisions.
Determine how the additi:on, deletion, or modification of a
responsibility or quality attribute will affect resource usage. This
involves, for example:
• Determining what changes m-ight tntroduce new resources or
remove old ones or affect existing resource usage
• Determining what resource limits will change and how
Ensure that the resources after the modification are sufficient to
meet the system requirements.
Encapsulate all resource managers and ensure that the policies
implemented by those resource managers are themselves
encapsulated and bindings are deferred to the extent pos.sible.
For each change or category of change:
• Determine the latest time at which the chang.e will need to be
made.
• Choose a defer-binding mechanism (see Section 7.2) that
delivers the appropriate capability at the time chosen.
• Determine the cost of introducing the mechanism and the cost
of making changes using the chosen mechanism. Use the
equation on page 1 1 8 to assess your choice of mechanism.
• Do not introduce so many binding choices that change is
impeded because the dependencies among the choices are
complex and unknown.
Determine what modifications are made easier or harder by your
technology choices.
• Wfll your technology choices help to make, test, and deploy
modifications?
• How easy is it to modify your choice of technot·ogies (In case
some of these technologies change or become obsolete)?
Choose your technologies to support the most likely modif1catlons.
For example, an -enterprise service bus makes it easier to change
how elements are connected but may introduce vendor lock-fn.
A a
7.4. Summary
·
·
·- 0 A a
Modifiability deals with change and the cost in time or money of making a change, including the extent
to which this modification affects other functions or quality attributes.
Changes can be made by developers, installers, or end users, and these changes need to be prepared
for. There is a cost of preparing for change as well as a cost of making a change. The modifiability
tactics are designed to prepare for subsequent changes.
Tactics to reduce the cost of making a change include making modules smaller, increasing cohesion,
and reducing coupling. Deferring binding will also reduce the cost of making a change.
Reducing coupling is a standard category of tactics that includes encapsulating, using an
intermediary, restricting dependencies, co-locating related responsibilities, refactoring, and abstracting
• common services.
Increasing cohesion is another standard tactic that involves separating responsibilities that do not
serve the same purpose.
Defer binding is a category of tactics that affect build time, load time, initialization time, or runtime.
7.5. For Further Reading
·
·
·- 0 A a
Serious students of software engineering should read two early papers about designing for
modifiability. The first is Edsger Dijkstra's 1 968 paper about the T.H.E. operating system [Dijkstra 68],
which is the first paper that talks about designing systems to be layered, and the modifiability benefits it
brings. The second is David Pamas' s 1 972 paper that introduced the concept of infonnation hiding
[Parnas 72]. Parnas prescribed defining modules not by their functionality but by their ability to
internalize the effects of changes.
The tactics that we have presented in this chapter are a variant on those introduced by [Bachmann
Ql}.
Additional tactics for modifiability within the avionics domain can be found in [EOSAN 07],
published by the European Organization for the Safety of Air Navigation.
7 .6. Discussion Questions
·
·
·- 0 A a
1. Modifiability comes in many flavors and is known by many names. Find one of the IEEE or ISO
standards dealing with quality attributes and compile a list of quality attributes that refer to some
form of modifiability. Discuss the differences.
2. For each quality attribute that you discovered as a result of the previous question, write a
modifiability scenario that expresses it.
3. In a certain metropolitan subway system, the ticket machines accept cash but do not give change.
There is a separate machine that dispenses change but does not sell tickets. In an average station
there are six or eight ticket machines for every change machine. What modifiability tactics do
you see at work in this arrangement? What can you say about availability?
4. For the subway system in the previous question, describe the specific form of modifiability (using
a modifiability scenario) that seems to be the aim of arranging the ticket and change machines as
described.
5. A wrapper is a common aid to modifiability. A wrapper for a component is the only element
allowed to use that component; every other piece of software uses the component's services by
going through the wrapper. The wrapper transforms the data or control information for the
component it wraps. For example, a component may expect input using English measures but
find itself in a system in which all of the other components produce metric measures. A wrapper
could be employed to translate. What modifiability tactics does a wrapper embody?
6. Once an intermediary has been introduced into an architecture, some modules may attempt to
circumvent it, either inadvertently (because they are not aware of the intermediary) or
intentionally (for performance, for convenience, or out of habit). Discuss some architectural
means to prevent inadvertent circumvention of an intermediary.
7. In some projects, deployability is an important quality attribute that measures how easy it is to get
a new version of the system into the hands of its users. This might mean a trip to your auto dealer
or transmitting updates over the Internet. It also includes the time it takes to install the update
once it arrives. In projects that measure deployability separately, should the cost of a modification
stop when the new version is ready to ship? Justify your answer.
8. The abstract common services tactic is intended to reduce coupling, but it also might reduce
cohesion. Discuss.
9. Identify particular change scenarios for an automatic teller machine. What modifications would
you make to your automatic teller machine design to accommodate these changes?
8. Performance
It's about titne.
·
·
·-
An ounce of performance is worth pounds of
• promzses.
-Mae West
0 A a
Performance, that is: It's about time and the software system's ability to meet timing requirements.
When events occur interrupts, messages, requests from users or other systems, or clock events
marking the passage of time the system, or some element of the system, must respond to them in time.
Characterizing the events that can occur (and when they can occur) and the system or element's timebased
response to those events is the essence is discussing performance.
Web-based system events come in the form of requests from users (numbering in the tens or tens of
millions) via their clients such as web browsers. In a control system for an internal combustion engine,
events come from the operator's controls and the passage of time; the system must control both the
firing of the ignition when a cylinder is in the correct position and the mixture of the fuel to maximize
power and efficiency and minimize pollution.
For a web-based system, the desired response might be expressed as number of transactions that can
be processed in a minute. For the engine control system, the response might be the allowable variation
in the firing time. In each case, the pattern of events arriving and the pattern of responses can be
characterized, and this characterization forms the language with which to construct performance
• scenanos.
For much of the history of software engineering, performance has been the driving factor in system
architecture. As such, it has frequently compromised the achievement of all other qualities. As the
price/performance ratio of hardware continues to plummet and the cost of developing software
continues to rise, other qualities have emerged as important competitors to performance.
Nevertheless, all systems have performance requirements, even if they are not expressed. For
example, a word processing tool may not have any explicit performance requirement, but no doubt
everyone would agree that waiting an hour (or a minute, or a second) before seeing a typed character
appear on the screen is unacceptable. Performance continues to be a fundamentally important quality
attribute for all software.
Performance is often linked to scalability that is, increasing your system's capacity for work,
while still performing well. Technically, scalability is making your system easy to change in a particular
way, and so is a kind of modifiability. In addition, we address scalability explicitly in Chapter 12.
8.1. Performance General Scenario
·
·
·- 0 A a
A performance scenario begins with an event arriving at the system. Responding correctly to the event
requires resources (including time) to be consumed. While this is happening, the system may be
simultaneously servicing other events.
Concurrency
Concurrency is one of the more important concepts that an architect must understand
and one of the least-taught in computer science courses. Concurrency refers to
operations occurring in parallel. For example, suppose there is a thread that executes
the statements
X : = 1 ;
x + + ;
and another thread that executes the same statements. What is the value of x after both
threads have executed those statements? It could be either 2 or 3 . I leave it to you to
figure out how the value 3 could occur or should I say I interleave it to you?
Concurrency occurs any time your system creates a new thread, because threads, by
definition, are independent sequences of control. Multi-tasking on your system is
supported by independent threads. Multiple users are simultaneously supported on your
system through the use of threads. Concurrency also occurs any time your system is
executing on more than one processor, whether the processors are packaged separately
or as multi-core processors. In addition, you must consider concurrency when parallel
algorithms, parallelizing infrastructures such as map-reduce, or NoSQL databases are
used by your system, or you utilize one of a variety of concurrent scheduling
algorithms. In other words, concurrency is a tool available to you in many ways.
Concurrency, when you have multiple CPUs or wait states that can exploit it, is a
good thing. Allowing operations to occur in parallel improves performance, because
delays introduced in one thread allow the processor to progress on another thread. But
because of the interleaving phenomenon just described (referred to as a race condition),
concurrency must also be carefully managed by the architect.
As the example shows, race conditions can occur when there are two threads of
control and there is shared state. The management of concurrency frequently comes
down to managing how state is shared. One technique for preventing race conditions is
to use locks to enforce sequential access to state. Another technique is to partition the
state based on the thread executing a portion of code. That is, if there are two instances
of x in our example, x is not shared by the two threads and there will not be a race
condition.
Race conditions are one of the hardest types of bugs to discover; the occurrence of
the bug is sporadic and depends on (possibly minute) differences in timing. I once had a
race condition in an operating system that I could not track down. I put a test in the
·
·
·- 0 A a
code so that the next time the race condition occurred, a debugging process was
triggered. It took over a year for the bug to recur so that the cause could be determined.
Do not let the difficulties associated with concurrency dissuade you from utilizing
this very important technique. Just use it with the knowledge that you must carefully
identify critical sections in your code and ensure that race conditions will not occur in
those sections.
-LB
Events can arrive in predictable patterns or mathematical distributions, or be unpredictable. An
arrival pattern for events is characterized as periodic, stochastic, or sporadic:
• Periodic events arrive predictably at regular time intervals. For instance, an event may arrive
every 1 0 milliseconds. Periodic event arrival is most often seen in real-time systems.
• Stochastic arrival means that events arrive according to some probabilistic distribution.
• Sporadic events arrive according to a pattern that is neither periodic nor stochastic. Even these
can be characterized, however, in certain circumstances. For example, we might know that at
most 600 events will occur in a minute, or that there will be at least 200 milliseconds between
the arrival of any two events. (This might describe a system in which events correspond to
keyboard strokes from a human user.) These are helpful characterizations, even though we
don't know when any single event will arrive.
The response of the system to a stimulus can be measured by the following:
• Latency. The time between the arrival of the stimulus and the system's response to it.
• Deadlines in processing. In the engine controller, for example, the fuel should ignite when the
cylinder is in a particular position, thus introducing a processing deadline.
• The throughput of the system, usually given as the number of transactions the system can
process in a unit of time.
• The jitter of the response the allowable variation in latency.
• The number of events not processed because the system was too busy to respond.
From these considerations we can now describe the individual portions of a general scenario for
performance:
• Source of stimulus. The stimuli arrive either from external (possibly multiple) or internal
sources.
• Stimulus. The stimuli are the event arrivals. The arrival pattern can be periodic, stochastic, or
sporadic, characterized by numeric parameters.
• Artifact. The artifact is the system or one or more of its components.
• Environment. The system can be in various operational modes, such as normal, emergency,
peak load, or overload.
• Response. The system must process the arriving events. This may cause a change in the system
environtnent (e.g., from normal to overload mode).
·
·
·- 0 A a
• Response measure. The response measures are the time it takes to process the arriving events
(latency or a deadline), the variation in this time (jitter), the number of events that can be
processed within a particular time interval (throughput), or a characterization of the events that
cannot be processed (miss rate).
The general scenario for performance is summarized in Table 8. 1 .
Table 8.1. Performance General Scenario
Portion of ScenarJo Possible Values
Source Internal or external to the system
Stimulus Arrival of a periodic, sporadic, or stochastic event
Artifact
Environment
Response
Response Measure
System or one or more components in the system
Operational mode: normaJ, emergency, peak load, overload
Process events, change level of service
Latency, deadline. throughput, jitter, miss rate
Figure 8 . 1 gives an example concrete performance scenario: Users initiate transactions under
normal operations. The system processes the transactions with an average latency of two seconds.
Source:
Users
Stimulus:
Initiate
Transactions
Artifact:
System
Environment:
Normal.
Operation
Response:
Transactions
Are Processed
Response
Measure:
Average
Latency
of Two
Seconds
Figure 8.1. Sample concrete performance scenario
8.2. Tactics for Performance
·
·
·- 0 A a
The goal of performance tactics is to generate a response to an event arriving at the system within some
time-based constraint. The event can be single or a stream and is the trigger to perform computation.
Performance tactics control the time within which a response is generated, as illustrated in Figure 8.2.
Event
Arrives
Tactics
to Control
Performance Response
Generated
within Time
Constraints
Figure 8.2. The goal of performance tactics
At any instant during the period after an event arrives but before the system's response f o it is
complete, either the system is working to respond to that event or the processing is blocked for some
reason. This leads to the two basic contributors to the response time: processing time (when the system
is working to respond) and blocked time (when the system is unable to respond).
• Processing time. Processing consumes resources, which takes titne. Events are handled by the
execution of one or more components, whose time expended is a resource. Hardware resources
include CPU, data stores, network communication bandwidth, and tnemory. Software resources
include entities defined by the systetn under design. For example, buffers must be managed and
access to critical sectionsl must be made sequential.
1. A critical section is a section of code in a multi-threaded system in which at most one thread may be active at any
time.
For example, suppose a message is generated by one component. It might be placed on the
network, after which it arrives at another component. It is then placed in a buffer; transformed
in some fashion; processed according to some algorithm; transformed for output; placed in an
output buffer; and sent onward to another component, another system, or some actor. Each of
these steps consumes resources and time and contributes to the overall latency of the processing
of that event.
Different resources behave differently as their utilization approaches their capacity that is,
as they become saturated. For example, as a CPU becomes more heavily loaded, performance
usually degrades fairly steadily. On the other hand, when you start to run out of memory, at
some point the page swapping becomes overwhelming and performance crashes suddenly.
• Blocked time. A computation can be blocked because of contention for some needed resource,
because the resource is unavailable, or because the computation depends on the result of other
computations that are not yet available:
• Contention for resources. Many resources can only be used by a single client at a time. This
·
·
·- 0 A a
means that other clients must wait for access to those resources. Figure 8.2 shows events
arriving at the system. These events may be in a single stream or in multiple streams.
Multiple streams vying for the same resource or different events in the same stream vying
for the same resource contribute to latency. The more contention for a resource, the more
likelihood of latency being introduced.
• Availability of resources. Even in the absence of contention, computation cannot proceed if
a resource is unavailable. Unavailability may be caused by the resource being offline or by
failure of the component or for some other reason. In any case, you must identify places
where resource unavailability might cause a significant contribution to overall latency.
Some of our tactics are intended to deal with this situation.
• Dependency on other computation. A computation may have to wait because it must
synchronize with the results of another computation or because it is waiting for the results
of a computation that it initiated. If a component calls another component and must wait for
that component to respond, the time can be significant if the called component is at the
other end of a network (as opposed to co-located on the same processor).
With this background, we tum to our tactic categories. We can either reduce demand for resources
or make the resources we have handle the demand more effectively:
• Control resource demand. This tactic operates on the demand side to produce smaller demand
on the resources that will have to service the events.
• Manage resources. This tactic operates on the response side to make the resources at hand
work more effectively in handling the demands put to them.
Control Resource Demand
One way to increase performance is to carefully manage the demand for resources. This can be done by
reducing the number of events processed by enforcing a sampling rate, or by limiting the rate at which
the system responds to events. In addition, there are a number of techniques for ensuring that the
resources that you do have are applied judiciously:
• Manage sampling rate. If it is possible to reduce the sampling frequency at which a stream of
environmental data is captured, then demand can be reduced, typically with some attendant loss
of fidelity. This is common in signal processing systems where, for example, different codecs
can be chosen with different sampling rates and data formats. This design choice is made to
maintain predictable levels of latency; you must decide whether having a lower fidelity but
consistent stream of data is preferable to losing packets of data.
• Limit event response. When discrete events arrive at the system (or element) too rapidly to be
processed, then the events must be queued until they can be processed. Because these events
are discrete, it is typically not desirable to ''downsample" them. In such a case, you may choose
to process events only up to a set maximum rate, thereby ensuring more predictable processing
when the events are actually processed. This tactic could be triggered by a queue size or
processor utilization measure exceeding some warning level. If you adopt this tactic and it is
unacceptable to lose any events, then you must ensure that your queues are large enough to
·
·
·- 0 A a
handle the worst case. If, on the other hand, you choose to drop events, then you need to choose
a policy for handling this situation: Do you log the dropped events, or simply ignore them? Do
you notify other systems, users, or administrators?
• Prioritize events. If not all events are equally important, you can impose a priority scheme that
ranks events according to how important it is to service them. If there are not enough resources
available to service them when they arise, low-priority events might be ignored. Ignoring
events consumes minimal resources (including time), and thus increases performance compared
to a system that services all events all the time. For example, a building management syste1n
may raise a variety of alarms. Life-threatening alarms such as a fire alarm should be given
higher priority than informational alarms such as a room is too cold.
• Reduce overhead. The use of intermediaries (so important for modifiability, as we saw in
Chapter 7) increases the resources consumed in processing an event stream, and so removing
them improves latency. This is a classic modifiability/performance tradeoff. Separation of
concerns, another linchpin of modifiability, can also increase the processing overhead
necessary to service an event if it leads to an event being serviced by a chain of components
rather than a single component. The context switching and intercomponent communication
costs add up, especially when the components are on different nodes on a network. A strategy
for reducing computational overhead is to co-locate resources. Co-location may mean hosting
cooperating components on the same processor to avoid the time delay of network
communication; it may mean putting the resources in the same runtime software component to
avoid even the expense of a subroutine call. A special case of reducing computational overhead
is to perform a periodic cleanup of resources that have become inefficient. For example, hash
tables and virtual memory maps may require recalculation and reinitialization. Another
common strategy is to execute single-threaded servers (for simplicity and avoiding contention)
and split workload across them.
• Bound execution times. Place a limit on how much execution time is used to respond to an
event. For iterative, data-dependent algorithms, limiting the number of iterations is a method
for bounding execution times. The cost is usually a less accurate computation. If you adopt this
tactic, you will need to assess its effect on accuracy and see if the result is "good enough." This
resource management tactic is frequently paired with the manage sampling rate tactic.
• Increase resource efficiency. Improving the algorithms used in critical areas will decrease
latency.
Manage Resources
Even if the demand for resources is not controllable, the management of these resources can be.
Sometimes one resource can be traded for another. For example, intermediate data may be kept in a
cache or it may be regenerated depending on time and space resource availability. This tactic is usually
applied to the processor but is also effective when applied to other resources such as a disk. Here are
some resource management tactics:
• Increase resources. Faster processors, additional processors, additional memory, and faster
networks all have the potential for reducing latency. Cost is usually a consideration in the
·
·
·- 0 A a
choice of resources, but increasing the resources is definitely a tactic to reduce latency and in
many cases is the cheapest way to get immediate improvement.
• Introduce concurrency. If requests can be processed in parallel, the blocked time can be
reduced. Concurrency can be introduced by processing different streams of events on different
threads or by creating additional threads to process different sets of activities. Once
concurrency has been introduced, scheduling policies can be used to achieve the goals you find
desirable. Different scheduling policies may maximize fairness (all requests get equal time),
throughput (shortest time to finish first), or other goals. (See the sidebar.)
• Maintain multiple copies of computations. Multiple servers in a client-server pattern are
replicas of computation. The purpose of replicas is to reduce the contention that would occur if
all computations took place on a single server. A load balancer is a piece of software that
assigns new work to one of the available duplicate servers; criteria for assignment vary but can
be as simple as round-robin or assigning the next request to the least busy server.
• Maintain multiple copies of data. Caching is a tactic that involves keeping copies of data
(possibly one a subset of the other) on storage with different access speeds. The different access
speeds may be inherent (memory versus secondary storage) or may be due to the necessity for
network communication. Data replication involves keeping separate copies of the data to
reduce the contention from multiple simultaneous accesses. Because the data being cached or
replicated is usually a copy of existing data, keeping the copies consistent and synchronized
becomes a responsibility that the system must assume. Another responsibility is to choose the
data to be cached. Some caches operate by merely keeping copies of whatever was recently
requested, but it is also possible to predict users' future requests based on patterns of behavior,
and begin the calculations or prefetches necessary to comply with those requests before the user
has made them.
• Bound queue sizes. This controls the maximum number of queued arrivals and consequently the
resources used to process the arrivals. If you adopt this tactic, you need to adopt a policy for
what happens when the queues overflow and decide if not responding to lost events is
acceptable. This tactic is frequently paired with the limit event response tactic.
• Schedule resources. Whenever there is contention for a resource, the resource must be
scheduled. Processors are scheduled, buffers are scheduled, and networks are scheduled. Your
goal is to understand the characteristics of each resource's use and choose the scheduling
strategy that is compatible with it. (See the sidebar.)
The tactics for performance are summarized in Figure 8.3.
Event
Arrives
·
·
·-
Performance Tactics
Control Resource Demand Manage Resources 􀂐 􀂑 Manage Sampling Rate Increase Resources
Limit Event Response Introduce Concurrency
Prioritize Events Maintain Multiple
Reduce Overhead
Copies of Computations
Bound Execution T:imes
Maintain Multiple
Copies of Data
Increase Resource Bound Queue Sizes
Efftclency
Schedule Resources
Figure 8.3. Performance tactics
Scheduling Policies
0 A a
Response
Ge·nerated wlthin
Time Constraints
A scheduling policy conceptually has two parts: a priority assignment and dispatching.
All scheduling policies assign priorities. In some cases the assignment is as simple as
first-in/first-out (or FIFO). In other cases, it can be tied to the deadline of the request or
its semantic importance. Competing criteria for scheduling include optimal resource
usage, request importance, minimizing the number of resources used, minimizing
latency, maximizing throughput, preventing starvation to ensure fairness, and so forth.
You need to be aware of these possibly conflicting criteria and the effect that the
chosen tactic has on meeting them.
A high-priority event stream can be dispatched only if the resource to which it is
being assigned is available. Sometimes this depends on preempting the current user of
the resource. Possible preemption options are as follows: can occur anytime, can occur
only at specific preemption points, and executing processes cannot be preempted. Some
common scheduling policies are these:
• First-in/first-out. FIFO queues treat all requests for resources as equals and satisfy
them in tum. One possibility with a FIFO queue is that one request will be stuck
behind another one that takes a long time to generate a response. As long as all of the
requests are truly equal, this is not a problem, but if some requests are of higher
priority than others, it is problematic.
• Fixed-priority scheduling. Fixed-priority scheduling assigns each source of resource
requests a particular priority and assigns the resources in that priority order. This
strategy ensures better service for higher priority requests. But it admits the
possibility of a lower priority, but important, request taking an arbitrarily long time to
be serviced, because it is stuck behind a series of higher priority requests. Three
common prioritization strategies are these:
·
·
·- 0 A a
• Semantic importance. Each stream is assigned a priority statically according to
some domain characteristic of the task that generates it.
• Deadline monotonic. Deadline monotonic. Deadline monotonic is a static priority
assignment that assigns higher priority to streams with shorter deadlines. This
scheduling policy is used when streams of different priorities with real-time
deadlines are to be scheduled.
• Rate monotonic. Rate monotonic is a static priority assignment for periodic
streams that assigns higher priority to streams with shorter periods. This
scheduling policy is a special case of deadline monotonic but is better known and
more likely to be supported by the operating system.
• Dynamic priority scheduling. Strategies include these:
• Round-robin. Round-robin is a scheduling strategy that orders the requests and
then, at every assignment possibility, assigns the resource to the next request in
that order. A special form of round-robin is a cyclic executive, where assignment
possibilities are at fixed time intervals.
• Earliest-deadline-first. Earliest-deadline-first. Earliest-deadline-first assigns
priorities based on the pending requests with the earliest deadline.
• Least-slack-first. This strategy assigns the highest priority to the job having the
least "slack time," which is the difference between the execution time remaining
and the time to the job's deadline.
For a single processor and processes that are preemptible (that is, it is possible to
suspend processing of one task in order to service a task whose deadline is drawing
near), both the earliest-deadline and least-slack scheduling strategies are optimal. That
is, if the set of processes can be scheduled so that all deadlines are met, then these
strategies will be able to schedule that set successfully.
• Static scheduling. A cyclic executive schedule is a scheduling strategy where the
preemption points and the sequence of assignment to the resource are determined
offline. The runtime overhead of a scheduler is thereby obviated.
Performance Tactics on the Road
Tactics are generic design principles. To exercise this point, think about the design of
the systems of roads and highways where you live. Traffic engineers employ a bunch of
design "tricks" to optimize the performance of these complex systems, where
performance has a number of measures, such as throughput (how many cars per hour
get from the suburbs to the football stadium), average-case latency (how long it takes,
on average, to get from your house to downtown), and worst-case latency (how long
does it take an emergency vehicle to get you to the hospital). What are these tricks?
None other than our good old buddies, tactics.
Let's consider some examples:
·
·
·- 0 A a
• Manage event rate. Lights on highway entrance ramps let cars onto the highway only
at set intervals, and cars must wait (queue) on the ramp for their turn.
• Prioritize events. Ambulances and police, with their lights and sirens going, have
higher priority than ordinary citizens; some highways have high-occupancy vehicle
(HOV) lanes, giving priority to vehicles with two or more occupants.
• Maintain multiple copies. Add traffic lanes to existing roads, or build parallel routes.
In addition, there are some tricks that users of the system can employ:
• Increase resources. Buy a Ferrari, for example. All other things being equal, the
fastest car with a competent driver on an open road will get you to your destination
more quickly.
• Increase efficiency. Find a new route that is quicker and/or shorter than your current
route.
• Reduce computational overhead. You can drive closer to the car in front of you, or
you can load more people into the same vehicle (that is, carpooling).
What is the point of this discussion? To paraphrase Gertrude Stein: performance is
performance is performance. Engineers have been analyzing and optimizing systems
for centuries, trying to improve their performance, and they have been employing the
same design strategies to do so. So you should feel some comfort in knowing that when
you try to improve the performance of your computer-based system, you are applying
tactics that have been thoroughly "road tested."
-RK
8.3. A Design Checklist for Performance
·
·
·- 0
Table 8.2 is a checklist to support the design and analysis process for performance.
A a
Table 8.2. Checklist to Support the Design and Analysis Process for Performance
Category
Allocation of
Responsibilities
Coordination
Model
Checklist
Dete􀇩mine the system's responsibilities th,at will involve heavy
loading, have time-critical response requirements, are heavily
used, or impact portions of the system where heavy load'S or
time-critical events occur.
For those responslbltities, Identify the processing requirements
of each respons1bility, and determine whether they may cause
bottlenecks.
Also, identify additional responsibilities to recognize and process
requests appropriately, including1
• Responsibilities that result from a thread of control crossing
process or processor boundaries
• Responsibilities to manage the threads of oontrol'-allocation
and deallocation of threads, malntalnlng thread pools, and so
forth
• ResponslbilitJes for scheduling shared resources or
managing performance-related artifacts such as queues,
buffers, and caches
For the responsibiliti.es and resources you identified, ensure that
the required performance response can be met (perhaps by
building a performance model to help in the evaluation}.
Determine the elements of the system that must coordinate with
each other-directly or indrrectly-and choose communication
and coordination mechanisms that do the following:
• Support any introduced concurrency (for example, is it thread
safe?), event prioritization, or scheduling strategy
• Ensure that the required performance response can be
delivered
• Can capture periodic, stochastic, or sporadic event arr,ivals,
as needed
• Have the appropriate properties of the communication
mechanlsms; for example, stateful, stateless, synchronous,
asynchronous, guaranteed delivery, throughput, or latency
Data Model
Mapping among
Architectural
Elements
Resource
Management
.Bindfng Time
Choice of
Technology
·
·
·- 0
Determine those port1;ons of the data model that will be heavily
loaded, have time-critical response requirements, are heavily
used, or impact portions of the system where heavy loads or
time-critical events occur.
For those data abstractions, determine the foHowing:
• Whe1her maintaining multiple copies of key data would
benefit performance
• Whether partitioning data would benefit performance
• Whether reducing the processing requirements for the
cr.eatlon, initialization, persistence, manipulation, translation,
or destruction of the enumerated data abstractions is
possible
• Whether adding resources to reduce bottlenecks for the
cr·eation, Initialization, persistence, manipulatton, translation,
or destruction of the enumerated data abstractions is feasible
Where heavy network loading will occur, determine whether
co-locating some components will reduce loading and improve
overall efficiency.
Ensure that components with heavy computation requirements
are assigned to processors with the most processing capaci1y.
Determine where introducing concurrency (that is, a�J:ocattng
a piece of tunc1ionallty to two or more copies of a component
running simultaneously) is feasible and has a significant posit'fve
effect on performance.
Determine whether the choice of threads of control and their
associated responsibilities introduces bottlenecks.
Determine which resources in your system are critical for
performance. For these resources, ensure that they will be
monitored and managed under normal and overloaded system
operation. For example:
• System elements that need to be aware of, and manage,
time and other performance-critical resources
• Process/thread models
• Prioritization of resources and access to resources
• Scheduling and looking strategies
• Deploying additional resources on demand to meet increased
loads
For each element that will be bound after compile time,
determine tfl.e following:
• Time necessary to complete the binding
• Additional overhead introduced by using the late binding
mechanism
Ensure that these values do not pose unacceptable performance
penalties on the system.
Will your choice of technology let you set and meet hard, realtime
deadlines? Do you know its characteristics under load and
lts limits?
Does your choice of technology g'ive you the ability to set the
following:
• Scheduling policy
• Priorities
• Policies for reducing demand
• Allocation of portions of the technology to processors
• Other performance-related parameters
Does your choice of technology introduce excessive overhead
for heavily used operations?
A a
8.4. Summary
·
·
·- 0 A a
Performance is about the management of system resources in the face of particular types of demand to
achieve acceptable timing behavior. Performance can be measured in terms of throughput and latency
for both interactive and embedded real-time systems, although throughput is usually more important in
interactive systems, and latency is more important in embedded systems.
Performance can be improved by reducing demand or by managing resources more appropriately.
Reducing demand will have the side effect of reducing fidelity or refusing to service some requests.
Managing resources more appropriately can be done through scheduling, replication, or just increasing
the resources available.
8.5. For Further Reading
·
·
·- 0
Performance has a rich body of literature. Here are some books we recommend:
A a
• Software Performance and Scalability: A Quantitative Approach [Liu 09]. This books covers
performance geared toward enterprise applications, with an emphasis on queuing theory and
measurement.
• Performance Solutions: A Practical Guide to Creating Responsive, Scalable Software [Smith
Ql].. This book covers designing with performance in mind, with emphasis on building (and
populating with real data) practical predictive performance models.
• Real-Time Design Patterns: Robust Scalable Architecture for Real-Time Systems [Douglass
22}.
• Real-Time Systems [Liu 00].
• Pattern-Oriented Software Architecture Volume 3: Patterns for Resource Management
[Kircher 03].
8.6. Discussion Questions
·
·
·- 0 A a
1. "Every system has real-time performance constraints." Discuss. Or provide a counterexample.
2. Write a performance scenario that describes the average on-time flight arrival performance for an
airline.
3. Write several performance scenarios for an automatic teller machine. Think about whether your
major concern is worst-case latency, average-case latency, throughput, or some other response
measure. How would you modify your automatic teller machine design to accommodate these
scenarios?
4. Web-based systems often use proxy servers, which are the first element of the system to receive a
request from a client (such as your browser). Proxy servers are able to serve up often-requested
web pages, such as a company' s home page, without bothering the real application servers that
carry out transactions. There may be many proxy servers, and they are often located
geographically close to large user communities, to decrease response time for routine requests.
What performance tactics do you see at work here?
5. A fundamental difference between coordination mechanisms is whether interaction is
synchronous or asynchronous. Discuss the advantages and disadvantages of each with respect to
each of the performance responses: latency, deadline, throughput, jitter, miss rate, data loss, or
any other required performance-related response you may be used to.
6. Find real-world (that is, nonsoftware) examples of applying each of the manage-resources tactics.
For example, suppose you were managing a brick-and-mortar big-box retail store. How would
you get people through the checkout lines faster using these tactics?
7. User interface frameworks typically are single-threaded. Why is this so and what are the
performance implications of this single-threading?
9. Security
·
·
·- 0
With Jungwoo Ryoo and Phil Laplante
Your personal identity isn 't worth quite as much
as it used to be-at least to thieves willing to
swipe it. According to experts who monitor such
markets, the value of stolen credit card data may
range from $3 to as little as 40 cents. That 's
down tenfold from a decade ago-even though
the cost to an individual who has a credit card
stolen can soar into the hundreds of dollars.
-Forbes.com (Taylor Buley. "Hackonomics,"
F orbes.com, October 27, 2008,
www.forbes.com/2008/ 1 0/25/credit-card-theft-techsecurity-
cz tb 1 024theft.html)
A a
Security is a measure of the system's ability to protect data and information from unauthorized access
while still providing access to people and systems that are authorized. An action taken against a
computer system with the intention of doing harm is called an attack and can take a number of forms. It
may be an unauthorized attempt to access data or services or to modify data, or it may be intended to
deny services to legitimate users.
The simplest approach to characterizing security has three characteristics: confidentiality, integrity,
and availability (CIA):
1. Confidentiality is the property that data or services are protected from unauthorized access.
For example, a hacker cannot access your income tax retun1s on a government computer.
2. Integrity is the property that data or services are not subject to unauthorized manipulation.
For example, your grade has not been changed since your instructor assigned it.
3. Availability is the property that the system will be available for legitimate use. For example, a
denial-of-service attack won't prevent you from ordering book from an online bookstore.
Other characteristics that are used to support CIA are these:
4. Authentication verifies the identities of the parties to a transaction and checks if they are truly
who they claim to be. For example, when you get an email purporting to come from a bank,
authentication guarantees that it actually comes from the bank.
5. Nonrepudiation guarantees that the sender of a message cannot later deny having sent the
message, and that the recipient cannot deny having received the message. For example, you
cannot deny ordering something from the Internet, or the merchant cannot disclaim getting
your order.
6. Authorization grants a user the privileges to perform a task. For example, an online banking
system authorizes a legitimate user to access his account.
·
·
·- 0 A a
We will use these characteristics in our general scenarios for security. Approaches to achieving security
can be characterized as those that detect attacks, those that resist attacks, those that react to attacks, and
those that recover from successful attacks. The objects that are being protected from attacks are data at
rest, data in transit, and computational processes.
9.1. Security General Scenario
·
·
·- 0 A a
One technique that is used in the security domain is threat modeling. An "attack tree," similar to a fault
tree discussed in Chapter 5, is used by security engineers to determine possible threats. The root is a
successful attack and the nodes are possible direct causes of that successful attack. Children nodes
decompose the direct causes, and so forth. An attack is an attempt to break CIA, and the leaves of attack
trees would be the stimulus in the scenario. The response to the attack is to preserve CIA or deter
attackers through monitoring of their activities. From these considerations we can now describe the
individual portions of a security general scenario. These are summarized in Table 9. 1 , and an example
security scenario is given in Figure 9 . 1 .
Portion of
Scenario
Source
Stimulus
Artifact
Environment
Response
Response
Measure
Table 9.1. Security General Scenario
Possible VaJues
Human or another system which may have been previously
idenUfied (either correctly or incorrectly} or may be currently
unknown. A human attacker may be from outside the organization or
from inside the organization.
Unauthorized attempt Is made to display data, change or delete
data, access system servi.ces, change the system's behavior, or
reduce availability.
System services, data within the system, a component or resources
of the system, data produced or consumed by the system
The system is either online or offline; either connected to or
disconnected from a network; either behind a firewall or open to a
network;. fully operational, parti·ally operational, or not operational.
Transactions are carried out In a fashion such that
• Data or services are protected from unauthorized access.
• Data or services are not being manipulated without authorization.
• Parties to a transaction are identified with assurance.
• The parties to the transaction cannot repudiate their
Involvements.
• The data, resources. and system services will be available for
legitimate use.
The system tracks activities within it by
• 'Recording access or modification
• Recording attempts to access data, resources. or services
• No1ifying appropriate entities (people or systems) when an
apparent attack is occurring
One or more of the following:
• How much of a system is compromised when a parti:cular
component or data value is compromised
• How much time passed before an attack was detected
• How many attacks were resisted
• How long does it take to rrecoverr from a successful attack
• How much data is vulnerable to a particular attack
Source:
Stimulus:
Attempts to
Modify Pay
Rate
Disgruntled
Employee from
Remote Location
Artifact:
Data Vllithin
1he System
Environment:
Normal
Operations
Response:
System
Maintains
Audit Trai l
·
·
·-
Figure 9.1. Sample concrete security scenario
0
Response
Measure􀃞
Correct Data Is
Restored within a
Day and Source
of Tampering
Identified
A a
• Source of stimulus. The source of the attack may be either a human or another system. It may
have been previously identified (either correctly or incorrectly) or may be currently unknown.
A human attacker may be from outside the organization or from inside the organization.
• Stimulus. The stimulus is an attack. We characterize this as an unauthorized attempt to display
data, change or delete data, access system services, change the system's behavior, or reduce
availability.
• Artifact. The target of the attack can be either the services of the system, the data within it, or
the data produced or consumed by the system. Some attacks are made on particular components
of the system known to be vulnerable.
• Environment. The attack can come when the system is either online or offline, either connected
to or disconnected from a network, either behind a firewall or open to a network, fully
operational, partially operational, or not operational.
• Response. The system should ensure that transactions are carried out in a fashion such that data
or services are protected from unauthorized access; data or services are not being manipulated
without authorization; parties to a transaction are identified with assurance; the parties to the
transaction cannot repudiate their involvements; and the data, resources, and system services
will be available for legitimate use.
The system should also track activities within it by recording access or modification;
attempts to access data, resources, or services; and notifying appropriate entities (people or
systems) when an apparent attack is occurring.
• Response measure. Measures of a system's response include how much of a system is
compromised when a particular component or data value is compromised, how much time
passed before an attack was detected, how many attacks were resisted, how long it took to
recover from a successful attack, and how much data was vulnerable to a particular attack.
Table 9 . 1 enumerates the elements of the general scenario, which characterize security, and Figure
ll shows a sample concrete scenario: A disgruntled employee from a remote location attempts to
·
·
·- 0 A a
modify the pay rate table during normal operations. The system maintains an audit trail, and the correct
data is restored within a day.
9.2. Tactics for Security
·
·
·- 0 A a
One method for thinking about how to achieve security in a system is to think about physical security.
Secure installations have limited access (e.g., by using security checkpoints), have means of detecting
intruders (e.g., by requiring legitimate visitors to wear badges), have deterrence mechanisms such as
armed guards, have reaction mechanisms such as automatic locking of doors, and have recovery
mechanisms such as off-site backup. These lead to our four categories of tactics: detect, resist, react,
and recover. Figure 9.2 shows these categories as the goal of security tactics.
Attack
Detect Attacks
Tactics
to Control
Security System Detects, Resists.
Reacts, or Recovers
Figure 9.2. The goal of security tactics
The detect attacks category consists of four tactics: detect intrusion, detect service denial, verify
message integrity, and detect message delay.
• Detect intrusion is the comparison of network traffic or service request patterns within a system
to a set of signatures or known patterns of malicious behavior stored in a database. The
signatures can be based on protocol, TCP flags, payload sizes, applications, source or
destination address, or port number.
• Detect service denial is the comparison of the pattern or signature of network traffic coming
into a system to historic profiles of known denial-of-service attacks.
• Verify message integrity. This tactic employs techniques such as checksums or hash values to
verify the integrity of messages, resource files, deployment files, and configuration files. A
checksum is a validation mechanism wherein the system maintains redundant information for
configuration files and messages, and uses this redundant information to verify the
configuration file or message when it is used. A hash value is a unique string generated by a
hashing function whose input could be configuration files or messages. Even a slight change in
the original files or messages results in a significant change in the hash value.
• Detect message delay is intended to detect potential man-in-the-middle attacks, where a
malicious party is intercepting (and possibly modifying) messages. By checking the time that it
takes to deliver a 1nessage, it is possible to detect suspicious timing behavior, where the time it
takes to deliver a 1nessage is highly variable.
Resist Attacks
There are a number of well-known means of resisting an attack:
·
·
·- 0 A a
• Identify actors. Identifying "actors" is really about identifying the source of any external input
to the system. Users are typically identified through user IDs. Other systems may be
"identified" through access codes, IP addresses, protocols, ports, and so on.
• Authenticate actors. Authentication means ensuring that an actor (a user or a remote computer)
is actually who or what it purports to be. Passwords, one-time passwords, digital certificates,
and biometric identification provide a means for authentication.
• Authorize actors. Authorization means ensuring that an authenticated actor has the rights to
access and modify either data or services. This mechanism is usually enabled by providing
some access control mechanisms within a system. Access control can be by an actor or by an
actor class. Classes of actors can be defined by actor groups, by actor roles, or by lists of
individuals.
• Limit access. Limiting access to computing resources involves limiting access to resources such
as metnory, network connections, or access points. This may be achieved by using memory
protection, blocking a host, closing a port, or rejecting a protocol. For example, a demilitarized
zone (DMZ) is used when an organization wants to let external users access certain services
and not access other services. It sits between the Internet and a firewall in front of the internal
intranet. The firewall is a single point of access to the intranet (limit exposure). It also restricts
access using a variety of techniques to authorize users (authorize actors).
• Limit exposure. The limit exposure tactic minimizes the attack surface of a system. This tactic
focuses on reducing the probability of and minimizing the effects of damage caused by a hostile
action. It is a passive defense because it does not proactively prevent attackers from doing
harm. Limit exposure is typically realized by having the least possible number of access points
for resources, data, or services and by reducing the number of connectors that may provide
unanticipated exposure.
• Encrypt data. Data should be protected from unauthorized access. Confidentiality is usually
achieved by applying some form of encryption to data and to communication. Encryption
provides extra protection to persistently maintained data beyond that available from
authorization. Communication links, on the other hand, may not have authorization controls. In
such cases, encryption is the only protection for passing data over publicly accessible
communication links. The link can be implemented by a virtual private network (VPN) or by a
Secure Sockets Layer (SSL) for a web-based link. Encryption can be symmetric (both parties
use the same key) or asymmetric (public and private keys).
• Separate entities. Separating different entities within the system can be done through physical
separation on different servers that are attached to different networks; the use of virtual
machines (see Chapter 26 for a discussion of virtual machines); or an "air gap," that is, by
having no connection between different portions of a system. Finally, sensitive data is
frequently separated from nonsensitive data to reduce the attack possibilities from those who
have access to nonsensitive data.
·
·
·- 0 A a
• Change default settings. Many systems have default settings assigned when the system is
delivered. Forcing the user to change those settings will prevent attackers from gaining access
to the system through settings that are, generally, publicly available.
React to Attacks
Several tactics are intended to respond to a potential attack:
• Revoke access. If the system or a system administrator believes that an attack is underway, then
access can be severely limited to sensitive resources, even for normally legitimate users and
uses. For example, if your desktop has been compromised by a virus, your access to certain
resources may be limited until the virus is removed from your system.
• Lock computer. Repeated failed login attempts may indicate a potential attack. Many systems
limit access from a particular computer if there are repeated failed attempts to access an
account from that computer. Legitimate users may make mistakes in attempting to log in.
Therefore, the limited access may only be for a certain time period.
• Inform actors. Ongoing attacks may require action by operators, other personnel, or
cooperating systems. Such personnel or systems the set of relevant actors must be notified
when the system has detected an attack.
Recover from Attacks
Once a system has detected and attempted to resist an attack, it needs to recover. Part of recovery is
restoration of services. For example, additional servers or network connections may be kept in reserve
for such a purpose. Since a successful attack can be considered a kind of failure, the set of availability
tactics (from Chapter 5) that deal with recovering from a failure can be brought to bear for this aspect of
security as well.
In addition to the availability tactics that permit restoration of services, we need to maintain an audit
trail. We audit that is, keep a record of user and system actions and their effects to help trace the
actions of, and to identify, an attacker. We may analyze audit trails to attempt to prosecute attackers, or
to create better defenses in the future.
The set of security tactics is shown in Figure 9.3.
..
Detect Attacks
I
Detect
Attack
lntrustion
Detect Service
Den􀁬al
Verify Message
Integrity
Detect Message
Delay
Restst Attacks
I
'
Identify
Actors
ALJthenticate
Actors
Authoriz·e
Actors
Limit Access
Limit Exposure
Encrypt Data
Separate
Entities
Change Defau􀁭t
Settings
React to
Attacks
t Revoke
Acoess
Lock
Computer
Inform
Actors
·
·
·-
Recove·r
from Attacks
Maintain Restore
AudnTra􀁮
See
Availability
Figure 9.3. Security tactics
0
System Detects.
Resists, Reacts,
or Recovers
A a
·
·
·- 0
9.3. A Design Checklist for Security
Table 9.2 is a checklist to support the design and analysis process for security.
Table 9.2. Checklist to Support the Design and Analysis Process for Security
Category
Allocation of
Responsibilities
Coordination
Model
Data Model
Checklist
Determine which system responsibiHties need to be secure.
For eacn of tnese responsibilitles, ensure that additional
responsibilities have been allocated to do the following:
• Identify the actor
• Authenticate the actor
• Authorize actors
• Grant or deny acc,ess to data or services
• Record attempts to access or modify data or services
• Encrypt data
• Recognize reduced availability for resources or services and
inform appropriate personnel and restrict access
• Recover from an attack
• Verify checksums and hash values
Determine mechanisms required to communicate and coordinate
with other systems or individua!ls. For these communications,
ensure that mechanisms for authenticating. and authorizing the
actor or system, and encrypting data for transmission across
the connection, are in place. Ensur·e also that mechanisms exist
for monttoring and recognizing unexpectedly high demands for
.resources or services as well as mechanisms for restricting or
terminating the connection.
Determine the sensitivity of different data fields. For each data
abstraction:
• Ensure that data of different sensitivity is separated.
• Ensure that data of different sensitivity has different access
rights and that access rights are checked prior to access􀈵
• Ensure that access to sensitive data is logged and that the log
file is suitably protected.
• Ensure that data is suitably encrypted and that keys are
separated from the encrypted data.
• Ensure that data can be restored if it is inappropriately
m-odified:.
A a
Mapping among
Architectural
Elements
Resource
Manag.ement
Binding Time
Choice of
Technology
·
·
·- 0
Determine how alternative mapp:ings of architectural ellements
that are under consideration may change how an individual or
system may read, write, or modify data; access system services or
resources; or reduce availability to system services or resources.
Determine how alternative mapplngs may affect the recording
of access to data, services or resources and the reoognltion of
unexpectedly high demands for resources.
For each such mapping, ensure that there are responsibilities to do
the following:
• Identify an actor
• Authenticate an actor
• Authorize actors
• Grant or deny access to data or services
• Record attempts to access or modify data or services
• Encrypt data
• Recognize reduced availability for resources or services, inform
appropriate personneJ, and restrict access
• Recover from an attack
Determine the system resources required to identify and monitor
a system or an individual who is internal or external, authorized or
not authorized, with access to specific resources or all resources.
Determine the resources required to authenticate the actor, grant
or deny access to data or resources, notify appropriate entities
(people or systems), record attempts to access data or resources,
encrypt data, recognize inexplicably high demand for resources,
inform users or systems, and restrict access.
For these resources consider Whether an external entity can
access a critical resource or ·exhaust a critical· resource; how to
monitor the resource; how to manage resource utilJzation; how
to log resource utilization; and ensure that there are sufficient
resources to perform the necessary security operations.
Ensure that a contaminated element can be prevented from
contaminating other elements.
Ensure that shared resources are not used for passing sensitive
data from an actor with access rights to that data to an actor
without access rights to that data.
Determine cases where an instance of a late-'bound component
may ·be untrusted. For such cases ensure that late-bound
components can be qualified; that is, if ownership certificates
for late-bound components are required, there are appropriate
mechanisms to manage and validate· them; that access to
late-bound data and services can be manag.ed; that access by
late-bound components to data and services can be blocked; that
mechanisms to record the access, modification. and attempts to
access data or services by late-bound components are in place;
and that system data is encrypted where the keys are intentionally
withhel·d for late-bound components
Determine what technologies are available to help user
authentication. data access rights, resource protection, and data
encryption.
Ensure that your chosen technologies support the tactics relevant
for your security needs.
A a
9.4. Summary
·
·
·- 0 A a
Attacks against a system can be characterized as attacks against the confidentiality, integrity, or
availability of a system or its data. Confidentiality means keeping data away from those who should not
have access while granting access to those who should. Integrity means that there are no unauthorized
modifications to or deletion of data, and availability means that the system is accessible to those who
are entitled to use it.
The emphasis of distinguishing various classes of actors in the characterization leads to many of the
tactics used to achieve security. Identifying, authenticating, and authorizing actors are tactics intended
to determine which users or systems are entitled to what kind of access to a system.
An assumption is made that no security tactic is foolproof and that systems will be compromised.
Hence, tactics exist to detect an attack, limit the spread of any attack, and to react and recover from an
attack.
Recovering from an attack involves many of the same tactics as availability and, in general,
involves returning the system to a consistent state prior to any attack.
9.5. For Further Reading
·
·
·- 0 A a
The architectural tactics that we have described in this chapter are only one aspect of making a system
secure. Other aspects are these:
• Coding. Secure Coding in C and C++ [Seacord 05] describes how to code securely. The
Common Weakness Enumeration [CWE 1 2] is a list of the most common vulnerabilities
discovered in systems.
• Organizational processes. Organizations must have processes that provide for responsibility for
various aspects of security, including ensuring that systems are patched to put into place the
latest protections. The National Institute of Standards and Technology (NIST) provides an
enumeration of organizational processes [NIST 09]. [Cappelli 1 2] discusses insider threats.
• Technical processes. Microsoft has a life-cycle development process (The Secure Development
Life Cycle) that includes modeling of threats. Four training classes are publicly available.
www.microsoft.com/download/en/details.aspx?id= 16420
NIST has several volumes that give definitions of security terms [NIST 04], categories of security
controls [NIST 06], and an enumeration of security controls that an organization could employ [NIST
.Q2J.. A security control could be a tactic, but it could also be organizational, coding-related, or a
technical process.
The attack surface of a system is the code that can be run by unauthorized users. A discussion of
how to minimize the attack surface for a system can be found at [Howard 04].
Encryption and certificates of various types and strengths are commonly used to resist certain types
of attacks. Encryption algorithms are particularly difficult to code correctly. A document produced by
NIST [NIST 02] gives requirements for these algorithms.
Good books on engineering systems for security have been written by Ross Anderson [Anderson
08] and Bruce Schneier [Schneier 08].
Different domains have different specific sets of practices. The Payment Card Industry (PCI) has a
set of standards intended for those involved in credit card processing (www.pcisecuritystandards.org).
There is also a set of recommendations for securing various portions of the electric grid
(www.smartgridipedia.org/index.php/ASAP-SG).
Data on the various sources of data breaches can be found in the Verizon 2 0 1 2 Data Breach
Investigations Report [Verizon 12].
John Viega has written several books about secure software development in various environments.
See, for example, [Vi ega 0 1].
9 .6. Discussion Questions
·
·
·- 0 A a
1. Write a set of concrete scenarios for security for an automatic teller machine. How would you
modify your design for the automatic teller machine to satisfy these scenarios?
2. One of the most sophisticated attacks on record was carried out by a virus known as Stuxnet.
Stuxnet first appeared in 2009 but became widely known in 20 1 1 when it was revealed that it had
apparently severely damaged or incapacitated the high-speed centrifuges involved in Iran's
uranium enrichment program. Read about Stuxnet and see if you can devise a defense strategy
against it based on the tactics in this chapter.
3. Some say that inserting security awareness into the software development life cycle is at least as
important as designing software with security countermeasures. What are some examples of
software development processes that can lead to more-secure systems?
4. Security and usability are often seen to be at odds with each other. Security often imposes
procedures and processes that seem like needless overhead to the casual user. But some say that
security and usability go (or should go) hand in hand and argue that making the system easy to
use securely is the best way to promote security to the user. Discuss.
5. List some examples of critical resources for security that might become exhausted.
6. List an example of a mapping of architectural elements that has strong security implications.
Hint: think of where data is stored.
7. Which of the tactics in our list will protect against an insider threat? Can you think of any that
should be added?
8. In the United States, Face book can account for more than 5 percent of all Internet traffic in a
given week. How would you recognize a denial-of-service attack on Facebook.com?
9. The public disclosure of vulnerabilities in production systems is a matter of controversy. Discuss
why this is so and the pros and cons of public disclosure of vulnerabilities.
10. Testability
·
·
·-
Testing leads to failure, and failure leads to
understanding
-Burt Rutan
0 A a
Industry estimates indicate that between 30 and 50 percent (or in some cases, even more) of the cost of
developing well-engineered systems is taken up by testing. If the software architect can reduce this cost,
the payoff is large.
Software testability refers to the ease with which software can be made to demonstrate its faults
through (typically execution-based) testing. Specifically, testability refers to the probability, assuming
that the software has at least one fault, that it will fail on its next test execution. Intuitively, a system is
testable if it "gives up" its faults easily. If a fault is present in a system, then we want it to fail during
testing as quickly as possible. Of course, calculating this probability is not easy and, as you will see
when we discuss response measures for testability, other measures will be used.
Figure 1 0 . 1 shows a model of testing in which a program processes input and produces output. An
oracle is an agent (human or mechanical) that decides whether the output is correct or not by comparing
the output to the program's specification. Output is not just the functionally produced value, but it also
can include derived measures of quality attributes such as how long it took to produce the output.
Figure 1 0 . 1 also shows that the program's internal state can also be shown to the oracle, and an oracle
can decide whether that is correct or not that is, it can detect whether the program has entered an
erroneous state and render a judgment as to the correctness of the program.
input
Program
output
Internal state
Oracle
Figure 10.1. A model of testing
{ approved } rejecled
Setting and examining a program's internal state is an aspect of testing that will figure prominently
in our tactics for testability.
For a system to be properly testable, it must be possible to control each component's inputs (and
possibly manipulate its internal state) and then to observe its outputs (and possibly its internal state,
either after or on the way to computing the outputs). Frequently this control and observation is done
through the use of a test harness, which is specialized software (or in some cases, hardware) designed to
exercise the software under test. Test harnesses come in various forms, such as a record-and-playback
capability for data sent across various interfaces, or a simulator for an external environment in which a
·
·
·- 0 A a
piece of embedded software is tested, or even during production (see sidebar). The test harness can
provide assistance in executing the test procedures and recording the output. A test harness can be a
substantial piece of software in its own right, with its own architecture, stakeholders, and quality
attribute requirements.
Testing is carried out by various developers, users, or quality assurance personnel. Portions of the
system or the entire system may be tested. The response measures for testability deal with how effective
the tests are in discovering faults and how long it takes to perform the tests to some desired level of
coverage. Test cases can be written by the developers, the testing group, or the customer. The test cases
can be a portion of acceptance testing or can drive the development as they do in certain types of Agile
methodologies.
Netflix's Simian Army
Netflix distributes movies and television shows both via DVD and via streaming video.
Their streaming video service has been extremely successful. In May 20 1 1 Netflix
streaming video accounted for 24 percent of the Internet traffic in North America.
Naturally, high availability is important to Netflix.
Netflix hosts their computer services in the Amazon EC2 cloud, and they utilize
what they call a "Simian Army" as a portion of their testing process. They began with a
Chaos Monkey, which randomly kills processes in the running system. This allows the
monitoring of the effect of failed processes and gives the ability to ensure that the
system does not fail or suffer serious degradation as a result of a process failure.
Recently, the Chaos Monkey got some friends to assist in the testing. Currently, the
Netflix Simian Army includes these:
• The Latency Monkey induces artificial delays in the client-server communication
layer to simulate service degradation and measures if upstream services respond
appropriate! y.
• The Conformity Monkey finds instances that don't adhere to best practices and shuts
them down. For example, if an instance does not belong to an auto-scaling group, it
will not appropriately scale when demand goes up.
• The Doctor Monkey taps into health checks that run on each instance as well as
monitors other external signs of health (e.g., CPU load) to detect unhealthy instances.
• The Janitor Monkey ensures that the Netflix cloud environment is running free of
clutter and waste. It searches for unused resources and disposes of them.
• The Security Monkey is an extension of Conformity Monkey. It finds security
violations or vulnerabilities, such as improperly configured security groups, and
terminates the offending instances. It also ensures that all the SSL and digital rights
management (DRM) certificates are valid and are not coming up for renewal.
• The 1 0 - 1 8 Monkey (localization-internationalization) detects configuration and
runtime problems in instances serving customers in 1nultiple geographic regions,
using different languages and character sets. The name 1 0 - 1 8 comes from Ll On-il8n,
·
·
·- 0
a sort of shorthand for the words localization and internationalization.
A a
Some of the members of the Simian Army use fault injection to place faults into the
running system in a controlled and monitored fashion. Other members monitor various
specialized aspects of the system and its environment. Both of these techniques have
broader applicability than just Netflix.
Not all faults are equal in terms of severity. More emphasis should be placed on
finding the most severe faults than on finding other faults. The Simian Army reflects a
determination by N etflix that the faults they look for are the most serious in terms of
their impact.
This strategy illustrates that some systems are too complex and adaptive to be tested
fully, because some of their behaviors are emergent. An aspect of testing in that arena is
logging of operational data produced by the system, so that when failures occur, the
logged data can be analyzed in the lab to try to reproduce the faults. Architecturally this
can require mechanisms to access and log certain system state. The Simian Army is one
way to discover and log behavior in systems of this ilk.
-LB
Testing of code is a special case of validation, which is making sure that an engineered artifact
meets the needs of its stakeholders or is suitable for use. In Chapter 2 1 we will discuss architectural
design reviews. This is another kind of validation, where the artifact being tested is the architecture. In
this chapter we are concerned only with the testability of a running system and of its source code.
10.1. Testability General Scenario
We can now describe the general scenario for testability.
·
·
·- 0 A a
• Source of stimulus. The testing is performed by unit testers, integration testers, or system testers
(on the developing organization side), or acceptance testers and end users (on the customer
side). The source could be human or an automated tester.
• Stimulus. A set of tests is executed due to the completion of a coding increment such as a class
layer or service, the completed integration of a subsystem, the complete implementation of the
whole system, or the delivery of the system to the customer.
• Artifact. A unit of code (corresponding to a module in the architecture), a subsystem, or the
whole system is the artifact being tested.
• Environment. The test can happen at development time, at compile time, at deployment time, or
while the system is running (perhaps in routine use). The environment can also include the test
harness or test environments in use.
• Response. The system can be controlled to perform the desired tests and the results from the
test can be observed.
• Response measure. Response measures are aimed at representing how easily a system under
test "gives up" its faults. Measures might include the effort involved in finding a fault or a
particular class of faults, the effort required to test a given percentage of statements, the length
of the longest test chain (a measure of the difficulty of performing the tests), measures of effort
to perform the tests, measures of effort to actually fmd faults, estimates of the probability of
finding additional faults, and the length of time or amount of effort to prepare the test
environment.
Maybe one measure is the ease at which the systetn can be brought into a specific state. In
addition, measures of the reduction in risk of the remaining errors in the systetn can be used.
Not all faults are equal in terms of their possible impact. Measures of risk reduction attempt to
rate the severity of faults found (or to be found).
Figure 1 0.2 shows a concrete scenario for testability. The unit tester completes a code unit during
development and performs a test sequence whose results are captured and that gives 85 percent path
coverage within three hours of testing.
Source:
Unit Tes1er
StimU lus:
Code Unit
Completed
Artifact:
Code Unit
Environment􀃟
Development
Response:
·
·
·-
Results Captured
0
Response
Measure:
85% Path Coverage
in Three Hours
Figure 10.2. Sample concrete testability scenario
Table 1 0 . 1 enumerates the elements of the general scenario that characterize testability.
Table 10.1. Testability General Scenario
Portion of Scenar,Jo Possible Values
Source
Stimulus
Environment
Artifacts
Response
Response Measure
Unit testers, integration testers, system testers, acceptance
testers, end users, either running tests manually or using
autom,ated testing tools
A set of tests is executed due to the completion of a coding
Increment such as a class layer or service, the completed
Integration of a subsystem, the complete implementation of the
whole system, or the delivery of the system to the customer.
Design time, development time, compile time, integration time.
deployment time, run time
The portion of the system being tested
One or more of the following: execute. test suite and capture
resutts, capture activity that resulted in the fault, control and
monitor the state of the system
One or more of the following: effort to find a fault or class of
faults, effort to achieve a given percentage of state space
coverage, probability of fault being revealed by the next
test, time to perform tests, effort to detect faults, length of
longest dependency chain in testl length of time to prepare
test environment, reduction in risk exposure (sfze(loss) x
prob (loss))
A a
10.2. Tactics for Testability
·
·
·- 0 A a
The goal of tactics for testability is to allow for easier testing when an increment of software
development is completed. Figure 10.3 displays the use of tactics for testability. Architectural
techniques for enhancing the software testability have not received as much attention as more mature
quality attribute disciplines such as modifiability, performance, and availability, but as we stated before,
anything the architect can do to reduce the high cost of testing will yield a significant benefit.
Tests
Executed
Tactics
to Control
Testability
Faults
Detected
Figure 10.3. The goal of testability tactics
There are two categories of tactics for testability. The first category deals with adding controllability
and observability to the system. The second deals with limiting complexity in the system's design.
Control and Observe System State
Control and observation are so central to testability that some authors even define testability in those
terms. The two go hand-in-hand; it makes no sense to control something if you can't observe what
happens when you do. The simplest form of control and observation is to provide a software component
with a set of inputs, let it do its work, and then observe its outputs. However, the control and observe
system state category of testability tactics provides insight into software that goes beyond its inputs and
outputs. These tactics cause a component to maintain some sort of state information, allow testers to
assign a value to that state information, and/or make that information accessible to testers on demand.
The state information might be an operating state, the value of some key variable, performance load,
intermediate process steps, or anything else useful to re-creating component behavior. Specific tactics
include the following:
• Specialized interfaces. Having specialized testing interfaces allows you to control or capture
variable values for a component either through a test harness or through normal execution.
Examples of specialized test routines include these:
• A set and get method for important variables, modes, or attributes (methods that might
otherwise not be available except for testing purposes)
• A report method that returns the full state of the object
• A reset method to set the internal state (for example, all the attributes of a class) to a
specified internal state
·
·
·- 0
• A method to tum on verbose output, various levels of event logging, performance
instrumentation, or resource monitoring
A a
Specialized testing interfaces and methods should be clearly identified or kept separate from
the access methods and interfaces for required functionality, so that they can be removed if
needed. (However, in performance-critical and some safety-critical systems, it is problematic to
field different code than that which was tested. If you remove the test code, how will you know
the code you field has the same behavior, particularly the same timing behavior, as the code
you tested? For other kinds of systems, however, this strategy is effective.)
• Record/playback. The state that caused a fault is often difficult to re-create. Recording the state
when it crosses an interface allows that state to be used to "play the system back" and to recreate
the fault. Record/playback refers to both capturing information crossing an interface and
using it as input for further testing.
• Localize state storage. To start a system, subsystem, or module in an arbitrary state for a test, it
is most convenient if that state is stored in a single place. By contrast, if the state is buried or
distributed, this becomes difficult if not impossible. The state can be fine-grained, even bitlevel,
or coarse-grained to represent broad abstractions or overall operational modes. The
choice of granularity depends on how the states will be used in testing. A convenient way to
"externalize" state storage (that is, to make it able to be manipulated through interface features)
is to use a state machine (or state machine object) as the mechanism to track and report current
state.
• Abstract data sources. Similar to controlling a program's state, easily controlling its input data
makes it easier to test. Abstracting the interfaces lets you substitute test data more easily. For
example, if you have a database of customer transactions, you could design your architecture so
that it is easy to point your test syste1n at other test databases, or possibly even to files of test
data instead, without having to change your functional code.
• Sandbox. "Sandboxing" refers to isolating an instance of the system from the real world to
enable experimentation that is unconstrained by the worry about having to undo the
consequences of the experiment. Testing is helped by the ability to operate the system in such a
way that it has no permanent consequences, or so that any consequences can be rolled back.
This can be used for scenario analysis, training, and simulation. (The Spring framework, which
is quite popular in the Java community, comes with a set of test utilities that support this. Tests
are run as a "transaction," which is rolled back at the end.)
A common form of sandboxing is to virtualize resources. Testing a system often involves
interacting with resources whose behavior is outside the control of the system. Using a
sandbox, you can build a version of the resource whose behavior is under your control. For
example, the system clock's behavior is typically not under our control it increments one
second each second which means that if we want to make the system think it's midnight on
the day when all of the data structures are supposed to overflow, we need a way to do that,
because waiting around is a poor choice. By having the capability to abstract system time from
clock time, we can allow the system (or components) to run at faster than wall-clock time, and
to allow the system (or components) to be tested at critical time boundaries (such as the next
·
·
·- 0 A a
shift on or off Daylight Savings Time). Similar virtualizations could be done for other
resources, such as memory, battery, network, and so on. Stubs, mocks, and dependency
injection are simple but effective forms of virtualization.
• Executable assertions. Using this tactic, assertions are (usually) hand-coded and placed at
desired locations to indicate when and where a program is in a faulty state. The assertions are
often designed to check that data values satisfy specified constraints. Assertions are defined in
terms of specific data declarations, and they must be placed where the data values are
referenced or modified. Assertions can be expressed as pre- and post-conditions for each
method and also as class-level invariants. This results in increasing observability, when an
assertion is flagged as having failed. Assertions systematically inserted where data values
change can be seen as a manual way to produce an "extended" type. Essentially, the user is
annotating a type with additional checking code. Any time an object of that type is modified,
the checking code is automatically executed, and warnings are generated if any conditions are
violated. To the extent that the assertions cover the test cases, they effectively embed the test
oracle in the code assuming the assertions are correct and correctly coded.
All of these tactics add capability or abstraction to the software that (were we not interested in
testing) otherwise would not be there. They can be seen as replacing bare-bones, get-the-job-done
software with more elaborate software that has bells and whistles for testing. There are a number of
techniques for effecting this replacement. These are not testability tactics, per se, but techniques for
replacing one component with a different version of itself. They include the following:
• Component replacement, which simply swaps the implementation of a component with a
different implementation that (in the case of testability) has features that facilitate testing.
Component replacetnent is often accomplished in a system's build scripts.
• Preprocessor macros that, when activated, expand to state-reporting code or activate probe
statements that return or display information, or return control to a testing console.
• Aspects (in aspect-oriented programs) that handle the cross-cutting concern of how state is
reported.
Limit Complexity
Complex software is harder to test. This is because, by the definition of complexity, its operating state
space is very large and (all else being equal) it is more difficult to re-create an exact state in a large state
space than to do so in a small state space. Because testing is not just about making the software fail but
about finding the fault that caused the failure so that it can be removed, we are often concerned with
making behavior repeatable. This category has three tactics:
• Limit structural complexity. This tactic includes avoiding or resolving cyclic dependencies
between components, isolating and encapsulating dependencies on the external environment,
and reducing dependencies between components in general (for example, reduce the number of
external accesses to a module 's public data). In object-oriented systems, you can simplify the
inheritance hierarchy: Limit the number of classes from which a class is derived, or the number
of classes derived from a class. Limit the depth of the inheritance tree, and the number of
children of a class. Limit polymorphism and dynamic calls. One structural metric that has been
·
·
·- 0 A a
shown etnpirically to correlate to testability is called the response of a class. The response of
class C is a count of the number of methods of C plus the number of tnethods of other classes
that are invoked by the methods of C. Keeping this metric low can increase testability.
Having high cohesion, loose coupling, and separation of concerns all modifiability tactics
(see Chapter 7) can also help with testability. They are a form of limiting the complexity of
the architectural elements by giving each element a focused task with limited interaction with
other elements. Separation of concerns can help achieve controllability and observability (as
well as reducing the size of the overall program's state space). Controllability is critical to
making testing tractable, as Robert Binder has noted: "A component that can act independently
of others is more readily controllable . . . . With high coupling among classes it is typically more
difficult to control the class under test, thus reducing testability . . . . If user interface capabilities
are entwined with basic functions it will be more difficult to test each function" [Binder 94].
Also, systems that require complete data consistency at all times are often more complex
than those that do not. If your requirements allow it, consider building your system under the
"eventual consistency" model, where sooner or later (but maybe not right now) your data will
reach a consistent state. This often makes system design simpler, and therefore easier to test.
Finally, some architectural styles lend themselves to testability. In a layered style, you can
test lower layers first, then test higher layers with confidence in the lower layers.
• Limit nondeterminism. The counterpart to limiting structural complexity is limiting behavioral
complexity, and when it comes to testing, nondeterminism is a very pernicious form of complex
behavior. Nondeterministic systems are harder to test than deterministic systems. This tactic
involves finding all the sources of nondeterministn, such as unconstrained parallelism, and
weeding them out as much as possible. Some sources of nondeterminism are unavoidable for
instance, in tnulti-threaded systems that respond to unpredictable events but for such systems,
other tactics (such as record/playback) are available.
Figure 1 0.4 provides a summary of the tactics used for testability.
Tests
Executed
·
·
·-
Testabi rity Tactics
Control and Observe
System State
􀛫
Specialized
lnterface·s
Record/
Playback
Locaflze State
Storage
Abstract Data
Sources
S􀅗ndbox
Executable
Assertions
Limit Comp1lexity
􀁄 Limit Structural
Complexity
Limit
Non determinism
Figure 10.4. Testability tactics
0 A a
Faults
Detected
10.3. A Design Checklist for Testability
·
·
·- 0
Table 1 0.2 is a checklist to support the design and analysis process for testability.
A a
Table 10.2. Checklist to Support the Design and Analysis Process for Testability
category
Allocation of
Responsibilities
Coordination Model
Checklist
Determine which system responsibilities are most crrtical
and hence need to be most thoroughly tested.
Ensure that additional system responsibilities have been
allocated to do the following::
• Execute test sune and capture results (external test or
self-test)
• Capture (log) the activity that resulted in a fault or that
resulted in unexpected (perhaps emergent) behavior
that was not necessarily a fault
• Contro·l and observe relevant system state for testing
Make sure the allocation of functionality provides high
cohesion, low coupling, strong separation of concerns, and
low structural complexity.
Ensure the system's coordination and communication
mechanisms:
• Support the execution of a test suite and capture the
results within a system or between systems
• Support capturing activity that resulted in a fault within
a system or between systems
• Support injection and moni1oring of state into the
communication channels for use in testing, within a
system or between systems
• Do not introduce needless nondeterminism
Data Model Determine the major data abstractions that must be tested
to ensure the correct operation of the system.
• :Ensure tlnat It is possible to capture the values of
instances of these data abstractions
• Ensure that the values of instances of these data
abstractions can be set when state is injected into the
system, so that system state leading to a fault may be
re-created
• Ensure that the creation, initialization, persistence,
manipulation, translation, and destruction of instances
of these data abstractions can be exercised and
captured
Mapping among Determ·ine how to test the possible mappings of
Architectural EJements architectural elements (especially mapplngs of processes
to processors, threads to processes, and modules to
components) so that the desired test response is achieved
and potential race conditions identified.
In addition, determine whether it is possible to test for
illegal mappings of architectural' elements.
Resource Management Ensure there are sufficient resources availab􀈶e to execute
a test suite and capture the results. Ensure that your test
environment is representative of (or better yet. identlcal to)
the environment in which the system will run. Ensure that
the system provides the means to do the following:
Binding Time
Choice of Technology
• Test resource l'imits
·
·
·- 0
• Capture detailed resource usage for analysis in the
event of a failure
• Inject new resource timlts Into the system for the
purposes of testing
• !Provide vi·rtualized resources for testing·
Ensure that components that are bound later than oompUe
time can be tested in the late-bound context.
Ensure that late bindings can be captured in the event of a
failure, so that you can re-create the system's state leading
to the failure.
Ensure t�hat the full range of binding possibilities can be
tested.
Determine what technologies are available to help achieve
the testability scenarios that apply to your architecture. Are
technologies available to help with regression testing. fault
injection, recording and playback, and so on?
Determine how testable the technologies are that you have
chosen (or are considering choosing in the future) and
ensure that your chosen technologies support the l'evel of
testing appropriate for your system. For example, if your
chosen technologies do not make it posstble to inject state,
It may be difficult to re-create fault scenarios.
Now That Your Architecture Is Set to Help You Test . . .
By Nick Rozanski, coauthor (with Eoin Woods) of Software Systems Architecture:
Working With Stakeholders Using Viewpoints and Perspectives
A a
In addition to architecting your system to make it amenable to testing, you will need to
overcome two more specific and daunting challenges when testing very large or
complex systems, namely test data and test automation.
Test Data
Your first challenge is how to create large, consistent and useful test data sets. This is a
significant problem in my experience, particularly for integration testing (that is, testing
a number of components to confirm that they work together correctly) and performance
testing (confirming that the system meets it requirements for throughput, latency, and
response time). For unit tests, and usually for user acceptance tests, the test data is
typically created by hand.
For example, you might need 50 products, 100 customers, and 500 orders in your
test database, so that you can test the functional steps involved in creating, amending,
or deleting orders. This data has to be sufficiently varied to make testing worthwhile, it
has to conform to all the referential integrity rules and other constraints of your data
model, and you need to be able to calculate and specify the expected results of the tests.
I've seen and been involved in two ways of doing this: you either write a system
to generate your test data, or you capture a representative data set from the production
environment and anonymize it as necessary. (Anonymizing test data involves removing
any sensitive information, such as personal data about people or organizations, financial
details, and so on.)
·
·
·- 0
Creating your own test data is the ideal, because you know what data you are using
and can ensure that it covers all of your edge cases, but it is a lot of effort. Capturing
data from the live environment is easier, assuming that there is a system there already,
but you don't know what data and hence what coverage you're going to get, and you
may have to take extra care to conform to privacy and data protection legislation.
A a
This can have an impact on the system's architecture in a number of ways, and
should be given due consideration early on by the architect. For example, the system
may need to be able to capture live transactions, or take "snapshots" of live data, which
can be used to generate test data. In addition, the test-data-generation system may need
an architecture of its own.
Test Automation
Your second challenge is around test automation. In practice it is not possible to test
large systems by hand because of the number of tests, their complexity, and the amount
of checking of results that's required. In the ideal world, you create a test automation
framework to do this automatically, which you feed with test data, and set running
every night, or even run every time you check in something (the continuous integration
model).
This is an area that is given too little attention on many large software development
projects. It is often not budgeted for in the project plan, with an unwritten assumption
that the effort needed to build it can be somehow "absorbed" into the development
costs. A test automation framework can be a significantly complex thing in its own
right (which raises the question of how you test it!). It should be scoped and planned
like any other project deliverable.
Due consideration should be given to how the framework will invoke functions on
the system under test, particularly for testing user interfaces, which is almost without
exception a nightmare. (The execution of a UI test is highly dependent on the layout of
the windows, the ordering of fields, and so on, which usually changes a lot in heavily
user-focused systems. It is sometimes possible to execute window controls
programmatically, but in the worst case you may have to record and replay keystrokes
or mouse movements.)
There are lots of tools to help with this nowadays, such as Quick Test Pro,
TestComplete, or Selenium for testing, and CruiseControl, Hudson, and TeamCity for
continuous integration. A comprehensive list on the web can be found here:
en. wikipedia. org/wiki/T est_ automation.
10.4. Summary
·
·
·- 0 A a
Ensuring that a system is easily testable has payoffs both in terms of the cost of testing and the
reliability of the system. A vehicle often used to execute the tests is the test harness. Test harnesses are
software systems that encapsulate test resources such as test cases and test infrastructure so that it is
easy to reapply tests across iterations and it is easy to apply the test infrastructure to new increments of
the system. Another vehicle is the creation of test cases prior to the development of a component, so
that developers know which tests their component must pass.
Controlling and observing the system state is a major class of testability tactics. Providing the
ability to do fault injection, to record system state at key portions of the system, to isolate the system
from its environment, and to abstract various resources are all different tactics to support the control and
observation of a system and its components.
Complex systems are difficult to test because of the large state space in which their computations
take place, and because of the larger number of interconnections among the elements of the system.
Consequently, keeping the system simple is another class of tactics that supports testability.
10.5. For Further Reading
·
·
·- 0 A a
An excellent general introduction to software testing is [Beizer 90]. For a more modem take on testing,
and from the software developer's perspective rather than the tester's, Freeman and Pryce cover testdriven
development in the object-oriented realm [Freeman 09].
Bertolino and Strigini [Bertolino 96] are the developers of the model of testing shown in Figure
1 0 . 1 .
Yin and Bieman [Yin 94] have written about executable assertions. Hartman [Hartman 1 0]
describes a technique for using executable assertions as a means for detecting race conditions.
Bruntink and van Deursen [Bruntink 06] write about the impact of structure on testing.
Jeff V oas 's foundational work on testability and the relationship between testability and reliability is
worthwhile. There are several papers to choose from, but [Voas 95] is a good start that will point you to
others.
10.6. Discussion Questions
·
·
·- 0 A a
1 . A testable system is one that gives up its faults easily. That is, if a system contains a fault, then it
doesn' t take long or much effort to make that fault show up. On the other hand, fault tolerance is
all about designing systems that jealously hide their faults; there, the whole idea is to make it very
difficult for a system to reveal its faults. Is it possible to design a system that is both highly
testable and highly fault tolerant, or are these two design goals inherently incompatible? Discuss.
2. "Once my system is in routine use by end users, it should not be highly testable, because if it still
contains faults and all systems probably do then I don't want them to be easily revealed."
Discuss.
3. Many of the tactics for testability are also useful for achieving modifiability. Why do you think
t h at 1. s?.
4. Write some concrete testability scenarios for an automatic teller machine. How would you
modify your design for the automatic teller machine to accommodate these scenarios?
5. What other quality attributes do you think testability is most in conflict with? What other quality
attributes do you think testability is most compatible with?
6. One of our tactics is to limit nondeterminism. One method is to use locking to enforce
synchronization. What impact does the use of locks have on other quality attributes?
7. Suppose you're building the next great social networking system. You anticipate that within a
month of your debut, you will have half a million users. You can't pay half a million people to
test your system, and yet it has to be robust and easy to use when all half a million are banging
away at it. What should you do? What tactics will help you? Write a testability scenario for this
social networking system.
8. Suppose you use executable assertions to improve testability. Make a case for, and then a case
against, allowing the assertions to run in the production system as opposed to removing them
after testing.
1 1 . Usability
·
·
·- 0
Any darn fool can make something complex; it
takes a genius to make something simple.
-Albert Einstein
A a
Usability is concerned with how easy it is for the user to accomplish a desired task and the kind of user
support the system provides. Over the years, a focus on usability has shown itself to be one of the
cheapest and easiest ways to improve a system's quality (or more precisely, the user's perception of
quality).
Usability comprises the following areas:
• Learning system features. If the user is unfamiliar with a particular system or a particular aspect
of it, what can the system do to make the task of learning easier? This might include providing
help features.
• Using a system efficiently. What can the syste1n do to make the user more efficient in its
operation? This might include the ability for the user to redirect the system after issuing a
command. For example, the user may wish to suspend one task, perform several operations, and
then resume that task.
• Minimizing the impact of errors. What can the system do so that a user error has minimal
impact? For example, the user may wish to cancel a command issued incorrectly.
• Adapting the system to user needs. How can the user (or the system itself) adapt to make the
user's task easier? For example, the system may automatically fill in URLs based on a user's
past entries.
• Increasing confidence and satisfaction. What does the system do to give the user confidence
that the correct action is being taken? For example, providing feedback that indicates that the
system is performing a long-running task and the extent to which the task is completed will
increase the user's confidence in the system.
11.1. Usability General Scenario
The portions of the usability general scenarios are these:
·
·
·- 0 A a
• Source of stimulus. The end user (who may be in a specialized role, such as a system or
network administrator) is always the source of the stimulus for usability.
• Stimulus. The stimulus is that the end user wishes to use a system efficiently, learn to use the
system, minimize the impact of errors, adapt the system, or configure the system.
• Environment. The user actions with which usability is concerned always occur at runtitne or at
system configuration time.
• Artifact. The artifact is the system or the specific portion of the system with which the user is
interacting.
• Response. The system should either provide the user with the features needed or anticipate the
user's needs.
• Response measure. The response is measured by task time, number of errors, number of tasks
accomplished, user satisfaction, gain of user knowledge, ratio of successful operations to total
operations, or amount of time or data lost when an error occurs.
Table 1 1 . 1 enumerates the elements of the general scenario that characterize usability.
Table 1 1 . 1 . Usability General Scenario
Portion of Scenario
Source
Stimulus
Environment
A rUt acts
Response
Response Measure
Possible Values
End user, possibly in a specialized role
End user tries to use a system efficiently, learn to use the
system, minimize the impact of errors, adapt the system, or
configure the system.
Runtime or configuration time
System or the specific portion of the system with which the
user is Interacting
The system should either provide the user with the features
needed or anticipate the user's needs.
One or more of the following : task time, number of errors,
number of tasks accomplished, user satisfaction, gain of user
knowledge, ratio of successful operations to total operations,
or amount of time or data lost when an error occurs
Figure 1 1 . 1 gives an example of a concrete usability scenario that you could generate using Table
1 1 . 1 : The user downloads a new application and is using it productively after two minutes of
experimentation.
Stimulus:
Downloads
a New
Source:
Application
User
A·rtifact:
System
E nv·ironment:
Runtime
Response:
User Uses
Application
Productive􀃱y
·
·
·-
Figure 1 1 . 1 . Sample concrete usability scenario
0
Response
Measure:
Within Two
Minutes of
Experimentation
A a
1 1 .2. Tactics for Usability
·
·
·- 0 A a
Recall that usability is concerned with how easy it is for the user to accomplish a desired task, as well as
the kind of support the system provides to the user. Researchers in human-computer interaction have
used the terms user initiative, system initiative, and mixed initiative to describe which of the humancomputer
pair takes the initiative in performing certain actions and how the interaction proceeds.
Usability scenarios can combine initiatives from both perspectives. For example, when canceling a
command, the user issues a cancel user initiative and the system responds. During the cancel,
however, the system may put up a progress indicator system initiative. Thus, cancel may demonstrate
mixed initiative. We use this distinction between user and system initiative to discuss the tactics that the
architect uses to achieve the various scenarios.
Figure 1 1 .2 shows the goal of the set of runtime usability tactics.
User
Request
Tactics
to Control
Usability
User Given
Appropriate
Feedback and
Assistance
Figure 11.2. The goal of runtime usability tactics
Separate the User Interface!
One of the most helpful things an architect can do to make a system usable is to
facilitate experimentation with the user interface via the construction of rapid
prototypes. Building a prototype, or several prototypes, to let real users experience the
interface and give their feedback pays enormous dividends. The best way to do this is
to design the software so that the user interface can be quickly changed.
Tactics for modifiability that we saw in Chapter 7 support this goal perfectly well,
especially these:
• Increase semantic coherence, encapsulate, and co-locate related responsibilities,
which localize user interface responsibilities to a single place
• Restrict dependencies, which minimizes the ripple effect to other software when the
user interface changes
• Defer binding, which lets you make critical user interface choices without having to
recode
Defer binding is especially helpful here, because you can expect that your product's
user interface will face pressure to change during testing and even after it goes to
market.
·
·
·- 0
User interface generation tools are consistent with these tactics; most produce a
single module with an abstract interface to the rest of the software. Many provide the
capability to change the user interface after compile time. You can do your part by
restricting dependencies on the generated module, should you later decide to adopt a
different tool.
A a
Much work in different user interface separation patterns occurred in the 1 980s and
90s. With the advent of the web and the modernization of the model-view-controller
(MVC) pattern to reflect web interfaces, MVC has become the dominant separation
pattern. Now the MVC pattern is built into a wide variety of different frameworks. (See
Chapter 14 for a discussion ofMVC.) MVC makes it easy to provide multiple views of
the data, supporting user initiative, as we discuss next.
Many times quality attributes are in conflict with each other. Usability and
modifiability, on the other hand, often complement each other, because one of the best
ways to make a system more usable is to make it modifiable. However, this is not
always the case. In many systems business rules drive the UI for example, specifying
how to validate input. To realize this validation, the UI may need to call a server (which
can negatively affect performance). To get around this performance penalty, the
architect may choose to duplicate these rules in the client and the server, which then
makes evolution difficult. Alas, the architect's life is never easy!
There is a connection between the achievement of usability and modifiability. The user interface
design process consists of generating and then testing a user interface design. Deficiencies in the design
are corrected and the process repeats. If the user interface has already been constructed as a portion of
the system, then the system must be modified to reflect the latest design. Hence the connection with
modifiability. This connection has resulted in standard patterns to support user interface design (see
sidebar).
Support User Initiative
Once a system is executing, usability is enhanced by giving the user feedback as to what the system is
doing and by allowing the user to make appropriate responses. For example, the tactics described next
-cancel, undo, pause/resume, and aggregate-support the user in either correcting errors or being
more efficient.
The architect designs a response for user initiative by enumerating and allocating the responsibilities
of the system to respond to the user command. Here are some common examples of user initiative:
• Cancel. When the user issues a cancel command, the system must be listening for it (thus, there
is the responsibility to have a constant listener that is not blocked by the actions of whatever is
being canceled); the command being canceled must be terminated; any resources being used by
the canceled command must be freed; and components that are collaborating with the canceled
command must be informed so that they can also take appropriate action.
• Undo. To support the ability to undo, the system must maintain a sufficient amount of
information about system state so that an earlier state may be restored, at the user's request.
·
·
·- 0 A a
Such a record may be in the form of state "snapshots" for example, checkpoints or as a set
of reversible operations. Not all operations can be easily reversed: for example, changing all
occurrences of the letter "a" to the letter "b" in a document cannot be reversed by changing all
instances of "b" to "a", because s01ne of those instances of "b" may have existed prior to the
original change. In such a case the system must maintain a more elaborate record of the change.
Of course, s01ne operations, such as ringing a bell, cannot be undone.
• Pause/resume. When a user has initiated a long-running operation say, downloading a large
file or set of files from a server it is often useful to provide the ability to pause and resume the
operation. Effectively pausing a long-running operation requires the ability to temporarily free
resources so that they may be reallocated to other tasks.
• Aggregate. When a user is performing repetitive operations, or operations that affect a large
number of objects in the same way, it is useful to provide the ability to aggregate the lowerlevel
objects into a single group, so that the operation may be applied to the group, thus freeing
the user from the drudgery (and potential for mistakes) of doing the same operation repeatedly.
For example, aggregate all of the objects in a slide and change the text to 14-point font.
Support System Initiative
When the system takes the initiative, it must rely on a model of the user, the task being undertaken by
the user, or the system state itself. Each model requires various types of input to accomplish its
initiative. The support system initiative tactics are those that identify the models the system uses to
predict either its own behavior or the user's intention. Encapsulating this information will make it easier
for it to be tailored or modified. Tailoring and modification can be either dynamically based on past
user behavior or offline during development. These tactics are the following:
• Maintain task model. The task model is used to determine context so the system can have some
idea of what the user is attempting and provide assistance. For example, knowing that sentences
start with capital letters would allow an application to correct a lowercase letter in that position.
• Maintain user model. This model explicitly represents the user's knowledge of the system, the
user's behavior in terms of expected response time, and other aspects specific to a user or a
class of users. For example, maintaining a user model allows the system to pace mouse
selection so that not all of the document is selected when scrolling is required. Or a model can
control the amount of assistance and suggestions automatically provided to a user. A special
case of this tactic is comtnonly found in user interface customization, wherein a user can
explicitly modify the system's user tnodel.
• Maintain system model. Here the system maintains an explicit model of itself. This is used to
determine expected system behavior so that appropriate feedback can be given to the user. A
common manifestation of a system model is a progress bar that predicts the time needed to
complete the current activity.
Figure 1 1 .3 shows a summary of the tactics to achieve usability.
Usability Tactics
Figure 1 1 .3. Usability tactics
·
·
·- 0 A a
·
·
·- 0
1 1 .3. A Design Checklist for Usability
Table 1 1 .2 is a checklist to support the design and analysis process for usability.
Table 1 1 .2. Checklist to Support the Design and Analysis Process for Usability
Category
Allocation of
Responsibilities
Coordination Model
Data Model
Mapping among
Architectural
Elements
Checklist
Ensure that additional system responsibiliities have been
allocated, as needed, to assist the user in the following:
• Learnrng how to use the system
• Effici·ently achieving the task at hand
• Adapting and configuring the system
• Recovering from user and system errors
Determine whether the properti.es of system elements'
coordination-timeliness, cu rrency, completeness,
correctness, consistency􀈷affect how a user learns to use
the system, achieves goals or completes tasks, adapts
and configures the system, recovers from user and system
errors, and gains increased confidence and satisfaction.
For example, can the system respond to mouse events
and give semantic feedbacK fn real time? Can long-running
events be canceled i n a masonable amount of time?
Determine the major data abstractions that are involved
with user-perceivable be:havior. Ensure these major data
abstractions, their operations, and their properties have
been designed to assist the user 1in achieving the tasK at
hand, adapting and config.uring the system. recovering from
user and system errors, learni'ng how to use the system, and
increasing satisfaction and user confidence.
For example, the data abstractions should be designed
to support undo and cancel operations: the transaction
granularity should not be so great that canceling or undoing
an operation takes an excessiveiy long time.
Determine· what mapping among architectural elements Is
visible to the end user (for example, the extent to which the
end user is aware of which services are local and which
are remote). For those that are visible, determine how this
affects the ways in which, or the ease with which, the user
A a
Resource
Management
Binding Time
Choice of Technology
·
·
·- 0
will team how to use the system, achieve the task at hand,
adapt and configure the system, recover from user and
system errors, and increase confidence and satisfaction.
Determlne how the user can adapt and configure the
system's use of resources. Ensure that resource limitations
under all user􀈸 controlled configurations will not make users
less likely to achieve their tasks. For example, attempt to
avoid con􀈹iguratlons that would result in excessively long
response times. Ensure that the level of resources will not
affect the users' ability to learn how to use the system, or
decrease their level of confidence and satisfaction with the
system.
Determine, which binding time decisions should oe under
user control and ensure that users can make decisions
that aid in usability. For example, if the user can choose. at
runtime, the system's conflgurat!on, or its communfcation
protocols, or its functionality via plug-ins, you need to ensure
that such choices do not adversely affect the user's ability to
learn system features, use the system effiCiently, minimize
the impact of errors. further adapt and configure the system,
or increase confidence and satisfaction.
Ensure the chosen technologies help to achieve the usability
scenarios that apply to your system. For example, do these
technologies aid in the creaUon of o111llne help, the production
of training materials, and the collection of user feedback?
How usable are any of your chosen technologies? Ensure
the chosen technologies do not adversely affect the usability
of the system (in terms of learning system features, using the
system efficJently, minimizing the impact of errors, adapting/
configuring the system, and increasing confidence and
satisfaction).
A a
1 1 .4. Summary
·
·
·- 0 A a
Architectural support for usability involves both allowing the user to take the initiative in
circumstances such as canceling a long-running command or undoing a completed command and
aggregating data and commands.
To be able to predict user or system responses, the system must keep an explicit model of the user,
the system, and the task.
There is a strong relationship between supporting the user interface design process and supporting
modifiability; this relation is promoted by patterns that enforce separation of the user interface from the
rest of the system, such as the MVC pattern.
1 1.5. For Further Reading
·
·
·- 0 A a
Claire Marie Karat has investigated the relation between usability and business advantage [Karat 94].
Jakob Nielsen has also written extensively on this topic, including a calculation on the ROI of
usability [Nielsen 08].
Bonnie John and Len Bass have investigated the relation between usability and software
architecture. They have enumerated around two dozen usability scenarios that have architectural impact
and given associated patterns for these scenarios [Bass 03].
Greg Hartman has defined attentiveness as the ability of the system to support user initiative and
allow cancel or pause/resume [Hartman 1 0].
Some of the patterns for separating the user interface are Arch/Slinky, Seeheim, and PAC. These are
discussed in Chapter 8 of Human-Computer Interaction [Dix 04].
1 1 .6. Discussion Questions
·
·
·- 0 A a
1. Write a concrete usability scenario for your automobile that specifies how long it takes you to set
your favorite radio stations? Now consider another part of the driver experience and create
scenarios that test other aspects of the response measures from the general scenario table.
2. Write a concrete usability scenario for an automatic teller machine. How would your design be
modified to satisfy these scenarios?
3. How might usability trade off against security? How might it trade off against performance?
4. Pick a few of your favorite web sites that do similar things, such as social networking or online
shopping. Now pick one or two appropriate responses from the usability general scenario (such as
"achieve the task at hand") and a correspondingly appropriate response measure. Using the
response and response measure you chose, compare the web sites' usability.
5. Specify the data model for a four-function calculator that allows undo.
6. Why is it that in so many systems, the cancel button in a dialog box appears to be unresponsive?
What architectural principles do you think were ignored in these systems?
7. Why do you think that progress bars frequently behave erratically, moving from 1 0 to 90 percent
in one step and then getting stuck on 90 percent?
8. Research the crash of Air France Flight 296 into the forest at Habsheim, France, on June 26,
1 988. The pilots said they were unable to read the digital display of the radio altimeter or hear its
audible readout. If they could have, do you believe the crash would have been averted? In this
context, discuss the relationship between usability and safety.
12. Other Quality Attributes
·
·
·-
Quality is not an act, it is a habit.
-Aristotle
0 A a
Chapters 5-1 1 each dealt with a particular quality attribute important to software systems. Each of those
chapters discussed how its particular quality attribute is defined, gave a general scenario for that quality
attribute, and showed how to write specific scenarios to express precise shades of meaning concerning
that quality attribute. And each gave a collection of techniques to achieve that quality attribute in an
architecture. In short, each chapter presented a kind of portfolio for specifying and designing to achieve
a particular quality attribute.
Those seven chapters covered seven of the most important quality attributes, in terms of their
occurrence in modem software-reliant systems. However, as is no doubt clear, seven only begins to
scratch the surface of the quality attributes that you might find needed in a software system you're
working on.
Is cost a quality attribute? It is not a technical quality attribute, but it certainly affects fitness for use.
We consider economic factors in Chapter 23.
This chapter will give a brief introduction to a few other quality attributes a sort of "B list" of quality
attributes but, more important, show how to build the same kind of specification or design portfolio
for a quality attribute not covered in our list.
12.1. Other Important Quality Attributes
·
·
·- 0
Besides the quality attributes we've covered in depth in Chapters 5-1 1 , some others that arise
frequently are variability, portability, development distributability, scalability and elasticity,
deployability, mobility, and monitorability. We discuss "green" computing in Section 12.3.
Variability
A a
Variability is a special form of modifiability. It refers to the ability of a system and its supporting
artifacts such as requirements, test plans, and configuration specifications to support the production of a
set of variants that differ from each other in a preplanned fashion. Variability is an especially important
quality attribute in a software product line (this will be explored in depth in Chapter 25), where it means
the ability of a core asset to adapt to usages in the different product contexts that are within the product
line scope. The goal of variability in a software product line is to make it easy to build and maintain
products in the product line over a period of time. Scenarios for variability will deal with the binding
time of the variation and the people time to achieve it.
Portability
Portability is also a special form of modifiability. Portability refers to the ease with which software that
was built to run on one platform can be changed to run on a different platform. Portability is achieved
by minimizing platform dependencies in the software, isolating dependencies to well-identified
locations, and writing the software to run on a "virtual machine" (such as a Java Virtual Machine) that
encapsulates all the platform dependencies within. Scenarios describing portability deal with moving
software to a new platform by expending no more than a certain level of effort or by counting the
number of places in the software that would have to change.
Development Distributability
Development distributability is the quality of designing the software to support distributed software
development. Many syste1ns these days are developed using globally distributed teams. One problem
that must be overcome when developing with distributed teams is coordinating their activities. The
system should be designed so that coordination among teams is minimized. This minimal coordination
needs to be achieved both for the code and for the data model. Teams working on modules that
communicate with each other may need to negotiate the interfaces of those modules. When a module is
used by many other modules, each developed by a different team, communication and negotiation
become more complex and burdensome. Similar considerations apply for the data model. Scenarios for
development distributability will deal with the compatibility of the communication structures and data
model of the system being developed and the coordination mechanisms of the organizations doing the
development.
Scalability
Two kinds of scalability are horizontal scalability and vertical scalability. Horizontal scalability (scaling
out) refers to adding more resources to logical units, such as adding another server to a cluster of
servers. Vertical scalability (scaling up) refers to adding more resources to a physical unit, such as
·
·
·- 0 A a
adding more memory to a single computer. The problem that arises with either type of scaling is how to
effectively utilize the additional resources. Being effective means that the additional resources result in a
measurable improvement of some system quality, did not require undue effort to add, and did not
disrupt operations. In cloud environments, horizontal scalability is called elasticity. Elasticity is a
property that enables a customer to add or remove virtual machines from the resource pool (see Chapter
26 for further discussion of such environments). These virtual machines are hosted on a large collection
of upwards of 1 0,000 physical machines that are managed by the cloud provider. Scalability scenarios
will deal with the impact of adding or removing resources, and the measures will reflect associated
availability and the load assigned to existing and new resources.
Deployability
Deployability is concerned with how an executable arrives at a host platform and how it is subsequently
invoked. Some of the issues involved in deploying software are: How does it arrive at its host (push,
where updates are sent to users unbidden, or pull, where users must explicitly request updates)? How is
it integrated into an existing system? Can this be done while the existing system is executing? Mobile
systems have their own problems in terms of how they are updated, because of concerns about
bandwidth. Deployment scenarios will deal with the type of update (push or pull), the form of the
update (medium, such as DVD or Internet download, and packaging, such as executable, app, or plugin),
the resulting integration into an existing system, the efficiency of executing the process, and the
associated risk.
Mobility
Mobility deals with the probletns of tnovement and affordances of a platform (e.g., size, type of display,
type of input devices, availability and volume of bandwidth, and battery life). Issues in mobility include
battery management, reconnecting after a period of disconnection, and the number of different user
interfaces necessary to support multiple platforms. Scenarios will deal with specifying the desired
effects of mobility or the various affordances. Scenarios may also deal with variability, where the same
software is deployed on multiple (perhaps radically different) platforms.
Monitorability
Monitorability deals with the ability of the operations staff to monitor the system while it is executing.
Items such as queue lengths, average transaction processing time, and the health of various components
should be visible to the operations staff so that they can take corrective action in case of potential
problems. Scenarios will deal with a potential problem and its visibility to the operator, and potential
corrective action.
Safety
In 2009 an employee of the Shushenskaya hydroelectric power station in Siberia sent commands over a
network to remotely, and accidentally, activate an unused turbine. The offline turbine created a "water
hammer" that flooded and then destroyed the plant and killed dozens of workers.
The thought that software could kill people used to belong in the realm of kitschy computers-runamok
science fiction. Sadly, it didn't stay there. As software has come to control more and more of the
devices in our lives, software safety has become a critical concern.
·
·
·- 0 A a
Safety is not purely a software concern, but a concern for any system that can affect its
environment. As such it receives mention in Section 12.3, where we discuss system quality attributes.
But there are means to address safety that are wholly in the software realm, which is why we discuss it
here as well.
Software safety is about the software 's ability to avoid entering states that cause or lead to damage,
injury, or loss of life to actors in the software 's environment, and to recover and limit the damage when
it does enter into bad states. Another way to put this is that safety is concerned with the prevention of
and recovery from hazardous failures. Because of this, the architectural concerns with safety are almost
identical to those for availability, which is also about avoiding and recovering from failures. Tactics for
safety, then, overlap with those for availability to a large degree. Both comprise tactics to prevent
failures and to detect and recover from failures that do occur.
Safety is not the same as reliability. A system can be reliable (consistent with its specification) but
still unsafe (for example, when the specification ignores conditions leading to unsafe action). In fact,
paying careful attention to the specification for safety-critical software is perhaps the most powerful
thing you can do to produce safe software. Failures and hazards cannot be detected, prevented, or
ameliorated if the software has not been designed with them in mind. Safety is frequently engineered by
performing failure mode and effects analysis, hazard analysis, and fault tree analysis. (These techniques
are discussed in Chapter 5.) These techniques are intended to discover possible hazards that could result
from the system's operation and provide plans to cope with these hazards.
12.2. Other Categories of Quality Attributes
·
·
·- 0 A a
We have primarily focused on product qualities in our discussions of quality attributes, but there are
other types of quality attributes that measure "goodness" of something other than the final product.
Here are three:
Conceptual Integrity of the Architecture
Conceptual integrity refers to consistency in the design of the architecture, and it contributes to the
understandability of the architecture and leads to fewer errors of confusion. Conceptual integrity
demands that the same thing is done in the same way through the architecture. In an architecture with
conceptual integrity, less is more. For example, there are countless ways that components can send
information to each other: messages, data structures, signaling of events, and so forth. An architecture
with conceptual integrity would feature one way only, and only provide alternatives if there was a
compelling reason to do so. Similarly, components should all report and handle errors in the same way,
log events or transactions in the same way, interact with the user in the same way, and so forth.
Quality in Use
ISO/IEC 250 1 0, which we discuss in Section 12.4, has a category of qualities that pertain to the use of
the system by various stakeholders. For example, time-to-market is an important characteristic of a
system, but it is not discernible from an examination of the product itself. Some of the qualities in this
category are these:
• Effectiveness. This refers to the distinction between building the system correctly (the system
performs according to its requirements) and building the correct system (the system performs in
the manner the user wishes). Effectiveness is a measure of whether the system is correct.
• Efficiency. The effort and time required to develop a system. Put another way, what is the
architecture's impact on the project's cost and schedule? Would a different set of architectural
choices have resulted in a system that would be faster or cheaper to bring to fruition? Efficiency
can include training time for developers; an architecture that uses technology unfamiliar to the
staff on hand is less buildable. Is the architecture appropriate for the organization in terms of its
experience and its available supporting infrastructure (such as test facilities or development
environments)?
• Freedom from risk. The degree to which a product or syste1n affects economic status, human
life, health, or the environment.
A special case of efficiency is how easy it is to build (that is, compile and assemble) the system
after a change. This becomes critical during testing. A recompile process that takes hours or overnight
is a schedule-killer. Architects have control over this by managing dependencies among modules. If the
architect doesn't do this, then what often happens is that some bright-eyed developer writes a make file
early on, it works, and people add to it and add to it. Eventually the project ends up with a seven-hour
compile step and very unhappy integrators and testers who are already behind schedule (because they
always are).
Marketability
·
·
·- 0 A a
An architecture's marketability is another quality attribute of concern. Some systems are well known by
their architectures, and these architectures sometimes carry a meaning all their own, independent of
what other quality attributes they bring to the system. The current craze in building cloud-based systems
has taught us that the perception of an architecture can be more important than the qualities the
architecture brings. Many organizations have felt they had to build cloud-based systems (or some other
technology dujour) whether or not that was the correct technical choice.
·
·
·- 0
12.3. Software Quality Attributes and System Quality Attributes
A a
Physical systems, such as aircraft or automobiles or kitchen appliances, that rely on software embedded
within are designed to meet a whole other litany of quality attributes: weight, size, electric
consumption, power output, pollution output, weather resistance, battery life, and on and on. For many
of these systems, safety tops the list (see the sidebar).
Sometimes the software architecture can have a surprising effect on the system's quality attributes.
For example, software that makes inefficient use of computing resources might require additional
memory, a faster processor, a bigger battery, or even an additional processor. Additional processors can
add to a system's power consumption, weight, required cabinet space, and of course expense.
Green computing is an issue of growing concern. Recently there was a controversy about how much
greenhouse gas was pumped into the atmosphere by Google's massive processor farms. Given the daily
output and the number of daily requests, it is possible to estimate how much greenhouse gas you cause
to be emitted each time you ask Google to perform a search. (Current estimates range from 0.2 grams to
7 grams of C02.) Green computing is all the rage. Eve Troeh, on the Atnerican Public Media show
"Marketplace" (July 5, 201 1 ), reports:
Two percent of all U.S. electricity now goes to data centers, according to the
Environmental Protection Agency. Electricity has become the biggest cost for processing
data more than the equipment to do it, more than the buildings to house that equipment. .
. . Google's making data servers that can float offshore, cooled by ocean breezes. HP has
plans to put data servers near farms, and power them with methane gas from cow pies.
The lesson here is that if you are the architect for software that resides in a larger system, you will
need to understand the quality attributes that are important for the containing syste1n to achieve, and
work with the system architects and engineers to see how your software architecture can contribute to
achieving them.
The Vanishing Line between Software and System Qualities
This is a book about software architecture, and so we treat quality attributes from a
software architect's perspective. But you may have already noticed that the quality
attributes that the software architect can bring to the party are limited by the
architecture of the system in which the software runs.
For example:
• The performance of a piece of software is fundamentally constrained by the
performance of the computer that runs it. No matter how well you design the
software, you just can't run the latest whole-earth weather forecasting models on
Grampa' s Commodore 64 and hope to know if it's going to rain tomorrow.
• Physical security is probably more important and more effective than software
security at preventing fraud and theft. If you don't believe this, write your laptop's
password on a slip of paper, tape it to your laptop, and leave it in an unlocked car
with the windows down. (Actually, don't really do that. Consider this a thought
experiment.)
·
·
·- 0 A a
• If we're being perfectly honest here, how usable is a device for web browsing that
has a screen smaller than a credit card and keys the size of a raisin?
For me, nowhere is the barrier between software and system more nebulous than in
the area of safety. The thought that software strings of 0' s and 1 's can kill or maim
or destroy is still an unnatural notion. Of course, it's not the 0' s and 1 's that wreak
havoc. At least, not directly. It's what they're connected to. Software, and the system in
which it runs, has to be connected to the outside world in some way before it can do
damage. That's the good news. The bad news is that the good news isn't all that good.
Software is connected to the outside world, always. If your program has no effect
whatsoever that is observable outside of itself, it probably serves no purpose.
There are notorious examples of software-related failures. The Siberian
hydroelectric plant catastrophe mentioned in the text, the Therac-25 fatal radiation
overdose, the Ariane 5 explosion, and a hundred lesser known accidents all caused
harm because the software was part of a system that included a turbine, an X-ray
emitter, or a rocket's steering controls, in the examples just cited. In these cases, flawed
software commanded some hardware in the system to take a disastrous action, and the
hardware simply obeyed. Actuators are devices that connect hardware to software; they
are the bridge between the world of 0' s and 1 's and the world of motion and control.
Send a digital value to an actuator (or write a bit string in the hardware register
corresponding to the actuator) and that value is translated to some mechanical action,
for better or worse.
But connection to an actuator is not required for software-related disasters.
Sometimes all the computer has to do is send erroneous information to its human
operators. In September 1 983, a Soviet satellite sent data to its ground system
computer, which interpreted that data as a missile launched from the United States
aimed at Moscow. Seconds later, the computer reported a second missile in flight.
Soon, a third, then a fourth, and then a fifth appeared. Soviet Strategic Rocket Forces
lieutenant colonel Stanislav Yevgrafovich Petrov made the astonishing decision to
ignore the warning system, believing it to be in error. He thought it extremely unlikely
that the U.S. would have fired just a few missiles, thereby inviting total retaliatory
destruction. He decided to wait it out, to see if the missiles were real that is, to see if
his country's capital city was going to be incinerated. As we know, it wasn't. The
Soviet system had mistaken a rare sunlight condition for missiles in flight. Similar
mistakes have occurred on the U.S. side.
Of course, the humans don't always get it right. On the dark and stormy night of
June 1 , 2009, Air France flight 447 from Rio de Janeiro to Paris plummeted into the
Atlantic Ocean, killing all on board. The Airbus A-330's flight recorders were not
recovered until May 20 1 1 , and as this book goes to publication it appears that the pilots
never knew that the aircraft had entered a high-altitude stall. The sensors that measure
airspeed had become clogged with ice and therefore unreliable. The software was
·
·
·- 0 A a
required to disengage the autopilot in this situation, which it did. The human pilots
thought the aircraft was going too fast (and in danger of structural failure) when in fact
it was going too slow (and falling). During the entire three-minute-plus plunge from
38,000 feet, the pilots kept trying to pull the nose up and throttles back to lower the
speed. It's a good bet that adding to the confusion was the way the A-330's stall
warning system worked. When the system detects a stall, it emits a loud audible alarm.
The computers deactivate the stall warning when they "think" that the angle of attack
measurements are invalid. This can occur when the airspeed readings are very low.
That is exactly what happened with Air France 44 7 : Its forward speed dropped below
60 knots, and the angle of attack was extremely high. As a consequence of a rule in the
flight control software, the stall warning stopped and started several times. Worse, it
came on whenever the pilot let the nose fall a bit (increasing the airspeed and taking the
readings into the "valid" range, but still in stall) and then stopped when he pulled back.
That is, doing the right thing resulted in the wrong feedback and vice versa.
Was this an unsafe system, or a safe system unsafely operated? Ultimately the courts
will decide.
Software that can physically harm us is a fact of our modern life. Sometimes the link
between software and physical harm is direct, as in the Ariane example, and sometimes
it's much more tenuous, as in the Air France 44 7 example. But as software
professionals, we cannot take refuge in the fact that our software can't actually inflict
harm any more than the person who shouts "Fire!" in a crowded theater can claim it
was the stampede, not the shout, that caused injury.
-PCC
·
·
·-
12.4. Using Standard Lists of Quality Attributes or Not
0 A a
Architects have no shortage of lists of quality attributes for software systems at their disposal. The
standard with the pause-and-take-a-breath title of "ISO/IEC FCD 25010: Systems and software
engineering Systems and software product Quality Requirements and Evaluation (SQuaRE) System
and software quality models," is a good example. The standard divides quality attributes into those
supporting a "quality in use" model and those supporting a "product quality" model. That division is a
bit of a stretch in some places, but nevertheless begins a divide-and-conquer march through a
breathtaking array of qualities. See Figure 1 2 . 1 for this array.
Functional
suilab<l<ly
Functoinal
correctness
Functional
appropriateii9ss "' •• - J
Compalib<lily I '
Usab<lily
Appropnateness
recogn<zabihty
Operablllly
Aeliablhty Securoty I \ PoN􀂅;IItY.,
.
,_
Matunty
Avatlab<llly ,-􀞝 'lnstallabitlty. 1
-·
'
Fault tolerance Aeplac9abihty
Figure 12.1. The ISO/IEC FCD 25010 product quality standard
The standard lists the following quality attributes that deal with product quality:
• Functional suitability. The degree to which a product or system provides functions that meet
stated and itnplied needs when used under specified conditions
• Performance efficiency. Performance relative to the amount of resources used under stated
conditions
• Compatibility. The degree to which a product, system, or component can exchange information
with other products, systems, or components, and/or perform its required functions, while
sharing the same hardware or software environment
• Usability. The degree to which a product or system can be used by specified users to achieve
specified goals with effectiveness, efficiency, and satisfaction in a specified context of use
• Reliability. The degree to which a system, product, or component performs specified functions
under specified conditions for a specified period of time
• Security. The degree to which a product or system protects information and data so that persons
or other products or systems have the degree of data access appropriate to their types and levels
of authorization
·
·
·- 0 A a
• Maintainability. The degree of effectiveness and efficiency with which a product or system can
be modified by the intended maintainers
• Portability. The degree of effectiveness and efficiency with which a system, product, or
component can be transferred from one hardware, software, or other operational or usage
environment to another
In ISO 250 1 0, these "quality characteristics" are each composed of "quality subcharacteristics" (for
example, nonrepudiation is a subcharacteristic of security). The standard slogs through almost five
dozen separate descriptions of quality subcharacteristics in this way. It defines for us the qualities of
"pleasure" and "comfort." It distinguishes among "functional correctness" and "functional
completeness," and then adds "functional appropriateness" for good measure. To exhibit
"compatibility," systems must either have "interoperability" or just plain "coexistence." "Usability" is a
product quality, not a quality-in-use quality, although it includes "satisfaction," which is a quality-inuse
quality. "Modifiability" and "testability" are both part of "maintainability." So is "modularity,"
which is a strategy for achieving a quality rather than a goal in its own right. "Availability" is part of
"reliability." "Interoperability" is part of "compatibility." And "scalability" isn't mentioned at all.
Got all that?
Lists like these and there are many do serve a purpose. They can be helpful checklists to assist
requirements gatherers in making sure that no important needs were overlooked. Even more useful than
standalone lists, they can serve as the basis for creating your own checklist that contains the quality
attributes of concern in your domain, your industry, your organization, and your products. Quality
attribute lists can also serve as the basis for establishing measures. If "pleasure" turns out to be an
important concern in your system, how do you measure it to know if your system is providing enough
of it?
However, general lists like these also have drawbacks. First, no list will ever be complete. As an
architect, you will be called upon to design a system to meet a stakeholder concern not foreseen by any
list-maker. For example, some writers speak of "manageability," which expresses how easy it is for
system administrators to manage the application. This can be achieved by inserting useful
instrumentation for monitoring operation and for debugging and performance tuning. We know of an
architecture that was designed with the conscious goal of retaining key staff and attracting talented new
hires to a quiet region of the American Midwest. That system's architects spoke of imbuing the system
with "Iowability." They achieved it by bringing in state-of-the-art technology and giving their
development teams wide creative latitude. Good luck finding "Iowability" in any standard list of quality
attributes, but that QA was as important to that organization as any other.
Second, lists often generate more controversy than understanding. You might argue persuasively
that "functional correctness" should be part of "reliability," or that "portability" is just a kind of
"modifiability," or that "maintainability" is a kind of"modifiability" (not the other way around). The
writers of ISO 250 1 0 apparently spent time and effort deciding to make security its own characteristic,
instead of a subcharacteristic of functionality, which it was in a previous version. We believe that effort
in making these arguments could be better spent elsewhere.
Third, these lists often purport to be taxonomies, which are lists with the special property that every
·
·
·- 0 A a
member can be assigned to exactly one place. Quality attributes are notoriously squishy in this regard.
We discussed denial of service as being part of security, availability, performance, and usability in
Chapter 4.
Finally, these lists force architects to pay attention to every quality attribute on the list, even if only
to finally decide that the particular quality attribute is irrelevant to their system. Knowing how to
quickly decide that a quality attribute is irrelevant to a specific system is a skill gained over time.
These observations reinforce the lesson introduced in Chapter 4 that quality attribute names, by
themselves, are largely useless and are at best invitations to begin a conversation; that spending time
worrying about what qualities are subqualities of what other qualities is also almost useless; and that
scenarios provide the best way for us to specify precisely what we mean when we speak of a quality
attribute.
Use standard lists of quality attributes to the extent that they are helpful as checklists, but don't feel
the need to slavishly adhere to their terminology.
·
·
·- 0 A a
12.5. Dealing with "X-ability'': Bringing a New Quality Attribute into the Fold
Suppose, as an architect, you must deal with a quality attribute for which there is no compact body of
knowledge, no "portfolio" like Chapters 5-1 1 provided for those seven QAs? Suppose you find yourself
having to deal with a quality attribute like "green computing" or "manageability" or even "Iowability"?
What do you do?
Capture Scenarios for the New Quality Attribute
The first thing to do is interview the stakeholders whose concerns have led to the need for this quality
attribute. You can work with them, either individually or as a group, to build a set of attribute
characterizations that refine what is meant by the QA. For example, security is often decomposed into
concerns such as confidentiality, integrity, availability, and others. After that refinement, you can work
with the stakeholders to craft a set of specific scenarios that characterize what is meant by that QA.
Once you have a set of specific scenarios, then you can work to generalize the collection. Look at
the set of stimuli you've collected, the set of responses, the set of response measures, and so on. Use
those to construct a general scenario by making each part of the general scenario a generalization of the
specific instances you collected.
In our experience, the steps described so far tend to consume about half a day.
Assemble Design Approaches for the New Quality Attribute
After you have a set of guiding scenarios for the QA, you can assemble a set of design approaches for
dealing with it. You can do this by
1. Revisiting a body of patterns you're familiar with and asking yourself how each one affects
the QA of interest.
2. Searching for designs that have had to deal with this QA. You can search on the name you've
given the QA itself, but you can also search for the terms you chose when you refined the QA
into subsidiary attribute characterizations (such as "confidentiality" for the QA of security).
3. Finding experts in this area and interviewing them or simply writing and asking them for
advice.
4. Using the general scenario to try to catalog a list of design approaches to produce the
responses in the response category.
5. Using the general scenario to catalog a list of ways in which a problematic architecture would
fail to produce the desired responses, and thinking of design approaches to head off those
cases.
Model the New Quality Attribute
If you can build a conceptual model of the quality attribute, this can be helpful in creating a set of
design approaches for it. By "model," we don't mean anything more than understanding the set of
parameters to which the quality attribute is sensitive. For example, a model of modifiability might tell
us that modifiability is a function of how many places in a system have to be changed in response to a
·
·
·- 0 A a
modification, and the interconnectedness of those places. A model for performance might tell us that
throughput is a function of transactional workload, the dependencies among the transactions, and the
number of transactions that can be processed in parallel.
Once you have a model for your QA, then you can work to catalog the architectural approaches
(tactics and patterns) open to you for manipulating each of the relevant parameters in your favor.
Assemble a Set of Tactics for the New Quality Attribute
There are two sources that can be used to derive tactics for any quality attribute: models and experts.
Figure 12.2 shows a queuing model for performance. Such models are widely used to analyze the
latency and throughput of various types of queuing systems, including manufacturing and service
environments, as well as computer systems.
--· Arrtvals
Queue
Scheduling _
algorithm
Server
Routtng of ·
messages
Figure 12.2. A generic queuing model
Results
Within this model, there are seven parameters that can affect the latency that the model predicts:
• Arrival rate
• Queuing discipline
• Scheduling algorithm
• Service time
• Topology
• Network bandwidth
• Routing algorithm
These are the only parameters that can affect latency within this model. This is what gives the model
its power. Furthermore, each of these parameters can be affected by various architectural decisions.
This is what makes the model useful for an architect. For example, the routing algorithm can be fixed or
it could be a load-balancing algorithm. A scheduling algorithm must be chosen. The topology can be
·
·
·-
affected by dynamically adding or removing new servers. And so forth.
The process of generating tactics based on a model is this:
• Enumerate the paratneters of the model
0 A a
• For each paratneter, enumerate the architectural decisions that can affect this parameter
What results is a list of tactics to, in the example case, control performance and, in the more general
case, to control the quality attribute that the model is concerned with. This makes the design problem
seem much more tractable. This list of tactics is finite and reasonably small, because the number of
parameters of the model is bounded, and for each parameter, the number of architectural decisions to
affect the parameter is limited.
Deriving tactics from models is fine as long as the quality attribute in question has a model.
Unfortunately, the number of such models is limited and is a subject of active research. There are no
good architectural models for usability or security, for example. In the cases where we had no model to
work from, we did four things to catalog the tactics:
1. We interviewed experts in the field, asking them what they do as architects to improve the
quality attribute response.
2. We examined systems that were touted as having high usability (or testability, or whatever
tactic we were focusing on).
3. We scoured the relevant design literature looking for common themes in design.
4. We examined documented architectural patterns to look for ways they achieved the quality
attribute responses touted for them.
Construct Design Checklists for the New Quality Attribute
Finally, examine the seven categories of design decisions in Chapter 4 and ask yourself (or your
experts) how to specialize your new quality of interest to these categories. In particular, think about
reviewing a software architecture and trying to figure out how well it satisfies your new qualities in
these seven categories. What questions would you ask the architect of that system to understand how
the design attempts to achieve the new quality? These are the basis for the design checklist.
12.6. For Further Reading
·
·
·- 0 A a
For most of the quality attributes we discussed in this chapter, the Internet is your friend. You can find
reasonable discussions of scalability, portability, and deployment strategies using your favorite search
engine. Mobility is harder to find because it has so many meanings, but look under "mobile computing"
as a start.
Distributed development is a topic covered in the International Conference on Global Software
Engineering, and looking at the proceedings of this conference will give you access to the latest
research in this area (www.icgse.org).
Release It! (Nygard 07] has a good discussion of monitorability (which he calls transparency) as
well as potential problems that are manifested after extended operation of a system. The book also
includes various patterns for dealing with some of the problems.
To gain an appreciation for the importance of software safety, we suggest reading some of the
disaster stories that arise when software fails. A venerable source i s the ACM Risks Forum news group,
known as comp.risks in the USENET community, available at www.risks.org. This list has been
moderated by Peter Neumann since 1 985 and is still going strong.
Nancy Leveson is an undisputed thought leader in the area of software and safety. If you're working
in safety-critical systems, you should become familiar with her work. You can start small with a paper
like [Leveson 04], which discusses a number of software-related factors that have contributed to
spacecraft accidents. Or you can start at the top with [Leveson 1 1], a book that treats safety in the
context oftoday's complex, sociotechnical, software-intensive systems.
The Federal Aviation Administration is the U.S. government agency charged with oversight of the
U.S. airspace system, and the agency is extremely concerned about safety. Their 2000 System Safety
Handbook is a good practical overview of the topic [FAA 00].
IEEE STD-1228- 1 994 ("Software Safety Plans") defines best practices for conducting software
safety hazard analyses, to help ensure that requirements and attributes are specified for safety-critical
software [IEEE 94]. The aeronautical standard D0-178B (due to be replaced by D0- 1 78C as this book
goes to publication) covers software safety requirements for aerospace applications.
A discussion of safety tactics can be found in the work of Wu and Kelly [Wu 06].
In particular, interlocks are an important tactic for safety. They enforce some safe sequence of
events, or ensure that a safe condition exists before an action is taken. Your microwave oven shuts off
when you open the door because of a hardware interlock. Interlocks can be implemented in software
also. For an interesting case study of this, see [Wozniak 07].
12.7. Discussion Questions
·
·
·- 0 A a
1. The Kingdom of Bhutan measures the happiness of its population, and government policy is
formulated to increase Bhutan's GNH (gross national happiness). Go read about how the GNH is
measured (try www.grossnationalhappiness.com) and then sketch a general scenario for the
quality attribute of happiness that will let you express concrete happiness requirements for a
software system.
2. Choose a quality attribute not described in Chapters 5-1 1 . For that quality attribute, assemble a
set of specific scenarios that describe what you mean by it. Use that set of scenarios to construct a
general scenario for it.
3. For the QA you chose for discussion question 2, assemble a set of design approaches (patterns
and tactics) that help you achieve it.
4. For the QA you chose for discussion question 2, develop a design checklist for that quality
attribute using the seven categories of guiding quality design decisions outlined in Chapter 4.
5. What might cause you to add a tactic or pattern to the sets of quality attributes already described
in Chapters 5-1 1 (or any other quality attribute, for that matter)?
6. According to slate.com and other sources, a teenage girl in Germany "went into hiding after she
forgot to set her Facebook birthday invitation to private and accidentally invited the entire
Internet. After 1 5,000 people confirmed they were coming, the girl 's parents canceled the party,
notified police, and hired private security to guard their home." Fifteen hundred people showed
up anyway; several minor injuries ensued. Is Facebook "unsafe"? Discuss.
7. Author James Gleick ("A Bug and a Crash," www.around.com/ariane.html) writes that "It took
the European Space Agency 1 0 years and $7 billion to produce Ariane 5, a giant rocket capable
of hurling a pair of three-ton satellites into orbit with each launch .. . . All it took to explode that
rocket less than a minute into its maiden voyage . . . was a small computer program trying to stuff
a 64-bit number into a 1 6-bit space. One bug, one crash. Of all the careless lines of code recorded
in the annals of computer science, this one may stand as the most devastatingly efficient." Write a
safety scenario that addresses the Ariane 5 disaster and discuss tactics that might have prevented
it.
8. Discuss how you think development distributability tends to "trade off' against the quality
attributes of performance, availability, modifiability, and interoperability.
Extra Credit: Close your eyes and, without peeking, spell "distributability." Bonus points for
successfully saying "development distributability" three times as fast as you can.
9. What is the relationship between mobility and security?
10. Relate monitorability to observability and controllability, the two parts of testability. Are they the
same? If you want to make your system more of one, can you just optimize for the other?
13. Architectural Tactics and Patterns
·
·
·- 0
I have not failed. I 've just found 10,000 ways that
won 't work.
-Thomas Edison
A a
There are many ways to do design badly, and just a few ways to do it well. Because success in
architectural design is complex and challenging, designers have been looking for ways to capture and
reuse hard-won architectural knowledge. Architectural patterns and tactics are ways of capturing proven
good design structures, so that they can be reused.
Architectural patterns have seen increased interest and attention, from both software practitioners
and theorists, over the past 1 5 years or more. An architectural pattern
• is a package of design decisions that is found repeatedly in practice,
• has known properties that permit reuse, and
• describes a class of architectures.
Because patterns are (by definition) found in practice, one does not invent them; one discovers
them. Cataloging patterns is akin to the job of a Linnaean botanist or zoologist: "discovering" patterns
and describing their shared characteristics. And like the botanist, zoologist, or ecologist, the pattern
cataloger strives to understand how the characteristics lead to different behaviors and different
responses to environmental conditions. For this reason there will never be a complete list of patterns:
patterns spontaneously emerge in reaction to environmental conditions, and as long as those conditions
change, new patterns will emerge.
Architectural design seldom starts from first principles. Experienced architects typically think of
creating an architecture as a process of selecting, tailoring, and combining patterns. The software
architect must decide how to instantiate a pattern how to make it fit with the specific context and the
constraints of the problem.
In Chapters 5-1 1 we have seen a variety of architectural tactics. These are simpler than patterns.
Tactics typically use just a single structure or computational mechanism, and they are meant to address
a single architectural force. For this reason they give more precise control to an architect when making
design decisions than patterns, which typically combine multiple design decisions into a package.
Tactics are the "building blocks" of design, from which architectural patterns are created. Tactics are
atoms and patterns are molecules. Most patterns consist of (are constructed from) several different
tactics. For this reason we say that patterns package tactics.
In this chapter we will take a very brief tour through the patterns universe, touching on some of the
most important and most commonly used patterns for architecture, and we will then look at the
relationships between patterns and tactics: showing how a pattern is constructed from tactics, and
showing how tactics can be used to tailor patterns when the pattern that you find in a book or on a
website doesn't quite address your design needs.

13.1. Architectural Patterns
An architectural pattern establishes a relationship between:
·
·
·- 0
• A context. A recurring, common situation in the world that gives rise to a problem.
A a
• A problem. The problem, appropriately generalized, that arises in the given context. The pattern
description outlines the problem and its variants, and describes any complementary or opposing
forces. The description of the problem often includes quality attributes that must be met.
• A solution. A successful architectural resolution to the problem, appropriately abstracted. The
solution describes the architectural structures that solve the problem, including how to balance
the many forces at work. The solution will describe the responsibilities of and static
relationships atnong elements (using a module structure), or it will describe the runtitne
behavior of and interaction between eletnents (laying out a component-and-connector or
allocation structure). The solution for a pattern is determined and described by:
• A set of element types (for example, data repositories, processes, and objects)
• A set of interaction mechanisms or connectors (for example, method calls, events, or
message bus)
• A topological layout of the components
• A set of semantic constraints covering topology, element behavior, and interaction
mechanisms
The solution description should also make clear what quality attributes are provided by the static
and runtime configurations of elements.
This {context, problem, solution} form constitutes a template for documenting a pattern.
Complex systems exhibit multiple patterns at once. A web-based system might employ a three-tier
client-server architectural pattern, but within this pattern it might also use replication (mirroring),
proxies, caches, firewalls, MVC, and so forth, each of which may employ more patterns and tactics.
And all of these parts of the client-server pattern likely employ layering to internally structure their
software modules.
13.2. Overview of the Patterns Catalog
·
·
·- 0 A a
In this section we list an assortment of useful and widely used patterns. This catalog is not meant to be
exhaustive in fact no such catalog is possible. Rather it is meant to be representative. We show
patterns of runtime elements (such as broker or client-server) and of design-time elements (such as
layers). For each pattern we list the context, problem, and solution. As part of the solution, we briefly
describe the elements, relations, and constraints of each pattern.
Applying a pattern is not an aU-or-nothing proposition. Pattern definitions given in catalogs are
strict, but in practice architects may choose to violate them in small ways when there is a good design
tradeoff to be had (sacrificing a little of whatever the violation cost, but gaining something that the
deviation gained). For example, the layered pattern expressly forbids software in lower layers from
using software in upper layers, but there may be cases (such as to gain some performance) when an
architecture might allow a few specific exceptions.
Patterns can be categorized by the dominant type of elements that they show: module patterns show
modules, component-and-connector (C&C) patterns show components and connectors, and allocation
patterns show a combination of software elements (modules, components, connectors) and nonsoftware
elements. Most published patterns are C&C patterns, but there are module patterns and allocation
patterns as well. We'll begin with the granddaddy of module patterns, the layered pattern.
Module Patterns
Layered Pattern
Context: All complex systems experience the need to develop and evolve portions of the system
independently. For this reason the developers of the system need a clear and well-documented
separation of concerns, so that modules of the system may be independently developed and maintained.
Problem: The software needs to be segmented in such a way that the modules can be developed and
evolved separately with little interaction among the parts, supporting portability, modifiability, and
reuse.
Solution: To achieve this separation of concerns, the layered pattern divides the software into units
called layers. Each layer is a grouping of modules that offers a cohesive set of services. There are
constraints on the allowed-to-use relationship among the layers: the relations must be unidirectional.
Layers completely partition a set of software, and each partition is exposed through a public interface.
The layers are created to interact according to a strict ordering relation. If (A,B) is in this relation, we
say that the implementation of layer A is allowed to use any of the public facilities provided by layer B.
In some cases, modules in one layer might be required to directly use modules in a nonadjacent lower
layer; normally only next-lower-layer uses are allowed. This case of software in a higher layer using
modules in a nonadjacent lower layer is called layer bridging. If many instances of layer bridging
occur, the system may not meet its portability and modifiability goals that strict layering helps to
achieve. Upward usages are not allowed in this pattern.
Of course, none of this comes for free. Someone must design and build the layers, which can often
add up-front cost and complexity to a system. Also, if the layering is not designed correctly, it may
·
·
·- 0 A a
actually get in the way, by not providing the lower-level abstractions that programmers at the higher
levels need. And layering always adds a performance penalty to a system. If a call is made to a function
in the top-tnost layer, this may have to traverse many lower layers before being executed by the
hardware. Each of these layers adds some overhead of their own, at minimum in the form of context
switching.
Table 1 3 . 1 summarizes the solution of the layered pattern.
Overview
Elements
Relations
Constraints
Weaknesses
Table 13.1. Layered Pattern Solution
The layered pattern defines layers (grouping·s of m·odules that offer
a cohesive set of services) and a unidirectional allowed-to-use
relation among the layers. The pattern ls usually shown graphically
by stacking boxes representing layers on top of each other.
Layer, a kind of module. The description of a layer should define
what modules the layer contains and a characterization of the
cohesive set of servi·ces that the layer provides.
Allowed to use, which is a speciaHzation of a more generic
depends-on relation. The design should define what the layer usage
rules are (e.g., "a layer is allowed to use any lower layer" or "a layer
is allowed to use only the layer Immediately below it") and any
a.llowable exceptions.
• Every piece of software is allocated to exactly one layer.
• There are at least two layers (but usually there are three or
more).
• The allowed-to-use relations should not be circular (i.e., a lower
layer cannot use a layer above).
• The addition of layers adds up-front cost and complexity to a
system.
• Layers contribute a performance penalty.
Layers are almost always drawn as a stack of boxes. The allowed-to-use relation is denoted by
geometric adjacency and is read from the top down, as in Figure 1 3 . 1 .
A
8
c
Key:
Layer
A layer Is allowed to use
the next lower layer.
Figure 13.1. Stack-of-boxes notation for layered designs
Some Finer Points of Layers
A layered architecture is one of the few places where connections among components
can be shown by adjacency, and where "above" and "below" matter. If you tum Figure
1 3 . 1 upside-down so that C is on top, this would represent a completely different
design. Diagrams that use arrows among the boxes to denote relations retain their
semantic meaning no matter the orientation.
·
·
·- 0
The layered pattern is one of the most commonly used patterns in all of software
engineering, but I'm often surprised by how many people still get it wrong.
A a
First, it is impossible to look at a stack of boxes and tell whether layer bridging is
allowed or not. That is, can a layer use any lower layer, or just the next lower one? It is
the easiest thing in the world to resolve this; all the architect has to do is include the
answer in the key to the diagram's notation (something we recommend for all
diagrams). For example, consider the layered pattern presented in Figure 1 3 .2 on the
next page.
Applications
c Services I
;
Data Bank I
Environmental Models
Environment Sensing
J�
OS and Hardware
Key:
j layer
Software In a layer Is aUowed to use software
in the same layer. or any layer immediately
t)elow or to lhe right.
Figure 13.2. A simple layer diagram, with a simple key answering the uses question
But I'm still surprised at how few architects actually bother to do this. And if they
don't, their layer diagrams are ambiguous.
Second, any old set of boxes stacked on top of each other does not constitute a
layered architecture. For instance, look at the design shown in Figure 1 3 .3 , which uses
arrows instead of adjacency to indicate the relationships among the boxes. Here,
everything is allowed to use everything. This is decidedly not a layered architecture.
The reason is that if Layer A is replaced by a different version, Layer C (which uses it
in this figure) might well have to change. We don't want our virtual machine layer to
change every time our application layer changes. But I'm still surprised at how many
people call a stack of boxes lined up with each other "layers" (or think that layers are
the same as tiers in a multi-tier architecture).
A
l
B
c
Key:
t=] Layer
·
·
·-
-•• Allowed to use
Figure 13.3. A wolf in layer's clothing
0 A a
Third, many architectures that purport to be layered look something like Figure 1 3 .4.
This diagram probably means that modules in A, B, or C can use modules in D, but
without a key to tell us for sure, it could mean anything. "Sidecars" like this often
contain common utilities (sometimes imported), such as error handlers, communication
protocols, or database access mechanisms. This kind of diagram makes sense only in
the case where no layer bridging is allowed in the main stack. Otherwise, D could
simply be made the bottom-most layer in the main stack, and the "sidecar" geometry
would be unnecessary. But I'm still surprised at how often I see this layout go
unexplained.
A
B D
c
Figure 13.4. Layers with a "sidecar"
Sometimes layers are divided into segments denoting a finer-grained decomposition
of the modules. Sometimes this occurs when a preexisting set of units, such as imported
modules, share the same allowed-to-use relation. When this happens, you have to
specify what usage rules are in effect among the segments. Many usage rules are
possible, but they must be made explicit. In Figure 1 3 .5, the top and the bottom layers
are segmented. Segments of the top layer are not allowed to use each other, but
segments of the bottom layer are. If you draw the same diagram without the arrows, it
will be harder to differentiate the different usage rules within segmented layers.
Layered diagrams are often a source of hidden ambiguity because the diagram does not
make explicit the allowed-to-use relations.
Ul
"' Rich .... Web Ul Command
􀛨 CHent \. Line
,,
Business Logl·c
,,
Data Access
local Data .. Remote Data
Access
""
Access
)
􀛩
Key:
·
·
·- 0
L-1 ___ ..] ..J Layer
( ) Layer
. ---'· segment
...,..,. Allowed to use
Figure 13.5. Layered design with segmented layers
A a
Finally, the most important point about layering is that a layer isn't allowed to use
any layer above it. A module "uses" another module when it depends on the answer it
gets back. But a layer is allowed to make upward calls, as long as it isn't expecting an
answer frmn the1n. This is how the common error-handling scheme of callbacks works.
A program in layer A calls a program in a lower layer B, and the parameters include a
pointer to an error-handling program in A that the lower layer should call in case of
error. The software in B makes the call to the program in A, but cares not in the least
what it does. By not depending in any way on the contents of A, B is insulated from
changes in A.
-PCC
Other Module Patterns
Designers in a particular domain often publish "standard" module decompositions for systems in that
domain. These standard decompositions, if put in the "context, problem, solution" form, constitute
module decomposition patterns.
Similarly in the object-oriented realm, "standard" or published class/object design solutions for a
class of system constitute object-oriented patterns.
Component-and-Connector Patterns
Broker Pattern
Context: Many systems are constructed from a collection of services distributed across multiple
servers. Implementing these systems is complex because you need to worry about how the systems will
interoperate how they will connect to each other and how they will exchange information as well as
the availability of the component services.
Problem: How do we structure distributed software so that service users do not need to know the nature
·
·
·- 0 A a
and location of service providers, making it easy to dynamically change the bindings between users and
providers?
Solution: The broker pattern separates users of services (clients) from providers of services (servers) by
inserting an intermediary, called a broker. When a client needs a service, it queries a broker via a
service interface. The broker then forwards the client's service request to a server, which processes the
request. The service result is communicated from the server back to the broker, which then returns the
result (and any exceptions) back to the requesting client. In this way the client remains completely
ignorant of the identity, location, and characteristics of the server. Because of this separation, if a server
becomes unavailable, a replacement can be dynamically chosen by the broker. If a server is replaced
with a different (compatible) service, again, the broker is the only component that needs to know of this
change, and so the client is unaffected. Proxies are commonly introduced as intermediaries in addition
to the broker to help with details of the interaction with the broker, such as marshaling and
unmarshaling messages.
The down sides of brokers are that they add complexity (brokers and possibly proxies must be
designed and implemented, along with messaging protocols) and add a level of indirection between a
client and a server, which will add latency to their communication. Debugging brokers can be difficult
because they are involved in highly dynamic environments where the conditions leading to a failure
may be difficult to replicate. The broker would be an obvious point of attack, from a security
perspective, and so it needs to be hardened appropriately. Also a broker, if it is not designed carefully,
can be a single point of failure for a large and complex system. And brokers can potentially be
bottlenecks for communication.
Table 1 3 .2 summarizes the solution of the broker pattern.
Table 13.2. Broker Pattern Solution
Overview
Elements
·
·
·- 0
The broker pattern defines a runtime component, called a broker, that
mediates the communication between a number of cllents and servers.
Client, a requester ot services
Server, a provider of services
Broker, an intermediary that locates an appropriate server to fulfill a
client's request, forwards the request to the server, and returns the
results to the client
Client-side proxy, an intermediary that manages the actual
communication with the broker, including mars·hating, sending, and
unmarshaling of messages
Server-side proxy, an intermediary that manag·es the actual
communication with the broker, including marshaling, sending, and
unmarshaling of messages
Relations The attachment retation associates clients (and, optionallyl client-side
proxies) and servers (and, optionally, server-side proxies) with brokers.
Constraints The client can only attach to a broker (potentially via a client-side
proxy). The server can only attach to a broker (potentially via a server$
side proxy) .
Weaknesses Brokers add a layer of fndirectlonf and hence latency, between clients
and servers, and that layer may be a communication bottleneck.
The broker can be a single point of ·failure.
A broker adds up-front complexity.
A broker may be a targe1 for security attacks.
A broker may be difficult to test.
A a
The broker is, of course, the critical component in this pattern. The pattern provides all of the
modifiability benefits of the use-an-intermediary tactic (described in Chapter 7), an availability benefit
(because the broker pattern makes it easy to replace a failed server with another), and a performance
benefit (because the broker pattern makes it easy to assign work to the least-busy server). However, the
pattern also carries with it some liabilities. For example, the use of a broker precludes performance
optimizations that you might make if you knew the precise location and characteristics of the server.
Also the use of this pattern adds the overhead of the intermediary and thus latency.
The original version of the broker pattern, as documented by Gamma, Helm, Johnson, and Vlissides
[Gamma 94], is given in Figure 1 3 .6.
-t ransfers - 1 1 Broker
Client-Side Proxy
1 +locate Server() 1
+locateCiieot()
+pack data() +registerServe r()
+unpack_ data() +unregisterServer()
+send_request{)
+return() -call 1
•
0 .. 1
1 -call
Bridge •
-uses
Client
•
'+pack_data(} ·U$9S
+call_serverO +unpack_data()
+start_ task() +forward_message{)
+use_Broker_API() +transmit_message{)
Figure 13.6. The broker pattern
·
·
·- 0
-transfers •
Server-Side Proxy
+pacKdataO
+tJnpack_data()
+call_service(}
+Sen d_response()
*
1 ·call
Server
+initialize()
+enter_main_loop()
+run_servioe()
+use_Broker_API ()
.
A a
The first widely used implementation of the broker pattern was in the Common Object Request
Broker Architecture (CORBA). Other common uses of this pattern are found in Enterprise Java Beans
(EJB) and Microsoft's .NET platform essentially any modern platform for distributed service
providers and consumers implements some form of a broker. The service-oriented architecture (SOA)
approach depends crucially on brokers, most commonly in the form of an enterprise service bus.
Model-View-Controller Pattern
Context: User interface software is typically the most frequently modified portion of an interactive
application. For this reason it is important to keep modifications to the user interface software separate
fro·m the rest of the system. Users often wish to look at data from different perspectives, such as a bar
graph or a pie chart. These representations should both reflect the current state of the data.
Problem: How can user interface functionality be kept separate from application functionality and yet
still be responsive to user input, or to changes in the underlying application's data? And how can
multiple views of the user interface be created, maintained, and coordinated when the underlying
application data changes?
Solution: The model-view-controller (MVC) pattern separates application functionality into three kinds
of components:
• A model, which contains the application's data
• A view, which displays some portion of the underlying data and interacts with the user
• A controller, which mediates between the model and the view and manages the notifications of
state changes
MVC is not appropriate for every situation. The design and implementation of three distinct kinds
of components, along with their various forms of interaction, may be costly, and this cost may not make
·
·
·- 0 A a
sense for relatively simple user interfaces. Also, the match between the abstractions of MVC and
commercial user interface toolkits is not perfect. The view and the controller split apart input and
output, but these functions are often combined into individual widgets. This may result in a conceptual
mismatch between the architecture and the user interface toolkit.
Table 1 3 . 3 summarizes the solution of the MVC pattern.
Table 13.3. Model-View-Controller Pattern Solution
Overview The MVC pattern breaks system functionality into three components: a
model, a view, and a controller that medi:ates between the model and
the view.
Elements The model is a representation of the application data or state, and it
contains (or provides an interface to) application logic.
The view is a user interface component that etther produces a
representation of the model for the user or allows for some form of
user Input, o r both.
The controller manages the Interaction between the model and th:e
view, translating user actions into changes to the model or changes to
the view.
Relations The notifies relation connects instances of model, view. and controller,
noti'fy,lng elements of relevant state changes.
Constraints There must be at least one instance each of model, view, and
controller.
The model component should not interact directly with the controller.
Weaknesses The complexity may not be worth it for simple user interfaces.
The model, view, and controller abstractions may not be good fits for
some user interface toolkits.
There may, in fact, be many views and many controllers associated with a model. For example, a set
of business data may be represented as columns of numbers in a spreadsheet, as a scatter plot, or as a
pie chart. Each of these is a separate view, and this view can be dynamically updated as the model
changes (for example, showing live transactions in a transaction processing system). A model may be
updated by different controllers; for example, a map could be zoo1ned and panned via mouse
movements, trackball movements, keyboard clicks, or voice commands; each of these different forms of
input needs to be managed by a controller.
The MVC components are connected to each other via some flavor of notification, such as events or
callbacks. These notifications contain state updates. A change in the model needs to be communicated
to the views so that they may be updated. An external event, such as a user input, needs to be
communicated to the controller, which may in tum update the view and/or the model. Notifications may
be either push or pull.
Because these components are loosely coupled, it is easy to develop and test the1n in parallel, and
changes to one have minimal impact on the others. The relationships between the components of MVC
are shown in Figure 13.7.
Key:
• •
State
Query
. -
I
I
Change
Notification
Method
Invocations
Events
View Selection
User Gestures
·
·
·- 0
State
Change
Figure 13.7. The model-view-controller pattern
A a
The MVC pattern is widely used in user interface libraries such as Java's Swing classes, Microsoft's
ASP.NET framework, Adobe 's Flex software development kit, Nokia's Qt framework, and many
others. As such, it is common for a single application to contain many instances of MVC (often one per
user interface object).
Pipe-and-Filter Pattern
Context: Many systems are required to transform streams of discrete data items, from input to output.
Many types of transformations occur repeatedly in practice, and so it is desirable to create these as
independent, reusable parts.
Problem: Such systems need to be divided into reusable, loosely coupled components with simple,
generic interaction mechanisms. In this way they can be flexibly combined with each other. The
components, being generic and loosely coupled, are easily reused. The components, being independent,
can execute in parallel.
Solution: The pattern of interaction in the pipe-and-filter pattern is characterized by successive
transfonnations of streams of data. Data arrives at a filter's input port(s), is transformed, and then is
passed via its output port(s) through a pipe to the next filter. A single filter can consume data from, or
produce data to, one or more ports.
There are several weaknesses associated with the pipe-and-filter pattern. For instance, this pattern is
typically not a good choice for an interactive system, as it disallows cycles (which are important for
user feedback). Also, having large numbers of independent filters can add substantial amounts of
computational overhead, because each filter runs as its own thread or process. Also, pipe-and-filter
·
·
·- 0 A a
systems may not be appropriate for long-running computations, without the addition of some form of
checkpoint/restore functionality, as the failure of any filter (or pipe) can cause the entire pipeline to fail.
The solution of the pipe-and-filter pattern is summarized in Table 1 3 .4.
Overview
E'lements
Table 13.4. Pipe-and-Filter Pattern Solution
Data is transformed from a system's external inputs to its external
outputs through a series of transformations performed by its filters
connected by pipes.
Filter, which is a component that transforms data read on its input
port(s) to data written on its output port(s). Filiers can execute
concurrently with each other. Fitters can Incrementally transform
data; that is, they can start p:roducing output as soon as they start
processing Input. Important characteristics include processing rates,
i nput/output data formats, and the transformation ·executed by the
filter.
Pipe, which is a connector that conveys data fmm a filter's output
port(s) to another filter's input port{s). A pipe has a single source
for its input and a single target for its output. A pipe preserves the
sequence of data items, and tt does not alter the data pass'ing
through. Important characteristics include buffer size, protocol of
interaction, transmission speed, and format of the data that passes
through a pipe.
Relations The attachme-nt relation associates the output of filters with the input
of pipes and vice versa.
Constraints Pipes connect filter output ports to filter input ports.
Connected filters must agree on the type of data being passed along
the connecting pipe.
Specializations of the pattern may restrict the association of
components to an acyclic graph or a linear sequence, sometimes
called a pipeline.
Other specializations may prescribe that components have certain
named ports, such as the stdln, stdout, and stderr ports of UNIX
filters.
Weaknesses The pipe-and-filter pattern is typically not a good choice for an
interactive system.
Having large numbers of independent filters can add substantial
amounts of computational overhead.
Pipe-and-fitter systems may not be appropriate for long-running
computations.
Pipes buffer data during communication. Because of this property, filters can execute
asynchronously and concurrently. Moreover, a filter typically does not know the identity of its upstream
or downstream filters. For this reason, pipeline pipe-and-filter syste1ns have the property that the overall
computation can be treated as the functional composition of the computations of the filters, making it
easier for the architect to reason about end-to-end behavior.
Data transformation systems are typically structured as pipes and filters, with each filter responsible
for one part of the overall transformation of the input data. The independent processing at each step
supports reuse, parallelization, and simplified reasoning about overall behavior. Often such systems
constitute the front end of signal-processing applications. These systems receive sensor data at a set of
initial filters; each of these filters compresses the data and performs initial processing (such as
·
·
·- 0 A a
smoothing). Downstream filters reduce the data further and do synthesis across data derived from
different sensors. The fmal filter typically passes its data to an application, for example providing input
to modeling or visualization tools.
Other systems that use pipe-and-filter include those built using UNIX pipes, the request processing
architecture of the Apache web server, the map-reduce pattern (presented later in this chapter), Yahoo!
Pipes for processing RSS feeds, many workflow engines, and many scientific computation systems that
have to process and analyze large streams of captured data. Figure 1 3 . 8 shows a UML diagram of a
pipe-and-filter system.
capacity ::o 40
capacity - 40 D end·af·datu = empty record
end-of-data = empty 􀁺cord when-full = bloc;k for 2 sec and rt;:try
when-full = bloc;k far 2 sec snd retry when-empty = bfock for 30 sec and retry
when-empry - block for 30 sec l!md retry
:[] I
' ((filtef)> I
' I
' 􀃭'1 :Calculate ""out ' I
' .1.
' ,,.. DirectDeposit ¥pips» {] g-l OU I 􀒞
dllter>l «filter» «pipe» :XmiToObject [.., «ptp9􀑑 r r ;Process
r' out / i􀞜 Payment I I\.
I OU! 'l'pipe» I I --e-. 􀒟fitter» u I I
C<Jpacity = 50 􀛦 j J :FormatRejected I end-ol-dara := "EOT" Strin9 ;;;t I Records I
when-full = block for 2 sec and rerry I
j
when-empty= bfock for 20 sec and retry
.-c - -apa-cft _y __- 10--'------,.
end-of-data "' empty record
whfJn-fulf = block for 2 sec and rfJtry
when-empty ,. block for 60 sec and retry
I
I
!I fitter» u
In, :Format L OirectOeposit
Figure 13.8. A UML diagram of a pipe-and-filter-based system
Client-Server Pattern
Context: There are shared resources and services that large numbers of distributed clients wish to
access, and for which we wish to control access or quality of service.
Problem: By managing a set of shared resources and services, we can promote modifiability and reuse,
by factoring out common services and having to modify these in a single location, or a small number of
locations. We want to improve scalability and availability by centralizing the control of these resources
and services, while distributing the resources themselves across multiple physical servers.
Solution: Clients interact by requesting services of servers, which provide a set of services. Some
components may act as both clients and servers. There may be one central server or multiple distributed
ones.
The client-server pattern solution is summarized in Table 1 3 .5; the component types are clients and
servers; the principal connector type for the client-server pattern is a data connector driven by a
request/reply protocol used for invoking services.
Table 13.5. Client-Server Pattern Solution
Overview
E'lements
Relations
Constraints
Weaknesses
·
·
·- 0
Clients initiate interactions with servers, invoking services as
needed from those servers and waiting for the results of those
requests.
Client, a component that invokes services of a server
component. Clients have ports that describe the services they
• reqUire.
SeNer; a component that provides serv1ces to clients. Servers
have ports that describe the services they prowde. Important
charactedstics include information about the nature of the
server ports (such as how many clients can connect) and
performance characteristics (e.g., maximum rates of service
invocation).
Request/reply connector, a data connector employing a
request/reply protocol, used by a client to invoke services on a
server. Important cha.racteristfcs include whether the calls are
local or remote, and whether data is encrypted.
The attachment relation associates clients with s·ervers.
Clients are connected to servers through request/reply
connectors.
Server components can be clients to other servers.
Specializations may impose restrictions:
• Numbers of attachments to a given port
• Allowed relations among servers
Components may be arranged' in tiers, whi:ch are logical
groupings of related functionality or functionality that will share
a host computing environment (covered more later in this
chapter).
Server can be a performance bottleneck.
Server can be a single point of failure.
Decisions about where to locate functionality (in the client or
In the server) are often complex and costly to change after a
system has been built.
A a
Some of the disadvantages of the client-server pattern are that the server can be a performance
bottleneck and it can be a single point of failure. Also, decisions about where to locate functionality (in
the client or in the server) are often complex and costly to change after a system has been built.
Some common examples of systems that use the client-server pattern are these:
• Information systems running on local networks where the clients are GUI -launched
applications and the server is a database management system
• Web-based applications where the clients are web browsers and the servers are components
running on an e-commerce site
The computational flow of pure client-server systems is asymmetric: clients initiate interactions by
invoking services of servers. Thus, the client must know the identity of a service to invoke it, and
clients initiate all interactions. In contrast, servers do not know the identity of clients in advance of a
service request and must respond to the initiated client requests.
In early forms of client-server, service invocation is synchronous: the requester of a service waits, or
is blocked, until a requested service completes its actions, possibly providing a return result. However,
variants of the client-server pattern may employ more-sophisticated connector protocols. For example:
• Web browsers don't block until the data request is served up.
·
·
·- 0 A a
• In some client-server patterns, servers are permitted to initiate certain actions on their clients.
This might be done by allowing a client to register notification procedures, or callbacks, that
the server calls at specific times.
• In other systems service calls over a request/reply connector are bracketed by a "session" that
delineates the start and end of a set of a client-server interaction.
The client-server pattern separates client applications from the services they use. This pattern
simplifies systems by factoring out common services, which are reusable. Because servers can be
accessed by any number of clients, it is easy to add new clients to a system. Similarly, servers may be
replicated to support scalability or availability.
The World Wide Web is the best-known example of a system that is based on the client-server
pattern, allowing clients (web browsers) to access information from servers across the Internet using
HyperText Transfer Protocol (HTTP). HTTP is a request/reply protocol. HTTP is stateless; the
connection between the client and the server is terminated after each response from the server.
Figure 1 3 . 9 uses an informal notation to describe the client-server view of an automatic teller
machine (ATM) banking system.
Bank
tra nsacuon
authorizer
ATM
monitoring
server
ATM
reconfigu ration
server
server
cl cl1eot
ATM main
process
Key:
Client
FTX server
daemon
Reconfigure
and update
prooess
Monitoring
station
program
TCP socket connector 'Wlttl
Server client and server ports
L.---1
ATM OS/.2
cllent process '----�
Wincklws
· application
Figure 13.9. The client-server architecture of an ATM banking system
Peer-to-Peer Pattern
Context: Distributed computational entities each of which is considered equally important in terms of
·
·
·- 0
initiating an interaction and each of which provides its own resources need to cooperate and
collaborate to provide a service to a distributed community of users.
A a
Problem: How can a set of "equal" distributed computational entities be connected to each other via a
common protocol so that they can organize and share their services with high availability and
scalability?
Solution: In the peer-to-peer (P2P) pattern, components directly interact as peers. All peers are "equal"
and no peer or group of peers can be critical for the health of the system. Peer-to-peer communication is
typically a request/reply interaction without the asymmetry found in the client-server pattern. That is,
any component can, in principle, interact with any other component by requesting its services. The
interaction may be initiated by either party that is, in client-server terms, each peer component is both
a client and a server. Sometimes the interaction is just to forward data without the need for a reply. Each
peer provides and consumes similar services and uses the same protocol. Connectors in peer-to-peer
systems involve bidirectional interactions, reflecting the two-way communication that may exist
between two or more peer-to-peer components.
Peers first connect to the peer-to-peer network on which they discover other peers they can interact
with, and then initiate actions to achieve their computation by cooperating with other peers by
requesting services. Often a peer's search for another peer is propagated from one peer to its connected
peers for a limited number of hops. A peer-to-peer architecture may have specialized peer nodes (called
supemodes) that have indexing or routing capabilities and allow a regular peer's search to reach a larger
number of peers.
Peers can be added and removed from the peer-to-peer network with no significant impact, resulting
in great scalability for the whole system. This provides flexibility for deploying the system across a
highly distributed platform.
Typically multiple peers have overlapping capabilities, such as providing access to the same data or
providing equivalent services. Thus, a peer acting as client can collaborate with multiple peers acting as
servers to complete a certain task. If one of these multiple peers becomes unavailable, the others can
still provide the services to complete the task. The result is improved overall availability. There are also
performance advantages: The load on any given peer component acting as a server is reduced, and the
responsibilities that might have required more server capacity and infrastructure to support it are
distributed. This can decrease the need for other communication for updating data and for central server
storage, but at the expense of storing the data locally.
The drawbacks of the peer-to-peer pattern are strongly related to its strengths. Because peer-to-peer
systems are decentralized, managing security, data consistency, data and service availability, backup,
and recovery are all more complex. In many cases it is difficult to provide guarantees with peer-to-peer
systems because the peers come and go; instead, the architect can, at best, offer probabilities that quality
goals will be met, and these probabilities typically increase with the size of the population of peers.
Table 1 3 . 6 on the next page summarizes the peer-to-peer pattern solution.
Table 13.6. Peer-to-Peer Pattern Solution
·
·
·- 0
Overview Computation is achieved by cooperating peers that request service
from and provide services to one another across a network.
E'lements Pee􀛪 which is an independent component running on a network
node. Special peer components can provide routing, indexing, and
peer search capability.
Request/reply connecto􀁊 which is used to connect to the peer
network􀉲 search for other peerst and lnvoke services from other
peers. In some cases, the need for a reply is done away with.
Relations The relation associates peers with their connectors. Attachments
may change at runtime.
Constraints Restrictions may be placed on the following:
• The number of allowable attachments to any given peer
• The number of hops used for searching for a peer
• Which peers know about which other peers
Some P2P networks are organized with star topologies, in which
peers only connect to supernodes.
Weaknesses Managing security, data consistency, data/service availability,
backup, and recovery are all more complex.
Small peer-to-peer systems may not be able to consistently achieve
quality goars such as performance and avaHabilfty.
A a
Peer-to-peer computing is often used in distributed computing applications such as file sharing,
instant messaging, desktop grid computing, routing, and wireless ad hoc networking. Examples of peerto-
peer systems include file-sharing networks such as BitTorrent and eDonkey, and instant messaging
and VoiP applications such as Skype. Figure 1 3 . 1 0 shows an example of an instantiation of the peer-topeer
pattern.
I
I
I
I
I
I
I
I
moldy
69.95.63.49
'- -
'
I
I
J
I
I
I
amid ala
70. 1 16.152.15
lambda
50.64.16.14 outr􀉳der
74.12.4 1 . 1 1 1
\
\
\
\
\
\
\
I
,
,
I
I
J
I 1
I
ana kin
- - - 􀀙 - - -t ,.,,
.
...,._192.20. 1
I
1.----
naboo
1 57.66.24.26
Key: 0 Leaf peer • Gnutella port RequesVreply using Gnutella
protocol over TOP or UDP D Ultrapeer A . ,. 8 HITP file tr􀀱nsfer
..,.,..r- -
.... ..,., from A to B
Figure 13.10. A peer-to-peer view of a Gnutella network using an informal C&C notation. For
·
·
·- 0 A a
brevity, only a few peers are identified. Each of the identified leaf peers uploads and downloads
files directly from other peers.
Service-Oriented Architecture Pattern
Context: A number of services are offered (and described) by service providers and consumed by
service consumers. Service consumers need to be able to understand and use these services without any
detailed knowledge of their implementation.
Problem: How can we support interoperability of distributed components running on different
platforms and written in different implementation languages, provided by different organizations, and
distributed across the Internet? How can we locate services and combine (and dynamically recombine)
them into meaningful coalitions while achieving reasonable performance, security, and availability?
Solution: The service-oriented architecture (SOA) pattern describes a collection of distributed
components that provide and/or consume services. In an SOA, service provider components and service
consumer components can use different implementation languages and platforms. Services are largely
standalone: service providers and service consumers are usually deployed independently, and often
belong to different systems or even different organizations. Components have interfaces that describe
the services they request from other components and the services they provide. A service's quality
attributes can be specified and guaranteed with a service-level agreement (SLA). In some cases, these
are legally binding. Components achieve their computation by requesting services from one another.
The elements in this pattern include service providers and service consumers, which in practice can
take different forms, from JavaScript running on a web browser to CICS transactions running on a
mainframe. In addition to the service provider and service consumer components, an SOA application
may use specialized components that act as intermediaries and provide infrastructure services:
• Service invocation can be mediated by an enterprise service bus (ESB). An ESB routes
messages between service consumers and service providers. In addition, an ESB can convert
messages from one protocol or technology to another, perform various data transformations
(e.g., format, content, splitting, merging), perform security checks, and manage transactions.
Using an ESB promotes interoperability, security, and modifiability. Of course, communicating
through an ESB adds overhead thereby lowering performance, and introduces an additional
point of failure. When an ESB is not in place, service providers and consumers communicate
with each other in a point-to-point fashion.
• To improve the independence of service providers, a service registry can be used in SOA
architectures. The registry is a component that allows services to be registered at runtime. This
enables runtime discovery of services, which increases system modifiability by hiding the
location and identity of the service provider. A registry can even permit multiple live versions
of the same service.
• An orchestration server (or orchestration engine) orchestrates the interaction among various
service consumers and providers in an SOA system. It executes scripts upon the occurrence of a
specific event (e.g., a purchase order request arrived). Applications with well-defined business
processes or workflows that involve interactions with distributed components or systems gain
in modifiability, interoperability, and reliability by using an orchestration server. Many
·
·
·- 0 A a
commercially available orchestration servers support various workflow or business process
language standards.
The basic types of connectors used in SOA are these:
• SOAP. The standard protocol for communication in the web services technology. Service
consumers and providers interact by exchanging request/reply XML messages typically on top
of HTTP.
• Representational State Transfer (REST). A service consumer sends nonblocking HTTP
requests. These requests rely on the four basic HTTP commands (POST, GET, PUT, DELETE)
to tell the service provider to create, retrieve, update, or delete a resource.
• Asynchronous messaging, a "fire-and-forget" information exchange. Participants do not have to
wait for an acknowledgment of receipt, because the infrastructure is assumed to have delivered
the message successfully. The messaging connector can be point-to-point or publish-subscribe.
In practice, SOA environments may involve a mix of the three connectors just listed, along with
legacy protocols and other communication alternatives (e.g., SMTP). Commercial products such as
IBM's WebSphere MQ, Microsoft's MSMQ, or Apache's ActiveMQ are infrastructure components that
provide asynchronous messaging. SOAP and REST are described in more detail in Chapter 6.
As you can see, the SOA pattern can be quite complex to design and implement (due to dynamic
binding and the concomitant use of metadata). Other potential problems with this pattern include the
performance overhead of the middleware that is interposed between services and clients and the lack of
performance guarantees (because services are shared and, in general, not under control of the requester).
These weaknesses are all shared with the broker pattern, which is not surprising because the SOA
pattern shares many of the design concepts and goals of broker. In addition, because you do not, in
general, control the evolution of the services that you use, you may have to endure high and unplannedfor
maintenance costs.
Table 1 3 . 7 summarizes the SOA pattern.
Table 13.7. Service-Oriented Architecture Pattern Solution
Overview
Elements
Relations
Constraints
Weaknesses
·
·
·- 0
Computation is achieved by a set of cooperating components
that provide and/or consume services over a network. The
computation is often described using a workflow language.
Components:
• Service providers, which provide one or more services
through published interfaces. Concerns ar,e onen tied to
the chosen lm plementatlon technology, and include perfor􀄡
manc-e, authorization constraints, availability, and cost. In
some cases these properties are speclfied in a service􀉵 level
agreement.
• Service consumers, which invoke services directly or 1hrough
an intermediary.
• Service providers may also be service consumers.
• ESB, which is an intermediary element tnat can route and
transform messag·es between service provfders and consum􀉶
ers.
• Registry of services, which may be used by providers to
register their services and by consumers to discover services
at runtime.
• Orchestration seNer, which coordinates the Interactions
between service consumers and providers based on
languages for business processes and workflows.
Connectors:
• SOAP connecto􀁋 which uses the SOAP protocol for
synchronous communication between web services, typically
over HTTP.
• REST connector, which relies on the basic request/reply
operations of the HTTP protocol.
• Asynchronous messaging connector, which uses a
messaging system to offer point􀄡to-polnt or publish􀉷subscribe
asynchronous message exchanges.
Attachment of the different kinds of -components available to the
respective connectors
Service consumers are connected to service providers, but
Intermediary components (e.g., ESB, registry, orchestration
server) may be used.
SOA-based systems are typically complex to build.
You don't control the evolution of independent servtces.
There is a performance overhead associated- with the
middleware, and services may be performance bottlenecks, and
typically do not provide performance guarantees.
A a
The main benefit and the major driver of SOA is interoperability. Because service providers and
service consumers may run on different platforms, service-oriented architectures often integrate a
variety of systems, including legacy systems. SOA also offers the necessary elements to interact with
external services available over the Internet. Special SOA components such as the registry or the ESB
also allow dynamic reconfiguration, which is useful when there's a need to replace or add versions of
components with no system interruption.
Figure 1 3 . 1 1 shows the SOA view of a system called Adventure Builder. Adventure Builder allows
a customer on the web to assemble a vacation by choosing an activity and lodging at and transportation
to a destination. The Adventure Builder system interacts with external service providers to construct the
·
·
·- 0 A a
vacation, and with bank services to process payment. The central OPC (Order Processing Center)
component coordinates the interaction with internal and external service consumers and providers. Note
that the external providers can be legacy mainframe systems, Java systems, .NET systems, and so on.
The nature of these external components is transparent because SOAP provides the necessary
interoperability.
,- - - - - - - - - - - - - - - - - - - - - - - - - - - -
1 Adventure Builder .
I
I
I Web
I browser
I
I- - - - - - - ,
I
I
OpcPurchase
OrderService
User's
e-mail ... - - r- - - -
client
Consumer
Web site
r-I .db<: ---1 Adventure
Catalog
DB
OpcOrder
TrackingService
OPC
􀞚􀞛J.d::.=;.bc --.... 11 ·A dv enture
OPC DB
CreditCard �..­ I
I
I
Web
Service
Broker
Service
Service Registry
Key:
Bank
t _ _
Ah1inePO
Service
AlrHne
Provider
Client·sicfe
application
Java EE
application
- -
External Web
service provider
o- Web services
endpoint
Data
repository
-
LodgingPO
Service
Lodging
Provider
ActivityPO
�\::l.e1 ce
Activity
Provider
HTTP/HTTPS
---:::>� SOAP call
---Data access
- - - 􀒝 SMTP
,- - - ....., Scope of the
-
1
I application (not
􀂷 - -. 1 a component)
Figure 1 3 . 1 1 . Diagram of the SOA view for the Adventure Builder system. OPC stands for
"Order Processing Center."
Publish-Subscribe Pattern
Context: There are a number of independent producers and consumers of data that must interact. The
·
·
·- 0 A a
precise number and nature of the data producers and consumers are not predetermined or fixed, nor is
the data that they share.
Problem: How can we create integration mechanisms that support the ability to transmit messages
among the producers and consumers in such a way that they are unaware of each other's identity, or
potentially even their existence?
Solution: In the publish-subscribe pattern, summarized in Table 1 3 .8, components interact via
announced messages, or events. Components may subscribe to a set of events. It is the job of the
publish-subscribe runtime infrastructure to make sure that each published event is delivered to all
subscribers of that event. Thus, the main form of connector in these patterns is an event bus. Publisher
components place events on the bus by announcing them; the connector then delivers those events to
the subscriber components that have registered an interest in those events. Any component may be both
a publisher and a subscriber.
Table 13.8. Publish-Subscribe Pattern Solution
Overview Components publish and subscribe to events. When an event is
announced by a component. the connector infrastructure dispatches
the event to all registered subscribers.
Elements Any C&C component with at least one publish or subscribe port.
Concerns include which events are published and subscribed to, and
the granularity of events.
The pub/ish-subscribe connector, which will have announce and listen
roles for components that wish to publish and subscribe to events.
Relations The attachment relation associates components with the publishsubscribe
connector by prescribing which components announce
events and which components are registered to receive events.
Constraints All components are connected to an event distributor that may be
viewed as either a bus-connector-or a component Publish ports
are attached to announce roles and subscribe ports are attached to
listen roles. Constraints may restrict which components can listen to
which events. whether a component can listen to its own events, and
how many publish-subscribe connectors can exist within a system.
A component may be both a publisher and a subscriber. by having
ports of both types.
Weaknesses Typically incr.eases latency and has a negative effect on scalability and
predictability of message delivery time.
Less control over ordering of messages, and delivery of messages is
not guaranteed,
Publish-subscribe adds a layer of indirection between senders and receivers. This has a negative
effect on latency and potentially scalability, depending on how it is implemented. One would typically
not want to use publish-subscribe in a system that had hard real-time deadlines to meet, as it introduces
uncertainty in message delivery times.
Also, the publish-subscribe pattern suffers in that it provides less control over ordering of messages,
and delivery of messages is not guaranteed (because the sender cannot know if a receiver is listening).
This can make the publish-subscribe pattern inappropriate for complex interactions where shared state
is critical.
·
·
·- 0 A a
There are some specific refinements of this pattern that are in common use. We will describe several
of these later in this section.
The computational model for the publish-subscribe pattern is best thought of as a system of
independent processes or objects, which react to events generated by their environment, and which in
turn cause reactions in other components as a side effect of their event announcements. An example of
the publish-subscribe pattern, implemented on top of the Eclipse platform, is shown in Figure 13 . 1 2.
-
c
Q)
>
Q)
-
Register
actfon
handlers
- - - - SEI.ArchEUl plug-In config
U J
event views and
editors
new or
setFieldO
core a. 􀛥
listener \%􀃫 􀃪􀃬 \ 0) a ? .....
\ 􀛤 o-2
\ (II
􀒜 \
·-
-􀞙 ihandle
ur event Arc hE
core
faQade
- -􀛬􀞘 1
action
handler CRUD
fac1 data
assert/modify/
retract lac'
Jess :
Key:
Action D handler
object
._ _ _ 0 Ul screen 0 Java ----;,. Register to <>--- Event send/
object obj,ect listen for event receive
- --.
11 I External [J XML file '--_ program
Event manager
(part of Ecfipse
platform)
-+II Java method
call
Figure 13.12. A typical publish-subscribe pattern realization
I
Typical examples of systems that employ the publish-subscribe pattern are the following:
• Graphical user interfaces, in which a user's low-level input actions are treated as events that are
routed to appropriate input handlers
• MVC-based applications, in which view components are notified when the state of a model
object changes
• Enterprise resource planning (ERP) systems, which integrate many components, each of which
is only interested in a subset of system events
• Extensible programming environments, in which tools are coordinated through events
• Mailing lists, where a set of subscribers can register interest in specific topics
• Social networks, where "friends" are notified when changes occur to a person's website
The publish-subscribe pattern is used to send events and messages to an unknown set of recipients.
Because the set of event recipients is unknown to the event producer, the correctness of the producer
·
·
·- 0 A a
cannot, in general, depend on those recipients. Thus, new recipients can be added without modification
to the producers.
Having components be ignorant of each other's identity results in easy modification of the system
(adding or removing producers and consumers of data) but at the cost of runtime performance, because
the publish-subscribe infrastructure is a kind of indirection, which adds latency. In addition, if the
publish-subscribe connector fails completely, this is a single point of failure for the entire system.
The publish-subscribe pattern can take several forms:
• List-based publish-subscribe is a realization of the pattern where every publisher maintains a
subscription list a list of subscribers that have registered an interest in receiving the event.
This version of the pattern is less decoupled than others, as we shall see below, and hence it
does not provide as much modifiability, but it can be quite efficient in terms of runtime
overhead. Also, if the components are distributed, there is no single point of failure.
• Broadcast-based publish-subscribe differs from list-based publish-subscribe in that publishers
have less (or no) knowledge of the subscribers. Publishers simply publish events, which are
then broadcast. Subscribers (or in a distributed system, services that act on behalf of the
subscribers) examine each event as it arrives and determine whether the published event is of
interest. This version has the potential to be very inefficient if there are lots of messages and
most messages are not of interest to a particular subscriber.
• Content-based publish-subscribe is distinguished from the previous two variants, which are
broadly categorized as "topic-based." Topics are predefined events, or messages, and a
component subscribes to all events within the topic. Content, on the other hand, is much more
general. Each event is associated with a set of attributes and is delivered to a subscriber only if
those attributes match subscriber-defined patterns.
In practice the publish-subscribe pattern is typically realized by some form of message-oriented
middleware, where the middleware is realized as a broker, managing the connections and channels of
information between producers and consumers. This middleware is often responsible for the
transformation of messages (or message protocols), in addition to routing and sometimes storing the
messages. Thus the publish-subscribe pattern inherits the strengths and weaknesses of the broker
pattern.
Shared-Data Pattern
Context: Various computational components need to share and manipulate large amounts of data. This
data does not belong solely to any one of those components.
Problem: How can systems store and manipulate persistent data that is accessed by multiple
independent components?
Solution: In the shared-data pattern, interaction is dominated by the exchange of persistent data
between multiple data accessors and at least one shared-data store. Exchange may be initiated by the
accessors or the data store. The connector type is data reading and writing. The general computational
model associated with shared-data systems is that data accessors perform operations that require data
from the data store and write results to one or more data stores. That data can be viewed and acted on
·
·
·- 0 A a
by other data accessors. In a pure shared-data system, data accessors interact only through one or more
shared-data stores. However, in practice shared-data systems also allow direct interactions between data
accessors. The data-store components of a shared-data system provide shared access to data, support
data persistence, manage concurrent access to data through transaction management, provide fault
tolerance, support access control, and handle the distribution and caching of data values.
Specializations of the shared-data pattern differ with respect to the nature of the stored dataexisting
approaches include relational, object structures, layered, and hierarchical structures.
Although the sharing of data is a critical task for most large, complex systems, there are a number of
potential problems associated with this pattern. For one, the shared-data store may be a performance
bottleneck. For this reason, performance optimization has been a common theme in database research.
The shared-data store is also potentially a single point of failure. Also, the producers and consumers of
the shared data may be tightly coupled, through their knowledge of the structure of the shared data.
The shared-data pattern solution is summarized in Table 1 3 .9.
Table 13.9. Shared-Data Pattern Solution
Overview Communication between data accessors is mediated by a shareddata
store. Control may be initiated by the data acc·essors or the
data store. Data is made persistent by the data store.
Elements Shared-data store. Concems inctude types of data stored, data
performance-oriented properties, data distribution, and number of
accessors permitted.
Data accessor component.
Data reading and writing connector. An Important choice here is
whether the connector is transactional or not, as well as the read/
write language, protocols, and semantics.
Relations Attachment relation determines which data accessors are
connected to whicn data stores.
Constraints Data accessors Interact with the data store(.s).
Weaknesses The shared-data store may be a performance bottlene,ck.
The shared-,data store may be a sfngle point of failure.
Producers and consumers of data may be tightly coupled.
The shared-data pattern is useful whenever various data items are persistent and have multiple
accessors. Use of this pattern has the effect of decoupling the producer of the data from the consumers
of the data; hence, this pattern supports modifiability, as the producers do not have direct knowledge of
the consumers. Consolidating the data in one or more locations and accessing it in a common fashion
facilitates performance tuning. Analyses associated with this pattern usually center on qualities such as
data consistency, performance, security, privacy, availability, scalability, and compatibility with, for
example, existing repositories and their data.
When a system has more than one data store, a key architecture concern is the mapping of data and
computation to the data. Use of multiple stores may occur because the data is naturally, or historically,
partitioned into separable stores. In other cases data may be replicated over several stores to improve
performance or availability through redundancy. Such choices can strongly affect the qualities noted
above.
·
·
·- 0 A a
Figure 1 3 . 1 3 shows the diagram of a shared-data view of an enterprise access management system.
There are three types of accessor components: Windows applications, web applications, and headless
progratns (programs or scripts that run in background and don't provide any user interface).
Password
synchro·nizer
Web
sign􀒛in
cootralized securit)' realm
Application
Web
appUcaeioo
Key:
Password
rese1
Wlndows GUI
application L)=;>Data
read r: J<:·
Self
registration
I
Data
write
Account
provlslonlng
Head tess ( program \...
Rights
et1 abtement
" Web
J apprication
,/' :!) Data
""� v read & write
Audit and
monitoring
Request
tracking
Delegated
administratioll
Enb!lemetu
mat1agemoot
Data
reposUory
Figure 13.13. The shared-data diagram of an enterprise access management system
Allocation Patterns
Map-Reduce Pattern
Context: Businesses have a pressing need to quickly analyze enormous volumes of data they generate
or access, at petabyte scale. Examples include logs of interactions in a social network site, massive
document or data repositories, and pairs of <source, target> web links for a search engine. Programs for
the analysis of this data should be easy to write, run efficiently, and be resilient with respect to
hardware failure.
Problem: For many applications with ultra-large data sets, sorting the data and then analyzing the
grouped data is sufficient. The problem the map-reduce pattern solves is to efficiently perform a
distributed and parallel sort of a large data set and provide a simple means for the programmer to
specify the analysis to be done.
·
·
·- 0 A a
Solution: The map-reduce pattern requires three parts: First, a specialized infrastructure takes care of
allocating software to the hardware nodes in a massively parallel computing environment and handles
sorting the data as needed. A node may be a standalone processor or a core in a multi-core chip. Second
and third are two programmer-coded functions called, predictably enough, map and reduce.
The map function takes as input a key (key 1 ) and a data set. The purpose of the map function is to
filter and sort the data set. All of the heavy analysis takes place in the reduce function. The input key in
the map function is used to filter the data. Whether a data record is to be involved in further processing
is determined by the map function. A second key (key2) is also important in the map function. This is
the key that is used for sorting. The output of the map function consists of a <key2, value> pair, where
the key2 is the sorting value and the value is derived from the input record.
Sorting is performed by a combination of the map and the infrastructure. Each record output by map
is hashed by key2 into a disk partition. The infrastructure maintains an index file for key2 on the disk
partition. This allows for the values on the disk partition to be retrieved in key2 order.
The performance of the map phase of map-reduce is enhanced by having multiple map instances,
each processing a different portion of the disk file being processed. Figure 1 3 . 1 4 shows how the map
portion of map-reduce processes data. An input file is divided into portions, and a number of map
instances are created to process each portion. The map function processes its portion into a number of
partitions, based on programmer-specified logic.
Portion i of Partition 1 Part1Uon 2 Partllion 3
Map instance i input file • •
+
Reduce
Output
Merge • • from ... instance 2 instance 2
j•
I
Portion jof Pattltioo 1 Parlltion 2 F'a riiiiOI\ 3
Key:
input file • • 0 Component
Map Instance 1
• Output D Disk fila
Figure 13.14. A component-and-connector view of map-reduce showing how the data processed
by map is partitioned and subsequently processed by reduce
The reduce function is provided with all the sets of <key2, value> pairs emitted by all the map
instances in sorted order. Reduce does some programmer-specified analysis and then emits the results
of that analysis. The output set is almost always much smaller than the input sets, hence the name
"reduce." The term "load" is sometimes used to describe the final set of data emitted. Figure 1 3 . 1 4 also
·
·
·- 0 A a
shows one instance (of many possible instances) of the reduce processing, called Reduce Instance 2.
Reduce Instance 2 is receiving data from all of the Partition 2s produced by the various map instances.
It is possible that there are several iterations of reduce for large files, but this is not shown in Figure
1 3 . 14.
A classic teaching problem for map-reduce is counting word occurrences in a document. This
example can be carried out with a single map function. The document is the data set. The map function
will find every word in the document and output a <word, 1> pair for each. For example, if the
document begins with the words "Having a whole book . . . ," then the first results of map will be
<Having, 1 >
<a , 1 >
<who l e , 1 >
<book, 1>
In practice, the "a" would be one of the words filtered by map.
Pseudocode for map might look like this:
map ( S tring key, S t r ing value ) :
I I key : document name
I I value : document contents
f o r each word w in value :
Emit ( w , " 1 " ) ;
The reduce function will take that list in sorted order, add up the 1 s for each word to get a count,
and output the result.
The corresponding reduce function would look like this:
reduce ( L i s t <key, value> ) :
I I key : a word
I I value : an integer
int r e s u l t = 0 ;
s o rt input
f o r each input value :
f o r each input pair with same word
r e s u l t ++ ;
Emit ( w o r d , r e s u l t )
r e s u l t = 0
Larger data sets lead to a much more interesting solution. Suppose we want to continuously analyze
Twitter posts over the last hour to see what topics are currently "trending." This is analogous to
counting word occurrences in millions of documents. In that case, each document (tweet) can be
assigned to its own instance of the map function. (If you don't have millions of processors handy, you
can break the tweet collection into groups that match the number of processors in your processor farm,
and process the collection in waves, one group after the other.) Or we can use a dictionary to give us a
list of words, and each map function can be assigned its own word to look for across all tweets.
There can also be multiple instances of reduce. These are usually arranged so that the reduction
happens in stages, with each stage processing a smaller list (with a smaller number of reduce instances)
than the previous stage. The final stage is handled by a single reduce function that produces the final
output.
·
·
·- 0 A a
Of course, the map-reduce pattern is not appropriate in all instances. Some considerations that
would argue against adopting this pattern are these:
• If you do not have large data sets, then the overhead of map-reduce is not justified.
• If you cannot divide your data set into similar sized subsets, the advantages of parallelism are
lost.
• If you have operations that require multiple reduces, this will be complex to orchestrate.
Commercial implementations of map-reduce provide infrastructure that takes care of assignment of
function instances to hardware, recovery and reassignment in case of hardware failure (a common
occurrence in massively parallel computing environments), and utilities like sorting of the massive lists
that are produced along the way.
Table 1 3 . 1 0 summarizes the solution of the map-reduce pattern.
Overview
Elements
Relations
Constraints
Table 13.10. Map-Reduce Pattern Solution
The map-reduce pattern provides a framework for analyzing a
large distributed set of data that will execute in parallet, on a set
of processors. This parallelization allows for low latency and high
availability. The map performs the extract and transform portions
of the analysis and the reduce performs the loading of the results.
(Extract-transform-load is sometimes used to describe the functions of
the map and reduce.)
Map is a function with multiple instances deployed across multiple,
processors that performs the extract and transformation portions of
the analysis.
Reduce rs a function that may be deployed as a slngl.e Instance or as
multiple lnstances across processors to perform the load portion of
extract-transform-load.
The infrastructure ts the framewor1k responsible for deploying map and
reduce instances, shepherding the data between them, and detecting
and recovering from failure.
Deploy on is the retatlon between an Instance of a map or reduce
function and the processor onto which it is installed.
Instantiate, monitor, and control is the relation between the
infrastructure and the Instances of map and reduce.
The data to be analyzed must exist as a set of files.
The map functions are stateless and do not communicate with each
other.
The only communication between the map instances and the reduce
instances is the data emitted from the map instances as <key, vaJue>
pairs.
Weaknesses If you do not have large data sets, the overhead of map􀉴reduce is not
justified.
If you cannot divide your data set into stmiJar sized subsets, the
advantages of parallelism are lost.
Operations that require multiple reduces are complex to orchestrate.
Map-reduce is a cornerstone of the software of some of the most familiar names on the web,
including Google, Facebook, eBay, and Yahoo!
Multi-tier Pattern
·
·
·- 0 A a
The multi-tier pattern is a C&C pattern or an allocation pattern, depending on the criteria used to define
the tiers. Tiers can be created to group components of similar functionality, in which case it is a C&C
pattern. However, in many, if not most, cases tiers are defined with an eye toward the computing
environment on which the software will run: A client tier in an enterprise system will not be running on
the computer that hosts the database. That makes it an allocation pattern, mapping software elements.perhaps
produced by applying C&C patterns to computing elements. Because of that reason, we have
chosen to list it as an allocation pattern.
Context: In a distributed deployment, there is often a need to distribute a system's infrastructure into
distinct subsets. This may be for operational or business reasons (for example, different parts of the
infrastructure may belong to different organizations).
Problem: How can we split the system into a number of computationally independent execution
structures groups of software and hardware connected by some communications media? This is
done to provide specific server environments optimized for operational requirements and resource
usage.
Solution: The execution structures of many systems are organized as a set of logical groupings of
components. Each grouping is termed a tier. The grouping of components into tiers may be based on a
variety of criteria, such as the type of component, sharing the same execution environment, or having
the same runtime purpose.
The use of tiers may be applied to any collection (or pattern) of runtime components, although in
practice it is most often used in the context of client-server patterns. Tiers induce topological constraints
that restrict which components may communicate with other components. Specifically, connectors may
exist only between components in the same tier or residing in adjacent tiers. The multi-tier pattern
found in 1nany Java EE and Microsoft .NET applications is an example of organization in tiers derived
fro·m the client-server pattern.
Additionally, tiers may constrain the kinds of communication that can take place across adjacent
tiers. For example, some tiered patterns require call-return communication in one direction but eventbased
notification in the other.
The main weakness with the multi-tier architecture is its cost and complexity. For simple systems,
the benefits of the multi-tier architecture may not justify its up-front and ongoing costs, in terms of
hardware, software, and design and implementation complexity.
Tiers are not components, but rather logical groupings of components. Also, don't confuse tiers with
layers! Layering is a pattern of modules (a unit of implementation), while tiers applies only to runtime
entities.
Table 1 3 . 1 1 summarizes the solution part of the multi-tier pattern.
Table 13.11. Multi-tier Pattern Solution
Overview
Elements
Relations
Constraints
Weaknesses
·
·
·- 0
The execution structures of many systems are organized as a
set of log.ical groupings of components. Each grouping is termed
a tier. The grouping of components into tiers may be based on a
variety of criteria, such as the type of component, sharing the same
execution environment, or having the same runtime purpose.
Tier, Which ls a logicaJ grouping of software· components.
Tiers may be formed on the basis of common computing platforms,
in which case those platforms are also elements of the pattern.
Is part o􀀩 to group components into tiers.
Communicates with, to show how ti.ers and the components they
contain Interact with each other.
Allocated to, in the case that Uers map to computing platforms.
A software component belongs to exactly one tier.
Substantial up-front cost and complex]ty.
A a
Tiers make it easier to ensure security, and to optimize performance and availability in specialized
ways. They also enhance the modifiability of the system, as the computationally independent subgroups
need to agree on protocols for interaction, thus reducing their coupling.
Figure 1 3 . 1 5 uses an informal notation to describe the multi-tier architecture of the Consumer
Website Java EE application. This application is part of the Adventure Builder system. Many
component-and-connector types are specific to the supporting platform, which is Java EE in this case.
􀑏 - - - 􀑐 - - - - - - - - - - - - - - - - 􀂀 􀃯 - - - - - - - 􀁿 - - - - -
·1 I ( mappings.xml I I \ I OpcOrder \ I I • I 1 Order I 1 T rackingService I I * do Main 􀃟·/ ,..,. -1 r Facade I I .
1 • �c- :- - - I I / I ... Servlet 1' ".. ... ........ lA I I1 •.screen ,"'- ........ ..7/.. 1 1
1 ... , " lr
Web\ 1I I =* --/.. .....- 1---( ( browser I 1.. 􀛢 Template .. Screen 􀃝 "I 1 I r 􀞗 Servlet r JSP 1'4-
Catalog
Facade OPC
I I
.9? 􀛣. I 1 " User
I I en ·•.•. screen I 1 Mgmt Adventure 1 • definitions.xml -1 Facade I 1 Catalog
􀆀 : 1 .. index.jsp .-.-- .--
.--
J
.{····· I I DB I I I .--
.--
. .. .... . . ··./'1 I I I I I . ..-
.... ·· I I I t 1 I I .-- sign-on- . .
• •·••••
1 I I 1 1 1 :<1' .. .. .. .. .. oonf􀃜.xm1 -·· 􀃠􀃡􀃢􀃣􀃤􀃥 · 1 1 1 1 I : 1 I J 1 1 l􀁖i:n􀁗i􀁘 J 􀃞- _ _ _ _ _ !"'.:b􀁙e􀁚 _ ...c _ _ _ _ _} 1 _ _ 􀁛􀁜t􀁝r _ ) I.. 􀃦􀃧􀃨􀃩 /
Key 0 cnent-s1de
applictltion
--t..• Hnw
HTIPS
Java D Q Stateless A Data
EE Servlet
session U store
filter bean
[]
File
D
Java. EE o􀀳ontext
application ltstener
- --7 Java -- JDBC .. .... .. [>File --:;..::::;..SOAP Web serv1ces I I I Coma1ner r ""
call 110 call endpomt 1._ J
Figure 13.15. A multi-tier view of the Consumer Website Java EE application, which is part of the
Adventure Builder system
Other Allocation Patterns
·
·
·- 0 A a
There are several published deployment styles. Microsoft publishes a "Tiered Distribution" pattern,
which prescribes a particular allocation of components in a multi-tier architecture to the hardware they
will run on. Similarly, ffiM's WebSphere handbooks describe a number of what they call "topologies"
along with the quality attribute criteria for choosing among them. There are 1 1 topologies (specialized
deployment patterns) described for W ebSphere version 6, including the "single machine topology
(stand-alone server)," "reverse proxy topology," "vertical scaling topology," "horizontal scaling
topology," and "horizontal scaling with IP sprayer topology."
There are also published work assignment patterns. These take the form of often-used team
structures. For example, patterns for globally distributed Agile projects include these:
• Platform. In software product line development, one site is tasked with developing reusable
core assets of the product line, and other sites develop applications that use the core assets.
• Competence center. Work is allocated to sites depending on the technical or domain expertise
located at a site. For example, user interface design is done at a site where usability engineering
experts are located.
• Open source. Many independent contributors develop the software product in accordance with
a technical integration strategy. Centralized control is minimal, except when an independent
contributor integrates his code into the product line.
13.3. Relationships between Tactics and Patterns
·
·
·- 0 A a
Patterns and tactics together constitute the software architect's primary tools of the trade. How do they
relate to each other?
Patterns Comprise Tactics
As we said in the introduction to this chapter, tactics are the "building blocks" of design from which
architectural patterns are created. Tactics are atoms and patterns are molecules. Most patterns consist of
(are constructed from) several different tactics, and although these tactics might all serve a common
purpose such as promoting modifiability, for example they are often chosen to promote difef rent
quality attributes. For example, a tactic might be chosen that makes an availability pattern more secure,
or that mitigates the performance impact of a modifiability pattern.
Consider the example of the layered pattern, the most common pattern in all of software architecture
(virtually all nontrivial systems employ layering). The layered pattern can be seen as the amalgam of
several tactics increase semantic coherence, abstract common services, encapsulate, restrict
communication paths, and use an intermediary. For example:
• Increase semantic coherence. The goal of ensuring that a layer's responsibilities all work
together without excessive reliance on other layers is achieved by choosing responsibilities that
have semantic coherence. Doing so binds responsibilities that are likely to be affected by a
change. For example, responsibilities that deal with hardware should be allocated to a hardware
layer and not to an application layer; a hardware responsibility typically does not have semantic
coherence with the application responsibilities.
• Restrict dependencies. Layers define an ordering and only allow a layer to use the services of
its adjacent lower layer. The possible communication paths are reduced to the number of layers
minus one. This limitation has a great influence on the dependencies between the layers and
makes it much easier to limit the side effects of replacing a layer.
Without any one of its tactics, the pattern might be ineffective. For example, if the restrict
dependencies tactic is not employed, then any function in any layer can call any other function in any
other layer, destroying the low coupling that makes the layering pattern effective. If the increase
semantic coherence tactic is not employed, then functionality could be randomly sprinkled throughout
the layers, destroying the separation of concerns, and hence ease of modification, which is the prime
motivation for employing layers in the first place.
Table 1 3 . 1 2 shows a number of the architectural patterns described in the book Pattern-Oriented
Software Architecture Volume 1 : A System of Patterns, by Buschmann et al., and shows which
modifiability tactics they employ.
Table 13.12. Architecture Patterns and Corresponding Tactics ([Bachmann 07])
Increase
Cohesion
.u c: ....
..c. 0 m e
E E
CI) Q,)
() Q)
-
W o () ca
-
ID C - (IJ 􀑍
(I.) G.) 􀞓 􀞔 0 m :a- Q. CI) <D .b ·:; ca
1- .&:, 􀞕 􀞖 (.) U Q c;
Pattern .E o oil( (I) UJ
Layered X X X
Pipes and Filters X X
Blackboard X X
Broker X X X
Model View X X
Controller
Presentation X X
Abstraction Control
Microkernel X X X
Reflection X X
Using Tactics to Augment Patterns
·
·
·-
ModiflabiiJty
Reduce Coupling
-
ell e >
1- G)
CJ) E 􀃝
...J
Q,
a. 0 c
cc (.)
ca . 0 ·- 1- "0 II)􀑎
:: - .r::.
-
(.l c
�» ... o
'C 0 m E ca
CD Q) ..
... . .. 0 - CD w £ <D Q) ·- 0
0 CD ca VI .,. ca .o
:::) a: c.. :::l £ a: ·ct
X X X
X X
X X X
X X X
X
X X
X X
0 A a
Defer Binding
Time
<1,)
e ·-
G.) c t-;- Cl)
s 0 Q. e ..... ·- 􀑌 ·-
- - 􀃛 en - c: m c tn ;::, ;: - c ::1 c: IJ: «< (/) ·- C:·-
·-
<IJ C) Q) "'C Cll 'C
(j!) CIJ ct) .c_ Cl). c ·-- :::) 0: 􀒚 en ::l· m
X
X X
X
X
A pattern is described as a solution to a class of problems in a general context. When a pattern is chosen
and applied, the context of its application becomes very specific. A documented pattern is therefore
underspecified with respect to applying it in a specific situation.
To make a pattern work in a given architectural context, we need to examine it from two
perspectives:
• The inherent quality attribute tradeoffs that the pattern makes. Patterns exist to achieve certain
quality attributes, and we need to compare the ones they promote (and the ones they diminish)
with our needs.
• Other quality attributes that the pattern isn't directly concerned with, but which it nevertheless
affects, and which are important in our application.
To illustrate these concerns in particular, and how to use tactics to augment patterns in general,
we'll use the broker pattern as a starting point.
The broker pattern is widely used in distributed systems and dates back at least to its critical role in
CORBA-based systems. Broker is a crucial component of any large-scale, dynamic, service-oriented
architecture.
Using this pattern, a client requesting some information from a server does not need to know the
location or APis of the server. The client simply contacts the broker (typically through a client-side
proxy); this is illustrated in the UML sequence diagram in Figure 13. 16.
process boundary
•
I
process boundary
·
·
·- 0
;Client :QiientPrQ�Y • :Broker :Server I
• •
I •
I I
I performFunctionA() t
fesultA .... - - - - - ' I
I I
• •
• '
I • I I I ' register6erver()
I I I I I
I IC I .II • OK ' I
I - - . -
'• I • I I
locateS􀁄 rver() I
I •-'-
_ _ s�v� D- '' I
' 􀑋 I
marshall Request()
4 I I
I
I
•
I
I
•
•
•
'
I
I '
I
'
I sendR􀛡quest() '
I •
' •
' I '
I I
' ' I
'
'I I
'I I
I I
'
I I I
I I
I l '
' I
• I
' locateCiient()
I I I ' • ' - ,.S:liEi􀛠tlp ' - •
'
sendR􀃚sponse()
• ' '
• '
I
·- I I
unmarsttj!JI Aespon;>eO
'
•
I
I
I
I '
•
I
I
I
•
I
•
I
I
I
•
I
I
I
I
I
•
I
I
I
I
I
I
I
I
I
I
I
I
'
I
I
I
I
I
I
I
I
unmarsh􀂏IIRequest()1
􀑊 I
I
r
pertormFunctionA , I f4- _ ':!S􀛧A _ T
-'
I
marshaUF esponse() 1
I
I
I
I
I
I
I
I
I
I
I
A a
Figure 13.16. A sequence diagram showing a typical client-server interaction mediated by a
broker
Weaknesses of the Broker Pattern
In Section 1 3 .2 we enumerated several weaknesses of the broker pattern. Here we will examine these
weaknesses in more detail. The broker pattern has several weaknesses with respect to certain quality
attributes. For example:
• Availability. The broker, if implemented as suggested in Figure 1 3 .6, is a single point of failure.
The liveness of servers, the broker, and perhaps even the clients need to be monitored, and
repair mechanisms must be provided.
• Performance. The levels of indirection between the client (requesting the information or
service) and the server (providing the information or service) add overhead, and hence add
latency. Also, the broker is a potential performance bottleneck if direct communication between
the client and server is not desired (for exatnple, for security reasons).
• Testability. Brokers are employed in complex multi-process and multi-processor systems. Such
systems are typically highly dynamic. Requests and responses are typically asynchronous. All
of this makes testing and debugging such systems extremely difficult. But the description of the
broker pattern provides no testing functionality, such as testing interfaces, state or activity
capture and playback capabilities, and so forth.
·
·
·- 0 A a
• Security. Because the broker pattern is primarily used when the system spans process and
processor boundaries such as on web-based systems security is a legitimate concern.
However, the broker pattern as presented does not offer any means to authenticate or authorize
clients or servers, and provides no means of protecting the communication between clients and
servers.
Of these quality attributes, the broker pattern is mainly associated with poor performance (the welldocumented
price for the loose coupling it brings to systems). It is largely unconcerned with the other
quality attributes in this list; they aren't mentioned in most published descriptions. But as the other
bullets show, they can be unacceptable "collateral damage" that come with the broker's benefits.
Improving the Broker Pattern with Tactics
How can we use tactics to plug the gaps between the "out of the box" broker pattern and a version of it
that will let us meet the requirements of a demanding distributed system? Here are some options:
• The increase available resources performance tactic would lead to multiple brokers, to help
with performance and availability.
• The maintain multiple copies tactic would allow each of these brokers to share state, to ensure
that they respond identically to client requests.
• Load balancing (an application of the scheduling resources tactic) would ensure that one broker
is not overloaded while another one sits idle.
• Heartbeat, exception detection, or ping/echo would give the replicated brokers a way of
notifying clients and notifying each other when one of them is out of service, as a means of
detecting faults.
Of course, each of these tactics brings a tradeoff. Each complicates the design, which will now take
longer to implement, be more costly to acquire, and be more costly to maintain. Load balancing
introduces indirection that will add latency to each transaction, thus giving back some of the
performance it was intended to increase. And the load balancer is a single point of failure, so it too must
be replicated, further increasing the design cost and complexity.
13.4. Using Tactics Together
·
·
·- 0 A a
Tactics, as described in Chapters 5-1 1 , are design primitives aimed at managing a single quality
attribute response. Of course, this is almost never true in practice; every tactic has its main effect to
manage modifiability or performance or safety, and so on and it has its side effects, its tradeoffs. On
the face of it, the situation for an architect sounds hopeless. Whatever you do to improve one quality
attribute endangers another. We are able to use tactics profitably because we can gauge the direct and
side effects of a tactic, and when the tradeoff is acceptable, we employ the tactic. In doing so we gain
some benefit in our quality attribute of interest while giving up something else (with respect to a
different quality attribute and, we hope, of a much smaller magnitude).
This section will walk through an example that shows how applying tactics to a pattern can produce
negative effects in one area, but how adding other tactics can bring relief and put you back in an
acceptable design space. The point is to show the interplay between tactics that you can use to your
advantage. Just as some combinations of liquids are noxious whereas others yield lovely things like
strawberry lemonade, tactics can either make things worse or put you in a happy design space. Here,
then, is a walkthrough of tactic mixology.
Consider a system that needs to detect faults in its components. A common tactic for detecting faults
is ping/echo. Let us assume that the architect has decided to employ ping/echo as a way to detect failed
components in the system. Every tactic has one or more side effects, and ping/echo is no different.
Common considerations associated with ping/echo are these:
• Security. How to prevent a ping flood attack?
• Performance. How to ensure that the performance overhead of ping/echo is small?
• Modifiability. How to add ping/echo to the existing architecture?
We can represent the architect's reasoning and decisions thus far as shown in Figure 1 3 . 1 7.
Add to
system
System
Ping/Echo
Ping
flood
Performance
overhead
Figure 13.17. Partial availability decisions
Suppose the architect determines that the performance tradeoff (the overhead of adding ping/echo to
the system) is the most severe. A tactic to address the performance side effect is increase available
·
·
·- 0
resources. Considerations associated with increase available resources are these:
• Cost. Increased resources cost more.
• Performance. How to utilize the increased resources efficiently?
This set of design decisions can now be represented as shown in Figure 1 3 . 1 8 .
Add to
system
System
Ping/Echo
Ping
flood
Performance
overhead
Increase Available
Resou rces
Cost Resource
utilization
Figure 13.18. More availability decisions
A a
Now the architect chooses to deal with the resource utilization consequence of employing increase
available resources. These resources must be used efficiently or else they are simply adding cost and
complexity to the system. A tactic that can address the efficient use of resources is the employment of a
scheduling policy. Considerations associated with the scheduling policy tactic are these:
• Modifiability. How to add the scheduling policy to the existing architecture?
• Modifiability. How to change the scheduling policy in the future?
The set of design decisions that includes the scheduling policy tactic can now be represented as in
Figure 1 3 . 19.
Add to
system
System
Ping/Echo
Ping
flood
Performance
overhead
·
·
·-
I ncrease Available
Resources
Cost Resource
utilization
0
Scheduling
Policy
Add to
system
Modify
policy
Figure 13.19. Still more availability decisions
A a
Next the architect chooses to deal with the modifiability consequence of employing a scheduling
policy tactic. A tactic to address the addition of the scheduler to the system is to use an intermediary,
which will insulate the choice of scheduling policy from the rest of the system. One consideration
associated with use an intermediary is this:
• Modifiability. How to ensure that all communication passes through the intermediary?
We can now represent the tactics-based set of architectural design decisions made thus far as in
Figure 1 3 .20.
Add to
system
System
Ping/Echo
Ping
flood
Performance
overhead
·
·
·-
Increase available
Resources
Cost Resource
utilization
Scheduling
Policy
Add to
system
Ensure usage
Modify
policy
0
Figure 13.20. As far as we go with availability decisions
A a
A tactic to address the concern that all communication passes through the intermediary is restrict
dependencies. One consideration associated with the restrict dependencies tactic is this:
• Performance. How to ensure that the performance overhead of the intermediary is not
excessive?
This design problem has now become recursive! At this point (or in fact, at any point in the tree of
design decisions that we have described) the architect might determine that the performance overhead
of the intermediary is small enough that no further design decisions need to be made.
Applying successive tactics is like moving through a game space, and it's a little like chess: Good
players are able to see the consequences of the move they're considering, and the very good players are
able to look several moves ahead. In Chapter 17 we'll see the activity of design treated as an exercise of
"generate and test": propose a design and test it to see if it's satisfactory. Applying tactics to an existing
design solution, such as a pattern, is one technique for generating a design for subsequent testing.
13.5. Summary
An architectural pattern
·
·
·-
• is a package of design decisions that is found repeatedly in practice,
• has known properties that permit reuse, and
• describes a class of architectures.
0 A a
Because patterns are (by definition) found repeatedly in practice, one does not invent them; one
discovers them.
Tactics are simpler than patterns. Tactics typically use just a single structure or computational
mechanism, and they are meant to address a single architectural force. For this reason they give more
precise control to an architect when making design decisions than patterns, which typically combine
multiple design decisions into a package. Tactics are the "building blocks" of design from which
architectural patterns are created. Tactics are atoms and patterns are molecules.
An architectural pattern establishes a relationship between:
• A context. A recurring, common situation in the world that gives rise to a problem.
• A problem. The problem, appropriately generalized, that arises in the given context.
• A solution. A successful architectural resolution to the problem, appropriately abstracted.
Complex systems exhibit multiple patterns at once.
Patterns can be categorized by the dominant type of elements that they show: module patterns show
modules, component-and-connector patterns show components and connectors, and allocation patterns
show a combination of software elements (modules, components, connectors) and nonsoftware
elements. Most published patterns are C&C patterns, but there are module patterns and allocation
patterns as well. This chapter showed examples of each type.
A pattern is described as a solution to a class of problems in a general context. When a pattern is
chosen and applied, the context of its application becomes very specific. A documented pattern is
therefore underspecified with respect to applying it in a specific situation. We can make a pattern more
specific to our problem by augmenting it with tactics. Applying successive tactics is like moving
through a game space, and is a little like chess: the consequences of the next move are important, and
looking several moves ahead is helpful.
13.6. For Further Reading
·
·
·- 0 A a
There are many existing repositories of patterns and books written about patterns. The original and most
well-known work on object-oriented design patterns is by the "Gang of Four" [Gamma 94].
The Gang ofF our's discussion of patterns included patterns at many levels of abstraction. In this
chapter we have focused entirely on architectural patterns. The patterns that we have presented here are
intended as representative examples. This chapter's inventory of patterns is in no way meant to be
exhaustive. For example, while we describe the SOA pattern, entire repositories of SOA patterns
(refinements of the basic SOA pattern) have been created. A good place to start is www.soapatterns.org.
Some good references for pattern-oriented architecture are [Buschmann 96], [Hanmer 07], [Schmidt
00], and [Kircher 03].
A good place to learn more about the map-reduce pattern is Google's foundational paper on it [Dean
04}.
Map-reduce is the tip of the spear of the so-called "NoSQL" movement, which seeks to displace the
relational database from its venerable and taken-for-granted status in large data-processing systems. The
movement has some of the revolutionary flavor of the Agile movement, except that NoSQL advocates
are claiming a better (for them) technology, as opposed to a better process. You can easily find NoSQL
podcasts, user forums, conferences, and blogs; it's also discussed in Chapter 26.
[Bachmann 07] discusses the use of tactics in the layered pattern and is the source for some of our
discussion of that.
The passage in this chapter about augmenting ping/echo with other tactics to achieve the desired
combination of quality attributes is based on the work of Kiran Kumar and TV Prabhakar [Kumar 1 Oa]
and [Kumar l Ob].
[Urdangarin 08] is the source of the work assignment patterns described in Section 1 3 .2.
The Adventure Builder system shown in Figures 1 3 . 1 1 and 1 3 . 1 5 comes from [AdvBuilder 1 0].
13.7. Discussion Questions
·
·
·- 0 A a
1. What's the difference between an architectural pattern, such as those described in this chapter
and in the Pattern-Oriented Software Architecture series of books, and design patterns, such as
those collected by the Gang of Four in 1994 and many other people subsequently? Given a
pattern, how would you decide whether it was an architectural pattern, a design pattern, a code
pattern, or something else?
2. SOA systems feature dynamic service registration and discovery. Which quality attributes does
this capability enhance and which does it threaten? If you had to make a recommendation to your
boss about whether your company's SOA system should use external services it discovers at
runtime, what would you say?
3. Write a complete pattern description for the "competence center" work assignment pattern
mentioned in Section 1 3 .2.
4. For a data set that i s a set of web pages, sketch a map function and a reduce function that together
provide a basic search engine capability.
5. Describe how the layered pattern makes use of these tactics: abstract common services,
encapsulate, and use an intermediary.
·
·
·-
14. Quality Attribute Modeling and Analysis
0
Do not believe in anything simply because you
have heard it . . . Do not believe in anything
merely on the authority of your teachers and
elders. Do not believe in traditions because they
have been handed down for many generations.
But after observation and analysis, when you find
that anything agrees with reason and is
conducive to the good and benefit of one and all,
then accept it and live up to it.
-Prince Gautama Siddhartha
A a
In Chapter 2 we listed thirteen reasons why architecture is important, worth studying, and worth
practicing. Reason 6 is that the analysis of an architecture enables early prediction of a system' s
qualities. This is an extraordinarily powerful reason! Without it, we would be reduced to building
systems by choosing various structures, implementing the system, measuring the system for its quality
attribute responses, and all along the way hoping for the best. Architecture lets us do better than that,
much better. We can analyze an architecture to see how the system or systems we build from it will
perform with respect to their quality attribute goals, even before a single line of code has been written.
This chapter will explore how.
The methods available depend, to a large extent, on the quality attribute to be analyzed. Some
quality attributes, especially performance and availability, have well-understood and strongly validated
analytic modeling techniques. Other quality attributes, for example security, can be analyzed through
checklists. Still others can be analyzed through back-of-the-envelope calculations and thought
experiments.
Our topics in this chapter range from the specific, such as creating models and analyzing checklists,
to the general, such as how to generate and carry out the thought experiments to perform early (and
necessarily crude) analysis. Models and checklists are focused on particular quality attributes but can
aid in the analysis of any system with respect to those attributes. Thought experiments, on the other
hand, can consider multiple quality attributes simultaneously but are only applicable to the specific
system under consideration.
·
·
·- 0
14.1. Modeling Architectures to Enable Quality Attribute Analysis
A a
Some quality attributes, most notably performance and availability, have well-understood, time-tested
analytic models that can be used to assist in an analysis. By analytic model, we mean one that supports
quantitative analysis. Let us first consider performance.
Analyzing Performance
In Chapter 1 2 we discussed the fact that models have parameters, which are values you can set to
predict values about the entity being modeled (and in Chapter 1 2 we showed how to use the parameters
to help us derive tactics for the quality attribute associated with the model). As an example we showed a
queuing model for performance as Figure 12.2, repeated here as Figure 14. 1 . The parameters of this
model are the following:
• The arrival rate of events
• The chosen queuing discipline
• The chosen scheduling algorithtn
• The service time for events
• The network topology
• The network bandwidth
• The routing algorithm chosen
--· Arrivals
Queue
Scheduling _
algorithm
Server
Routrng of ..........,
messages
Figure 14.1. A queuing model of performance
Results
In this section, we discuss how such a model can be used to understand the latency characteristics of
an architectural design.
To apply this model in an analytical fashion, we also need to have previously made some
architecture design decisions. We will use model-view-controller as our example here. MVC, as
·
·
·- 0 A a
presented in Section 1 3 .2, says nothing about its deployment. That is, there is no specification of how
the model, the view, and the controller are assigned to processes and processors; that's not part of the
pattern's concern. These and other design decisions have to be made to transform a pattern into an
architecture. Until that happens, one cannot say anything with authority about how an MVC-based
implementation will perform. For this example we will assume that there is one instance each of the
model, the view, and the controller, and that each instance is allocated to a separate processor. Figure
14.2 shows MVC following this allocation scheme.
1\ I
I
Internet
I <<deploy>>
I
I (]
<<component>>
VIew
intranet
<<deploy>>
I
<<component>>
Controller
1\ I
I
I <<deploy>>
I
I
<<Component>:>
Model
Key: UML 2.Q
Figure 14.2. An allocation view, in UML, of a model-view-controller architecture
Given that quality attribute models such as the performance model shown in Figure 14. 1 already
exist, the problem becomes how to map these allocation and coordination decisions onto Figure 14. 1 .
Doing this yields Figure 14.3. There are requests coming from users outside the system labeled as 1 in
Figure 14.3 arriving at the view. The view processes the requests and sends some transformation of
the requests on to the controller labeled as 2. Some actions of the controller are returned to the viewlabeled
as 3 . The controller sends other actions on to the model labeled 4. The model performs its
activities and sends information back to the view labeled 5 .
Users
generate
requests
1
View
2
·
·
·-
5
3
0
Model
Controller
Figure 14.3. A queuing model of performance for MVC
4
To analyze the model in Figure 14.3, a number of items need to be known or estimated:
• The frequency of arrivals from outside the system
• The queuing discipline used at the view queue
• The time to process a message within the view
• The number and size of messages that the view sends to the controller
• The bandwidth of the network that connects the view and the controller
• The queuing discipline used by the controller
• The time to process a message within the controller
• The number and size of messages that the controller sends back to the view
• The bandwidth of the network used for messages from the controller to the view
• The number and size of messages that the controller sends to the model
• The queuing discipline used by the model
• The time to process a message within the model
• The number and size of messages the model sends to the view
• The bandwidth of the network connecting the model and the view
A a
Given all of these assumptions, the latency for the system can be estimated. Sometimes well-known
formulas from queuing theory apply. For situations where there are no closed-form solutions, estimates
can often be obtained through simulation. Simulations can be used to make more-realistic assumptions
such as the distribution of the event arrivals. The estimates are only as good as the assumptions, but
they can serve to provide rough values that can be used either in design or in evaluation; as better
information is obtained, the estimates will improve.
A reasonably large number of parameters must be known or estimated to construct the queuing
·
·
·- 0 A a
model shown in Figure 14.3. The model must then be solved or simulated to derive the expected
latency. This is the cost side of the cost/benefit of performing a queuing analysis. The benefit side is
that as a result of the analysis, there is an estimate for latency, and ''what if' questions can be easily
answered. The question for you to decide is whether having an estimate of the latency and the ability to
answer "what if' questions is worth the cost of performing the analysis. One way to answer this
question is to consider the importance of having an estimate for the latency prior to constructing either
the system or a prototype that simulates an architecture under an assumed load. If having a small
latency is a crucial requirement upon which the success of the system relies, then producing an estimate
is appropriate.
Performance is a well-studied quality attribute with roots that extend beyond the computer industry.
For example, the queuing model given in Figure 1 4 . 1 dates from the 1 930s. Queuing theory has been
applied to factory floors, to banking queues, and to many other domains. Models for real-time
performance, such as rate monotonic analysis, also exist and have sophisticated analysis techniques.
Analyzing Availability
Another quality attribute with a well-understood analytic framework is availability.
Modeling an architecture for availability or to put it more carefully, modeling an architecture to
determine the availability of a system based on that architecture is a matter of determining the failure
rate and the recovery time. As you may recall from Chapter 5, availability can be expressed as
MTBF
(MTBF + MTTR)
This models what is known as steady-state availability, and it is used to indicate the uptime of a
system (or component of a system) over a sufficiently long duration. In the equation, MTBF is the mean
time between failure, which is derived based on the expected value of the implementation's failure
probability density function (PDF), and MTTR refers to the mean time to repair.
Just as for performance, to model an architecture for availability, we need an architecture to
analyze. So, suppose we want to increase the availability of a syste1n that uses the broker pattern, by
applying redundancy tactics. Figure 14.4 illustrates three well-known redundancy tactics from Chapter
.5.: active redundancy, passive redundancy, and cold spare. Our goal is to analyze each redundancy
option for its availability, to help us choose one.
BrokerAcrtve
Active
Redundancy
Broker
ACTIVE
Passive
Redundancy
Broker
ACTIVE
(Cold) Spare
...
ooiL
State
Synchronization
Client-Server
Proxy Traffic
Periodic
•
, -•
Checkpoint Data
·
Clisnt-Server
Proxy Traffic
Client-Server
Proxy Traffic
Broker SPARE
Broker SPARE
Broker SPARE
·
·
·-
Key:
0
process
--+Jill message
Figure 14.4. Redundancy tactics, as applied to a broker pattern
A a
As you recall, each of these tactics introduces a backup copy of a component that will take over in
case the primary component suffers a failure. In our case, a broker replica is employed as the redundant
spare. The difference among them is how up to date with current events each backup keeps itself:
• In the case of active redundancy, the active and redundant brokers both receive identical copies
of the 1nessages received from the client and server proxies. The internal broker state is
synchronously maintained between the active and redundant spare in order to facilitate rapid
fail over upon detection of a fault in the active broker.
• For the passive redundancy implementation, only the active broker receives and processes
messages from the client and server proxies. When using this tactic, checkpoints of internal
broker state are periodically transmitted from the active broker process to the redundant spare,
using the checkpoint-based rollback tactic.
• Finally, when using the cold spare tactic, only the active broker receives and processes
messages from the client and server proxies, because the redundant spare is in a dormant or
even powered-off state. Recovery strategies using this tactic involve powering up, booting, and
·
·
·- 0 A a
loading the broker implementation on the spare. In this scenario, the internal broker state is
rebuilt organically, rather than via synchronous operation or checkpointing, as described for the
other two redundancy tactics.
Suppose further that we will detect failure with the heartbeat tactic, where each broker (active and
spare) periodically transmits a heartbeat message to a separate process responsible for fault detection,
correlation, reporting, and recovery. This fault manager process is responsible for coordinating the
transition of the active broker role from the failed broker process to the redundant spare.
You can now use the steady state model of availability to assign values for MTBF and MTTR for
each of the three redundancy tactics we are considering. Doing so will be an exercise left to the reader
(as you'll see when you reach the discussion questions for this chapter). Because the three tactics differ
primarily in how long it takes to bring the backup copy up to speed, MTTR will be where the difference
among the tactics shows up.
More sophisticated models of availability exist, based on probability. In these models, we can
express a probability of failure during a period of time. Given a particular MTBF and a time duration T,
the probability of failure R is given by
R(T) = exp�(M7-�F)
You will recall from Statistics 1 0 1 that:
• When two events A and B are independent, the probability that A or B will occur is the sutn of
the probability of each event: P (A o r B ) = P (A) + P ( B ) .
• When two events A and B are independent, the probability of both occurring is P (A and B ) =
P (A ) • P ( B ) .
• When two events A and B are dependent, the probability of both occurring is P (A and B ) = P (A)
• P ( B 1 A) , where the last term means "the probability of B occurring, given that A occurs."
We can apply simple probability arithmetic to an architecture pattern for availability to determine
the probability of failure of the pattern given the probability of failure of the individual components
(and an understanding of their dependency relations). For example, in an architecture pattern employing
the passive redundancy tactic, let's assume that the failure of a component (which at any moment might
be acting as either the primary or backup copy) is independent of a failure of its counterpart, and that
the probability of failure of either is the same. Then the probability that both will fail is 1 - P ( F ) * * 2 .
Still other models take into account different levels of failure severity and degraded operating states
of the system. Although the derivation of these formulas is outside the scope of this chapter, you end up
with formulas that look like the following for the three redundancy tactics we've been discussing,
where the values C2 through C5 are references to the MTBF column of Table 1 4. 1 , D2 through D4
refer to the Active column, E2 through E3 refer to the Passive column, and F2 through F3 refer to the
Spare column.
• Active redundancy:
• Availability(MTTR): 1 -((SUM(C2:C5) + D3) x D2)/((C2 x (C2 + C4 + D3) + ((C2 + C4 +
D2) X (C3 + C5)) + ((C2 + C4) X (C2 + C4 + D3))))
·
·
·- 0 A a
• P(Degraded) = ((C3 + C5) x D2)/((C2 x (C2 + C4 + D3) + ((C2 + C4 + D2) x (C3 + C5)) +
((C2 + C4) X (C2 + C4 + D3))))
• Passive redundancy:
• Availability(MTTR_passive) = 1 - ((SUM(C2:C5) + E3) x E2)/((C2 x (C2 + C4 + E3) +
((C2 + C4 + E2) X (C3 + C5)) + ((C2 + C4) X (C2 + C4 + E3))))
• P(Degraded) = ((C3 + C5) x E2)/((C2 x (C2 + C4 + E3) + ((C2 + C4 + E2) x (C3 + C5)) +
((C2 + C4) X (C2 + C4 + E3))))
• Spare:
• Availability(MTTR) = 1 - ((SUM(C2:C5) + F3) x F2)/((C2 x (C2 + C4 + F3) + ((C2 + C4
+ F2) X (C3 + C5)) + ((C2 + C4) X (C2 + C4 + F3))))
• P(Degraded) = ((C3 + C5) x F2)/((C2 x (C2 + C4 + F3) + ((C2 + C4 + F2) x (C3 + C5)) +
((C2 + C4) X (C2 + C4 + F3))))
Table 14.1. Calculated Availability for an Availability-Enhanced Broker Implementation
Failure MTBF
FuncUon Severity (Hours)
Hardware I 1 250,000
'
2 50,000
Software I 1 50,000
2 10,000
Availability I
Active
Redundancy
(Hot Spare)
5
30
5
30
0.9999998
MMTR (Seconds)
Passive
Redundancy Spare
(W'arm Spare) (Cold Spare)
5 300
30 30
5 300
30 30
0.999990 0.9'994
Plugging in these values for the parameters to the equations listed above results in a table like Table
1 4. 1 , which can be easily calculated using any spreadsheet tool. Such a calculation can help in the
selection of tactics.
The Analytic Model Space
As we discussed in the preceding sections, there are a growing number of analytic models for some
aspects of various quality attributes. One of the quests of software engineering is to have a sufficient
number of analytic models for a sufficiently large number of quality attributes to enable prediction of
the behavior of a designed system based on these analytic models. Table 14.2 shows our current status
with respect to this quest for the seven quality attributes discussed in Chapters 5-1 1 .
Table 14.2. A Summary of the Analytic Model Space
Quality
Attribute
Availability
l'ntellectoal Basis
Markov models;
statistical models
Maturity/Gaps
·
·
·- 0
Moderate maturity; mature in the
hardware reliability domain, less mature
in the software domain. Requires models
that speak to state recovery and for which
failure percentages can be attributed to
software.
lnteroperability Conceptual framework Low maturity; models require substantial
human interpretation and input.
Modifiability
Performance
Security
Testability
Usability
Coupling and cohesion
metrics; cost models
Queuing theory; realtime
scheduling theory
No architectural models
Component interaction
me tries
No arch;tectural models
Substantial research in academia; still
requires more empirical support in realworld
environments.
High maturity; requires constderable
education and training to use properly.
Low maturrty; little empirical validation.
A a
As the table shows, the field still has a great deal of work to do to achieve the quest for wellvalidated
analytic models to predict behavior, but there is a great deal of activity in this area (see the
"For Further Reading" section for additional papers). The remainder of this chapter deals with
techniques that can be used in addition to analytic models.
14.2. Quality Attribute Checklists
·
·
·- 0 A a
For some quality attributes, checklists exist to enable the architect to test compliance or to guide the
architect when making design decisions. Quality attribute checklists can come from industry consortia,
from government organizations, or from private organizations. In large organizations they may be
developed in house.
These checklists can be specific to one or more quality attributes; checklists for safety, security, and
usability are common. Or they may be focused on a particular domain; there are security checklists for
the financial industry, industrial process control, and the electric energy sector. They may even focus on
some specific aspect of a single quality attribute: cancel for usability, as an example.
For the purposes of certification or regulation, the checklists can be used by auditors as well as by
the architect. For example, two of the items on the checklist of the Payment Card Industry (PCI) are to
only persist credit card numbers in an encrypted form and to never persist the security code from the
back of the credit card. An auditor can ask to examine stored credit card data to determine whether it
has been encrypted. The auditor can also examine the schema for data being stored to see whether the
security code has been included.
This example reveals that design and analysis are often two sides of the same coin. By considering
the kinds of analysis to which a system will be subjected (in this case, an audit), the architect will be led
into making important early architectural decisions (making the decisions the auditors will want to
find).
Security checklists usually have heavy process components. For example, a security checklist might
say that there should be an explicit security policy within an organization, and a cognizant security
officer to ensure compliance with the policy. They also have technical components that the architect
needs to examine to determine the implications on the architecture of the system being designed or
evaluated. For example, the following is an item from a security checklist generated by a group
chartered by an organization of electric producers and distributors. It pertains to embedded systems
delivering electricity to your ho-me:
A designated syste1n or systems shall daily or on request obtain current version numbers,
installation date, configuration settings, patch level on all elements of a [portion of the
electric distribution] system, compare these with inventory and configuration databases,
and log all discrepancies.
In Search of a Grand Unified Theory for Quality Attributes
How do we create analytic models for those quality attribute aspects for which none
currently exist? I do not know the answer to this question, but if we had a basis set for
quality attributes, we would be in a better position to create and validate quality
attribute models. By basis set I mean a set of orthogonal concepts that allow one to
define the existing set of quality attributes. Currently there is much overlap among
quality attributes; a basis set would enable discussion of tradeoffs in terms of a
common set of fundamental and possibly quantifiable concepts. Once we have a basis
·
·
·- 0 A a
set, we could develop analytic models for each of the elements of the set, and then an
analytic model for a particular quality attribute becomes a composition of the models of
the portions of the basis set that make up that quality attribute.
What are some of the elements of this basis set? Here are some of my candidates:
• Time. Time is the basis for performance, some aspects of availability, and some
aspects of usability. Time will surely be one of the fundamental concepts for defining
quality attributes.
• Dependencies among structural elements. Modifiability, security, availability, and
performance depend in some form or another on the strength of connections among
various structural elements. Coupling is a form of dependency. Attacks depend on
being able to move frotn one compromised element to a currently uncompromised
element through some dependency. Fault propagation depends on dependencies. And
one of the key elements of performance analysis is the dependency of one
computation on another. Enumeration of the fundamental forms of dependency and
their properties will enable better understanding of many quality attributes and their
interaction.
• Access. How does a system promote or deny access through various mechanisms?
Usability is concerned with allowing smooth access for humans; security is
concerned with allowing smooth access for some set of requests but denying access
to another set of requests. Interoperability is concerned with establishing connections
and accessing information. Race conditions, which undermine availability, come
about through unmediated access to critical computations.
These are some of my candidates. I am sure there are others. The general problem is
to define a set of candidates for the basis set and then show how current definitions of
various quality attributes can be recast in terms of the elements of the basis set. I am
convinced that this is a problem that needs to be solved prior to making substantial
progress in the quest for a rich enough set of analytic models to enable prediction of
system behavior across the quality attributes important for a system.
-LB
This kind of rule is intended to detect mal ware masquerading as legitimate components of a system.
The architect will look at this item and conclude the following:
• The embedded portions of the system should be able to report their version number, installation
date, configuration settings, and patch levels. One technique for doing this is to use "reflection"
for each component in the system. Reflection now becomes one of the important patterns used
in this system.
• Each software update or patch should maintain this information. One technique for doing this is
to have automated update and patch mechanisms. The architecture could also realize this
functionality through reflection.
·
·
·- 0 A a
• A system must be designated to query the embedded components and persist the information.
This means
• There must be overall inventory and configuration databases.
• Logs of discrepancies between current values and overall inventory must be generated and
sent to appropriate recipients.
• There must be network connections to the embedded components. This affects the network
topology.
The creation of quality attribute checklists is usually a time-consuming activity, undertaken by
multiple individuals and typically refined and evolved over time. Domain specialists, quality attribute
specialists, and architects should all contribute to the development and validation of these checklists.
The architect should treat the items on an applicable checklist as requirements, in that they need to
be understood and prioritized. Under particular circumstances, an item in a checklist may not be met,
but the architect should have a compelling case as to why it is not.
·
·
·- 0
14.3. Thought Experiments and Back-of-the-Envelope Analysis
A a
A thought experiment is a fancy name for the kinds of discussions that developers and architects have
on a daily basis in their offices, in their meetings, over lunch, over whiteboards, in hallways, and around
the coffee machine. One of the participants might draw two circles and an arrow on the white board and
make an assertion about the quality attribute behavior of these two circles and the arrow in a particular
context; a discussion ensues. The discussion can last for a long time, especially if the two circles are
augmented with a third and one more arrow, or if some of the assumptions underlying a circle or an
arrow are still in flux. In this section, we describe this process somewhat more formally.
The level of formality one would use in performing a thought experiment is, as with most
techniques discussed in this book, a question of context. If two people with a shared understanding of
the system are performing the thought experiment for their own private purposes, then circles and lines
on a whiteboard are adequate, and the discussion proceeds in a kind of shorthand. If a third person is to
review the results and the third person does not share the common understanding, then sufficient details
must be captured to enable the third person to understand the argument perhaps a quick legend and a
set of properties need to be added to the diagram. If the results are to be included in documentation as
design rationale, then even more detail must be captured, as discussed in Chapter 1 8. Frequently such
thought experiments are accompanied by rough analyses back-of-the-envelope analyses based on
the best data available, based on past experiences, or even based on the guesses of the architects,
without too much concern for precision.
The purpose of thought experiments and back-of-the-envelope analysis is to find problems or
confirmation of the nonexistence of problems in the quality attribute requirements as applied to sunnyday
use cases. That is, for each use case, consider the quality attribute requirements that pertain to that
use case and analyze the use case with the quality attribute requirements in mind. Models and checklists
focus on one quality attribute. To consider other quality attributes, one must model or have a checklist
for the second quality attribute and understand how those models interact. A thought experiment may
consider several of the quality attribute requirements simultaneously; typically it will focus on just the
most important ones.
The process of creating a thought experiment usually begins with listing the steps associated with
carrying out the use case under consideration; perhaps a sequence diagram is employed. At each step of
the sequence diagram, the (mental) question is asked: What can go wrong with this step with respect to
any of the quality attribute requirements? For example, if the step involves user input, then the
possibility of erroneous input must be considered. Also the user may not have been properly
authenticated and, even if authenticated, may not be authorized to provide that particular input. If the
step involves interaction with another system, then the possibility that the input format will change after
some time must be considered. The network passing the input to a processor may fail; the processor
performing the step may fail; or the computation to provide the step may fail, take too long, or be
dependent on another computation that may have had problems. In addition, the architect must ask
about the frequency of the input, and the anticipated distribution of requests (e.g., Are service requests
regular and predictable or irregular and "bursty"?), other processes that might be competing for the
same resources, and so forth. These questions go on and on.
·
·
·- 0 A a
For each possible problem with respect to a quality attribute requirement, the follow-on questions
consist of things like these:
• Are there mechanisms to detect that problem?
• Are there mechanisms to prevent or avoid that problem?
• Are there mechanistns to repair or recover from that problem if it occurs?
• Is this a problem we are willing to live with?
The problems hypothesized are scrutinized in terms of a cost/benefit analysis. That is, what is the
cost of preventing this problem compared to the benefits that accrue if the problem does not occur?
As you tnight have gathered, if the architects are being thorough and if the problems are significant
(that is, they present a large risk for the system), then these discussions can continue for a long time.
The discussions are a normal portion of design and analysis and will naturally occur, even if only in the
mind of a single designer. On the other hand, the time spent performing a particular thought experiment
should be bounded. This sounds obvious, but every grey-haired architect can tell you war stories about
being stuck in endless meetings, trapped in the purgatory of"analysis paralysis."
Analysis paralysis can be avoided with several techniques:
• "Time boxing": setting a deadline on the length of a discussion.
• Estimating the cost if the problem occurs and not spending more than that cost in the analysis.
In other words, do not spend an inordinate amount of time in discussing minor or unlikely
potential problems.
Prioritizing the requirements will help both with the cost estimation and with the time estimation.
14.4. Experiments, Simulations, and Prototypes
·
·
·- 0 A a
In many environments it is virtually impossible to do a purely top-down architectural design; there are
too many considerations to weigh at once and it is too hard to predict all of the relevant technological
barriers. Requirements may change in dramatic ways, or a key assumption may not be met: We have
seen cases where a vendor-provided API did not work as specified, or where an API exposing a critical
function was simply missing.
Finding the sweet spot within the enormous architectural design space of complex systems is not
feasible by reflection and mathematical analysis alone; the models either aren't precise enough to deal
with all of the relevant details or are so complicated that they are impractical to analyze with tractable
mathematical techniques.
The purpose of experiments, simulations, and prototypes is to provide alternative ways of analyzing
the architecture. These techniques are invaluable in resolving tradeoffs, by helping to tum unknown
architectural parameters into constants or ranges. For example, consider just a few of the questions that
might occur when creating a web-conferencing system a distributed client-server infrastructure with
real-time constraints:
• Would moving to a distributed database from local flat files negatively impact feedback time
(latency) for users?
• How many participants could be hosted by a single conferencing server?
• What is the correct ratio between database servers and conferencing servers?
These sorts of questions are difficult to answer analytically. The answers to these questions rely on
the behavior and interaction of third-party components such as commercial databases, and on
performance characteristics of software for which no standard analytical models exist. The approach
used for the web-conferencing architecture was to build an extensive testing infrastructure that
supported simulations, experiments, and prototypes, and use it to compare the performance of each
incremental modification to the code base. This allowed the architect to determine the effect of each
form of improvement before committing to including it in the final system. The infrastructure includes
the following:
• A client simulator that makes it appear as though tens of thousands of clients are
simultaneously interacting with a conferencing server.
• Instrumentation to measure load on the conferencing server and database server with differing
numbers of clients.
The lesson from this experience is that experimentation can often be a critical precursor to making
significant architectural decisions. Experimentation must be built into the development process:
building experimental infrastructure can be time-consuming, possibly requiring the development of
custom tools. Carrying out the experiments and analyzing their results can require significant time.
These costs must be recognized in project schedules.
14.5. Analysis at Different Stages of the Life Cycle
·
·
·- 0 A a
Depending on your project's state of development, different forms of analysis are possible. Each form
of analysis comes with its own costs. And there are different levels of confidence associated with each
analysis technique. These are summarized in Table 14.3.
Table 14.3. Forms of Analysis, Their Life-Cycle Stage, Cost, and Confidence in Their Outputs
Life-Cycle Stage Form of Analysis Cost Confidence
Requirements Experience-based analogy Low Low-High
Requirements Back-of-the-envelope Low Low-Medium
Architecture Thought experiment Low Low-Medium
Architecture Checklist Low Medium
Architecture Analytlc model Low-Medium Medium
Architecture Simulation Medium Medium
Architecture Prototype Medium Medium-High
Implementation Experiment Medium-High Medium-High
Fielded System Instrumentation Medium-High High
The table shows that analysis performed later in the life cycle yields results that merit high
confidence. However, this confidence comes at a price. First, the cost of performing the analysis also
tends to be higher. But the cost of changing the system to fix a problem uncovered by analysis
skyrockets later in the life cycle.
Choosing an appropriate form of analysis requires a consideration of all of the factors listed in Table
14.3: What life-cycle stage are you currently in? How important is the achievement of the quality
attribute in question and how worried are you about being able to achieve the goals for this attribute?
And finally, how much budget and schedule can you afford to allocate to this form of risk mitigation?
Each of these considerations will lead you to choose one or more of the analysis techniques described in
this chapter.
14.6. Summary
·
·
·- 0 A a
Analysis of an architecture enables early prediction of a system's qualities. We can analyze an
architecture to see how the system or systems we build from it will perform with respect to their quality
attribute goals. Some quality attributes, most notably performance and availability, have wellunderstood,
time-tested analytic models that can be used to assist in quantitative analysis. Other quality
attributes have less sophisticated models that can nevertheless help with predictive analysis.
For some quality attributes, checklists exist to enable the architect to test compliance or to guide the
architect when making design decisions. Quality attribute checklists can come from industry consortia,
from government organizations, or from private organizations. In large organizations they may be
developed in house. The architect should treat the items on an applicable checklist as requirements, in
that they need to be understood and prioritized.
Thought experiments and back-of-the-envelope analysis can often quickly help find problems or
confirm the nonexistence of problems with respect to quality attribute requirements. A thought
experiment may consider several of the quality attribute requirements simultaneously; typically it will
focus on just the most important ones. Experiments, simulations, and prototypes allow the exploration
of tradeoffs, by helping to tum unknown architectural parameters into constants or ranges whose values
may be measured rather than estimated.
Depending on your project's state of development, different forms of analysis are possible. Each
form of analysis comes with its own costs and its own level of confidence associated with each analysis
technique.
14.7. For Further Reading
·
·
·- 0 A a
There have been many papers and books published describing how to build and analyze architectural
models for quality attributes. Here are just a few examples.
Availability
Many availability models have been proposed that operate at the architecture level of analysis. Just a
few of these are [Gokhale 05] and [Yacoub 02].
A discussion and comparison of different black-box and white-box models for determining software
reliability can be found in [Chandran 1 0].
A book relating availability to disaster recovery and business recovery is [Schmidt 1 0].
Interoperability
An overview of interoperability activities can be found in [Brownsword 04].
Modifiability
Modifiability is typically measured through complexity metrics. The classic work on this topic is
[C hidamber 94].
More recently, analyses based on design structure matrices have begun to appear [MacCormack 06].
Performance
Two of the classic works on software performance evaluation are [Smith 0 1] and [Klein 93].
A broad survey of architecture-centric performance evaluation approaches can be found in
[Koziolek 1 0].
Security
Checklists for security have been generated by a variety of groups for different domains. See for
example:
• Credit cards, generated by the Payment Card Industry:
www .pc isecuri tystandards. org/ security_ standards/
• Information security, generated by the National Institute of Standards and Technology (NIST):
[NIST 09].
• Electric grid, generated by Advanced Security Acceleration Project for the Smart Grid:
www .smartgridipedia.o rg/indexp. hp/ASAP -SG
• Common Criteria. An international standard (ISO/IEC 1 5408) for computer security
certification: www.commoncriteriaportal.org
Testability
Work in measuring testability from an architectural perspective includes measuring testability as the
measured complexity of a class dependency graph derived from UML class diagrams, and identifying
·
·
·- 0 A a
class diagrams that can lead to code that is difficult to test [Baudry 05]; and measuring controllability
and observability as a function of data flow [Le Traon 97].
Usability
A checklist for usability can be found at www.stcsig.org/usability/topics/articles/he-checklist.html
Safety
A checklist for safety is called the Safety Integrity Level: en. wikipedia.org/wiki/Safety_Integrity_ Level
Applications of Modeling and Analysis
For a detailed discussion of a case where quality attribute modeling and analysis played a large role in
determining the architecture as it evolved through a number of releases, see [Graham 07].
14.8. Discussion Questions
·
·
·- 0 A a
1. Build a spreadsheet for the steady-state availability equation MTBF I (MTBF + MTTR). Plug in
different but reasonable values for MTBF and MTTR for each of the active redundancy, passive
redundancy, and cold spare tactics. Try values for MTBF that are very large compared to MTTR,
and also try values for MTBF that are much closer in size to MTTR. What do these tell you about
which tactics you might want to choose for availability?
2. Enumerate as many responsibilities as you can that need to be carried out for providing a
"cancel" operation in a user interface. Hint: There are at least 2 1 of them, as indicated in a
publication by (strong hint!) one of the authors of this book whose last name (unbelievably strong
hint!) begins with "B."
3. The M/M/1 (look it up!) queuing model has been employed in computing systems for decades.
Where in your favorite computing system would this model be appropriate to use to predict
latency?
4. Suppose an architect produced Figure 14.5 while you were sitting watching him. Using thought
experiments, how can you determine the performance and availability of this system? What
assumptions are you making and what conclusions can you draw? How definite are your
conclusions?
Figure 14.5. Capture of a whiteboard sketch from an architect
·
·
·- 0 A a
Part Three. Architecture in the Life Cycle
Part I of this book introduced architecture and the various contextual lenses through which it could be
viewed. To recap from Chapter 3, those contexts include the following:
• Technical. What technical role does the software architecture play in the system or systems of
which it's a part? Part of the answer to this is what Part II of our book is about and the rest is
included in Part IV. Part II describes how decisions are made, and Part IV describes the
environment that determines whether the results of the decisions satisfy the needs of the
organization.
• Project life cycle. How does a software architecture relate to the other phases of a software
development life cycle? The answer to this is what Part III of our book is about.
• Business. How does the presence of a software architecture affect an organization's business
environment? The answer to this is what Part IV of our book is about.
• Professional. What is the role of a software architect in an organization or a development
project? The answer to this is threaded throughout the entire book, but especially in Chapter 24,
where we treat the duties, skills, and knowledge of software architects.
Part II concentrated on the technical context of software architecture. In our philosophy, this is
tantamount to understanding quality attributes. If you have a deep understanding of how architecture
affects quality attributes, then you have mastered most of what you need to know about making design
decisions.
Here in Part III we tum our attention to how to constructively apply that knowledge within the
context of a particular software development project. Here is where software architecture meets
software engineering: How do architecture concerns affect the gathering of requirements, the carrying
out of design decisions, the validation and capturing of the design, and the transformation of design into
implementation? In Part III, we'll find out.
A Word about Methods
·
·
·- 0 A a
Because this is a book about software architecture in practice, we've tried to spell out specific methods
in enough detail so that you can emulate them. You' II see PALM, a method for eliciting business goals
that an architecture should accommodate. You'll see Views and Beyond, an approach for documenting
architecture in a set of views that serve stakeholders and their concerns. You'll see AT AM, a method
for evaluating an architecture against stakeholders' ideas of what quality attributes it should provide.
You'll see CBAM, a method for assessing which evolutionary path of an architecture will best serve
stakeholders' needs.
All of these methods rely in some way or another on tapping stakeholders' knowledge about what
an architecture under development should provide. As presented in their respective chapters, each of
these methods includes a similar process of identifying the relevant stakeholders, putting them in a
room together, presenting a briefing about the method that the stakeholders have been assembled to
participate in, and then launching into the method.
So why is it necessary to put all of the stakeholders in the same room? The short answer is that it
isn't. There are (at least) three major engagement models for conducting an architecture-focused
method. Why three? Because we have identified two important factors, each of which has two values,
that describe four potential engagement models for gathering information from stakeholders. These two
factors are
1. Location (co-located or distributed)
2. Synchronicity (synchronous or asynchronous)
One option (co-located and asynchronous) makes no sense, and so we are left with three viable
engagement models. The advantages and disadvantages we've observed of each engagement model
follow.
Why has the big-meeting format (co-located, synchronous) tended to prevail? There are several
reasons:
• It compresses the time required for the method. Time on site for remote participants is
minimized, although as we will see, travel time is not considered in this argument. All of the
stakeholders are available with minimal external distractions.
• It emphasizes the importance of the method. Any meeting important enough to bring multiple
people together for an extended time must be judged by management to be important.
• It benefits from the helpful group mentality that emerges when people are in the same room
working toward a common goal. The group mentality fosters buy-in to the architecture and
buy-in to the reasons it exists. Putting stakeholders in the same room lets them open
communication paths with the architect and with each other, paths that will often remain open
long after the meeting has run its course. We always enjoy seeing business cards exchanged
with handshakes when stakeholders meet each other for the first time. Putting the architect in a
room full of stakeholders for a couple of days is a very healthy thing for any project.
Model
All stakeholders in
the same room for
the durati,on of the
exercise (co-located
and synchronous).
Some stakeholders
participate in exercise
remotely (distributed
and synchronous) .
Facilitators
interviewing
stakeho􀍛ders
individually or in small
groups (distributed
and asynchronous}.
Advantages
All stakehotders participate
equally.
Group mentality produces
buy􀍜in for architecture and
the resufts of the exercise.
Enduring communication
paths are opened among
stakeholders.
Thrs option takes the
shortest calendar time.
Saves travel costs for remote
participants; this option
might permit participati'on by
stakeholders who otherwise
would not be able to
contribute.
Allows for inedepth
interaction between
facilitators and stakeholders.
Eliminates group factors that
might inhibit a stakeholder
from speaking In public.
·
·
·-
Disadvantages
0
Scheduling can be
problematic.
Some stakeholders mfght not
be forthcoming in a crowd.
Stakeholders might incur
substantial travel costs to
attend.
Technology is a limiting
factor; remote participants
almost always are secondclass
citizens in terms of
their participation and afterexercise
"connection" to other
participants.
If stakeholders are widely
distributed, increased travel
costs incurred by fadlitator(s}.
Reduced group buy-in.
Reduced group mentality.
Reduced after-exercise
communication among
stakeholders.
Exerdse stretched out over a
longer period of calendar time.
A a
But there are, as ever, tradeoffs. The big-meeting format can be costly and difficult to fit into an
already crowded project schedule. Often the hardest aspect of executing any of our methods is finding
two contiguous days when all the important stakeholders are available. Also, the travel costs associated
with a big meeting can be substantial in a distributed organization. And some stakeholders might not be
as forthcoming as we would like if they are in a room surrounded by strong-willed peers or higher-ups
(although our methods use facilitation techniques to try to correct for this).
So which model is best? You already know the answer: It depends. You can see the tradeoffs among
the different approaches. Pick the one that does the best job for your organization and its particularities.
Conclusion
·
·
·- 0 A a
As you read Part III and learn about architecture methods, remember that the form of the method we
present is the one in which the most practical experience resides. But:
1. You can always adjust the engagement model to be something other than everybody-in-thesame-
room if that will work better for you.
2. Whereas the steps of a method are nominally carried out in sequential order according to a set
agenda, sometimes there must be dynamic modifications to the schedule to accommodate
personnel availability or architectural information. Every situation is unique, and there may
be times when you need to return briefly to an earlier step, jump forward to a later step, or
iterate among steps, as the need dictates.
P.S.: We do provide one example of a shortened version of one of our methods the ATAM. We call
this Lightweight Architecture Evaluation, and it is described in Chapter 2 1 .
15. Architecture in Agile Projects
·
·
·- 0
It is not the strongest of the species that survives,
nor the most intelligent that survives. It is the one
that is the most adaptable to change.
-Charles Darwin
A a
Since their first appearance over a decade ago, the various flavors of Agile methods and processes have
received increasing attention and adoption by the worldwide software community. New software
processes do not just emerge out of thin air; they evolve in response to a palpable need. In this case, the
software development world was responding to a need for projects to be more responsive to their
stakeholders, to be quicker to develop functionality that users care about, to show more and earlier
progress in a project's life cycle, and to be less burdened by documenting aspects of a project that
would inevitably change. Is any of this inimical to the use of architecture? We emphatically say "no." In
fact, the question for a software project is not "Should I do Agile or architecture?", but rather questions
such as "How much architecture should I do up front versus how much should I defer until the project's
requirements have solidified somewhat?", "When and how should I refactor?", and "How much of the
architecture should I formally document, and when?" We believe that there are good answers to all of
these questions, and that Agile and architecture are not just well suited to live together but in fact
critical companions for many software projects.
The Agile software movement began to receive considerable public attention approximately a
decade ago, with the release of the "Agile Manifesto." Its roots extend at least a decade earlier than that,
in practices such as Extreme Programming and Serum. The Agile Manifesto, originally signed by 1 7
developers, was however a brilliant public relations move; it is brief, pithy, and sensible:
Manifesto for Agile Software Development
We are uncovering better ways of developing software by doing it and helping others
do it. Through this work we have come to value:
Individuals and interactions
Working SO'ftware
Customer collaboration
Responding to chang,e
over
over
over
over
processes and tools
comprehensive documentation
contract negotiatton
following a p:lan
That is, while there is value in the items on the right, we value the items on the left
more. [ agilemanifesto.org]
The authors of the Manifesto go on to describe the twelve principles that underlie their reasoning:
1. Our highest priority is to satisfy the customer through early and continuous delivery of
valuable software.
2. W elcmne changing requirements, even late in development. Agile processes harness change
for the custotner's competitive advantage.
·
·
·- 0 A a
3. Deliver working software frequently, from a couple of weeks to a couple of months, with a
preference to the shorter timescale.
4. Business people and developers must work together daily throughout the project.
5. Build projects around motivated individuals. Give them the environment and support they
need, and trust them to get the job done.
6. The most efficient and effective method of conveying information to and within a
development team is face-to-face conversation.
7. Working software is the primary measure of progress.
8. Agile processes promote sustainable development. The sponsors, developers, and users
should be able to maintain a constant pace indefinitely.
9. Continuous attention to technical excellence and good design enhances agility.
10. Simplicity the art of maximizing thev amount of work not done is essential.
1 1 . The best architectures, requirements, and designs emerge from self-organizing teams.
12. At regular intervals, the team reflects on how to become more effective, then tunes and
adjusts its behavior accordingly.
There has been considerable elaboration of the Agile Manifesto, and Agile processes, since its first
release, but the basic principles have remained solid. The Agile movement (and its predecessors) have
gained considerable attention and have enjoyed widespread adoption over the past two decades. These
processes were initially employed on small- to medium-sized projects with short time frames and
enjoyed considerable success. They were not often used for larger projects, particularly those with
distributed development. This is not surprising, given the twelve principles.
In particular principles 4 and 6 imply the need for co-location or, if co-location is not possible, then
at least a high level of co1nmunication among the distributed teams. Indeed, one of the core practices of
Agile projects is frequent (often daily) face-to-face meetings. Principle 1 1 says that, for best results,
teams should be self-organizing. But self-organization is a social process that is much more
cumbersome if those teams are not physically co-located. In this case we believe that the creators of the
twelve Agile principles got it wrong. The best teams may be self-organizing, but the best architectures
still require much more than this technical skill, deep experience, and deep know ledge.
Principle 1 argues for "early and continuous delivery of valuable software" and principle 7 claims
that "Working software is the primary measure of progress." One might argue that a focus on early and
continuous release of software, where "working" is measured in terms of customer-facing features,
leaves little time for addressing the kinds of cross-cutting concerns and infrastructure critical to a highquality
large-scale system.
It has been claimed by some that there is an inherent tension between being agile and doing a
conscientious job of architecting. But is there truly a tension? And if so, how do you go about
characterizing it and reasoning about it? In short, how much architecture is the "right" amount of
architecture?
·
·
·- 0
Our brief answer, in this chapter, is that there is no tension. This issue is not "Agile versus
Architecture" but rather "how best to blend Agile and Architecture."
A a
One more point, before we dive into the details: The Agile Manifesto is itself a compromise: a
pronouncement created by a committee. The fact that architecture doesn't clearly live anywhere within
it is most likely because they had no consensus opinion on this topic and not because there is any
inherent conflict.
15.1. How Much Architecture?
·
·
·- 0 A a
We often think of the early software development methods that emerged in the 1 970s such as the
Waterfall method as being plan-driven and inflexible. But this inflexibility is not for nothing. Having
a strong up-front plan provides for considerable predictability (as long as the requirements don't change
too much) and makes it easier to coordinate large numbers of teams. Can you imagine a large
construction or aerospace project without heavy up-front planning? Agile methods and practitioners, on
the other hand, often scorn planning, preferring instead teamwork, frequent face-to-face
communication, flexibility, and adaptation. This enhances invention and creativity.
Garden Shed or Skyscraper?
A few years ago I built a small shed in my back yard, for holding gardening tools, the
lawn mower, the fertilizer cart, and so forth. I had a plan in my head, a small team of
physically co-located "developers," and excellent access to the customer (me) for
making any last-minute decisions and for incorporating any late-breaking feature
requests. What was my architecture? For sure, nothing was written down; I had an
image in my head. I went to the local big-box hardware store/lumberyard and bought a
bunch of building materials, primarily wood. I already owned a fine collection of
hammers, saws, and drills. The boys and I started hammering and sawing and drilling.
In short order I had a garden shed which has served its purpose, with the occasional
repair, for quite a few years. My process was agile: I was able to accommodate the
knowledge, skills, and characteristics of my developers; we were a self-organizing
team; and I was able to easily accommodate feature requests that emerged late in the
process.
Would I recommend this process for the construction of a 20-story office building,
or even a building-code-compliant single-family house? Of course not. All of these are
built using the much-maligned BDUF (Big Design Up Front) process.
My ad hoc process for building the shed was ultimately agile, but it had little
analysis or forethought. It did, however, have just enough forethought and planning.
Doing BDUF hiring an architect, a structural engineer, and a surveyor, and doing a
detailed analysis of soil conditions, potential snow loads, and options for future
modifications would have been folly; really expensive folly!
So too with software. As with everything that we recommend in this book, the
amount of up-front planning and analysis should be justified by the potential risks. In
the end, everything in architecture is about cost/benefit tradeoffs.
-RK
Let us consider a specific case, to illustrate the tradeoff between up-front planning and agility: the
Agile technique of employing user stories. User stories are a cornerstone of the Agile approach. Each
user story describes a set of features visible to the user. Implementing user stories is a way of
·
·
·- 0 A a
demonstrating progress to the customer. This can easily lead to an architecture in which every feature is
independently designed and implemented. In such an environment, concerns that cut across more than
one feature become hard to capture. For example, suppose there is a utility function that supports
multiple features. To identify this utility function, coordination is required among the teams that
develop the different features, and it also requires a role in which a broad overview across all of the
features is maintained. If the development team is geographically distributed and the system being
developed is a large one, then emphasis on delivering features early will cause massive coordination
problems. In an architecture-centric project, a layered architecture is a way to solve this problem, with
features on upper layers using shared functionality of the lower layers, but that requires up-front
planning and design and feature analysis.
Successful projects clearly need a successful blend of the two approaches. For the vast majority of
nontrivial projects, this is not and never should be an either/or choice. Too much up-front planning and
commitment can stifle creativity and the ability to adapt to changing requirements. Too much agility
can be chaos. No one would want to fly in an aircraft where the flight control software had not been
rigorously planned and thoroughly analyzed. Similarly, no one would want to spend 1 8 months
planning an e-commerce website for their latest cell-phone model, or video game, or lipstick (all of
which are guaranteed to be badly out of fashion in 1 8 months). What we all want is the sweet spotwhat
George Fairbanks calls "j ust enough architecture." This is not just a matter of doing the right
amount of architecture work, but also doing it at the right time. Agile projects tend to want to evolve the
architecture, as needed, in real time, whereas large software projects have traditionally favored
considerable up-front analysis and planning.
An Analytic Perspective on Up-front Work vs. Agility
Boehm and Turner, analyzing historical data from 1 6 1 industrial projects, examine the
effects of up-front architecture and risk resolution effort. This corresponds to the
COnstructive COst MOdel II (COCOMO II) scale factor called "RESL." There are two
activities that can add time to the basic project schedule:
• Up-front design work on the architecture and up-front risk identification, planning,
and resolution work
• Rework due to fixing defects and addressing modification requests.
Intuitively, these two trade off against each other: The more we invest in planning,
the less (we hope) rework is needed.
So Boehm and Turner synthesized a model that allowed them to plot these two
values against each other. The axes of their graph (Figure 1 5 . 1) show percent of time
added for RESL and percent of time added to the schedule. The amount of architecture
and risk resolution effort is plotted as the dashed line, moving up and to the right from
near the origin, and ranges from 5 to 50 percent of project effort. This effort is plotted
against three hypothetical projects, measured in thousands of source lines of code
(KSLOC):
• One project of 1 0 KSLOC
• One project of 1 00 KSLOC
• One project of 1 ,000 KSLOC
1 00
1 0,000 KSLOC
90
80
41
- 70 :J. 'lJQ) .c0
(JJ
- - 60
􀛟
Q)>
0
0 - 50
'lJQ) 'lJ 1 00 KSLOC
"C
c(
Ql 40
e ·-
1-
-
0
....
t: 30
·� ...
Ql
a.
20 1 0 KSLOC ,􀅍 􀅍'
·
·
·- 0
Sweet Spot Drivers:
Rapid Change: Leftward
High Assurance: Rightward
1 0
0 1 0 20 30 40 50
Percent of Time Added for Architecture and Ris:k Resolution
- - - Percent of Project Schedule Oevoted to
Initial Architecture and Ris·k Resolution
Added Schedule Devoted to Rework
(COCOMO II RESl Factor)
Total Percent Added to Schedule
0 Sweet Spot
Figure 15.1. Architecture effort vs. rework
60
A a
There is one line representing each of these three projects, starting near the Y axis
and descending, at different rates, to the X axis at the 50 mark. This shows that adding
time for up-front work reduces later rework. No surprise: that is exactly the point of
doing more up-front work. However, when you sum each of those downward-trending
lines (for the 1 0, 1 00, and 1 ,000 KSLOC projects) with the upward sloping line for the
up-front (initial architecture and risk resolution) work, you get the second set of three
·
·
·- 0
lines, which start at the Y axis and meet the upward sloping line at the 50 mark on the
X axis.
A a
These lines show that there is a sweet spot for each project. For the 1 0 KSLOC
project, the sweet spot is at the far left. This says that devoting much, if any, time to upfront
work is a waste for a small project (assuming that the inherent domain complexity
is the same for all three sets of lines). For the 1 00 KSLOC project, the sweet spot is at
around 20 percent of the project schedule. And for the 1 ,000 KSLOC project, the sweet
spot is at around 40 percent of the project schedule. These results are fairly intuitive. A
project with a million lines of code is enormously complex, and it is difficult to imagine
how Agile principles alone can cope with this complexity if there is no architecture to
guide and organize the effort.
The graph shows that no one answer is appropriate for all situations, so you need
methods to guide you to decide how much up-front work is right for you. Boehm and
Turner's work is a start, but expected lines of code is not the only determinant for
appropriateness of up-front planning. The domain, the reliability or safety required, and
the experience of your development team all play a role.
The whole point of choosing how much time to budget for architecture is to reduce risk. Risk may
be financial, political, operational, or reputational. Some risks might involve human life or the chance
of legal action. Chapter 22 covers risk management and budgets for planning in the context of
architecture.
15.2. Agility and Architecture Methods
·
·
·- 0 A a
Throughout this book we emphasize methods for architecture design, analysis, and documentation. We
unabashedly like methods! And so does the Agile community: dozens of books have been written on
Serum, Extreme Programming, Crystal Clear, and other Agile methods. But how should we think of
architecture-centric techniques and methods in an Agile context? How well do they fit with the twelve
Agile principles, for example?
We believe that they fit very well. The methods we present are based on the essential elements
needed to perform the activity. If you believe that architecture needs to be designed, analyzed, and
documented, then the techniques we present are essential regardless of the project in which they are
embedded. The methods we present are essentially driven by the motivation to reduce risk, and by
considerations of costs and benefits.
Among all of our methods for extracting architecturally significant requirements, for architecture
design, for architecture evaluation, for architecture documentation that you '11 see in subsequent
chapters, one might expect the greatest Agile friction from evaluation and documentation. And so the
rest of this section will examine those two practices in an Agile context.
Architecture Documentation and Y AGNI
Our approach to architecture documentation is called Views and Beyond, and it will be discussed in
Chapter 1 8 . Views and Beyond and Agile agree emphatically on the following point: If information
isn't needed, don't spend the resources to document it. All documentation should have an intended use
and audience in mind, and be produced in a way that serves both.
One of our fundamental principles of technical documentation is "Write for the reader." That means
understanding who will read the documentation and how they will use it. If there is no audience, there is
no need to produce the documentation. This principle is so important in Agile methods that it has been
given its own name: YAGNI. YAGNI means "you ain't gonna need it," and it refers to the idea that you
should only implement or document something when you actually have the need for it. Do not spend
time attempting to anticipate all possible needs.
The Views and Beyond approach uses the architectural view as the "unit" of documentation to be
produced. Selecting the views to document is an example of applying this principle. The Views and
Beyond approach prescribes producing a view if and only if it addresses substantial concerns of an
important stakeholder community. And because documentation is not a monolithic activity that holds
up all other progress until it is complete, the view selection method prescribes producing the
documentation in prioritized stages to satisfy the needs of the stakeholders who need it now.
We document the portions of the architecture that we need to teach to newcomers, that embody
significant potential risks if not properly managed, and that we need to change frequently. We
document what we need to convey to readers so they can do their job. Although "classic" Agile
emphasizes documenting the minimum amount needed to let the current team of developers make
progress, our approach emphasizes that the reader might be a maintainer assigned to make a technology
upgrade years after the original development team has disbanded.
Architecture Evaluation
·
·
·- 0 A a
Could an architecture evaluation work as part of an Agile process? Absolutely. In fact, doing so is
perfectly Agile-consistent, because meeting stakeholders' important concerns is a cornerstone of Agile
philosophy.
Our approach to architecture evaluation is exemplified by the Architecture Tradeoff Analysis
Method (AT AM) of Chapter 2 1 . It does not endeavor to analyze all, or even most, of an architecture.
Rather, the focus is determined by a set of quality attribute scenarios that represent the most important
(but by no means all) of the concerns of the stakeholders. "Most important" is judged by the amount of
value the scenario brings to the architecture' s stakeholders, or the amount of risk present in achieving
the scenario. Once these scenarios have been elicited, validated, and prioritized, they give us an
evaluation agenda based on what is important to the success of the system, and what poses the greatest
risk for the system's success. Then we only delve into those areas that pose high risk for the
achievement of the system's main functions and qualities.
And as we will see in Chapter 2 1 , it is easy to tailor a lightweight architecture evaluation, for
quicker and less-costly analysis and feedback whenever in the project it is called for.
15.3. A Brief Example of Agile Architecting
·
·
·- 0 A a
Our claim is that architecture and agility are quite compatible. Now we will look at a brief case study of
just that. This project, which one of the authors worked on, involved the creation and evolution of a
web-conferencing system. Throughout this project we practiced "agile architecting" and, we believe, hit
the sweet spot between up-front planning where possible, and agility where needed.
Web-conferencing systems are complex and demanding systems. They must provide real-time
responsiveness, competitive features, ease of installation and use, lightweight footprint, and much more.
For example:
• They must work on a wide variety of hardware and software platforms, the details of which are
not under the control of the architect.
• They must be reliable and provide low-latency response times, particularly for real-time
functionality such as voice over IP (VoiP) and screen sharing.
• They must provide high security, but do so over an unknown network topology and an
unknown set of firewalls and firewall policies.
• They must be easily modified and easily integrated into a wide variety of environments and
applications.
• They must be highly usable and easily installed and learned by users with widely varying IT
skills.
Many of the above-mentioned goals trade off against each other. Typically security (in the form of
encryption) comes at the expense of real-time performance (latency). Modifiability comes at the
expense of time-to-market. Availability and performance typically come at the expense of modifiability
and cost.
Even if it is possible to collect, analyze, and prioritize all relevant data, functional requirements, and
quality attribute requirements, the stringent time-to-market constraints that prevail in a competitive
climate such as web-conferencing would have prevented us from doing this. Trying to support all
possible uses is intractable, and the users themselves were poorly equipped for envisioning all possible
potential uses of the system. So just asking the users what they wanted, in the fashion of a traditional
requirements elicitation, was not likely to work.
This results in a classic "agility versus commitment" problem. On the one hand the architect wants
to provide new capabilities quickly, and to respond to customer needs rapidly. On the other hand, longterm
survival of the system and the co1npany means that it must be designed for extensibility,
modifiability, and portability. This can best be achieved by having a simple conceptual model for the
architecture, based on a small number of regularly applied patterns and tactics. It was not obvious how
we would "evolve" our way to such an architecture. So, how is it possible to find the "sweet spot"
between these opposing forces?
The W ebArrow web-conferencing system faced precisely this dilemma. It was impossible for the
architect and lead designers to do purely top-down architectural design; there were too many
considerations to weigh at once, and it was too hard to predict all of the relevant technological
·
·
·- 0 A a
challenges. For example, they had cases where they discovered that a vendor-provided API did not
work as specified imagine that! or that an API exposing a critical function was simply missing. In
such cases, these problems rippled through the architecture, and workarounds needed to be fashioned . .
. fast!
To address the complexity of this domain, the WebArrow architect and developers found that they
needed to think and work in two different modes at the same time:
• Top-down designing and analyzing architectural structures to meet the demanding quality
attribute requirements and tradeoffs
• Bottom-up analyzing a wide array of implementation-specific and environment-specific
constraints and fashioning solutions to them
To compensate for the difficulty in analyzing architectural tradeoffs with any precision, the team
adopted an agile architecture discipline combined with a rigorous program of experiments aimed at
answering specific tradeoff questions. These experiments are what are called "spikes" in Agile
terminology. And these experiments proved to be the key in resolving tradeoffs, by helping to tum
unknown architectural parameters into constants or ranges. Here 's how it worked:
1. First, the WebArrow team quickly created and crudely analyzed an initial software and
system architecture concept, and then they implemented and fleshed it out incrementally,
starting with the 1nost critical functionality that could be shown to a customer.
2. They adapted the architecture and refactored the design and code whenever new requirements
popped up or a better understanding of the problem domain emerged.
3. Continuous experimentation, empirical evaluation, and architecture analysis were used to
help determine architectural decisions as the product evolved.
For example, incremental improvement in the scalability and fault-tolerance of WebArrow was
guided by significant experimentation. The sorts of questions that our experiments (spikes) were
designed to answer were these:
• Would moving to a distributed database from local flat files negatively impact feedback time
(latency) for users?
• What (if any) scalability improvement would result from using mod_perl versus standard Perl?
How difficult would the development and quality assurance effort be to convert to mod_perl?
• How many participants could be hosted by a single meeting server?
• What was the correct ratio between database servers and meeting servers?
Questions like these are difficult to answer analytically. The answers rely on the behavior and
interactions of third-party components, and on performance characteristics of software for which no
standard analytic models exist. The Web-Arrow team's approach was to build an extensive testing
infrastructure (including both simulation and instrumentation), and to use this infrastructure to compare
the performance of each modification to the base system. This allowed the team to determine the effect
of each proposed improvement before committing it to the final system.
The lesson here is that making architecture processes agile does not require a radical re-invention of
either Agile practices or architecture methods. The Web-Arrow team's emphasis on experimentation
·
·
·- 0
proved the key factor; it was our way of achieving an agile fonn of architecture conception,
implementation, and evaluation.
A a
This approach meant that the W ebArrow architecture development approach was in line with many
of the twelve principles, including:
• Principle 1 , providing early and continuous delivery of working software
• Principle 2, welcoming changing requirements
• Principle 3, delivering working software frequently
• Principle 8, promoting sustainable development at a constant pace
• Principle 9, giving continuous attention to technical excellence and good design
15.4. Guidelines for the Agile Architect
·
·
·- 0 A a
Barry Boehm and colleagues have developed the Incremental Commitment Model a hybrid process
model framework that attempts to find the balance between agility and commitment. This model is
based upon the following six principles:
1. Commitment and accountability of success-critical stakeholders
2. Stakeholder "satisficing" (meeting an acceptability threshold) based on success-based
negotiations and tradeoffs
3. Incremental and evolutionary growth of system definition and stakeholder commitment
4. Iterative system development and definition
5. Interleaved system definition and development allowing early fielding of core capabilities,
continual adaptation to change, and timely growth of complex systems without waiting for
every requirement and subsystem to be defined
6. Risk management risk -driven anchor point milestones, which are key to synchronizing and
stabilizing all of this concurrent activity
Grady Booch has also provided a set of guidelines for an agile architecture (which in turn imply
some duties for the agile architect). Booch claims that all good software-intensive architectures are
agile. What does he mean by this? He means that a successful architecture is resilient and loosely
coupled. It is composed of a core set of well-reasoned design decisions but still contains some "wiggle
room" that allows modifications to be made and refactorings to be done, without ruining the original
structure.
Booch also notes that an effective agile process will allow the architecture to grow incrementally as
the system is developed and matures. The key to success is to have decomposability, separation of
concerns, and near-independence of the parts. (Sound familiar? These are all modifiability tactics.)
Finally, Booch notes that to be agile, the architecture should be visible and self-evident in the code;
this means making the design patterns, cross-cutting concerns, and other important decisions obvious,
well communicated, and defended. This may, in tum, require documentation. But whatever architectural
decisions are made, the architect must make an effort to "socialize" the architecture.
Ward Cunningham has coined the term "technical debt." Technical debt is an analogy to the normal
debt that we acquire as consumers: we purchase something now and (hope to) pay for it later. In
software the equivalent of "purchasing something now" is quick-and-dirty implementation. Such
implementation frequently leaves technical debt that incurs penalties in the future, in terms of increased
maintenance costs. When technical debt becomes unacceptably high, projects need to pay down some
of this debt, in the form of refactoring, which is a key part of every agile architecting process.
What is our advice?
1. If you are building a large and complex system with relatively stable and well-understood
requirements, it is probably optimal to do a large amount of architecture work up front (see
Figure 1 5 . 1 for some sample values for "large").
2. On big projects with vague or unstable requirements, start by quickly designing a complete
·
·
·- 0 A a
candidate architecture even if it is just a "Power Point architecture," even if it leaves out many
details, and even if you design it in just a couple of days. Alistair Cockburn has introduced a
similar idea in his Crystal Clear method, called a "walking skeleton," which is enough
architecture to be able to demonstrate end-to-end functionality, linking together the major
system functions. Be prepared to change and elaborate this architecture as circumstances
dictate, as you perform your spikes and experiments, and as functional and quality attribute
requirements emerge and solidify. This early architecture will help guide development, help
with early problem understanding and analysis, help in requirements elicitation, help teams
coordinate, and help in the creation of coding templates and other project standards.
3. On smaller projects with uncertain requirements, at least try to get agreement on the central
patterns to be employed, but don't spend too much time on construction, documentation, or
analysis up front. In Chapter 2 1 we will show how analysis can be done in a relatively
lightweight and "just-in-time" fashion.
15.5. Summary
·
·
·- 0 A a
The Agile software movement is emblemized by the Agile Manifesto and a set of principles that assign
high value to close-knit teams and continuous and frequent delivery of working software. Agile
processes were initially employed on small- to medium-sized projects with short time frames and
enjoyed considerable success. They were not often used for larger projects, particularly those with
distributed development.
Although there might appear to be an inherent tension between being agile and architecture
practices of the sort prescribed in this book, the underlying philosophies are not at odds and can be
married to great effect. Successful projects need a successful blend of the two approaches. Too much
up-front planning and commitment can be stifling and unresponsive to customers' needs, whereas too
much agility can simply result in chaos. Agile architects tend to take a middle ground, proposing an
initial architecture and running with that, until its technical debt becomes too great, at which point they
need to refactor.
Boehtn and Turner, analyzing historical data from 1 6 1 industrial projects, examined the effects of
up-front architecture and risk resolution effort. They found that projects tend to have a "sweet spot"
where some up-front architecture planning pays off and is not wasteful.
Among this book's architecture methods, documentation and evaluation might seem to be where the
most friction with Agile philosophies might lie. However, our approaches to these activities are riskbased
and embodied in methods that help you focus effort where it will most pay off.
The WebArrow example showed how adding experimentation to the project's processes enabled it
to obtain benefits from both architecture and classic Agile practices, and be responsive to ever-changing
requirements and domain understanding.
15.6. For Further Reading
·
·
·-
Agile comes in many flavors. Here are some of the better-known ones:
• Extreme Programming [Beck 04]
• Serum [Schwaber 04]
• Feature-Driven Development [Palmer 02]
• Crystal Clear [Cockburn 04]
0 A a
The journal IEEE Software devoted an entire special issue in 20 1 0 to the topic of agility and
architecture. The editor's introduction [Abrahams son 1 0] discusses many of the issues that we have
raised here.
George Fairbanks in his book Just Enough Architecture [Fairbanks 1 0] provides techniques that are
very compatible with Agile methods.
Barry Boehm and Richard Turner [Boehm 04] offer a data- and analysis-driven perspective on the
risks and tradeoffs involved in the continuum of choices regarding agility and what they called
"discipline." The choice of "agility versus discipline" in the title of the book has angered and alienated
many practitioners of Agile methods, most of which are quite disciplined. While this book does not
focus specifically on architecture, it does touch on the subject in many ways. This work was expanded
upon in 20 1 0, when Boehm, Lane, Koolmanojwong, and Turner described the Incremental
Commitment Model and its relationship to agility and architecture [Boehm 1 0]. All of Boehm and
colleagues' work is informed by an active attention to risk. The seminal article on software risk
management [Boehm 9 1] was written by Barry Boehm, more than 20 years ago, and it is still relevant
and compelling reading today.
Carriere, Kazman, and Ozkaya [Carriere 1 0] provide a way to reason about when and where in an
architecture you should do refactoring to reduce technical debt based on an analysis of the
propagation cost of anticipated changes.
The article by Graham, Kazman, and Walmsley [Graham 07] provides substantially more detail on
the We bArrow case study of agile architecting, including a number of architectural diagrams and
additional description of the experimentation performed.
Ward Cunninghatn first coined the term "technical debt" in 1 992 [Cunningham 92]. Brown et al.
[Brown 1 0], building in part on Cunningham's work, offer an economics-driven perspective on how to
enable agility through architecture.
Robert Nord, Jim Tomayko, and Rob Wojcik [Nord 04] have analyzed the relationship between
several of the Software Engineering Institute's architecture methods and Extreme Programming. Grady
Booch has blogged extensively on the relationship between architecture and Agile in his blog, for
example [Booch 1 1].
Felix Bachtnann [Bachmann 1 1] has provided a concrete example of a lightweight version of the
ATAM that fits well with Agile projects and principles.
15.7. Discussion Questions
·
·
·- 0 A a
1. How would you employ the Agile practices of pair programming, frequent team interaction, and
dedicated customer involvement in a distributed development environment?
2. Suppose, as a supporter of architecture practices, you were asked to write an Architecture
Manifesto that was modeled on the Agile Manifesto. What would it look like?
3. Agile projects must be budgeted and scheduled like any other. How would you do that? Does an
architecture help or hinder this process?
4. What do you think are the essential skills for an architect operating in an Agile context? How do
you suppose they differ for an architect working in a non-Agile project?
5. The Agile Manifesto professes to value individuals and interactions over processes and tools.
Rationalize this statement in terms of the role of tools in the modem software development
process: compilers, integrated development environments, de buggers, configuration managers,
automatic test tools, and build and configuration tools.
6. Critique the Agile Manifesto in the context of a 200-developer, 5-million-line project with an
expected lifetime of 20 years.
16. Architecture and Requirements
·
·
·- 0
The two most important requirements for major
success are: first, being in the right place at the
right time, and second, doing something about it.
-Ray Kroc
A a
Architectures exist to build systems that satisfy requirements. That's obvious. What may be less
obvious is that, to an architect, not all requirements are created equal. Some have a much more
profound effect on the architecture than others. An architecturally significant requirement (ASR) is a
requirement that will have a profound effect on the architecture that is, the architecture might well be
dramatically different in the absence of such a requirement.
You cannot hope to design a successful architecture if you do not know the ASRs. ASRs often, but
not always, take the form of quality attribute requirements the performance, security, modifiability,
availability, usability, and so forth, that the architecture must provide to the system. In Chapters 5-13.
we introduced patterns and tactics to achieve quality attributes. Each time you select a pattern or tactic
to use in your architecture, you are changing the architecture as a result of the need to meet quality
attribute requirements. The more difficult and important the QA requirement, the more likely it is to
significantly affect the architecture, and hence to be an ASR.
Architects have to identify ASRs, usually after doing a significant bit of work to uncover candidate
ASRs. Competent architects know this, and as we observe experienced architects going about their
duties, we notice that the first thing they do is start talking to the important stakeholders. They're
gathering the information they need to produce the architecture that will respond to the project's needs
-whether or not this information has already been identified.
This chapter provides some systematic means for identifying the ASRs and other factors that will
shape the architecture.
16.1. Gathering ASRs from Requirements Documents
·
·
·- 0 A a
An obvious location to look for candidate ASRs is in the requirements documents or in user stories.
After all, we are looking for requirements, and requirements should be in requirements documents.
Unfortunately, this is not usually the case, although as we will see, there is information in the
requirements documents that can be of use.
Don't Get Your Hopes Up
Many projects don't create or maintain the kind of requirements document that professors in software
engineering classes or authors of traditional software engineering books love to prescribe. Whether
requirements are specified using the "MoSCoW" style (must, should, could, won't), or as a collection of
"user stories," neither of these is much help in nailing down quality attributes.
Furthermore, no architect just sits and waits until the requirements are "finished" before starting
work. The architect must begin while the requirements are still in flux. Consequently, the QA
requirements are quite likely to be up in the air when the architect starts work. Even where they exist
and are stable, requirements documents often fail an architect in two ways.
First, most of what is in a requirements specification does not affect the architecture. As we've seen
over and over, architectures are mostly driven or "shaped" by quality attribute requirements. These
determine and constrain the most important architectural decisions. And yet the vast bulk of most
requirements specifications is focused on the required features and functionality of a system, which
shape the architecture the least. The best software engineering practices do prescribe capturing quality
attribute requirements. For example, the Software Engineering Body of Knowledge (SWEBOK) says
that quality attribute requirements are like any other requirements. They must be captured if they are
important, and they should be specified unambiguously and be testable.
In practice, though, we rarely see adequate capture of quality attribute requirements. How many
times have you seen a requirement of the form "The system shall be modular" or "The system shall
exhibit high usability" or "The system shall meet users' performance expectations"? These are not
requirements, but in the best case they are invitations for the architect to begin a conversation about
what the requirements in these areas really are.
Second, much of what is useful to an architect is not in even the best requirements document. Many
concerns that drive an architecture do not manifest themselves at all as observables in the system being
specified, and so are not the subject of requirements specifications. ASRs often derive from business
goals in the development organization itself; we'll explore this in Section 1 6.3. Developmental qualities
are also out of scope; you will rarely see a requirements document that describes teaming assumptions,
for example. In an acquisition context, the requirements document represents the interests of the
acquirer, not that of the developer. But as we saw in Chapter 3, stakeholders, the technical environment,
and the organization itself all play a role in influencing architectures.
Sniffing Out ASRs from a Requirements Document
Although requiretnents documents won't tell an architect the whole story, they are an important source
of ASRs. Of course, ASRs aren't going to be conveniently labeled as such; the architect is going to
·
·
·-
have to perform a bit of excavation and archaeology to ferret them out.
0 A a
Chapter 4 categorizes the design decisions that architects have to make. Table 1 6 . 1 summarizes
each category of architectural design decision, and it gives a list of requirements to look for that might
affect that kind of decision. If a requirement affects the making of a critical architectural design
decision, it is by definition an ASR.
Table 16.1. Early Design Decisions and Requirements That Can Affect Them
Design Decision Category
.Allocation of Responsibilities
Coordination Model
Data Mode'l
Management of Resources
Mapping among Architectural
Elements
Binding Time Decisions
Choice of Technology
Look for Requirements Addressing . . .
Planned evolution of responsibWties, user roles,
system modes, major processing steps, commercial
packages
Properties of the coordination (timeliness, currency,
completeness, correctness, and consistency)
Names of extemal elements, protocols. sensors
or actuators (devices), middleware. network
configurations (including their S·ecurity properties)
Evolution requirements on the list above
Processing steps, information flows, major domain
entities, access rights, persistence, evolution
requirements
Time. concurrency, memory footprint, scheduling,
multiple users, multiple activities. devices, energy
usage, soft resources (buffers, queues, etc.)
Scalabiltty requirements on the list above
Plans for teaming, processors, families of
processors, evolution of processors, network
configurations
Extension of or flexibility of functionality, regional
distinctions, language d,istlnctions, portability,
calibrations, conNgurations
Named technologies, changes to technologies
(planned and unplanned}
16.2. Gathering ASRs by Interviewing Stakeholders
·
·
·- 0 A a
Say your project isn't producing a comprehensive requirements document. Or it is, but it's not going to
have the QAs nailed down by the time you need to start your design work. What do you do?
Architects are often called upon to help set the quality attribute requirements for a system. Projects
that recognize this and encourage it are much more likely to be successful than those that don't. Relish
the opportunity. Stakeholders often have no idea what QAs they want in a system, and no amount of
nagging is going to suddenly instill the necessary insight. If you insist on quantitative QA requirements,
you're likely to get numbers that are arbitrary, and there' s a good chance that you'll find at least some
of those requirements will be very difficult to satisfy.
Architects often have very good ideas about what QAs are exhibited by similar systems, and what
QAs are reasonable (and reasonably straightforward) to provide. Architects can usually provide quick
feedback as to which quality attributes are going to be straightforward to achieve and which are going
to be problematic or even prohibitive. And architects are the only people in the room who can say, "I
can actually deliver an architecture that will do better than what you had in mind would that be useful
to you?"
Interviewing the relevant stakeholders is the surest way to learn what they know and need. Once
again, it behooves a project to capture this critical information in a systematic, clear, and repeatable
way. Gathering this information from stakeholders can be achieved by many methods. One such
method is the Quality Attribute Workshop (QAW), described in the sidebar.
The results of stakeholder interviews should include a list of architectural drivers and a set of QA
scenarios that the stakeholders (as a group) prioritized. This information can be used to do the
following:
• Refine system and software requirements
• Understand and clarify the system's architectural drivers
• Provide rationale for why the architect subsequently made certain design decisions
• Guide the development of prototypes and simulations
• Influence the order in which the architecture is developed
The Quality Attribute Workshop
The QAW is a facilitated, stakeholder-focused method to generate, prioritize, and refine
quality attribute scenarios before the software architecture is completed. The QA W is
focused on system-level concerns and specifically the role that software will play in the
system. The QA W is keenly dependent on the participation of system stakeholders.l
1. This material was adapted from [Barbacci 03].
The QA W involves the following steps:
Step 1. QA W Presentation and Introductions. QA W facilitators describe the motivation
for the QAW and explain each step of the method. Everyone introduces
·
·
·- 0 A a
themselves, briefly stating their background, their role in the organization, and
their relationship to the system being built.
Step 2. Business/Mission Presentation. The stakeholder representing the business
concerns behind the system (typically a manager or management representative)
spends about one hour presenting the system's business context, broad
functional requirements, constraints, and known quality attribute requirements.
The quality attributes that will be refined in later steps will be derived largely
from the business/mission needs presented in this step.
Step 3. Architectural Plan Presentation. Although a detailed system or software
architecture might not exist, it is possible that broad system descriptions, context
drawings, or other artifacts have been created that describe some of the system's
technical details. At this point in the workshop, the architect will present the
system architectural plans as they stand. This lets stakeholders know the current
architectural thinking, to the extent that it exists.
Step 4. Identification of Architectural Drivers. The facilitators will share their list of key
architectural drivers that they assembled during steps 2 and 3, and ask the
stakeholders for clarifications, additions, deletions, and corrections. The idea is
to reach a consensus on a distilled list of architectural drivers that includes
overall requirements, business drivers, constraints, and quality attributes.
Step 5. Scenario Brainstorming. Each stakeholder expresses a scenario representing his
or her concerns with respect to the system. Facilitators ensure that each scenario
has an explicit stimulus and response. The facilitators ensure that at least one
representative scenario exists for each architectural driver listed in step 4.
Step 6. Scenario Consolidation. After the scenario brainstorming, similar scenarios are
consolidated where reasonable. Facilitators ask stakeholders to identify those
scenarios that are very similar in content. Scenarios that are similar are merged,
as long as the people who proposed them agree and feel that their scenarios will
not be diluted in the process. Consolidation helps to prevent votes from being
spread across several scenarios that are expressing the same concern.
Consolidating almost-alike scenarios assures that the underlying concern will
get all of the votes it is due.
Step 7. Scenario Prioritization. Prioritization of the scenarios is accomplished by
allocating each stakeholder a number of votes equal to 30 percent of the total
number of scenarios generated after consolidation. Stakeholders can allocate any
number of their votes to any scenario or combination of scenarios. The votes are
counted, and the scenarios are prioritized accordingly.
Step 8. Scenario Refinement. After the prioritization, the top scenarios are refined and
elaborated. Facilitators help the stakeholders put the scenarios in the six-part
scenario form of source-stimulus-artifact-environment-response-response
measure that we described in Chapter 4. As the scenarios are refined, issues
surrounding their satisfaction will emerge. These are also recorded. Step 8 lasts
as long as time and resources allow.
·
·
·- 0 A a
·
·
·-
16.3. Gathering ASRs by Understanding the Business Goals
0 A a
Business goals are the raison d 'etre for building a system. No organization builds a system without a
reason; rather, the organization's leaders want to further the mission and ambitions of their organization
and themselves. Common business goals include making a profit, of course, but most organizations
have many more concerns than simply profit, and in other organizations (e.g., nonprofits, charities,
governments), profit is the farthest thing from anyone's mind.
Business goals are of interest to architects because they often are the precursor or progenitor of
requirements that may or may not be captured in a requirements specification but whose achievement
(or lack) signals a successful (or less than successful) architectural design. Business goals frequently
lead directly to ASRs.
There are three possible relationships between business goals and an architecture:
1. Business goals often lead to quality attribute requirements. Or to put it another way, every
quality attribute requirement such as user-visible response time or platform flexibility or
ironclad security or any of a dozen other needs should originate from some higher purpose
that can be described in terms of added value. If we ask, for example, "Why do you want this
system to have a really fast response time?'', we might hear that this will differentiate the
product from its competition and let the developing organization capture market share; or that
this will make the soldier a more effective warfighter, which is the mission of the acquiring
organization; or other reasons having to do with the satisfaction of some business goal.
2. Business goals may directly affect the architecture without precipitating a quality attribute
requirement at all. In Chapter 3 we told the story of the architect who designed a system
without a database until the manager informed him that the database team needed work. The
architecture was importantly affected without any relevant quality attribute requirement.
3. No influence at all. Not all business goals lead to quality attributes. For example, a business
goal to "reduce cost" may be realized by lowering the facility's thermostats in the winter or
reducing employees' salaries or pensions.
Figure 1 6 . 1 illustrates the major points just described. In the figure, the arrows mean "leads to." The
solid arrows are the ones highlighting relationships of most interest to architects.
I
'I
•
1
I
􀃙
Nonarchitectural Solutions
Quality Attributes
Architecture
Figure 16.1. Some business goals may lead to quality attribute requirements (which lead to
architectures), or lead directly to architectural decisions, or lead to nonarchitectural solutions.
·
·
·- 0 A a
Architects often become aware of an organization' s business and business goals via osmosisworking,
listening, talking, and soaking up the goals that are at work in an organization. Osmosis is not
without its benefits, but more systematic ways are possible. We describe one such way in the sidebar "A
Method for Capturing Business Goals."
A Categorization of Business Goals
Business goals are worth capturing explicitly. This is because they often imply ASRs that would
otherwise go undetected until it is too late or too expensive to address them. Capturing business goals is
well served by having a set of candidate business goals handy to use as conversation starters. If you
know that many businesses want to gain market share, for instance, you can use that to engage the right
stakeholders in your organization to ask, ''What are our ambitions about market share for this product,
and how could the architecture contribute to meeting them?"
Our research in business goals has led us to adopt the categories shown in Table 1 6.2. These
categories can be used as an aid to brainstorming and elicitation. By employing the list of categories,
and asking the stakeholders about possible business goals in each category, some assurance of coverage
is gained.
Table 16.2. A List of Standard Business Goal Categories
1 . Contributing to the growth and continuity of the organization
2. Meeting financial objectives
3. Meeting personal objectives
4. Meeting responsibility to employees
5. Meeting responsibility to society
6. Meeting responsibility to state
7. Meeting responsibility to shareholders
8. Managing market position
9. 1·mproving business processes
10. Managing the quality and reputation of products
1 1 . Managing change in environmentai factors
These categories are not completely orthogonal. Some business goals may fit into more than one
category, and that's all right. In an elicitation method, the categories should prompt questions about the
existence of organizational business goals that fall into that category. If the categories overlap, then this
might cause us to ask redundant questions. This is not harmful and could well be helpful. The utility of
these categories is to help identify all business goals, not to provide a taxonomy.
1 . Contributing to the growth and continuity of the organization. How does the system being
developed contribute to the growth and continuity of the organization? In one experience
using this business goal category, the system being developed was the sole reason for the
existence of the organization. If the system was not successful, the organization would cease
to exist. Other topics that might come up in this category deal with market share, product
lines, and international sales.
·
·
·- 0 A a
2. Meeting financial objectives. This category includes revenue generated or saved by the
system. The system may be for sale, either in standalone fonn or by providing a service, in
which case it generates revenue. The system tnay be for use in an internal process, in which
case it should make those processes more effective or more efficient. Also in this category is
the cost of development, deployment, and operation of the system. But this category can also
include financial objectives of individuals: a tnanager hoping for a raise, for example, or a
shareholder expecting a dividend.
3. Meeting personal objectives. Individuals have various goals associated with the construction
of a system. They may range from "I want to enhance my reputation by the success of this
system" to "I want to learn new technologies" to "I want to gain experience with a different
portion of the development process than in the past." In any case, it is possible that technical
decisions are influenced by personal objectives.
4. Meeting responsibility to employees. In this category, the employees in question are usually
those employees involved in development or those involved in operation. Responsibility to
employees involved in development might include ensuring that certain types of employees
have a role in the development of this system, or it might include providing employees the
opportunities to learn new skills. Responsibility to employees involved in operating the
system might include safety, workload, or skill considerations.
5. Meeting responsibility to society. Some organizations see themselves as being in business to
serve society. For these organizations, the system under development is helping them meet
those responsibilities. But all organizations must discharge a responsibility to society by
obeying relevant laws and regulations. Other topics that might come up under this category
are resource usage, "green computing," ethics, safety, open source issues, security, and
• pnvacy.
6. Meeting responsibility to state. Government systems, almost by definition, are intended to
meet responsibility to a state or country. Other topics that tnight come up in this category deal
with export controls, regulatory conformance, or supporting government initiatives.
7. Meeting responsibility to shareholders. There is overlap between this category and the
financial objectives category, but additional topics that might come up here are liability
protection and certain types of regulatory conformance such as, in the United States,
adherence to the Sarbanes-Oxley Act.
8. Managing market position. Topics that might come up in this category are the strategy used
to increase or hold market share, various types of intellectual property protection, or the time
to market.
9. Improving business processes. Although this category partially overlaps with meeting
financial objectives, reasons other than cost reduction exist for improving business processes.
It may be that improved business processes enable new markets, new products, or better
customer support.
10. Managing the quality and reputation of products. Topics that might come up in this category
include branding, recalls, types of potential users, quality of existing products, and testing
support and strategies.
·
·
·- 0 A a
1 1 . Managing change in environmental factors. As we said in Chapter 3 , the business context for
a system might change. This item is intended to encourage the stakeholders to consider what
might change in the business goals for a system.
Expressing Business Goals
How will you write down a business goal once you've learned it? Just as for quality attributes, a
scenario makes a convenient, uniform, and clarifying way to express business goals. It helps ensure that
all business goals are expressed clearly, in a consistent fashion, and contain sufficient information to
enable their shared understanding by relevant stakeholders. Just as a quality attribute scenario adds
precision and meaning to an otherwise vague need for, say, "modifiability," a business goal scenario
will add precision and meaning to a desire to "meet financial objectives."
Our business goal scenario template has seven parts. They all relate to the system under
development, the identity of which is implicit. The parts are these:
1 . Goal-source. These are the people or written artifacts providing the goal.
2. Goal-subject. These are the stakeholders who own the goal and wish it to be true. Each
stakeholder might be an individual or (in the case of a goal that has no one owner and has
been assimilated into an organization) the organization itself. If the business goal is, for
example, "Maximize dividends for the shareholders," who is it that cares about that? It is
probably not the programmers or the system's end users (unless they happen to own stock).
Goal-subjects can and do belong to different organizations. The developing organization, the
custmner organizations, subcontractors, vendors and suppliers, standards bodies, regulatory
agencies, and organizations responsible for systems with which ours must interact are all
potential goal-subjects.
3. Goal-object. These are the entities to which the goal applies. "Object" is used in the sense of
the object of a verb in a sentence. All goals have goal-objects: we want something to be true
about something (or someone) that (or whom) we care about. For example, for goals we
would characterize as furthering one's self-interest, the goal-object can be "myself or my
family." For some goals the goal-object is clearly the development organization, but for some
goals the goal-object can be more refined, such as the rank-and-file employees of the
organization or the shareholders of the organization. Table 1 6.3 is a representative crosssection
of goal-objects. Goal-objects in the table start small, where the goal-object is a single
individual, and incrementally grow until the goal-object is society at large.
Table 16.3. Business Goals and Their Goal-Objects
Goal-Object
Individual
System
Portfolio
Organization's
Employees
Organization's
Shareholders
Organization
Nation
Society
Business Goals That Often Have This Goal-Object
Personal wealth, power, honor/face/reputation, game and gambling spirit,
maintain or improve reputaMon (personal), family interests
Manage flexibility, distributed development, portability, open systems/standards,
testability, product lines, integrability, interoperabillty, ease of Installation and
ease of repair, flexibillty/contigurabllity, performance, rellabllily/availability, ease
of use, security, safety, scalabillty/extendibility. functionality, system constraints,
internationalization, reduce lime to market
Reduce cost of development, cost leadership, differentiation, reduce cost of
retirement, smooth transition to follow-on systems, replace legacy systems,
replace labor with automation, diversify operational sequence, eliminate
intermediate stages, automate tracking of business events, collect/communicate/
retrieve operational knowledge. improve decision making, coordinate across
distance, align task and process, manage on basis of process measurements,
operate effectively within the competitive environment, the technological
environment, or the customer environment
Create something new, provide the best quality products and services possible,
be the leading Innovator in lhe industry
Provide high rewards and benefits to employees, create a pleasant and friendly
workplace, have satisfied employees, fulfill responsibility toward employees,
maintain jobs of workforce on legacy systems
Maximize dividends tor the shareholders
Growth of the business, continuity of the business, maximize profits over the
short run, maximize profits over the long run, survival of the organization,
maximize the company's net assets and reserves, be a market leader, maximize
the market share, expand or retain market share, enter new markets, maximize
the company's rate of growth. keep tax payments to a minimum, increase sales
growth, maintain or improve reputation, achieve business goals through financial
objectives, run a stable organization
Patriotism, national pride, national security, national welfare
Run an ethical organization, responsibility toward society, be a socially
responsible company, be of service to the community, operate effectively within
social environment, operate effectively within legal environment
Remarks
·
·
·- 0
The individual who has these goals has them for
him/herself or his/her family.
These can be goals for a system being developed or
acquired. The fist applies to systems In general, but
the quantification of any one item likely applies to a
single system being developed or acquired.
These goals live on the cusp between an individual
system and the entire organization. They apply either
to a single system or to an organization's entire
portfolio that the organization is building or acquiring
to achieve Its goals.
Before we get to the organization as a whole, there
are some goals aimed at specific subsets of the
organization.
These are goals for the organization as a whole. The
organization can be a development or acquisition
organization, although most were undoubtedly
created with the former in mind.
Before we get to society at large, this goal-object is
specifically limited to the goal owner's own country.
Some interpret •society" as "my society," which puts
this category closer to the nation goal-object, but we
are taking a broader view.
A a
4. Environment. This is the context for this goaL For example, there are social, legal,
competitive, customer, and technological environments. Sometimes the political environment
is key; this is as a kind of social factor. Upcoming technology may be a major factor.
5. Goal. This is any business goal articulated by the goal-source.
6. Goal-measure. This is a testable measurement to determine how one would know if the goal
has been achieved. The goal-measure should usually include a time component, stating the
time by which the goal should be achieved.
7. Pedigree and value. The pedigree of the goal tells us the degree of confidence the person who
stated the goal has in it, and the goal 's volatility and value. The value of a goal can be
expressed by how much its owner is willing to spend to achieve it or its relative importance
compared to other goals. Relative itnportance may be given by a ranking from 1 (most
important) to n (least important), or by assigning each goal a value on a fixed scale such as 1
to 1 0 or high-medium-low. We combine value and pedigree into one part although it certainly
is possible to treat them separately. The important concern is that both are captured.
Elements 2-6 can be cotnbined into a sentence that reads:
For the system being developed, <goal-subject> desires that <goal-object> achieve <goal> in the
context of <environment> and will be satisfied if <goal-measure>.
The sentence can be augmented by the goal's source (element 1 ) and the goal 's pedigree and value
(element 7). Some sample business goal scenarios include the following:
• For MySys, the project manager has the goal that his family's stock in the company will rise by
5 percent (as a result of the success of MySys).
• For MySys, the developing organization's CEO has the goal that MySys will make it 50
percent less likely that his nation will be attacked.
• For MySys, the portfolio manager has the goal that MySys will make the portfolio 30 percent
more profitable.
·
·
·- 0 A a
• For MySys, the project manager has the goal that customer satisfaction will rise by 1 0 percent
(as a result of the increased quality ofMySys).
In many contexts, the goals of different stakeholders may conflict. By identifying the stakeholder
who owns the goal, the sources of conflicting goals can be identified.
A General Scenario for Business Goals
A general scenario (see Chapter 4) is a template for constructing specific or "concrete" scenarios. It
uses the generic structure of a scenario to supply a list of possible values for each non-boilerplate part
of a scenario. See Table 1 6.4 for a general scenario for business goals.
Table 16.4. General Scenario Generation Table for Business Goals
2. Goal-subject
Any stakeholder
or staKeholder
group identified
as having a
legitimate
interest in the
system
.
: 3. Goal-object
-􀂢 Individual
.,.. 􀛝 System
􀛜 Portfolio 􀞒 Organization's
􀂢 employees
: Organization's
· shareholders
Organization
Nation
Society
.
: 5. Goal
II> 􀒗 Contributing to the
.!!! growth and continuity
'5 of the organization
"'
: Meeting financial
· objectives
Meeting personal
objectives
Meeting responsibility
to employees
Meeting responsibility
to society
Meeting responsibility
to state
Meeting responsibility
to shareholders
Managing market
position
Improving business
processes
Managing quality and
reputation of products
Managing change In
environmental factors
. . 6. Goal·measure (examples,
: 4. Environment : based on goal categories)
- ----- =
2 Social (includes .., Time that business remains
􀒙 political) 􀂙 viable
- "'
5 Legal '(;; Financial performance vs.
u
J! Competitive ��> objectives .,
.,.􀛞 . Customer 􀒘 Promotion or raise achieved In 'j period
: Technological -g Employee satisfaction; turnover
􀒖 rate
· Contribution to trade deficit/
surplus
Stock price, dividends
Market share
Time to carry out a business
process
Quality measures of products
Technology-related problems
Time window for achievement
7. Value
1-n
1-10
H·M·L
Resources
willing to
expend
For each of these scenarios you might want to additionally capture its source (e.g., Did this come
directly from the goal-subject, a document, a third party, a legal requirement?), its volatility, and its
importance.
Capturing Business Goals
Business goals are worth capturing because they can hold the key to discovering ASRs that emerge in
no other context. One method for eliciting and documenting business goals is the Pedigreed Attribute
eLicitation Method, or PALM. The word "pedigree" means that the business goal has a clear derivation
or background. PALM uses the standard list of business goals and the business goal scenario format we
described earlier.
PALM can be used to sniff out missing requirements early in the life cycle. For example, having
stakeholders subscribe to the business goal of improving the quality and reputation of their products
may very well lead to (for example) security, availability, and performance requiretnents that otherwise
might not have been considered.
PALM can also be used to discover and carry along additional information about existing
requirements. For example, a business goal might be to produce a product that outcompetes a rival' s
·
·
·- 0 A a
market entry. This might precipitate a performance requirement for, say, half-second turnaround when
the rival features one-second turnaround. But if the competitor releases a new product with half-second
turnaround, then what does our requirement become? A conventional requirements document will
continue to carry the half-second requirement, but the goal-savvy architect will know that the real
requirement is to beat the competitor, which may mean even faster performance is needed.
Finally, PALM can be used to examine particularly difficult quality attribute requirements to see if
they can be relaxed. We know of more than one system where a quality attribute requirement proved
quite expensive to provide, and only after great effort, money, and time were expended trying to meet it
was it revealed that the requirement had no actual basis other than being someone' s best guess or fond
wish at the time.
16.4. Capturing ASRs in a Utility Tree
·
·
·- 0 A a
As we have seen, ASRs can be extracted from a requirements document, captured from stakeholders in
a workshop such as a QA W, or derived from business goals. It is helpful to record them in one place so
that the list can be reviewed, referenced, used to justify design decisions, and revisited over time or in
the case of major system changes.
To recap, an ASR must have the following characteristics:
• A profound impact on the architecture. Including this requirement will very likely result in a
different architecture than if it were not included.
• A high business or mission value. If the architecture is going to satisfy this requirementpotentially
at the expense of not satisfying others it must be of high value to important
stakeholders.
Using a single list can also help evaluate each potential ASR against these criteria, and to make sure
that no architectural drivers, stakeholder classes, or business goals are lacking ASRs that express their
needs.
A Method for Capturing Business Goals
PALM is a seven-step method, nominally carried out over a day and a half in a
workshop attended by architects and stakeholders who can speak to the business goals
of the organizations involved. The steps are these:
1 . PALM overview presentation. Overview of PALM, the problem it solves, its steps,
and its expected outcomes.
2. Business drivers presentation. Briefing of business drivers by project management.
What are the goals of the customer organization for this system? What are the goals
of the development organization? This is normally a lengthy discussion that allows
participants to ask questions about the business goals as presented by project
management.
3. Architecture drivers presentation. Briefing by the architect on the driving business
and quality attribute requirements: the ASRs.
4. Business goals elicitation. Using the standard business goal categories to guide
discussion, we capture the set of important business goals for this system. Business
goals are elaborated and expressed as scenarios. We consolidate almost-alike
business goals to eliminate duplication. Participants then prioritize the resulting set
to identify the most important goals.
5. Identification of potential quality attributes from business goals. For each
important business goal scenario, participants describe a quality attribute that (if
architected into the system) would help achieve it. If the QA is not already a
requirement, this is recorded as a finding.
6. Assignment of pedigree to existing quality attribute drivers. For each architectural
·
·
·- 0 A a
driver named in step 3, we identify which business goals it is there to support. If
none, that's recorded as a finding. Otherwise, we establish its pedigree by asking
for the source of the quantitative part. For example: Why is there a 40-millisecond
performance requirement? Why not 60 milliseconds? Or 80 milliseconds?
7. Exercise conclusion. Review of results, next steps, and participant feedback.
Architects can use a construct called a utility tree for all of these purposes. A utility tree begins with
the word "utility" as the root node. Utility is an expression of the overall "goodness" of the system. We
then elaborate this root node by listing the major quality attributes that the system is required to exhibit.
(We said in Chapter 4 that quality attribute names by themselves were not very useful. Never fear: we
are using them only as placeholders for subsequent elaboration and refinement!)
Under each quality attribute, record a specific refinement of that QA. For example, performance
might be decomposed into "data latency" and "transaction throughput." Or it might be decomposed into
"user wait time" and "time to refresh web page." The refinements that you choose should be the ones
that are relevant to your system. Under each refinement, record the appropriate ASRs (usually
expressed as QA scenarios).
Some ASRs might express more than one quality attribute and so might appear in more than one
place in the tree. That is not necessarily a problem, but it could be an indication that the ASR tries to
cover too much diverse territory. Such ASRs may be split into constituents that each attach to smaller
concerns.
Once the ASRs are recorded and placed in the tree, you can now evaluate them against the two
criteria we listed above: the business value of the candidate ASR and the architectural impact of
including it. You can use any scale you like, but we find that a simple "H" (high), "M" (medium), and
"L" (low) suffice for each criterion.
For business value, High designates a must-have requirement, Medium is for a requirement that is
important but would not lead to project failure were it omitted. Low describes a nice requirement to
have but not something worth much effort.
For architectural impact, High means that meeting this ASR will profoundly affect the architecture.
Medium means that meeting this ASR will smnewhat affect the architecture. Low means that meeting
this candidate ASR will have little effect on the architecture.
Table 1 6.5 shows a portion of a sample utility tree drawn from a health care application called
Nightingale. Each ASR is labeled with a pair of "H," "M," and "L" values indicating (a) the ASR's
business value and (b) its effect on the architecture.
Table 16.5. Tabular Form of the Utility Tree for the Nightingale ATAM Exercise
Quality
Attribute
Performance
Usability
Attribute
Refinement
Transaction
response time
Throughput
Proficiency
training
Norma'!
operations
Configurability User-defined
changes
Maintainability Routine
Extensibility
Security
changes
Upgrades to
commercial
components
Adding new
product
Confidentiality
Integrity
ASR
·
·
·- 0
A user updates a patient's account in response to a
change-of-address notification while the system is under
peak load, and the transaction completes ,Jn less than
0.75 second. (H,M)
A user updates a patient's account in response to a
change-of-address notlftcation while the system is under
double the peak load, and the transaction completes in
less than 4 seconds. (L,M)
At peak 'load, the system is able to compl·ete 150
normalized transactions per second. (M,M)
A new hire with two or more years' experience in the
business becomes proficient in Nightingale's core
functions in less than 1 week. (M,L)
A user in a particular context asks for help, and the
system provides help for that context, wfthjn 3 seconds.
(H ,M)
A hospital payment officer initiates a payment plan for a
patient while interacting wlth that patient and completes
the process without the system introducing delays. (M,M)
A hospital increases the fee for a particular service. The
configuration team makes the change in 1 working day;
no source code needs to chan9e. (H,L}
A maintainer encounters search- and response-time
deficiencies, fixes the bug, and distributes the bug fix with
no more than 3 person-days of effort. (H,M)
A reporting requirement requires a change to the reportgenerating
metadata. Change is made in 4 person-hours
of effort. (M,L)
The database vendor releas·es a new version that must
be installed in less than 3 person-weeks. (H,M)
A product that tracks blood bank donors is created within
2 person-months. ( M,M)
A physical therapist is allowed to see that part of a
patient's record dealing with orthopedic treatment but not
other parts nor any financial information. (HJM)
The system resists unauthorized intrusion and reports the
intrusion attempt to author,ities within 90 seconds. { H,M}
Availability No downtime The database vendor releases new software, which is
hot-swapp.ed into place, with no downtime. (H,L)
The system supports 24/7 web-based account access by
patients. (l,L)
A a
Once you have a utility tree filled out, you can use it to make important checks. For instance:
• A QA or QA refinement without any ASR is not necessarily an error or omission that needs to
be rectified, but it is an indication that attention should be paid to finding out for sure if there
are unrecorded ASRs in that area.
·
·
·- 0 A a
• ASRs that rate a (H,H) rating are obviously the ones that deserve the most attention from you;
these are the most significant of the significant requirements. A very large number of these
might be a cause for concern about whether the system is achievable.
• Stakeholders can review the utility tree to make sure their concerns are addressed. (An
alternative to the organization we have described here is to use stakeholder roles rather than
quality attributes as the organizing rule under "Utility.")
16.5. Tying the Methods Together
·
·
·- 0
How should you employ requirements documents, stakeholder interviews, Quality Attribute
Workshops, PALM, and utility trees in concert with each other?
A a
As for most complex questions, the answer to this one is "It depends." If you have a requirements
process that gathers, identifies, and prioritizes ASRs, then use that and consider yourself lucky.
If you feel your requirements fall short of this ideal state, then you can bring to bear one or more of
the other approaches. For example, if nobody has captured the business goals behind the system you're
building, then a PALM exercise would be a good way to ensure that those goals are represented in the
system's ASRs.
If you feel that important stakeholders have been overlooked in the requirements-gathering process,
then it will probably behoove you to capture their concerns through interviews. A Quality Attribute
Workshop is a structured method to do that and capture their input.
Building a utility tree is a good way to capture ASRs along with their prioritization something that
many requirements processes overlook.
Finally, you can blend all the methods together: PALM makes an excellent "subroutine call" from a
Quality Attribute Workshop for the step that asks about business goals, and a quality attribute utility
tree makes an excellent repository for the scenarios that are the workshop ' s output.
It is unlikely, however, that your project will have the time and resources to support this do-it-all
approach. Better to pick the approach that fills in the biggest gap in your existing requirements:
stakeholder representation, business goal manifestation, or ASR prioritization.
16.6. Summary
·
·
·- 0 A a
Architectures are driven by architecturally significant requirements: requirements that will have
profound effects on the architecture. Architecturally significant requirements may be captured from
requirements documents, by interviewing stakeholders, or by conducting a Quality Attribute Workshop.
In gathering these requirements, we should be mindful of the business goals of the organization.
Business goals can be expressed in a common, structured form and represented as scenarios. Business
goals may be elicited and documented using a structured facilitation method called PALM.
A useful representation of quality attribute requirements is in a utility tree. The utility tree helps to
capture these requirements in a structured form, starting from coarse, abstract notions of quality
attributes and gradually refining them to the point where they are captured as scenarios. These scenarios
are then prioritized, and this prioritized set defines your "tnarching orders" as an architect.
16.7. For Further Reading
·
·
·- 0 A a
PALM can be used to capture the business goals that conform to a business goal viewpoint; that is, you
can use PALM to populate a business goal view of your system, using the terminology of ISO Standard
42010. We discuss this in The Business Goals Viewpoint [Clements 1 0c]. Complete details of PALM
can be found in CMU/SEI-201 0-TN-0 18, Relating Business Goals to Architecturally Significant
Requirements for Software Systems [Clements 1 Ob].
The Open Group Architecture Framework, available at www.opengroup.org/togaf/, provides a very
complete template for documenting a business scenario that contains a wealth of useful information.
Although we believe architects can make use of a lighter-weight means to capture a business goal, it's
worth a look.
The definitive reference source for the Quality Attribute Workshop is [Barbacci 03].
The term architecturally significant requirement was created by the Software Architecture Review
and Assessment (SARA) group [Obbink 02].
When dealing with systems of systems (SoS), the interaction and handoffbetween the systems can
be a source of problems. The Mission Thread Workshop and Business Thread Workshop focus on a
single thread of activity within the overall SoS context and identify potential problems having to do
with the interaction of the disparate systems. Descriptions of these workshops can be found at [Klein
1 0] and [Gagliardi 09].
16.8. Discussion Questions
·
·
·- 0 A a
1 . Interview representative stakeholders for your business's or university's expense recovery
system. Capture the business goals that are motivating the system. Use the seven-part business
goal scenario outline given in Section 1 6.3.
2. Draw a relation between the business goals you uncovered for the previous question and ASRs.
3. Consider an automated teller machine (ATM) system. Attempt to apply the 1 1 categories of
business goals to that system and infer what goals might have been held by various stakeholders
involved in its development.
4. Create a utility tree for the A TM system above. (Interview some of your friends and colleagues if
you like, to have them contribute quality attribute considerations and scenarios.) Consider a
minimutn of four different quality attributes. Ensure that the scenarios that you create at the leaf
nodes have explicit responses and response measures.
5. Restructure the utility tree given in Section 1 6.4 using stakeholder roles as the organizing
principle. What are the benefits and drawbacks of the two representations?
6. Find a software requirements specification that you consider to be of high quality. Using colored
pens (real ones if the document is printed, virtual ones if the document is online), color red all the
material that you find completely irrelevant to a software architecture for that system. Color
yellow all of the material that you think might be relevant, but not without further discussion and
elaboration. Color green all of the material that you are certain is architecturally significant.
When you're done, every part of the document that's not white space should be red, yellow, or
green. Approximately what percentage of each color did your document end up being? Do the
results surprise you?
1 7. Designing an Architecture
·
·
·- 0
In most people 's vocabularies, design means
veneer. It's interior decorating. It's the fabric of
the curtains or the sofa. But to me, nothing could
be further from the meaning of design. Design is
the fundamental soul of a human-made creation
that ends up expressing itself in successive outer
layers of the product or service.
-Steve Jobs
A a
We have discussed the building blocks for designing a software architecture, which principally are
locating architecturally significant requirements; capturing quality attribute requirements; and choosing,
generating, tailoring, and analyzing design decisions for achieving those requirements. All that's
missing is a way to pull the pieces together. The purpose of this chapter is to provide that way.
We begin by describing our strategy for designing an architecture and then present a packaging of
these ideas into a method: the Attribute-Driven Design method.
17.1. Design Strategy
·
·
·- 0 A a
We present three ideas that are key to architecture design methods: decomposition, designing to
architecturally significant requirements, and generate and test.
Decomposition
Architecture determines the quality attributes of a system. Hopefully, we have convinced you of that by
now. The quality attributes are properties of the system as a whole. Latency, for example, is the time
between the arrival of an event and the output of the processing of that event. Availability refers to the
system providing services, and so forth.
Given the fact that quality attributes refer to the system as a whole, if we wish to design to achieve
quality attribute requirements, we must begin with the system as a whole. As the design is decomposed,
the quality attribute requirements can also be decomposed and assigned to the elements of the
decomposition.
A dec01nposition strategy does not mean that we are assuming the design is a green-field design or
that there are no constraints on the design to use particular preexisting components either externally
developed or legacy. Just as when you choose a route from one point to another, you may choose to
stop at various destinations along the route, constraints on the design can be accommodated by a
decomposition strategy. You as the designer must keep in mind the constraints given to you and arrange
the dec01nposition so that it will accommodate those constraints. In some contexts, the system may end
up being constructed mostly from preexisting components; in others, the preexisting components may
be a smaller portion of the overall system. In either case, the goal of the design activity is to generate a
design that accommodates the constraints and achieves the quality and business goals for the system.
We have already talked about module decomposition, but there are other kinds of decompositions
that one regularly finds in an architecture, such as the decomposition of a component in a componentsand-
connectors (C&C) pattern into its subcomponents. For example, a user interface implemented using
the model-view-controller (MVC) pattern would be decomposed into a number of components for the
model, one or more views, and one or more controllers.
Designing to Architecturally Significant Requirements
In Chapter 1 6, we discussed architecturally significant requirements (ASRs) and gave a technique for
collecting and structuring them. These are the requirements that drive the architectural design; that is
why they are significant. Driving the design means that these requirements have a profound effect on
the architecture. In other words, you must design to satisfy these requirements. This raises two
questions: What happens to the other requirements? and Do I design for one ASR at a time or all at
once?
1 . What about the non-ASR requirements? The choice of ASRs implies a prioritization of the
requirements. Once you have produced a design that satisfies the ASRs, you know that you
are in good shape. However, in the real world, there are other requirements that, while not
ASRs, you would like to be able to satisfy. You have three options with respect to meeting
these other requirements: (a) You can still meet the other requirements. (b) You can meet the
·
·
·- 0 A a
other requirements with a slight adjustment of the existing design, and this slight adjustment
does not keep the higher priority requirements from being met. (c) You cannot meet the other
requirements under the current design. In case (a) or (b), there is nothing more to be done.
You are happy. In case (c), you now have three options: (i) If you are close to meeting the
requirement, you can see if the requirement can be relaxed. (ii) You can reprioritize the
requirements and revisit the design. (iii) You can report that you cannot meet the
requirement. All of these latter three options involve adjusting either the requiretnent or its
priority. Doing so may have a business impact, and it should be reported up the management
chain.
2. Design for all of the ASRs or one at a time? The answer to this question is a matter of
experience. When you learn chess, you begin by learning that the horsey goes up two and
over one. After you have been playing for a while, you internalize the moves of the knight
and you can begin to look further ahead. The best players may look ahead a dozen or more
moves. This situation applies to when you are designing to satisfy ASRs. Left to their own
devices, novice architects will likely focus on one ASR at a time. But you can do better than
that. Eventually, through experience and education, you will develop an intuition for
designing, and you will employ patterns to aid you in designing for multiple ASRs.
Generate and Test
One way of viewing design is as a process of "generate and test." This generate-and-test approach
views a particular design as a hypothesis: namely, the design satisfies the requirements. Testing is the
process of determining whether the design hypothesis is correct. If it is not, then another design
hypothesis must be generated. Figure 17. 1 shows this iteration.
Generate
Initial
Hypothesis
Test
Hypothesis
Generate
Next
Hypothesis
Figure 17.1. The generate-and-test process of architecture design
For this process to be effective, the generation of the next design hypothesis must build on the
results of the tests. That is, the things wrong with the current design hypothesis are fixed in the next
design hypothesis, and the things that are right are kept. If there is no coupling between the testing and
the generation of the next design hypothesis, then this process becomes ''guess and test" and that is not
effective.
Generate and test as a design strategy leads to the following questions:
1 . Where does the initial hypothesis come from?
2. What are the tests that are applied?
3. How is the next hypothesis generated?
4. When are you done?
·
·
·- 0 A a
We have already seen many of the elements of the answers to these questions. But now we can think
about them and organize them more systematically.
Creating the Initial Hypothesis
Design solutions are created using "collateral" that is available to the project. Collateral can include
existing systems, frameworks available to the project, known architecture patterns, design checklists, or
a domain decomposition.
• Existing systems. Very few systems are completely unprecedented, even within a single
organization. Organizations are in a particular business, their business leads to specialization,
and specialization leads to the development of variations on a theme. It is likely that systems
already exist that are similar to the system being constructed in your company.
Existing systems are likely to provide the most powerful collateral, because the business
context and requirements for the existing system are likely to be similar to the business context
and requirements for the new system, and many of the problems that occur have already been
solved in the existing design.
A common special case is when the existing system you're drawing on for knowledge is the
same one that you're building. This occurs when you're evolving a system, not building one
from scratch. The existing design serves as the initial design hypothesis. The "test" part of this
process will reveal the parts that don't work under the current (presumably changed) set of
requirements and will therefore pinpoint the parts of the system's design that need to change.
Another special case is when you have to combine existing legacy systems into a single
system. In this case, the collection of legacy systems can be mined to determine the initial
design hypothesis.
• Frameworks. A framework is a partial design (accompanied by code) that provides services
that are common in particular domains. Fratneworks exist in a great many d01nains, ranging
from web applications to middleware systems to decision support systems. The design of the
framework (especially the architectural assumptions it makes) provides the initial design
hypothesis. For exatnple, a design framework might constrain all communication to be via a
broker, or via a publish-subscribe bus, or via callbacks. In each case this design framework has
constrained your initial design hypothesis.
• Patterns and tactics. As we discussed in Chapter 1 3 , a pattern is a known solution to a common
problem in a given context. Cataloged architectural patterns, possibly augmented with tactics,
should be considered as candidates for the design hypothesis you're building.
• Domain decomposition. Another option for the initial design hypothesis comes from
performing a domain decomposition. For exatnple, most object-oriented analysis and design
processes begin this way, identifying actors and entities in the domain. This decomposition will
divide the responsibilities to make certain modifications easier, but by itself it does not speak to
many other quality attribute requirements.
·
·
·- 0 A a
• Design checklists. The design checklists that we presented in Chapters 5-1 1 can guide an
architect to making quality-attribute-targeted design choices. The point of using a checklist is to
ensure completeness: Have I thought about all of the issues that might arise with respect to the
many quality attribute concerns that I have? The checklist will provide guidance and
confidence to an architect.
Choosing the Tests
Three sources provide the tests to be applied to the hypothesis:
1 . The analysis techniques described in Chapter 14.
2. The design checklists for the quality attributes that we presented in Chapters 5-1 1 can also be
used to test the design decisions already made, from the sources listed above. For the
important quality attribute requirements, use the design checklists to assess whether the
decisions you've made so far are sound and complete. For example, if testability is important
for your system, the checklist says to ensure that the coordination model supports capturing
the activity that led to a fault.
3. The architecturally significant requirements. If the hypothesis does not provide a solution for
the ASRs, then it must be improved.
Generating the Next Hypothesis
After applying the tests, you might be done everything looks good. On the other hand, you might still
have some concerns; specifically, you might have a list of quality attribute problems associated with
your analysis of the current hypothesis. This is the problem that tactics are intended to solve: to improve
a design with respect to a particular quality attribute. Use the sets of tactics described in each of
Chapters 5-1 1 to help you to choose the ones that will improve your design so that you can satisfy these
outstanding quality attribute requirements.
Terminating the Process
You are done with the generate-and-test process when you either have a design that satisfies the ASRs
or when you exhaust your budget for producing the design. In Chapter 22, we discuss how much time
should be budgeted for producing the architecture.
If you do not produce such a design within budget, then you have two options depending on the set
of ASRs that are satisfied. Your first option is to proceed to implementation with the best hypothesis
you were able to produce, with the realization that some ASRs may not be met and may need to be
relaxed or eliminated. This is the most common case. Your second option is to argue for more budget
for design and analysis, potentially revisiting some of the major early design decisions and resuming
generate and test from that point. If all else fails, you could suggest that the project be terminated. If all
of the ASRs are critical and you were not able to produce an acceptable or nearly acceptable design,
then the system you produce from the design will not be satisfactory and there is no sense in producing
it.
17 .2. The Attribute-Driven Design Method
·
·
·- 0 A a
The Attribute-Driven Design (ADD) method is a packaging of the strategies that we have just
discussed. ADD is an iterative method that, at each iteration, helps the architect to do the following:
• Choose a part of the system to design.
• Marshal all the architecturally significant requirements for that part.
• Create and test a design for that part.
The output of ADD is not an architecture complete in every detail, but an architecture in which the
main design approaches have been selected and vetted. It produces a "workable" architecture early and
quickly, one that can be given to other project teams so they can begin their work while the architect or
architecture team continues to elaborate and refine.
Inputs to ADD
Before beginning a design process, the requirements functional, quality, and constraints should be
known. In reality, waiting for all of the requirements to be known means the project will never be
finished, because requirements are continually arriving to a project as a result of increased knowledge
on the part of the stakeholders and changes in the environment (technical, social, legal, financial, or
political) over time. ADD can begin when a set of architecturally significant requirements is known.
This increases the importance of having the correct set of ASRs. If the set of ASRs changes after
design has begun, then the design may well need to be reworked (a truth under any design method, not
just ADD). To the extent that you have any influence over the requirements-gathering process, it would
behoove you to lobby for collection of ASRs first. Although these can't all be known a priori, as we
saw in Chapter 1 6, quality attribute requirements are a good start.
In addition to the ASRs, input to ADD should include a context description. The context description
gives you two vital pieces of information as a designer:
1 . What are the boundaries of the system being designed? What is inside the system and what is
outside the system must be known in order to constrain the problem and establish the scope
of the architecture you are designing. The system's scope is unknown or unclear surprisingly
often, and it will help the architecture to nail down the scope as soon as you can.
2. What are the external systems, devices, users, and environmental conditions with which the
system being designed must interact? By "environmental conditions" here we are referring to
the system's runtime environment. The system's environmental conditions are an
enumeration of factors such as where the input comes from, where the output goes, what
forms they take, what quality attributes they have, and what forces may affect the operation
of the system. It is possible that not all of the external systems are known at design time. In
this case, the system must have some discovery mechanisms, but the context description
should enumerate the assumptions that can be made about the external systems even if their
specifics are not yet known. An example of accommodating environment conditions can be
seen in a system that must be sent into space. In addition to handling its inputs, outputs, and
quality attributes, such a system must accommodate failures caused by stray gamma rays,
certainly a force affecting the operation of the system.
Output of ADD
·
·
·- 0 A a
The output of ADD is a set of sketches of architectural views. The views together will identify a
collection of architectural elements and their relationships or interactions. One of the views produced
will be a module decomposition view, and in that view each element will have an enumeration of its
responsibilities listed.
Other views will be produced according to the design solutions chosen along the way. For example,
if at one point in executing the method, you choose the service-oriented architecture (SOA) pattern for
part of the system, then you will capture this in an SOA view (whose scope is that part of the system to
which you applied the pattern).
The interactions of the elements are described in terms of the information being passed between the
elements. For example, we might specify protocol names, synchronous, asynchronous, level of
encryption, and so forth.
The reason we refer to "sketches" above is that ADD does not take the design so far as to include
full-blown interface specifications, or even so far as choosing the names and parameter types of
interface programs (methods). That can co1ne later. ADD does identify the information that passes
through the interfaces and important characteristics of the information. If any aspects of an interface
have quality attribute implications, those are captured as annotations.
When the method reaches the end, you will have a full-fledged architecture that is roughly
documented as a set of views. You can then polish this collection, perhaps merging some of the views
as appropriate, to the extent required by your project. In an Agile project, this set of rough sketches may
be all you need for quite a while, or for the life of the project.
17.3. The Steps of ADD
ADD is a five-step method:
1 . Choose an element of the system to design.
2. Identify the ASRs for the chosen element.
3. Generate a design solution for the chosen element.
·
·
·- 0
4. Inventory remaining requirements and select the input for the next iteration.
5. Repeat steps 1-4 until all the ASRs have been satisfied.
Step 1 : Choose an Element of the System to Design
A a
ADD works by beginning with a part of the system that has not yet been designed, and designing it. In
this section, we'll discuss how to make that choice.
For green-field designs, the "element" to begin with is simply the entire system. The first trip
through the ADD steps will yield a broad, shallow design that will produce a set of newly identified
architectural elements and their interactions. These elements will almost certainly require more design
decisions to flesh out what they do and how they satisfy the ASRs allocated to them; during the next
iteration of ADD, those elements become candidates for the "choose an element" step.
So, nominally, the first iteration of ADD will create a collection of elements that together constitute
the entire system. The second iteration will take one of these elements what we call the "chosen
element" and design it, resulting in still finer-grained elements. The third iteration will take another
element either one of the children of the whole system or one of the children that was created from the
design of one of the children of the whole system and so forth. For example, if you choose an SOA
pattern in the first iteration, you might choose child elements such as service clients, service providers,
and the SOA infrastructure components. In the next iteration through the loop, you would refine one of
these child elements, perhaps the infrastructure components. In the next iteration you now have a
choice: refine another child of the SOA pattern, such as a service provider, or refine one of the child
elements of the infrastructure components. Figure 17.2 shows these choices as a decomposition tree,
annotated with the ADD iteration that applies to each node. (The example cmnponents are loosely
based on the Adventure Builder system, introduced in Chapter 1 3 .) Figure 17.2 is a decomposition view
of our hypothetical system after two iterations of ADD.
·
·
·- 0
SOA
Orde:r
tracktng
service
Airline
provider
�,_ __ J , __ _J
servfce
Order
database
Consumer
website
lodging Activity
pro.vider provider
service service
Bank.
credlt
card
service
SOA lnfrastructure
compon􀁐nts
Iteration "1: SOA pattern applied
Iteration #3: Refine another SOA element?
Or an SOA infrastructure component?
ESB Service
registry
Iteration #2:
SOA infrastructure
components refi,ned
Service
broker
A a
Figure 17 .2. Iteration 1 applied the SOA pattern. Iteration 2 refined the infrastructure
components. Where will iteration 3 take you?
There are cases when the first iteration of ADD is different Perhaps you are not creating a system
but evolving an existing one_ Perhaps you are required to use a piece of software that your company
already owns, and therefore must fit it into the design_ There are many reasons why some of the design
might already be done for you, and the first time through the steps of ADD you won't pick "whole
system" as the starting point Nevertheless, step 1 still holds: All it requires is that at least one of the
elements you know about needs further design_
There are two main refinement strategies to pursue with ADD: breadth first and depth first Breadth
first 1neans that all of the second-level elements are designed before any of the third-level elements, and
so forth. Depth first means that one downward chain is completed before beginning a second downward
chain. The order that you should work through ADD is influenced by the business and technical
contexts within which the project is operating. Some of the important factors include the following:
• Personnel availability may dictate a refinement strategy. If an important group or team has a
window of availability that will close soon and will work on a particular part of the system,
then it behooves the architect to design that part of the system to the point where it can be
handed off for implementation depth first. But if the team is not currently available but will
be available at some definite time in the future, then you can defer their part of the design until
later.
• Risk mitigation may dictate a refinement strategy. The idea is to design the risky parts of the
system to enough depth so that problems can be identified and solved early. For example, if an
unfamiliar technology is being introduced on the project, prototypes using that technology will
likely be developed to gain understanding of its implications. These prototypes are most useful
if they reflect the design of the actual system. A depth-frrst strategy can provide a context for
technology prototyping. Using this context you can build the prototype in a fashion that allows
·
·
·- 0 A a
for its eventual integration into the architecture. On the other hand, if the risk is in how
elements at the same level of the design interact with each other to meet critical quality
attributes, then a breadth-first strategy is in order.
• Deferral of some functionality or quality attribute concerns may dictate a mixed approach. For
example, suppose the system being constructed has a medium-priority availability requirement.
In this case you might adopt a strategy of employing redundancy for availability but defer
detailed consideration of this redundancy strategy to allow for the rapid generation of the highpriority
functionality in an intermediate release. You might therefore apply a breadth-first
approach for everything but availability, and then in subsequent design iterations you revisit
some of the elements to enable the addition of the responsibilities to support availability. In
reality this approach will require some backtracking, where you revisit earlier decisions and
refme them or modify them to accommodate this new requirement.
All else being equal, a breadth-first refinement strategy is preferred because it allows you to
apportion the most work to the most teams soonest. Breadth first allows for consideration of the
interaction among the elements at the same level.
Step 2 : Identify the ASRs for This Element
In Chapter 1 6 we described a number of methods for discovering the ASRs for a system. One of those
methods involved building a utility tree. To support the design process, the utility tree has an advantage
over the other methods: it guides the stakeholders in prioritizing the QA requirements. The two factors
used to prioritize the ASRs in a utility tree are business value and architectural impact. The business
value of an ASR typically will not change throughout the design process and does not need to be
reconsidered.
If the chosen element for design in step 1 is the whole system, then a utility tree can be a good
source for the ASRs. Otherwise, construct a utility tree specifically focused on this chosen element,
using the quality attribute requirements that apply to this element (you'll see how to assign those in step
4). Those that are labeled (High, High) are the ASRs for this element. As an architect you will also need
to pay attention to the (High, Medium) and (Medium, High) utility tree leaves as well. These will
almost certainly also be ASRs for this element.
Step 3 : Generate a Design Solution for the Chosen Element
This step is the heart of the ADD. It is the application of the generate-and-test strategy. Upon entry to
this step, we have a chosen element for design and a list of ASRs that apply to it. For each ASR, we
develop a solution by choosing a candidate design approach.
Your initial candidate design will likely be inspired by a pattern, possibly augmented by one or
more tactics. You may then refine this candidate design by considering the design checklists that we
gave for the quality attributes in Chapters 5-1 1 . For ASRs that correspond to quality attributes, you can
invoke those checklists to help you instantiate or refine the major design approach (such as a pattern)
that you've chosen. For example, the layered pattern is helpful for building systems in which
modifiability is important, but the pattern does not tell you how many layers you should have or what
each one's responsibility should be. But the checklist for the "allocation of responsibilities" design
·
·
·- 0 A a
decision category for modifiability in Chapter 7 will help you ask the right questions to make that
determination.
Although this step is performed for each ASR in turn, the sources of design candidates outlined
above patterns, tactics, and checklists will usually do much better than that. That is, you're likely to
find design candidates that address several of your ASRs at once. This is because to the extent that the
system you're building is similar to others you know about, or to the extent that the problem you are
solving is similar to the problems solved by patterns, it is likely that the solutions you choose will be
solving a whole collection ofASRs simultaneously. If you can bring a solution to bear that solves more
than one of your ASRs at once, so much the better.
The design decisions made in this step now become constraints on all future steps of the method.
Step 4: Verify and Refine Requirements and Generate Input for the Next Iteration
It's possible that the design solution you came up with in the prior step won't satisfy all the ASRs. Step
4 of ADD is a test step that is applied to your design for the element you chose to elaborate in step 1 of
this iteration. One of the possible outcomes of step 4 is "backtrack," meaning that an important
requirement was not satisfied and cannot be satisfied by further elaborating this design. In this case, the
design needs to be reconsidered.
The ASRs you have not yet satisfied could be related to the following:
1 . A quality attribute requirement allocated to the parent element
2. A functional responsibility of the parent element
3. One or more constraints on the parent element
Table 1 7 . 1 summarizes the types of problems and the actions we recommend for each.
Table 17.1. Recommended Actions for Problems with the Current Hypothesis
·
·
·- 0
Type of ASR Not Met Action Recommended
1. Quality attribute requirement Consider app·ly·ing (m.ore) tactics to improve the
design with respect to the quality attribute. For
each candidate tactic, ask:
• Will this tactic Jmprove the quality attribute
behavior of the current design sufficiently?
• Should this tactic be used' in conjunction with
another tactic?
• What are the tradeoff considerations when
applying this tactic?
2. Functional responsibility Add responsibilities either to existing modules or to
newly created modules:
3 . Constraint
• Assign the responsibility to a module containing
similar responsibilities.
• Break a module into portions when it Is too
complex.
• Assign the responsibility to a module containing
responsibilities with similar quality attribute
characteristics-for example, similar timing
behavior, similar security requirements, or
similar avaitabiHty requirements.
Modtfy the design or 1ry to relax the constraint:
• Modify the design to accommodate the
constraint.
• Relax the constraint.
A a
In most real-world systems, requirements outstrip available time and resources. Consequently you
will find yourself unable to meet some of the QA requirements, functional requirements, and
constraints. These kinds of decisions are outside the scope of the ADD method, but they are clearly
important drivers of the design process, and as an architect you will be continually negotiating decisions
of this form.
Step 4 is about taking stock and seeing what requirements are left that still have not been satisfied
by our design so far. At this point you should sequence through the quality attribute requirements,
responsibilities, and constraints for the element just designed. For each one there are four possibilities:
1 . The quality attribute requirement, functional requirement, or constraint has been satisfied. In
this case, the design with respect to that requirement is complete; the next time around, when
you further refine the design, this requirement will not be considered. For example, if a
constraint is to use a particular middleware and the system is decomposed into elements that
all use this middleware, the constraint has been satisfied and can be removed from
consideration. An example of a quality attribute requirement being satisfied is a requirement
to make it easy to modify elements and their interactions. If a publish-subscribe pattern can
be shown to have been employed throughout the system, then this QA requirement can be
said to be satisfied.
2. The quality attribute requirement, functional requirement, or constraint is delegated to one of
the children. For example, if a constraint is to use a particular middleware and the
decomposition has a child element that acts as the infrastructure, then delegating that
constraint to that child will retain the constraint and have it be reconsidered when the
infrastructure element is chosen for subsequent design. Similarly, with the example we gave
·
·
·- 0 A a
earlier about providing extensibility, if there is as yet no identifiable plug-in manager, then
this requirement is delegated to the child where the plug-in manager is likely to appear.
3. The quality attribute requirement, functional requirement, or constraint is distributed among
the children. For example, a constraint might be to use .NET. In this case, .NET Remoting
might become a constraint on one child and ASP .NET on another. Or a quality attribute
requirement that constrains end-to-end latency of a certain operation to 2 seconds could be
distributed among the element's three children so that the latency requirement for one
element is 0.8 seconds, the latency for a second element is 0.9 seconds, and the latency for a
third is 0.3 seconds. When those elements are subsequently chosen for further design, those
times will serve as constraints on them individually.
4. The quality attribute requirement, functional requirement, or constraint cannot be satisfied
with the current design. In this case there are the same two options we discussed previously:
you can either backtrack revisit the design to see if the constraint or quality attribute
requirement can be satisfied some other way or push back on the requirement. This will
almost certainly involve the stakeholders who care about that requirement, and you should
have convincing arguments as to why the dropping of the requirement is necessary.
Report to the project manager that the constraint cannot be satisfied without jeopardizing other
requirements. You must be prepared to justify such an assertion. Essentially, this is asking,
"What's more important the constraint or these other requirements?"
Step 5: Repeat Steps 1-4 Until Done
After the prior steps, each element has a set of responsibilities, a set of quality attribute requirements,
and a set of constraints assigned to it. If it's clear that all of the requirements are satisfied, then this
unequivocally ends the ADD process.
In projects in which there is a high degree of trust between you and the implementation teams, the
ADD process can be terminated when only a sketch of the architecture is available. This could be as
soon as two levels of breadth-first design, depending on the size of the system. In this case, you trust the
implementation team to be able to flesh out the architecture design in a manner consistent with the
overall design approaches you have laid out. The test for this is if you believe that you could begin
implementation with the level of detail available and trust the implementation team to that extent. If you
have less trust in the implementation team, then an additional level (or levels) of design may be
necessary. (And, of course, you will need to subsequently ensure that the implementation is faithfully
followed by the team.)
On the other hand, if there is a contractual arrangement between your organization and the
implementation organization, then the specification of the portion of the system that the implementers
are providing must be legally enforceable. This means that the ADD process must continue until that
level of specificity has been achieved.
Finally, another condition for terminating ADD is when the project's design budget has been
exhausted. This happens more often than you might think.
Choosing when to terminate ADD and when to start releasing the architecture that you've sketched
·
·
·- 0 A a
out are not the same decision. You can, and in many cases should, start releasing early architectural
views based on the needs of the project (such as scheduled design reviews or customer presentations)
and your confidence in the design so far. The unpalatable alternative is to make everyone wait until the
architecture design is finished. You-can't-have-it-until-it' s-done is particularly unpalatable in Agile
projects, as we discussed in Chapter 1 5 .
You should release the documentation with a caveat as to how likely you think it is to change. But
even early broad-and-shallow architectural descriptions can be enormously helpful to implementers and
other project staff. A first- or second-level module decomposition view, for instance, lets experts start
scouring the marketplace for commercial products that provide the responsibilities of the identified
modules. Managers can start making budgets and schedules for implementation that are based on the
architecture and not just the requirements. Support staff can start building the infrastructure and file
systems to hold project artifacts (these are often structured to mirror the module decomposition view).
And early release invites early feedback.
17.4. Summary
·
·
·- 0 A a
The Attribute-Driven Design method is an application of the generate-and-test philosophy. It keeps the
number of requirements that must be satisfied to a humanly achievable quantity. ADD is an iterative
method that, at each iteration, helps the architect to do the following:
• Choose an element of the system to design.
• Marshal all the architecturally significant requirements for the chosen element.
• Create and test a design for that chosen element.
The output of ADD is not an architecture complete in every detail, but an architecture in which the
main design approaches have been selected and validated. It produces a "workable" architecture early
and quickly, one that can be given to other project teams so they can begin their work while the
architect or architecture team continues to elaborate and refine.
ADD is a five-step method:
1 . Choose the element of the system to design. For green-field designs, the "part" to begin with
is simply the entire system. For designs that are already partially completed (either by
external constraints or by previous iterations through ADD), the part is an element that is not
yet designed. Choosing the next element can proceed in a breadth-first, depth-first, or mixed
manner.
2. Identify the ASRs for the chosen element.
3. Generate a design solution for the chosen element, using design collateral such as existing
systems, frameworks, patterns and tactics, and the design checklists from Chapters 5-1 1 .
4. Verify and refine requirements and generate input for the next iteration. Either the design in
step 3 will satisfy all of the chosen element's ASRs or it won't. If it doesn't, then either they
can be allocated to elements that will be elaborated in future iterations of ADD, or the
existing design is inadequate and we must backtrack. Furthermore, non-ASR requirements
will either be satisfied, allocated to children, or indicated as not achievable.
5. Repeat steps 1-4 until all the ASRs have been satisfied, or until the architecture has been
elaborated sufficiently for the implementers to use it.
17 .5. For Further Reading
·
·
·- 0 A a
You can view design as the process of making decisions; this is another philosophy of design. This
view of design leads to an emphasis on design rationale and tools to capture design rationale. The view
of design as the process of making decisions dates to the 1940s [Mettler 9 1], but it has been recently
applied to architecture design most prominently by Philippe Kruchten [Kruchten 04], and Hans van
Vliet and Jan Bosch [van Vliet 05].
The Software Engineering Institute has produced a number of reports describing the ADD method
and its application in a variety of contexts. These include [Wojcik 06], [Kazman 04], and [Wood 07].
George Fairbanks has written an engaging book that describes a risk-driven process of architecture
design, entitled Just Enough Software Architecture: A Risk-Driven Approach [Fairbanks 1 0].
Tony Lattanze has created an Architecture-Centric Design Method (ACDM), described in his book
Architecting Software Intensive Systems: A Practitioners Guide [Lattanze 08].
Ian Gorton's Essential Architecture, Second Edition, emphasizes the middleware aspects of a design
[Gorton 1 0].
Woods and Rozanski have written Software Systems Architecture, Second Edition, which interprets
the design process through the prism of different views [Woods 1 1].
A number of authors have compared five different industrial architecture design methods. You can
find this comparison at [Hofmeister 07].
Raghvinder Sangwan and his colleagues describe the design of a building management system that
was originally designed using object-oriented techniques and then was redesigned using ADD
[Sangwan 08].
17 .6. Discussion Questions
·
·
·- 0 A a
1 . ADD does not help with the detailed design of interfaces for the architectural elements it
identifies. Details of an interface include what each method does, whether you need to call a
single all-encompassing method to perform the work of the element or many methods of
finer-grained function, what exceptions are raised on the interface, and more. What are some
examples where the specific design of an interface might bring more or less performance,
security, or availability to a system? (By the way, if there are quality attribute implications to
an interface, you can capture those as annotations on the element.)
2. What sets a constraint apart from other (even high-priority) requirements is that it is not
negotiable. Should this consideration guide the design process? For example, would it be
wise to design to satisfy all of the constraints before worrying about other ASRs?
3. In discussion question 4 of Chapter 1 6 you were asked to create a utility tree for an ATM.
Now choose the two most important ASRs from that utility tree and create a design fragment
using the ADD method employing and instantiating a pattern.
18. Documenting Software Architectures
·
·
·-
If it is not written down, it does not exist.
-Philippe Kruchten
0 A a
Even the best architecture, the most perfectly suited for the job, will be essentially useless if the people
who need to use it do not know what it is; cannot understand it well enough to use, build, or modify it;
or (worst of all) misunderstand it and apply it incorrectly. And all of the effort, analysis, hard work, and
insightful design on the part of the architecture team will have been wasted. They might as well have
gone on vacation for all the good their architecture will do.
Creating an architecture isn't enough. It has to be communicated in a way to let its stakeholders use
it properly to do their jobs. If you go to the trouble of creating a strong architecture, one that you expect
to stand the test of time, then you must go to the trouble of describing it in enough detail, without
ambiguity, and organizing it so that others can quickly find and update needed information.
Documentation speaks for the architect. It speaks for the architect today, when the architect should
be doing other things besides answering a hundred questions about the architecture. And it speaks for
the architect tomorrow, when he or she has left the project and now someone else is in charge of its
evolution and maintenance.
The sad truth is that architectural documentation today, if it is done at all, is often treated as an
afterthought, something people do because they have to. Maybe a contract requires it. Maybe a
customer demands it. Maybe a company's standard process calls for it. In fact, these may all be
legitimate reasons. But none of them are compelling enough to produce high-quality documentation.
Why should the architect spend valuable time and energy just so a manager can check off a deliverable?
The best architects produce good documentation not because it's "required" but because they see
that it is essential to the matter at hand producing a high-quality product, predictably and with as little
rework as possible. They see their immediate stakeholders as the people most intimately involved in
this undertaking: developers, deployers, testers, and analysts.
But architects also see documentation as delivering value to themselves. Documentation serves as
the receptacle to hold the results of major design decisions as they are confirmed. A well-thought-out
documentation scheme can make the process of design go much more smoothly and systematically.
Documentation helps the architect(s) reason about the architecture design and communicate it while the
architecting is in progress, whether in a six-month design phase or a six-day Agile sprint.
·
·
·-
18.1. Uses and Audiences for Architecture Documentation
0 A a
Architecture documentation must serve varied purposes. It should be sufficiently transparent and
accessible to be quickly understood by new employees. It should be sufficiently concrete to serve as a
blueprint for construction. It should have enough information to serve as a basis for analysis.
Architecture documentation is both prescriptive and descriptive. For some audiences, it prescribes
what should be true, placing constraints on decisions yet to be made. For other audiences, it describes
what is true, recounting decisions already made about a system's design.
The best architecture documentation for, say, performance analysis may well be different from the
best architecture documentation we would wish to hand to an implementer. And both of these will be
different from what we put in a new hire's "welcome aboard" package or a briefing we put together for
an executive. When planning and reviewing documentation, you need to ensure support for all the
relevant needs.
We can see that many different kinds of people are going to have a vested interest in an architecture
document. They hope and expect that the architecture document will help them do their respective jobs.
Understanding their uses of architecture documentation is essential, as those uses determine the
important information to capture.
Fundamentally, architecture documentation has three uses:
1 . Architecture documentation serves as a means of education. The educational use consists of
introducing people to the system. The people may be new members of the team, external
analysts, or even a new architect. In many cases, the "new" person is the customer to whom
you're showing your solution for the first time, a presentation you hope will result in funding
or go-ahead approval.
2. Architecture documentation serves as a primary vehicle for communication among
stakeholders. An architecture' s precise use as a communication vehicle depends on which
stakeholders are doing the communicating.
Perhaps one of the most avid consumers of architecture documentation is none other than the
architect in the project's future. The future architect may be the same person or may be a
replacement, but in either case he or she is guaranteed to have an enormous stake in the
documentation. New architects are interested in learning how their predecessors tackled the
difficult issues of the system and why particular decisions were made. Even if the future architect
is the same person, he or she will use the documentation as a repository of thought, a storehouse
of design decisions too numerous and hopelessly intertwined to ever be reproducible from
memory alone. See the sidebar "Schmucks and Jerks."
3. Architecture documentation serves as the basis for system analysis and construction.
Architecture tells implementers what to implement. Each module has interfaces that must be
provided and uses interfaces from other modules. Not only does this provide instructions
about the provided and used interfaces, but it also determines with what other teams the
development team for the module must communicate.
During development, an architecture can be very complex, with many issues left to resolve.
·
·
·- 0 A a
Documentation can serve as a receptacle for registering and communicating these issues that
might otherwise be overlooked.
For those interested in the ability of the design to meet the system's quality objectives, the
architecture documentation serves as the fodder for evaluation. It must contain the information
necessary to evaluate a variety of attributes, such as security, performance, usability, availability,
and modifiability.
For system builders who use automatic code-generation tools, the documentation may
incorporate the models used for generation. These models provide guidance to those who wish to
understand the behavior of the module in more detail than is normally documented but in less
detail than examining the code would provide.
18.2. Notations for Architecture Documentation
·
·
·- 0 A a
Notations for documenting views differ considerably in their degree of formality. Roughly speaking,
there are three main categories of notation:
• Informal notations. Views are depicted (often graphically) using general-purpose diagramming
and editing tools and visual conventions chosen for the system at hand. The semantics of the
description are characterized in natural language, and they cannot be formally analyzed. In our
experience, the most common tool for informal notations is Power Point.
• Semiformal notations. Views are expressed in a standardized notation that prescribes graphical
elements and rules of construction, but it does not provide a complete semantic treatment of the
meaning of those elements. Rudimentary analysis can be applied to determine if a description
satisfies syntactic properties. UML is a semiformal notation in this sense.
• Formal notations. Views are described in a notation that has a precise (usually mathematically
based) semantics. Formal analysis of both syntax and semantics is possible. There are a variety
of formal notations for software architecture available. Generally referred to as architecture
description languages (ADLs), they typically provide both a graphical vocabulary and an
underlying semantics for architecture representation. In s01ne cases these notations are
specialized to particular architectural views. In others they allow many views, or even provide
the ability to formally define new views. The usefulness of ADLs lies in their ability to support
automation through associated tools: automation to provide useful analysis of the architecture
or assist in code generation. In practice, the use of such notations is rare.
Schmucks and Jerks
One day I was sitting in a meeting with a well-known compiler guru. He was
recounting some of his favorite war stories from his long career. One of these stories
particularly stuck with me. He was talking about the time that he was chasing down a
very nasty and subtle bug in the code of a compiler that he was maintaining. After a
long and exasperating search, he finally located and eventually fixed the bug. But the
search itself had gotten him so worked up, and he was so infuriated at the irresponsible
thought and programming that led to the bug, that he decided to do a bit more detective
work and figure out who was the jerk responsible for that bug.
By going backward through the revision history, he found the culprit. It was him. He
was the jerk. It turns out that he was the one who eight years earlier had written that
offending piece of code. The trouble was, he had no recollection of writing the code
and no recollection of the rationale for writing it the way he had done. Perhaps there
was a good reason to do so at the time, but if so it was lost now.
That is why we document. The documentation helps the poor schmuck who has to
maintain your code in the future, and that schmuck might very well be you!
-RK
·
·
·- 0 A a
Determining which form of notation to use involves making several tradeoffs. Typically, more
formal notations take more time and effort to create and understand, but they repay this effort in
reduced ambiguity and more opportunities for analysis. Conversely, more informal notations are easier
to create, but they provide fewer guarantees.
Regardless of the level of formality, always remember that different notations are better (or worse)
for expressing different kinds of information. Formality aside, no UML class diagram will help you
reason about schedulability, nor will a sequence chart tell you very much about the system's likelihood
of being delivered on time. You should choose your notations and representation languages always
keeping in mind the important issues you need to capture and reason about.
18.3. Views
·
·
·- 0 A a
Perhaps the most important concept associated with software architecture documentation is that of the
view. A software architecture is a complex entity that cannot be described in a simple one-dimensional
fashion. A view is a representation of a set of system elements and relations among them not all
system elements, but those of a particular type. For example, a layered view of a system would show
elements of type "layer" that is, it would show the system's decomposition into layers and the
relations among those layers. A pure layered view would not, however, show the system's services, or
clients and servers, or data model, or any other type of element.
Thus, views let us divide the multidimensional entity that is a software architecture into a number of
(we hope) interesting and manageable representations of the system. The concept of views gives us our
most fundamental principle of architecture documentation:
Documenting an architecture is a matter of documenting the relevant views and then adding
documentation that applies to more than one view.
This maxim gives our approach to documentation its name: Views and Beyond.
What are the relevant views? This depends entirely on your goals. As we saw previously,
architecture documentation can serve many purposes: a mission statement for implementers, a basis for
analysis, the specification for automatic code generation, the starting point for system understanding
and asset recovery, or the blueprint for project planning.
Different views also expose different quality attributes to different degrees. Therefore, the quality
attributes that are of most concern to you and the other stakeholders in the system's development will
affect the choice of what views to document. For instance, a layered view will let you reason about your
system's portability, a deployment view will let you reason about your system's performance and
reliability, and so forth.
Different views support different goals and uses. This is why we do not advocate a particular view
or collection of views. The views you should document depend on the uses you expect to make of the
documentation. Different views will highlight different system elements and relations. How many
different views to represent is the result of a cost/benefit decision. Each view has a cost and a benefit,
and you should ensure that the benefits of maintaining a particular view outweigh its costs.
Views may be driven by the need to document a particular pattern in your design. Some patterns are
composed of modules, others of components and connectors, and still others have deployment
considerations. Module views, component-and-connector (C&C) views, and allocation views are the
appropriate mechanisms for representing these considerations.
Module Views
A module is an itnpletnentation unit that provides a coherent set of responsibilities. A module might
take the form of a class, a collection of classes, a layer, an aspect, or any decomposition of the
implementation unit. Example module views are decomposition, uses, and layers. Every module has a
collection of properties assigned to it. These properties are intended to express the important
information associated with the module, as well as constraints on the module. Sample properties are
·
·
·- 0 A a
responsibilities, visibility information, and revision history. The relations that modules have to one
another include is part of, depends on, and is a.
The way in which a system's software is decomposed into manageable units remains one of the
important forms of system structure. At a minimum, this determines how a system's source code is
decomposed into units, what kinds of assumptions each unit can make about services provided by other
units, and how those units are aggregated into larger ensembles. It also includes global data structures
that impact and are impacted by multiple units. Module structures often determine how changes to one
part of a system might affect other parts and hence the ability of a system to support modifiability,
portability, and reuse.
It is unlikely that the documentation of any software architecture can be complete without at least
one module view.
Table 1 8 . 1 summarizes the elements, relations, constraints, and purpose of the module views in
general. Later we provide this information specific to each of a number of often used module views.
Elements
Relations
Constraints
Usage
Table 18.1. Summary of the Module Views
Mod'ules, which are lmp'lementatlon units of software that
provide a coherent set of responsibilities.
• ts part of, which defines a parVwh.ole relationship between
the submodule the part-and the aggregate module the
whole.
• Depends on, which defines a dependency relationship between
two modules. Specific module views elaborate what
dependency is meant.
• Is a, which defines a genera'llzation/speciallzation relationship
between a more specific module the child-and a
more general module the parent.
Different module views may impose specific topological
constraints, such as l'imJ.tations on the visibnity between
mod'ules.
• Blueprint for construction of the code
• Change-impact analysis
• Planning incremental dev�elopment
• Requirements traceability analysi,s
• Communicating the funct􀍓onal lty of a system and' the structure
of its code base
• Supporting the definition of work assignments, implementation
schedu'ies, and budget information
• Showing the structure of information that the system needs
to manage
Properties of modules that help to guide implementation or are input to analysis should be recorded
as part of the supporting documentation for a module view. The list of properties may vary but is likely
to include the following:
• Name. A module's name is, of course, the primary means to refer to it. A module's name often
suggests something about its role in the system. In addition, a module's name may reflect its
position in a decomposition hierarchy; the name A.B.C, for example, refers to a module C that
is a submodule of a module B, itself a submodule of A.
·
·
·- 0 A a
• Responsibilities. The responsibility property for a module is a way to identify its role in the
overall system and establishes an identity for it beyond the name. Whereas a module's name
may suggest its role, a statement of responsibility establishes it with much more certainty.
Responsibilities should be described in sufficient detail to make clear to the reader what each
module does.
• Visibility ofinterface(s) . When a module has submodules, some interfaces of the submodules
are public and some may be private; that is, the interfaces are used only by the submodules
within the enclosing parent module. These private interfaces are not visible outside that context.
• Implementation information. Modules are units of implementation. It is therefore useful to
record information related to their implementation from the point of view of managing their
development and building the system that contains them. This might include the following:
• Mapping to source code units. This identifies the files that constitute the implementation of a
module. For example, a module Account, if implemented in Java, might have several files that
constitute its implementation: IAccount.java (an interface), Accountimpl.java (the
implementation of Account functionality), AccountBean.java (a class to hold the state of an
account in memory), AccountOrmMapping.xml (a file that defines the mapping between
AccountBean and a database table object-relational mapping), and perhaps even a unit test
AccountTest.j a va.
• Test information. The module's test plan, test cases, test scaffolding, and test data are important
to document. This information may simply be a pointer to the location of these artifacts.
• Management information. A manager may need information about the module's predicted
schedule and budget. This information may simply be a pointer to the location of these artifacts.
• Implementation constraints. In many cases, the architect will have an implementation strategy
in mind for a module or may know of constraints that the implementation must follow.
• Revision history. Knowing the history of a module including authors and particular changes
may help when you perfonn maintenance activities.
Because modules partition the system, it should be possible to determine how the functional
requirements of a syste1n are supported by module responsibilities. Module views that show
dependencies among modules or layers (which are groups of modules that have a specific pattern of
allowed usage) provide a good basis for change-impact analysis. Modules are typically modified as a
result of problem reports or change requests. Impact analysis requires a certain degree of design
completeness and integrity of the module description. In particular, dependency information has to be
available and correct to be able to create useful results.
A module view can be used to explain the system's functionality to someone not familiar with it.
The various levels of granularity of the module decomposition provide a top-down presentation of the
system's responsibilities and therefore can guide the learning process. For a system whose
implementation is already in place, module views, if kept up to date, are helpful, as they explain the
structure of the code base to a new developer on the team. Thus, up-to-date module views can simplify
and regularize system maintenance.
·
·
·- 0 A a
On the other hand, it is difficult to use the module views to make inferences about runtime behavior,
because these views are just a static partition of the functions of the software. Thus, a module view is
not typically used for analysis of performance, reliability, and many other runtime qualities. For those,
we rely on component-and-connector and allocation views.
Module views are commonly mapped to component-and-connector views. The implementation units
shown in module views have a mapping to components that execute at runtime. Sometimes, the
mapping is quite straightforward, even one-to-one for small, simple applications. More often, a single
module will be replicated as part of many runtime components, and a given component could map to
several modules. Module views also provide the software elements that are mapped to the diverse
nonsoftware elements of the system environment in the various allocation views.
Component-and-Connector Views
Component-and-connector views show elements that have some runtime presence, such as processes,
objects, clients, servers, and data stores. These elements are termed components. Additionally,
component-and-connector views include as elements the pathways of interaction, such as
communication links and protocols, information flows, and access to shared storage. Such interactions
are represented as connectors in C&C views. Sample C&C views are service-oriented architecture
(SOA), client-server, or communicating process views.
Components have interfaces called ports. A port defines a point of potential interaction of a
component with its environment. A port usually has an explicit type, which defines the kind of behavior
that can take place at that point of interaction. A component may have many ports of the same type,
each forming a different input or output channel at runtime. In this respect ports differ from interfaces
of modules, whose interfaces are never replicated. You can annotate a port with a number or range of
numbers to indicate replication; for example, " 1 . .4" might mean that an interface could be replicated up
to four times. A component's ports should be explicitly documented, by showing them in the diagram
and defining them in the diagram's supporting documentation.
A component in a C&C view may represent a complex subsystem, which itself can be described as
a C&C subarchitecture. This subarchitecture can be depicted graphically in situ when the substructure is
not too co1nplex, by showing it as nested inside the component that it refines. Often, however, it is
documented separately. A component's subarchitecture may employ a different pattern than the one in
which the component appears.
Connectors are the other kind of element in a C&C view. Simple examples of connectors are service
invocation; asynchronous message queues; event multicast supporting publish-subscribe interactions;
and pipes that represent asynchronous, order-preserving data streams. Connectors often represent much
more complex forms of interaction, such as a transaction-oriented communication channel between a
database server and a client, or an enterprise service bus that mediates interactions between collections
of service users and providers.
Connectors have roles, which are its interfaces, defining the ways in which the connector may be
used by components to carry out interaction. For example, a client-server connector might have
invokes-services and provides-services roles. A pipe might have writer and reader roles. Like
component ports, connector roles differ from module interfaces in that they can be replicated, indicating
·
·
·- 0 A a
how many components can be involved in its interaction. A publish-subscribe connector might have
many instances of the publisher and subscriber roles.
Like components, complex connectors may in turn be decomposed into collections of components
and connectors that describe the architectural substructure of those connectors. Connectors need not be
binary. That is, they need not have exactly two roles. For example, a publish-subscribe connector might
have an arbitrary number of publisher and subscriber roles. Even if the connector is ultimately
implemented using binary connectors, such as a procedure call, it can be useful to adopt n-ary connector
representations in a C&C view. Connectors embody a protocol of interaction. When two or more
components interact, they must obey conventions about order of interactions, locus of control, and
handling of error conditions and timeouts. The protocol of interaction should be documented.
The primary relation within a C&C view is attachment. Attachments indicate which connectors are
attached to which components, thereby defining a system as a graph of components and connectors.
Specifically, an attachment is denoted by associating (attaching) a component's port to a connector's
role. A valid attachment is one in which the ports and roles are compatible with each other, under the
semantic constraints defined by the view. Compatibility often is defined in terms of information type
and protocol. For example, in a call-return architecture, you should check to make sure that all "calls"
ports are attached to some call-return connector. At a deeper semantic level, you should check to make
sure that a port's protocol is consistent with the behavior expected by the role to which it is attached.
An element (component or connector) of a C&C view will have various associated properties. Every
element should have a name and type. Additional properties depend on the type of component or
connector. Define values for the properties that support the intended analyses for the particular C&C
view. For example, if the view will be used for performance analysis, latencies, queue capacities, and
thread priorities may be necessary. The following are examples of some typical properties and their
uses:
• Reliability. What is the likelihood of failure for a given component or connector? This property
might be used to help determine overall system availability.
• Performance. What kinds of response time will the component provide under what loads? What
kind of bandwidth, latency, jitter, transaction volume, or throughput can be expected for a given
connector? This property can be used with others to determine system-wide properties such as
response times, throughput, and buffering needs.
• Resource requirements. What are the processing and storage needs of a component or a
connector? This property can be used to determine whether a proposed hardware configuration
will be adequate.
• Functionality. What functions does an element perform? This property can be used to reason
about overall computation performed by a system.
• Security. Does a component or a connector enforce or provide security features, such as
encryption, audit trails, or authentication? This property can be used to determine system
security vulnerabilities.
• Concurrency. Does this component execute as a separate process or thread? This property can
help to analyze or simulate the performance of concurrent components and identify possible
deadlocks.
·
·
·- 0 A a
• Modifiability. Does the messaging structure support a structure to cater for evolving data
exchanges? Can the components be adapted to process those new messages? This property can
be defined to extend the functionality of a component.
• Tier. For a tiered topology, what tier does the component reside in? This property helps to
define the build and deployment procedures, as well as platform requirements for each tier.
C&C views are commonly used to show to developers and other stakeholders how the system works
-one can "animate" or trace through a C&C view, showing an end-to-end thread of activity. C&C
views are also used to reason about runtime system quality attributes, such as performance and
availability. In particular, a well-documented view allows architects to predict overall system properties
such as latency or reliability, given estimates or measurements of properties of the individual elements
and their interactions.
Table 1 8.2 summarizes the elements, relations, and properties that can appear in C&C views. This
table is followed by a more detailed discussion of these concepts, together with guidelines concerning
their documentation.
Table 18.2. Summary of Component-and-Connector Views
E'lements • Components. Principal processing units and data stores. A component
has a set of ports through which It interacts with other compo·
nents (via connectors).
• Connectors. Pathways of interaction between components. Connec ..
tors have a set of roles (interfaces) that indicate how components
may use a connector in interactions.
Relations • Attachments. Component ports are associated with connector :roles
to yield a graph of components and connectors_
• Interface delegation I n some situations component ports are associ􀍔
ated with one or more ports in an "internal.
,
subarchitecture. The case
is similar for the roles of a connector.
Constraints • Components can only be attached to connectors, not directly to other
components,
• Connectors can only be attached to components, not directly to other
connectors.
• Attachments can only be made between compatible ports and roles.
• fnterface delegation can only be defined between two compatible
ports {or two compatible roles).
• Connectors cannot appear in isolalion; a connector must be attached
to a component.
Usage • Show how the system works.
• Guide development by spec-ifying structure and behavior of runtime
elements.
• Help reason about runtime system quality attributes, such as performance
and availability.
Notations for C&C Views
As always, box-and-line drawings are available to represent C&C views. Although informal notations
are limited in the semantics that can be conveyed, following some simple guidelines can lend rigor and
·
·
·- 0 A a
depth to the descriptions. The primary guideline is simple: assign each component type and each
connector type a separate visual form (symbol), and list each of the types in a key.
UML components are a good semantic match to C&C components because they permit intuitive
documentation of important information like interfaces, properties, and behavioral descriptions. UML
components also distinguish between component types and component instances, which is useful when
defining view-specific component types.
UML ports are a good semantic match to C&C ports. A UML port can be decorated with a
multiplicity, as shown in the left portion of Figure 1 8 . 1 , though this is typically only done on
component types. The number of ports on component instances, as shown in the right portion of Figure
1 8 . 1 , is typically bound to a specific number. Components that dynamically create and manage a set of
ports should retain a multiplicity descriptor on instance descriptions.
Serrver [1 . . 5] Server Server
<<Repositoryn []
Accou nt Da,uabase
Key: UML
J
Admin
: Acco u n t U
Database Admin
Figure 18.1. A UML representation of the ports on a C&C component type (left) and component
instance (right). The Account Database component type has two types of ports, Server and Admin
(noted by the boxes on the component's border). The Server port is defined with a multiplicity,
meaning that multiple instances of the port are permitted on any corresponding component
instance.
While C&C connectors are as semantically rich as C&C components, the same is not true of UML
connectors. UML connectors cannot have substructure, attributes, or behavioral descriptions. This
makes choosing how to represent C&C connectors more difficult, as UML connectors are not always
rich enough.
You should represent a "simple" C&C connector using a UML connector a line. Many commonly
used C&C connectors have well-known, application-independent semantics and implementations, such
as function calls or data read operations. If the only information you need to supply is the type of the
connector, then a UML connector is adequate. Call-return connectors can be represented by a UML
assembly connector, which links a component's required interface (socket) to the other component's
provided interface (lollipop). You can use a stereotype to denote the type of connector. If all connectors
in a primary presentation are of the same type, you can note this once in a comment rather than
explicitly on each connector to reduce visual clutter. Attachment is shown by connecting the endpoints
of the connector to the ports of components. Connector roles cannot be explicitly represented with a
UML connector because the UML connector element does not allow the inclusion of interfaces (unlike
the UML port, which does allow interfaces). The best approximation is to label the connector ends and
use these labels to identify role descriptions that must be documented elsewhere.
You should represent a "rich" C&C connector using a UML component, or by annotating a line
·
·
·- 0 A a
UML connector with a tag or other auxiliary documentation that explains the meaning of the complex
connector.
Allocation Views
Allocation views describe the mapping of software units to elements of an environment in which the
software is developed or in which it executes. The environment might be the hardware, the operating
environment in which the software is executed, the file systems supporting development or deployment,
or the development organization(s).
Table 1 8 . 3 summarizes the characteristics of allocation views. Allocation views consist of software
elements and environmental elements. Examples of environmental elements are a processor, a disk
farm, a file or folder, or a group of developers. The software elements come from a module or C&C
• vtew.
Table 18.3. Summary of the Characteristics of Allocation Views
Elements
Relations
Constraints
Usage
• Software element. A software element has properties that are
required of the environment.
• Environmental element. An environmental element has properties
that are provided to the software.
Allocated to. A software element is mapped (allocated to) an
environmental element. Properties are dependent on the particular
vrew.
Varies by view
• For reasoning about performance, availability, security, and safety.
11 For reasoning about distribut,ed development and allocation of
work to teams.
• For reasoning about concurrent access to software versions.
• For reasoning about the form and mechanisms of system
installa1ion .
The relation in an allocation view is allocated to. We usually talk about allocation views in terms of
a mapping from software elements to environmental elements, although the reverse mapping can also
be relevant and interesting. A single software element can be allocated to multiple environmental
elements, and multiple software elements can be allocated to a single environmental element. If these
allocations change over time, either during development or execution of the system, then the
architecture is said to be dynamic with respect to that allocation. For example, processes might migrate
from one processor or virtual machine to another. Similarly modules might migrate from one
development team to another.
Software elements and environmental elements have properties in allocation views. The usual goal
of an allocation view is to compare the properties required by the software element with the properties
provided by the environmental elements to determine whether the allocation will be successful or not.
For example, to ensure a component's required response time, it has to execute on (be allocated to) a
processor that provides sufficiently fast processing power. For another example, a computing platform
might not allow a task to use more than 1 0 kilobytes of virtual memory. An execution model of the
software element in question can be used to determine the required virtual memory usage. Similarly, if
·
·
·- 0 A a
you are migrating a module from one team to another, you might want to ensure that the new team has
the appropriate skills and background knowledge.
Allocation views can depict static or dynamic views. A static view depicts a fixed allocation of
resources in an environment. A dynamic view depicts the conditions and the triggers for which
allocation of resources changes according to loading. Some systems recruit and utilize new resources as
their load increases. An example is a load-balancing system in which new processes or threads are
created on another machine. In this view, the conditions under which the allocation changes, the
allocation of runtime software, and the dynamic allocation mechanism need to be documented. (Recall
from Chapter 1 that one of the allocation structures is the work assignment structure, which allocates
modules to teams for development. That relationship, too, can be allocated dynamically, depending on
"load" in this case, the load on development teams.)
Quality Views
Module, C&C, and allocation views are all structural views: They primarily show the structures that the
architect has engineered into the architecture to satisfy functional and quality attribute requirements.
These views are excellent for guiding and constraining downstream developers, whose primary job
it is to implement those structures. However, in systems in which certain quality attributes (or, for that
matter, some other kind of stakeholder concerns) are particularly important and pervasive, structural
views may not be the best way to present the architectural solution to those needs. The reason is that the
solution may be spread across multiple structures that are inconvenient to combine (for example,
because the element types shown in each are different).
Another kind of view, which we call a quality view, can be tailored for specific stakeholders or to
address specific concerns. These quality views are formed by extracting the relevant pieces of structural
views and packaging them together. Here are five examples:
• A security view can show all of the architectural measures taken to provide security. It would
show the components that have some security role or responsibility, how those components
communicate, any data repositories for security information, and repositories that are of
security interest. The view's context information would show other security measures (such as
physical security) in the system's environment. The behavior part of a security view would
show the operation of security protocols and where and how humans interact with the security
elements. It would also capture how the system would respond to specific threats and
vulnerabilities.
• A communications view might be especially helpful for systems that are globally dispersed and
heterogeneous. This view would show all of the component-to-component channels, the various
network channels, quality-of-service parameter values, and areas of concurrency. This view can
be used to analyze certain kinds of performance and reliability (such as deadlock or race
condition detection). The behavior part of this view could show (for example) how network
bandwidth is dynamically allocated.
• An exception or error-handling view could help illuminate and draw attention to error reporting
and resolution mechanisms. Such a view would show how components detect, report, and
resolve faults or errors. It would help identify the sources of errors and appropriate corrective
·
·
·- 0 A a
actions for each. Root-cause analysis in those cases could be facilitated by such a view.
• A reliability view would be one in which reliability mechanisms such as replication and
switchover are modeled. It would also depict timing issues and transaction integrity.
• A performance view would include those aspects of the architecture useful for inferring the
system's performance. Such a view might show network traffic models, maximum latencies for
operations, and so forth.
These and other quality views reflect the documentation philosophy of ISO/IEC/IEEE standard
420 1 0:20 1 1 , which prescribes creating views driven by stakeholder concerns about the architecture.
18.4. Choosing the Views
·
·
·- 0 A a
Documenting decisions during the design process (something we strongly recommend) produces views,
which are the heart of an architecture document. It is most likely that these views are rough sketches
more than fmished products ready for public release; this will give you the freedom to back up and
rethink design decisions that tum out to be problematic without having wasted time on broad
dissemination and cosmetic polish. They are documented purely as your own memory aid.
By the time you're ready to release an architecture document, you're likely to have a fairly wellworked-
out collection of architecture views. At some point you'll need to decide which to take to
completion, with how much detail, and which to include in a given release. You'll also need to decide
which views can be usefully combined with others, so as to reduce the total number of views in the
document and reveal important relations among the views.
You can determine which views are required, when to create them, and how much detail to include
if you know the following:
• What people, and with what skills, are available
• Which standards you have to comply with
• What budget is on hand
• What the schedule is
• What the information needs of the important stakeholders are
• What the driving quality attribute requirements are
• What the size of the system is
At a minimum, expect to have at least one module view, at least one C&C view, and for larger
systems, at least one allocation view in your architecture document. Beyond that basic rule of thumb,
however, there is a three-step method for choosing the views:
• Step 1. Build a stakeholder/view table. Enumerate the stakeholders for your project's
software architecture documentation down the rows. Be as comprehensive as you can. For the
columns, enumerate the views that apply to your system. (Use the structures discussed in
Chapter I, the views discussed in this chapter, and the views that your design work in ADD has
suggested as a starting list of candidates.) Some views (such as decomposition, uses, and work
assignment) apply to every system, while others (various C&C views, the layered view) only
apply to some systems. For the columns, make sure to include the views or view sketches you
already have as a result of your design work so far.
Once you have the rows and columns defined, fill in each cell to describe how much
information the stakeholder requires from the view: none, overview only, moderate detail, or
high detail. The candidate view list going into step 2 now consists of those views for which
some stakeholder has a vested interest.
• Step 2. Combine views. The candidate view list from step I is likely to yield an impractically
large number of views. This step will winnow the list to manageable size. Look for marginal
views in the table: those that require only an overview, or that serve very few stakeholders.
·
·
·- 0
Combine each marginal view with another view that has a stronger constituency.
A a
• Step 3. Prioritize and stage. After step 2 you should have the minimum set of views needed to
serve your stakeholder community. At this point you need to decide what to do first. What you
do first depends on your project, but here are some things to consider:
• The decomposition view (one of the module views) is a particularly helpful view to release
early. High-level (that is, broad and shallow) decompositions are often easy to design, and
with this information the project manager can start to staff development teams, put training
in place, determine which parts to outsource, and start producing budgets and schedules.
• Be aware that you don't have to satisfy all the information needs of all the stakeholders to
the fullest extent. Providing 80 percent of the information goes a long way, and this might
be good enough so that the stakeholders can do their job. Check with the stakeholder to see
if a subset of information would be sufficient. They typically prefer a product that is
delivered on time and within budget over getting the perfect documentation.
• You don't have to complete one view before starting another. People can make progress
with overview-level information, so a breadth-first approach is often the best.
18.5. Combining Views
·
·
·- 0 A a
The basic principle of documenting an architecture as a set of separate views brings a divide-andconquer
advantage to the task of documentation, but if the views were irrevocably different, with no
association with one another, nobody would be able to understand the system as a whole.
Because all views in an architecture are part of that same architecture and exist to achieve a
common purpose, many of them have strong associations with each other. Managing how architectural
structures are associated is an important part of the architect's job, independent of whether any
documentation of those structures exists.
Sometimes the most convenient way to show a strong association between two views is to collapse
them into a single combined view, as dictated by step 2 of the three-step method just presented to
choose the views. A combined view is a view that contains elements and relations that come from two
or more other views. Combined views can be very useful as long as you do not try to overload them
with too many mappings.
The easiest way to merge views is to create an overlay that combines the information that would
otherwise have been in two separate views. This works well if the coupling between the two views is
tight; that is, there are strong associations between elements in one view and elements in the other view.
If that is the case, the structure described by the combined view will be easier to understand than the
two views seen separately. For an example, see the overlay of decomposition and uses sketches shown
in Figure 1 8.2. In an overlay, the elements and the relations keep the types as defined in their
constituent views.
I
«subsystem»
adlsc
Server-Side Application Modules
'I I
controller business
·
·
·- 0
l _ _ 􀃒 _ 􀅐u􀅑e􀅒>- _ _ 􀂍 _ 􀅓􀅔e􀅕>- _
I
servlet
l r--.
I t-.1 ... __ ,
I
I
utils
I
I , -----.
"subsystem))
itc
<<USeS>> _
_
_ J
1 <<uses>> 􀁼 - ---- 􀁽 􀁽 􀁽 􀁽 􀁾 I I t-..-_----- ..1----. . __ ,
client facades
I
enUty
I
I
I
I
I
I
I
I
I
I
<<uses>> I 􀛛 - - - - .... 'i' I j<<usei>>.-1 - ----. 1 I<<Usas>- -�� 􀂎 􀃓􀃔􀃕--􀃖􀃗􀃘 I I
objects test
I
I l
webservice portal
«subsystem •>
tdc
I< subsystem�·
GCS
I
securfty
taglibs
I
common
Notation: UML
Figure 18.2. A decomposition view overlaid with "uses" information, to create a
decomposition/uses overlay.
The views below often combine naturally:
A a
• Various C& C views. Because C&C views all show runtime relations among components and
connectors of various types, they tend to combine well. Different (separate) C&C views tend to
show different parts of the system, or tend to show decomposition refinements of components
in other views. The result is often a set of views that can be combined easily.
• Deployment view with either SOA or communicating-processes views. An SOA view shows
services, and a communicating-processes view shows processes. In both cases, these are
components that are deployed onto processors. Thus there is a strong association between the
elements in these views.
·
·
·- 0 A a
• Decomposition view and any of work assignment, implementation, uses, or layered views. The
decomposed modules fonn the units of work, developtnent, and uses. In addition, these
modules populate layers.
18.6. Building the Documentation Package
·
·
·- 0 A a
Remember the principle of architecture documentation, with which we started this chapter. This
principle tells us that our task is to document the relevant views and to document the information that
applies to more than one view.
Documenting a View
Figure 1 8 . 3 shows a template for documenting a view.
Term plate for a View
Section 1. IP�rimary Presentation
Sm:tion 2'" Element Catalog
S ect1io n 2. A. E I e me nts and Their Pro perties
Section 2.8. Relations and Their Propert􀞑es
Section 2.C. Element Interfaces
Sectjon 2. D. Eh􀀡n1ent Behavior
Section 3. Context Diagram
'I D o - I
Section 4. Variability Guide
Section 5. Rationa'le
Figure 18.3. View template
No matter what the view, the documentation for a view can be placed into a standard organization
consisting of these parts:
• Section 1: The Primary Presentation. The primary presentation shows the eletnents and
relations of the view. The primary presentation should contain the information you wish to
convey about the system in the vocabulary of that view. It should certainly include the
primary elements and relations but under s01ne circumstances might not include all of them.
·
·
·- 0 A a
For example, you may wish to show the elements and relations that come into play during
normal operation but relegate error handling or exception processing to the supporting
documentation.
The primary presentation is most often graphical. It might be a diagram you've drawn in an
informal notation using a simple drawing tool, or it might be a diagram in a semiformal or
formal notation imported from a design or modeling tool that you're using. If your primary
presentation is graphical, make sure to include a key that explains the notation. Lack of a key is
the most common mistake that we see in documentation in practice.
Occasionally the primary presentation will be textual, such as a table or a list. If that text is
presented according to certain stylistic rules, these rules should be stated or incorporated by
reference, as the analog to the graphical notation key. Regardless of whether the primary
presentation is textual instead of graphical, its role is to present a terse summary of the most
important information in the view.
· Section 2: The Element Catalog. The element catalog details at least those elements depicted
in the primary presentation. For instance, if a diagram shows elements A, B, and C, then the
element catalog needs to explain what A, B, and C are. In addition, if elements or relations
relevant to this view were omitted from the primary presentation, they should be introduced and
explained in the catalog. Specific parts of the catalog include the following:
• Elements and their properties. This section names each element in the view and lists the
properties of that element. Each view introduced in Chapter 1 listed a set of suggested
properties associated with that view. For example, elements in a decomposition view might
have the property of "responsibility" an explanation of each module's role in the system
-and elements in a communicating-processes view 1night have timing parameters, among
other things, as properties. Whether the properties are generic to the view chosen or the
architect has introduced new ones, this is where they are documented and given values.
• Relations and their properties. Each view has specific relation types that it depicts among
the elements in that view. Mostly, these relations are shown in the primary presentation.
However, if the primary presentation does not show all the relations or if there are
exceptions to what is depicted in the primary presentation, this is the place to record that
information.
• Element interfaces. This section documents element interfaces.
• Element behavior. This section documents element behavior that is not obvious from the
primary presentation.
• Section 3 : Context Diagram. A context diagram shows how the system or portion of the
system depicted in this view relates to its environment. The purpose of a context diagram is to
depict the scope of a view. Here "context" means an environment with which the part of the
system interacts. Entities in the environment may be humans, other c01nputer systems, or
physical objects, such as sensors or controlled devices.
• Section 4: Variability Guide. A variability guide shows how to exercise any variation points
that are a part of the architecture shown in this view.
·
·
·- 0 A a
• Section 5: Rationale. Rationale explains why the design reflected in the view came to be. The
goal of this section is to explain why the design is as it is and to provide a convincing argument
that it is sound. The choice of a pattern in this view should be justified here by describing the
architectural probletn that the chosen pattern solves and the rationale for choosing it over
another.
Documenting Information Beyond Views
As shown in Figure 1 8 .4, documentation beyond views can be divided into two parts:
1 . Overview of the architecture documentation. This tells how the documentation is laid out and
organized so that a stakeholder of the architecture can find the information he or she needs
efficiently and reliably.
2. Information about the architecture. Here, the information that remains to be captured beyond
the views themselves is a short system overview to ground any reader as to the purpose of the
system and the way the views are related to one another, an overview of and rationale behind
system-wide design approaches, a list of elements and where they appear, and a glossary and
an acronym list for the entire architecture.
Architecture
docum entat.i on
infonnatlo·n
Architecture
info nn at ion
·�
Template for Documentation
Beyond Views
Section 1. Documentation Roadmap
Section 2 . How a View Is Documented
Section 3. System Overview
Section 4. Mapping Between Views
Section !5. Rationale
.. Section 6. Directory - index, g lossary,
acronym list
Figure 18.4. Summary of documentation beyond views
Figure 1 8 .4 summarizes our template for documentation beyond views. Documentation beyond
views consists of the following sections:
• Document control information. List the issuing organization, the current version number, date
of issue and status, a change history, and the procedure for submitting change requests to the
document. Usually this is captured in the front matter. Change control tools can provide much
of this information.
• Section 1: Documentation Roadmap. The documentation roadmap tells the reader what
information is in the documentation and where to find it. A documentation map consists of four
sections:
·
·
·- 0 A a
• Scope and summary. Explain the purpose of the document and briefly summarize what is
covered and (if you think it will help) what is not covered. Explain the relation to other
documents (such as downstream design documents or upstream system engineering
documents).
• How the documentation is organized. For each section in the documentation, give a short
synopsis of the information that can be found there. An alternative to this is to use an
annotated table of contents. This is a table that doesn't just list section titles and page
numbers, but also gives a synopsis with each entry. It provides one-stop shopping for a
reader attempting to look up a particular kind of information.
• View overview. The major part of the map describes the views that the architect has
included in the package. For each view, the map gives the following information:
• The name of the view and what pattern it instantiates, if any.
• A description of the view's element types, relation types, and property types. This lets
a reader begin to understand the kind of information that is presented in the view.
• A description of language, modeling techniques, or analytical methods used in
constructing the view.
• How stakeholders can use the documentation. The map follows with a section describing
which stakeholders and concerns are addressed by each view; this is conveniently captured
as a table. This section shows how various stakeholders might use the documentation to
help address their concerns. Include short scenarios, such as "A maintainer wishes to know
the units of software that are likely to be changed by a proposed modification. The
maintainer consults the decomposition view to understand the responsibilities of each
module in order to identify the modules likely to change. The maintainer then consults the
uses viewl to see what modules use the affected modules (and thus might also have to
change)." To be compliant with ISO/IEC 420 1 0-2007, you must consider the concerns of at
least users, acquirers, developers, and maintainers.
1. The uses view is a module view. It shows the uses structure discussed in Chapter 1 .
• Section 2 : How a View Is Documented. This is where you explain the standard organization
you're using to document views either the one described in this chapter or one of your own. It
tells your readers how to find information in a view. If your organization has standardized on a
template for a view, as it should, then you can simply refer to that standard. If you are lacking
such a template, then text such as that given above describing our view template should appear
in this section of your architecture documentation.
• Section 3 : System Overview. This is a short prose description of the system's function, its
users, and any important background or constraints. This section provides your readers with a
consistent mental model of the system and its purpose. This might be just a pointer to a
concept-of-operations document.
• Section 4: Mapping Between Views. Because all the views of an architecture describe the
same system, it stands to reason that any two views will have much in common. Helping a
reader understand the associations between views will help that reader gain a powerful insight
·
·
·-
into how the architecture works as a unified conceptual whole.
0 A a
The associations between elements across views in an architecture are, in general, many-tomany.
For instance, each module may map to multiple runtime elements, and each runtime
element may map to multiple modules.
View-to-view associations can be conveniently captured as tables. List the elements of the
first view in some convenient lookup order. The table itself should be annotated or introduced
with an explanation of the association that it depicts; that is, what the correspondence is
between the elements across the two views. Examples include "is implemented by" for
mapping from a component-and-connector view to a module view, "implements" for mapping
from a module view to a component-and-connector view, "included in" for mapping from a
decomposition view to a layered view, and many others.
• Section 5: Rationale. This section documents the architectural decisions that apply to more
than one view. Prime candidates include documentation of background or organizational
constraints or major requirements that led to decisions of system-wide import. The decisions
about which fundamental architecture patterns to use are often described here.
• Section 6: Directory. The directory is a set of reference material that helps readers find more
information quickly. It includes an index of terms, a glossary, and an acronym list.
Online Documentation, Hypertext, and Wikis
A document can be structured as linked web pages. Compared with documents written with a textediting
tool, web-oriented documents typically consist of short pages (created to fit on one screen) with
a deeper structure. One page usually provides some overview information and has links to more
detailed information. When done well, a web-based document is easier to use for people who just need
overview information. On the other hand, it can become more difficult for people who need detail.
Finding information can be more difficult in multi-page, web-based documents than in a single-file,
text-based document, unless a search engine is available.
Using readily available tools, it's possible to create a shared document that many stakeholders can
contribute to. The hosting organization needs to decide what permissions it wants to give to various
stakeholders; the tool used has to support the permissions policy. In the case of architecture
documentation, we would want all stakeholders to comment on and add clarifying information to the
architecture, but we would only want architects to be able to change the architecture or at least provide
architects with a "fmal approval" mechanism. A special kind of shared document that is ideal for this
purpose is a wiki.
Follow a Release Strategy
Your project's development plan should specify the process for keeping the important documentation,
including architecture documentation, current. The architect should plan to issue releases of the
documentation to support major project milestones, which usually means far enough ahead of the
milestone to give developers time to put the architecture to work. For example, the end of each iteration
or sprint or incremental release could be associated with providing revised documentation to the
development team.
Documenting Patterns
·
·
·- 0 A a
Architects can, and typically do, use patterns as a starting point for their design, as we have discussed in
Chapter 1 3 . These patterns might be published in existing catalogs or in an organization's proprietary
repository of standard designs, or created specifically for the problem at hand by the architect. In each
of these cases, they provide a generic (that is, incomplete) solution approach that the architect will have
to refine and instantiate.
First, record the fact that the given pattern is being used. Then say why this solution approach was
chosen why it is a good fit to the problem at hand. If the chosen approach comes from a pattern, this
will consist essentially of showing that the problem at hand fits the problem and context of the pattern.
Using a pattern means making successive design decisions that eventually result in an architecture.
These design decisions manifest themselves as newly instantiated elements and relations among them.
The architect can document a snapshot of the architecture at each stage. How many stages there are
depends on many things, not the least of which is the ability of readers to follow the design process in
case they have to revisit it in the future.
18.7. Documenting Behavior
·
·
·- 0 A a
Documenting an architecture requires behavior documentation that complements structural views by
describing how architecture elements interact with each other. Reasoning about characteristics such as a
system's potential to deadlock, a system's ability to complete a task in the desired amount of time, or
maximum memory consumption requires that the architecture description contain information about
both the characteristics of individual elements as well as patterns of interaction among them that is,
how they behave with each other. In this section, we provide guidance as to what types of things you
will want to document in order to reap these benefits. In our architecture view template, behavior has its
own section in the element catalog.
There are two kinds of notations available for documenting behavior. The first kind of notation is
called trace-oriented languages; the second is called comprehensive languages.
Traces are sequences of activities or interactions that describe the system's response to a specific
stimulus when the system is in a specific state. A trace describes a sequence of activities or interactions
between structural elements of the system. Although it is conceivable to describe all possible traces to
generate the equivalent of a comprehensive behavioral model, it is not the intention of trace-oriented
documentation to do so. Below we describe four notations for documenting traces: use cases, sequence
diagrams, c01nmunication diagrams, and activity diagrams. Although other notations are available (such
as message sequence charts, timing diagrams, and the Business Process Execution Language), we have
chosen these four as a representative sample of trace-oriented languages.
• Use cases describe how actors can use a system to accomplish their goals. Use cases are
frequently used to capture the functional requirements for a system. UML provides a graphical
notation for use case diagrams but does not say how the text of a use case should be written.
The UML use case diagram can be used effectively as an overview of the actors and the
behavior of a system. The use case description is textual and should contain the use case name
and brief description, the actor or actors who initiate the use case (primary actors), other actors
who participate in the use case (secondary actors), flow of events, alternative flows, and
nonsuccess cases.
• A UML sequence diagram shows a sequence of interactions among instances of elements
pulled from the structural documentation. It shows only the instances participating in the
scenario being documented. A sequence diagram has two dimensions: vertical, representing
time, and horizontal, representing the various instances. The interactions are arranged in time
sequence from top to bottom. Figure 1 8 . 5 is an example of a sequence diagram that illustrates
the basic UML notation.
·
·
·- 0
:User I
I
I
I
I
I
log-in
:Logi n
Page
:Lo·gin
Controller
:UserDao
!logi n{ . . . l I checkPwcd{. .. ) 1
<E:---------
new
........
:User
1---- ---f----- -...,..... Session
􀂋--------J----------􀂌 rebister User Login(􀃑 .. )
I fE:--------...... <!(- --- - - - - 􀂶 I
I 1 I
Key ( U M L)
Actor
---Ill� Synchronous
message
Object
-----=>�- Asynchronous
message
I LifeHne
Exec1Jtion
occurrence
_ _ _ ;::> Return
messag·e
:Logger
I
I
I
I
I
_ I
Figure 18.5. A simple example of a UML sequence diagram
A a
Objects (i.e., element instances) have a lifeline, drawn as a vertical dashed line along the
time axis. The sequence is usually started by an actor on the far left. The instances interact by
sending messages, which are shown as horizontal arrows. A message can be a method or
function call, an event sent through a queue, or something else. The message usually maps to a
resource (operation) in the interface of the receiver instance. A filled arrowhead on a solid line
represents a synchronous message, whereas the open arrowhead represents an asynchronous
message. The dashed arrow is a return message. The execution occurrence bars along the
lifeline indicate that the instance is processing or blocked waiting for a return.
• A UML communication diagram shows a graph of interacting elements and annotates each
interaction with a number denoting order. Similarly to sequence diagrams, instances shown in a
communication diagram are elements described in the accompanying structural documentation.
Communication diagrams are useful when the task is to verify that an architecture can fulfill the
functional requirements. The diagrams are not useful if the understanding of concurrent actions
is important, as when conducting a performance analysis.
• UML activity diagrams are similar to flow charts. They show a business process as a sequence
of steps (called actions) and include notation to express conditional branching and concurrency,
as well as to show sending and receiving events. Arrows between actions indicate the flow of
·
·
·- 0 A a
control. Optionally, activity diagrams can indicate the architecture element or actor performing
the actions. Activity diagrams can express concurrency. A fork node (depicted as a thick bar
orthogonal to the flow arrows) splits the flow into two or more concurrent flows of actions. The
concurrent flows may later be synchronized into a single flow through a join node (also
depicted as an orthogonal bar). The join node waits for all incoming flows to complete before
proceeding. Different from sequence and communication diagrams, activity diagrams don't
show the actual operations being perfonned on specific objects. Activity diagrams are useful to
broadly describe the steps in a specific workflow. Conditional branching (diamond symbol)
allows a single diagram to represent multiple traces, although it's not usually the intent of an
activity diagram to show all possible traces or the complete behavior for the system or part of it.
In contrast to trace notations, comprehensive models show the complete behavior of structural
elements. Given this type of documentation, it is possible to infer all possible paths from initial state to
final state. The state machine formalism represents the behavior of architecture elements because each
state is an abstraction of all possible histories that could lead to that state. State machine languages
allow you to complement a structural description of the elements of the system with constraints on
interactions and timed reactions to both internal and environmental stimuli.
UML state machine diagram notation is based on the statechart graphical formalism developed by
David Harel for modeling reactive systems; it allows you to trace the behavior of your system, given
specific inputs. A UML state machine diagram shows states represented as boxes and transitions
between states represented as arrows. The state machine diagrams help to model elements of the
architecture and help to illustrate their runtime interactions. Figure 1 8.6 is a simple example showing
the states of a vehicle cruise control system.
IJ press "'cruise press "set" or
􀛙 on/off" button ...,!' "' "resume" buttons .r
􀛚
off
... on ,
disengaged
- -
'- ..1 - press "cruise '- ..1 tap brake pedaJ '
J ' on/off" button
press " cruise on/off" button
press "+"
to accelerate
,I .....
on,
engaged
.-
.I '
II\
'- .I
press "-"
to coast
' )
push
ttlrottle
ped.al
Ke y: UML
Figure 18.6. UML state machine diagram for the cruise control system of a motor vehicle
Each transition in a state machine diagram is labeled with the event causing the transition. For
example, in Figure 1 8 .6, the transitions correspond to the buttons the driver can press or driving actions
that affect the cruise control system. Optionally, the transition can specify a guard condition, which is
enclosed in brackets. When the event corresponding to the transition occurs, the guard condition is
evaluated and the transition is only enabled if the guard is true at that time. Transitions can also have
consequences, called actions or effects, indicated by a slash. When an action is noted, it indicates that
the behavior following the slash will be performed when the transition occurs. The states may also
specify entry and exit actions.
·
·
·- 0 A a
Other notations exist for describing comprehensive behavior. For example, Architecture Analysis
and Design Language (AADL) can be used to reason about runtime behavior. Specification and
Description Language (SDL) is used in telephony.
·
·
·-
18.8. Architecture Documentation and Quality Attributes
0 A a
If architecture is largely about the achievement of quality attributes and if one of the main uses of
architecture documentation is to serve as a basis for analysis (to make sure the architecture will achieve
its required quality attributes), where do quality attributes show up in the documentation? Short of a
full-fledged quality view (see page 340), there are five major ways:
1 . Any major design approach (such as an architecture pattern) will have quality attribute
properties associated with it. Client-server is good for scalability, layering is good for
portability, an information-hiding-based decomposition is good for modifiability, services are
good for interoperability, and so forth. Explaining the choice of approach is likely to include
a discussion about the satisfaction of quality attribute requirements and tradeoffs incurred.
Look for the place in the documentation where such an explanation occurs. In our approach,
we call that rationale.
2. Individual architectural elements that provide a service often have quality attribute bounds
assigned to them. Consumers of the services need to know how fast, secure, or reliable those
services are. These quality attribute bounds are defined in the interface documentation for the
elements, sometimes in the form of a service-level agreement. Or they may simply be
recorded as properties that the elements exhibit.
3. Quality attributes often impart a "language" of things that you would look for. Security
involves security levels, authenticated users, audit trails, firewalls, and the like. Performance
brings to mind buffer capacities, deadlines, periods, event rates and distributions, clocks and
timers, and so on. Availability conjures up 1nean time between failure, fail over 1nechanisms,
primary and secondary functionality, critical and noncritical processes, and redundant
elements. Someone fluent in the "language" of a quality attribute can search for the kinds of
architectural elements (and properties of those elements) that were put in place precisely to
satisfy that quality attribute requirement.
4. Architecture documentation often contains a mapping to requirements that shows how
requirements (including quality attribute requirements) are satisfied. If your requirements
document establishes a requirement for availability, for instance, then you should be able to
look it up by name or reference in your architecture document to see the places where that
requirement is satisfied.
5. Every quality attribute requirement will have a constituency of stakeholders who want to
know that it is going to be satisfied. For these stakeholders, the architect should provide a
special place in the documentation's introduction that either provides what the stakeholder is
looking for, or tells the stakeholder where in the document to fmd it. It would say something
like this: ''If you are a performance analyst, you should pay attention to the processes and
threads and their properties (defined [here]), and their deployment on the underlying
hardware platform (defined [here])." In our documentation approach, we put this here's-whatyou're-
looking-for information in a section called the documentation roadmap.
·
·
·- 0 A a
18.9. Documenting Architectures That Change Faster Than You Can Document
Them
When your web browser encounters a file type it's never seen before, odds are that it will go to the
Internet, search for and download the appropriate plug-in to handle the file, install it, and reconfigure
itself to use it. Without even needing to shut down, let alone go through the code-integrate-test
development cycle, the browser is able to change its own architecture by adding a new component.
Service-oriented systems that utilize dynamic service discovery and binding also exhibit these
properties. More challenging systems that are highly dynamic, self-organizing, and reflective (meaning
self-aware) already exist. In these cases, the identities of the components interacting with each other
cannot be pinned down, let alone their interactions, in any static architecture document.
Another kind of architectural dynamism, equally challenging from a documentation perspective, is
found in systems that are rebuilt and redeployed with great rapidity. Some development shops, such as
those responsible for commercial websites, build and "go live" with their system many times every day.
Whether an architecture changes at runtime, or as a result of a high-frequency release-and-deploy
cycle, the changes occur much faster than the documentation cycle. In either case, nobody is going to
hold up things until a new architecture document is produced, reviewed, and released.
But knowing the architecture of these systems is every bit as important, and arguably more so, than
for systems in the world of more traditional life cycles. Here's what you can do if you're an architect in
a highly dynamic environment:
• Document what is true about all versions of your system. Your web browser doesn't go out and
grab just any piece of software when it needs a new plug-in; a plug-in must have specific
properties and a specific interface. And it doesn't just plug in anywhere, but in a predetermined
location in the architecture. Record those invariants as you would for any architecture. This
may make your documented architecture more a description of constraints or guidelines that
any compliant version of the system must follow. That's fine.
• Document the ways the architecture is allowed to change. In the previous examples, this will
usually mean adding new components and replacing components with new implementations. In
the Views and Beyond approach, the place to do this is called the variability guide (captured in
Section 4 of our view template).
·
·
·- 0
18.10. Documenting Architecture in an Agile Development Project
A a
"Agile" refers to an approach to software development that emphasizes rapid and flexible development
and de-emphasizes project and process infrastructure for their own sake. In Chapter 1 5 we discuss the
relationships between architecture and Agile. Here we focus just on how to document architecture in an
Agile environment.
The Views and Beyond and Agile philosophies agree strongly on a central point: If information isn't
needed, don't document it. All documentation should have an intended use and audience in mind, and
be produced in a way that serves both. One of the fundamental principles of technical documentation is
"Write for the reader." That means understanding who will read the documentation and how they will
use it. If there is no audience, there is no need to produce the documentation.
Architecture view selection is an example of applying this principle. The Views and Beyond
approach prescribes producing a view if and only if it addresses the concerns of an explicitly identified
stakeholder community.
Another central idea to remember is that documentation is not a monolithic activity that holds up all
other progress until it is complete. The view selection method given earlier prescribes producing the
documentation in prioritized stages to satisfy the needs of the stakeholders who need it now.
When producing Views and Beyond-based architecture documentation using Agile principles, keep
the following in mind:
• Adopt a template or standard organization to capture your design decisions.
• Plan to document a view if (but only if) it has a strongly identified stakeholder constituency.
• Fill in the sections of the template for a view, and for information beyond views, when (and in
whatever order) the information becomes available. But only do this if writing down this
information will make it easier (or cheaper or make success more likely) for someone
downstream doing their job.
• Don't worry about creating an architectural design document and then a finer-grained design
document. Produce just enough design information to allow you to move on to code. Capture
the design information in a format that is simple to use and simple to change a wiki, perhaps.
• Don't feel obliged to fill up all sections of the template, and certainly not all at once. We still
suggest you define and use rich templates because they may be useful in some situations. But
you can always write "N/ A" for the sections for which you don't need to record the information
(perhaps because you will convey it orally).
• Agile teams sometimes make models in brief discussions by the whiteboard. When
documenting a view, the primary presentation may consist of a digital picture of the
white board. Further information about the elements (element catalog), rationale discussion
(architecture background), variability mechanisms being used (variability guide), and all else
can be communicated verbally to the team at least for now. Later on, if you find out that it's
useful to record a piece of information about an element, a context diagram, rationale for a
certain design decision, or something else, the template will have the right place ready to
receive it.
·
·
·- 0
The Software You're Delivering Isn't the Only Software That Matters
About ninety-nine percent of the treatment of architecture in this book (and others) is
concerned with the software elements that make up the operational system that is
delivered to its customer. Component-and-connector views show the units of runtime
behavior of that system. Module views show the units of implementation that have to
be built in order to create that system.
A a
A colleague of mine is a project manager for a Fortune 500 software company. On
the day I wrote this sidebar, she found out that the development platform her project
relied on had been infected with a virulent new virus, and the company's IT department
was removing it from service, along with all the backup images, until the virus could be
completely removed. That was going to take about five days. After that, all of her
project's software and tooling would have to be reinstalled and brought back up to
latest-version status. Her project was in user final acceptance test, racing against a
delivery deadline, and the IT department's decision doomed her project to join the
countless others in our industry that are delivered late. The snarling email she sent to
the IT department for (a) allowing the platform to become infected and (b) not
providing a backup platform (real or virtual) in a timely fashion would melt your
screen.
The treatment of software architecture we describe in this book is perfectly capable
of representing and usefully incorporating software other than the software that your
cust01ner is paying you to deliver. Allocation views, recall, are about mapping that
software to structures in the environment. "Uses" views show which software elements
rely on the correct presence of other software in order to work. Context diagrams are all
about showing relations between your system and important elements of its
environment. It would be the easiest thing in the world to use these constructs to
represent support software including, in my friend's case, the development platform.
An avionics project I worked on years ago included in our decomposition view a
module called the System Generation Module. This consisted of all of the software we
needed to construct a loadable image of the product we were building. Not a single byte
of code from the System Generation Module made it onto the aircraft, but it was as
important as any other. Even if you don't build any of your support software but use
off-the-shelf development tools from your favorite vendor, someone in your
organization is responsible for the care and feeding of that software: its acquisition,
installation, configuration, and upgrade. That constitutes a nontrivial work assignment,
which suggests that support software also belongs in the work assignment view (a kind
of allocation view). And of course you always build some of it yourself test scripts,
build scripts, and so forth so it's even more deserving of a place in your architecture.
Promoting support and development software to first-class architectural status
makes us ask the right questions about it, especially the most important one: What
·
·
·- 0 A a
quality attributes do we require of it? Will it provide us with the right security if (for
example) we want to exclude our subcontracting partners from access to some of our IP
during development? Will it have the availability to be up and running at 2 a.m. Sunday
morning when our project goes into its inevitable final delivery crunch? And if it
crashes, will the IT folks have someone standing by to bring it back up? Will it be
modifiable or configurable enough to support the way your project intends to use it?
Think about what other software and environmental resources your project depends
on, and consider using the architectural tools, models, views, and concepts at your
disposal to help you do what architecture always helps you do: Ask the right questions
at the right time to expose risks and begin to mitigate them. These concepts include
quality attribute scenarios, "uses" views, and deployment and work assignment views
that include support software.
-PCC
18.1 1. Summary
·
·
·- 0 A a
Writing architectural documentation is much like other types of writing. You must understand the uses
to which the writing is to be put and the audience for the writing. Architectural documentation serves as
a means for communication among various stakeholders, not only up the management chain and down
to the developers but also across to peers.
An architecture is a complicated artifact, best expressed by focusing on particular perspectives
depending on the message to be communicated. These perspectives are called views, and you must
choose the views to document, must choose the notation to document these views, and must choose a
set of views that is both minimal and adequate. This may involve combining various views that have a
large overlap. You must document not only the structure of the architecture but also the behavior.
Once you have decided on the views, you must decide how to package the documentation. The
packaging will depend on the media used for expressing the documentation. Print has different
characteristics for understanding and grouping than various online media. Different online media will
also have different characteristics.
The context of the project will also affect the documentation. Some of the contextual factors are the
important quality attributes of the system, the rate of change of the system, and the project management
strategy.
18.12. For Further Reading
·
·
·- 0 A a
Documenting Software Architectures (second edition) [Clements lOa] is a comprehensive treatment of
the Views and Beyond approach. It describes a multitude of different views and notations for them. It
also describes how to package the documentation into a coherent whole.
ISO/IECIIEEE 42010:20 1 1 ("eye-so-forty-two-ten" for short) is the ISO (and IEEE) standard [ISO
ill Systems and software engineering-Architecture description. The first edition of that standard,
IEEE Std. 1471 -2000, was developed by an IEEE working group drawing on experience from industry,
academia, and other standards bodies between 1 995 and 2000. ISO/IEC/IEEE 420 1 0 is centered on two
key ideas: a conceptual framework for architecture description and a statement of what information
must be found in any ISO/IEC/IEEE 4201 0-compliant architecture description, using multiple
viewpoints driven by stakeholders' concerns.
Under ISO/IEC/IEEE 42010, as in the Views and Beyond approach, views have a central role in
documenting software architecture. The architecture description of a system includes one or more
• vtews.
If you want to use the Views and Beyond approach to produce an ISO/IECIIEEE 4201 0-compliant
architecture document, you certainly can. The main additional obligation is to choose and document a
set of viewpoints, identifying the stakeholders, their concerns, and the elements catalog for each view,
and (to a lesser degree) address ISO/IEC/IEEE 420 1 0 ' s other required information content.
AADL is an SAE standard. The SAE is an organization for engineering professionals in the
aerospace, automotive, and commercial vehicle industries. The website for the AADL standard is at
www.aadl.info.
SDL is a notation used in the telecom industry. It is targeted at describing the behavior of reactive
and distributed systems in general and telecom systems in particular. A real-time version of SDL can be
found at www.sdl-rt.org/standard/V2.2/pdf/SDL-RT.pdf.
UML 2.0 added several features specifically to allow architecture to be modeled, such as ports. It is
managed by the Object Management Group and can be found at www.omg.org/spec/UML/.
18.13. Discussion Questions
·
·
·- 0 A a
1 . Go to the website of your favorite open source system. On the site, look for the architectural
documentation for that system. What is there? What is missing? How would this affect your
ability to contribute code to this project?
2. Banks are justifiably cautious about security. Sketch the documentation you would need for an
automatic teller machine (ATM) in order to reason about its security architecture.
3. Suppose your company has just purchased another company and that you have been given the
task of merging a system in your company with a similar system in the other company. What
views of the other system's architecture would you like to see and why? Would you ask for the
same views of both systems?
4. When would you choose to document behavior using trace models or using comprehensive
models? What value do you get and what effort is required for each of them?
5. How much of a project's budget would you devote to software architecture documentation?
Why? How would you measure the cost and the benefit?
6. Antony Tang, an architect and one of the reviewers of this book, says that he has used a
development view a kind of quality view that describes how the software should be developed
in relation to the use of tools and development workflows, the use of standard library routines
such as for exception handling, some coding conventions and standards, and some testing and
deployment conventions. Sketch a definition of a development view.
·
·
·-
19. Architecture, Implementation, and Testing
You don 't make progress by standing on the
sidelines, whimpering and complaining. You
make progress by implementing ideas.
-Shirley Hufstedler
0 A a
Although this is a book about software architecture you've noticed that by now, no doubt we need
to remind ourselves from time to time that architecture is not a goal unto itself, but only the means to an
end. Building systems from the architecture is the end game, systems that have the qualities necessary
to meet the concerns of their stakeholders.
This chapter covers two critical areas in system-building implementation and testing from the
point of view of architecture. What is the relationship of architecture to implementation (and vice
versa)? What is the relationship of architecture to testing (and vice versa)?
19.1. Architecture and Implementation
·
·
·- 0 A a
Architecture is intended to serve as the blueprint for implementation. The sidebar "Potayto, Potahto . .
􀗕" makes the point that architectures and implementations rely on different sets of vocabulary, which
results in development tools usually serving one community or the other fairly well, but not both.
Frequently the implementers are so engrossed in their immediate task at hand that they make
implementation choices that degrade the modular structure of the architecture, for example.
This leads to one of the most frustrating situations for architects. It is very easy for code and its
intended architecture to drift apart; this is sometimes called "architecture erosion." This section talks
about four techniques to help keep the code and the architecture consistent.
Embedding the Design in the Code
A key task for implementers is to faithfully execute the prescriptions of the architecture. George
Fairbanks, in Just Enough Architecture, prescribes using an "architecturally-evident coding style."
Throughout the code, implementers can document the architectural concept or guidance that they're
reifying. That is, they can "embed" the architecture in their implementations. They can also try to
localize the implementation of each architectural element, as opposed to scattering it across different
implementation entities.
This practice is made easier if implementers (consistently across a project) adopt a set of
conventions for how architectural concepts "show up" in code. For example, identifying the layer to
which a code unit belongs will make it more likely that implementers and maintainers will respect (and
hence not violate) the layering.
Frameworks
"Framework" is a terribly overused term, but here we mean a reusable set of libraries or classes for a
software system. "Library" and "class" are implementation-like terms, but frameworks have broad
architectural implications they are a place where architecture and implementation meet. The classes
(in an object-oriented framework) are appropriate to the application domain of the system that is being
constructed. Frameworks can range from small and straightforward (such as ones that provide a set of
standard and commonly used data types to a system) to large and sophisticated. For example, the
AUTomotive Open System ARchitecture (AUTOSAR) is a framework for automotive software, jointly
developed by automobile manufacturers, suppliers, and tool developers.
Frameworks that are large and sophisticated often encode architectural interaction mechanisms, by
encoding how the classes (and the objects derived from them) communicate and synchronize with each
other. For example, AUTOSAR is an architecture and not (just) an architecture framework.
A framework amounts to a substantial (in some cases, enormous) piece of reusable software, and it
brings with it all of the advantages of reuse: saving time and cost, avoiding a costly design task,
encoding domain knowledge, and decreasing the chance of errors from individual implementers coding
the same thing differently and erroneously. On the other hand, frameworks are difficult to design and
get correct. Adopting a framework means investing in a selection process as well as training, and the
framework may not provide all the functionality that you require. The learning curve for a framework is
·
·
·- 0 A a
often extremely steep. A framework that provides a complete set of functionality for implementing an
application in a particular domain is called a "platform."
Code Templates
A template provides a structure within which some architecture-specific functionality is achieved, in a
consistent fashion system-wide. Many code generators, such as user interface builders, produce a
template into which a developer inserts code, although templates can also be provided by the
development environment.
Suppose that an architecture for a high-availability system prescribes that every component that
implements a critical responsibility must use a failover technique that switches control to a backup copy
of itself in case a fault is detected in its operation.
The architecture could, and no doubt would, describe the failover protocol. It might go something
like this:
In the event that a failure is detected in a critical-application component, a switch over occurs as
follows:
1 . A secondary copy, executing in parallel in background on a different processor, is promoted
to the new primary.
2. The new primary reconstitutes with the application's clients by sending them a message that
means, essentially: The operational unit that was serving you has had a failure. Were you
waiting for anything from us at the time? It then proceeds to service any requests received in
response.
3. A new secondary is started to serve as a backup for the new primary.
4. The newly started secondary announces itself to the new primary, which starts sending it
messages as appropriate to keep it up to date while it is executing in background.
Iff ailure is detected within a secondary, a new one is started on some other processor. It
coordinates with its primary and starts receiving state data.
Even though the primary and secondary copies are never doing the same thing at the same time (the
primary is performing its duty and sending state updates to its backups, and the secondaries are waiting
to leap into action and accepting state updates), both components come from identical copies of the
same source code.
To accomplish this, the coders of each critical component would be expected to implement that
protocol. However, a cleverer way is to give the coder a code template that contains the tricky failover
part as boilerplate and contains fill-in-the-blank sections where coders can fill in the implementation for
the functionality that is unique to each application. This template could be embedded in the
development environment so that when the developer specifies that the module being developed is to
support a fail over protocol, the template appears as the initial code for the module.
An example of such a template, taken frmn an air traffic control system, is illustrated in Figure 1 9 . 1 .
The structure is a continuous loop that services incoming events. If the event is one that causes the
application to take a normal (non-fault-tolerance-related) action, it carries out the appropriate action,
·
·
·- 0 A a
followed by an update of its backup counterparts' data so that the counterpart can take over if
necessary. Most applications spend most of their time processing normal events. Other events that may
be received involve the transfer (transmission and reception) of state and data updates. Finally, there is
a set of events that involves both the announcement that this unit has become the primary and requests
from clients for services that the former (now failed) primary did not complete.
terminate : = f a l s e
i n i t i a l i z e app l i cation/application protoco l s
a s k for current s tate ( image reque s t )
Loop
Get event
C a s e Event
_
Type i s
"norma l " ( n o n - f a u l t - t o l erant-related) requ e s t s to
- - p e r form a c t i ons ; only happens i f t h i s unit i s the
- - current primary addre s s space
when X => P r o c e s s X
Send state data updates to other addre s s spaces
when Y => P r o c e s s Y
Send state data updates to other addre s s spaces
• • •
when Terminate
_
Directive => c l ean up resource s ; terminate
: = true
when State
_
Data
_
Update => app l y to state data
w i l l o n l y happen i f t h i s unit i s a s e condary addre s s
s p a c e , receiving the update from the primary after i t
has completed a " no rma l " action sending, receiving
state data
when Image
_
Request => send current state data t o new
addr e s s space
when State
_
Data
_
Image => I n i t i a l i z e s t a t e data
when Switch Di rective => n o t i f y s e rvice packages o f
change in rank
these are reque s t s that come in a f t e r a PAS/ SAS
switchover; they report s e r v i c e s that they had
requested from the o l d ( fa i led) PAS which this unit
(now the PAS ) must complete . A, B , etc . are the names
o f the clients .
when Recon from A => reconstitute A
- -
when Recon from B => reconstitute B
• • •
when othe r s => l o g e r r o r
end c a s e
exit when terminate
end loop
Figure 19.1. A code template for a failover protocol. ''Process X" and "Process Y" are
placeholders for application-specific code.
Using a template has architectural implications: it makes it simple to add new applications to the
system with a minimum of concern for the actual workings of the fault-tolerant mechanisms designed
·
·
·- 0 A a
into the approach. Coders and maintainers of applications do not need to know about message-handling
mechanisms except abstractly, and they do not need to ensure that their applications are fault tolerantthat
has been handled architecturally.
Code templates have implications for reliability: once the template is debugged, then entire classes
of coding errors across the entire system disappear. But in the context of this discussion, templates
represent a true common ground where the architecture and the implementation come together in a
consistent and useful fashion.
Keeping Code and Architecture Consistent
Code can drift away from architecture in a depressingly large number of ways. First, there may be no
constraints imposed on the coders to follow the architecture. This makes no apparent sense, for why
would we bother to invest in an architecture if we aren't going to use it to constrain the code? However,
this happens more often than you might think. Second, some projects use the published architecture to
start out, but when problems are encountered (either technical or schedule-related), the architecture is
abandoned and coders scramble to field the system as best they can. Third (and perhaps most common),
after the system has been fielded, changes to it are accomplished with code changes only, but these
changes affect the architecture. However, the published architecture is not updated to guide the changes,
nor updated afterward to keep up with them.
One simple method to remedy the lack of updating the architecture is to not treat the published
architecture as an all-or-nothing affair it's either all correct or all useless. Parts of the architecture may
become out of date, but it will help enormously if those parts are marked as "no longer applicable" or
"to be revised." Conscientiously marking sections as out of date keeps the architecture documentation a
living document and (paradoxically) sends a stronger message about the remainder: it is still correct and
can still be trusted.
In addition, strong management and process discipline will help prevent erosion. One way is to
mandate that changes to the system, no matter when they occur, are vetted through the architecture first.
The alternatives for achieving code alignment with the architecture include the following:
• Sync at life-cycle milestone. Developers change the code until the end of some phase, such as a
release or end of an iteration. At that point, when the schedule pressure is less, the architecture
is updated.
• Sync at crisis. This undesirable approach happens when a project has found itself in a technical
quagmire and needs architectural guidance to get itself going again.
• Sync at check-in. Rules for the architecture are codified and used to vet any check-in. When a
change to the code "breaks" the architecture rules, key project stakeholders are informed and
then either the code or the architecture rules must be modified. This process is typically
automated by tools.
These alternatives can work only if the implementation follows the architecture mostly, departing
from it only here and there and in small ways. That is, it works when syncing the architecture involves
an update and not a wholesale overhaul or do-over.
·
·
·- 0
Potayto, Potahto, Tomayto, Tomahto Let's Call the Whole Thing Off!
A a
One of the most vexing realities about architecture-based software development is the
gulf between architectural and implementation ontologies, the set of concepts and terms
inherent in an area. Ask an architect what concepts they work with all day, and you're
likely to hear things like modules, components, connectors, stakeholders, evaluation,
analysis, documentation, views, modeling, quality attributes, business goals, and
technology roadmaps.
Ask an implementer the same question, and you likely won't hear any of those
words. Instead you'll hear about objects, methods, algorithms, data structures,
variables, debugging, statements, code comments, compilers, generics, operator
overloading, pointers, and build scripts.
This is a gap in language that reflects a gap in concepts. This gap is, in turn,
reflected in the languages of the tools that each community uses. UML started out as a
way to model object-oriented designs that could be quickly converted to code that is,
UML is conceptually "close" to code. Today it is a de facto architecture description
language, and likely the most popular one. But it has no built-in concept for the most
ubiquitous of architectural concepts, the layer. If you want to represent layers in UML,
you have to adopt some convention to do it. Packages stereotyped as <<layer>>,
associated with stereotyped <<allowed to use>> dependencies do the trick. But it is a
trick, a workaround for a language deficiency. UML has "connectors," two of them in
fact. But they are a far cry from what architects think of as connectors. Architectural
connectors can and do have rich functionality. For instance, an enterprise service bus
(ESB) in a service-oriented architecture handles routing, data and format
transformation, technology adaptation, and a host of other work. It is most natural to
depict the ESB as a connector tying together services that interact with each other
through it. But UML connectors are impoverished things, little more than bookkeeping
mechanisms that have no functionality whatsoever. The delegation connector in UML
exists merely to associate the ports of a parent component with ports of its nested
children, to send inputs from the outside into a child's input port, and outputs from a
child to the output port of the parent. And the assembly connector simply ties together
one component's "requires" interface with another's "provides" interface. These are no
more than bits of string to tie two components together. To represent a true architectural
connector in UML, you have to adopt a convention another workaround such as
using simple associations tagged with explanatory annotations, or abandon the
architectural concept completely and capture the functionality in another component.
Part of the concept gap between architecture and implementation is inevitable.
Architectures, after all, are abstractions of systems and their implementations. Back in
Chapter 2, we said that was one of the valuable properties of architecture: you could
build many different systems from one. And that' s what an abstraction is: a one-tomany
mapping. One abstraction, many instances; one architecture, many
implementations. That architecture is an abstraction of implementation is almost its
·
·
·- 0
whole point: architecture lets us achieve intellectual control over a system without
having to capture, let alone master, all of the countless and myriad truths about its
implementation.
A a
And here comes the gap again: All of those truths about its implementation are what
coders produce for a living, without which the system remains but an idea. Architects,
on the other hand, dismiss all of that reality by announcing that they are not interested
in implementation "details."
Can't we all get along?
We could. There is nothing inherently impossible about a language that embraces
architectural as well as coding concepts, and several people have proposed some. But
UML is beastly difficult to change, and programming language purveyors all seem to
focus their attention down on the underlying machine and not up to the architecture that
is directing the implementation.
Until this gap is resolved, until architects and coders (and their tools) speak the same
conceptual language, we are likely to continue to deal with the most vexing result of
this most vexing reality: writing code (or introducing a code change) that ignores the
architecture is the easiest thing in the world.
The good news is that even though architecture and implementation speak different
languages, they aren't languages from different planets. Concepts in one ontology
usually correspond pretty well to concepts in another. Frameworks are an area where
the languages enjoy a fair amount of overlap. So are interfaces. These constructs live on
the cusp of the two domains, and provide hope that we might one day speak the same
language.
-PCC
19.2. Architecture and Testing
·
·
·- 0 A a
What is the relationship between architecture and testing? One possible answer is "None," or "Not
much." Testing can be seen as the process of making sure that a software system meets its requirements,
that it brings the necessary functionality (endowed with the necessary quality attributes) to its user
community. Testing, seen this way, is simply connected to requirements, and hardly connected to
architecture at all. As long as the system works as expected, who cares what the architecture is? Yes,
the architecture played the leading role in getting the system to work as expected, thank you very much,
but once it has played that role it should make a graceful exit off the stage. Testers work with
requirements: Thanks, architecture, but we'll take it from here.
Not surprisingly, we don't like that answer. This is an impoverished view of testing, and in fact an
unrealistic one as well. As we'll see, architecture cannot help but play an important role in testing.
Beyond that, though, we'll see that architecture can help make testing less costly and more effective
when embraced in testing activities. We'll also see what architects can do to help testers, and what
testers can do to take advantage of the architecture.
Levels of Testing and How Architecture Plays a Role in Each
There are "levels" of testing, which range from testing small, individual pieces in isolation to an entire
system.
• Unit testing refers to tests run on specific pieces of software. Unit testing is usually a part of the
job of implementing those pieces. In fact, unit tests are typically written by developers
themselves. When the tests are written before developing the unit, this practice is known as
test-driven development.
Certifying that a unit has passed its unit tests is a precondition for delivery of that unit to
integration activities. Unit tests test the software in a standalone fashion, often relying on
"stubs" to play the role of other units with which the tested unit interacts, as those other units
may not yet be available. Unit tests won't usually catch errors dealing with the interaction
between elements that comes later but unit tests provide confidence that each of the
system's building blocks is exhibiting as much correctness as is possible on its own.
A unit corresponds to an architectural element in one of the architecture 's module views. In
object-oriented software, a unit might correspond to a class. In a layered system, a unit might
correspond to a layer, or a part of a layer. Most often a unit corresponds to an element at the
leaf of a module decomposition tree.
Architecture plays a strong role in unit testing. First, it defines the units: they are
architectural elements in one or more of the module views. Second, it defines the
responsibilities and requirements assigned to each unit.
Modifiability requirements can also be tested at unit test time. How long it will take to make
specified changes can be tested, although this is seldom done in practice. If specified changes
take too long for the developers to make, imagine how long they will take when a new and
separate maintenance group is in charge without the intimate knowledge of the modules.
·
·
·- 0 A a
Although unit testing goes beyond architecture (tests are based on nonarchitectural
information such as the unit's internal data structures, algorithms, and control flows), they
cannot begin their work without the architecture.
• Integration testing tests what happens when separate software units start to work together.
Integration testing concentrates on finding problems related to the interfaces between elements
in a design. Integration testing is intimately connected to the specific increments or subsets that
are planned in a system's development.
The case where only one increment is planned, meaning that integration of the entire system
will occur in a single step, is called "big bang integration" and has largely been discredited in
favor of integrating many incrementally larger subsets. Incremental integration makes locating
errors much easier, because any new error that shows up in an integrated subset is likely to live
in whatever new parts were added this time around.
At the end of integration testing, the project has confidence that the pieces of software work
together correctly and provide at least some correct system-wide functionality (depending on
how big a subset of the system is being integrated). Special cases of integration testing are
these:
• System testing, which is a test of all elements of the system, including software and
hardware in their intended environment
• Integration testing that involves third-party software
Once again, architecture cannot help but play a strong role in integration testing. First, the
increments that will be subject to integration testing must be planned, and this plan will be
based on the architecture. The uses view is particularly helpful for this, as it shows what
elements must be present for a particular piece of functionality to be fielded. That is, if the
project requires that (for example) in the next increment of a social networking system users
will be able to manage photographs they've allowed other users to post in their own member
spaces, the architect can report that this new functionality is part of the user_permissions
module, which will use a new part of the photo_sharing module, which in tum will use a new
structure in the master user_links database, and so forth. Project management will know, then,
that all of the software must be ready for integration at the same time.
Second, the interfaces between elements are part of the architecture, and those interfaces
determine the integration tests that are created and run.
Integration testing is where runtitne quality attribute requirements can be tested.
Performance and reliability testing can be accomplished. A sophisticated test harness is useful
for performing these types of tests. How long does an end-to-end synchronization of a local
database with a global database take? What happens if faults are injected into the system? What
happens when a process fails? All of these conditions can be tested at integration time.
Integration testing is also the time to test what happens when the system runs for an
extended period. You could monitor resource usage during the testing and look for resources
that are consumed but not freed. Does your pool of free database connections decrease over
time? Then maybe database connections should be managed more aggressively. Does the
thread pool show signs of degradation over time? Ditto.
·
·
·- 0 A a
• Acceptance testing is a kind of system testing that is performed by users, often in the setting in
which the system will run. Two special cases of acceptance testing are alpha and beta testing. In
both of these, users are given free rein to use the system however they like, as opposed to
testing that occurs under a preplanned regimen of a specific suite of tests. Alpha testing usually
occurs in-house, whereas beta testing makes the system available to a select set of end users
under a "User beware" proviso. Systems in beta test are generally quite reliable after all, the
developing organization is highly motivated to make a good first impression on the user
community but users are given fair warning that the system might not be bug-free or (if "bugfree"
is too lofty a goal) at least not up to its planned quality level.
Architecture plays less of a role in acceptance testing than at the other levels, but still an
important one. Acceptance testing involves stressing the system's quality attribute behavior by
running it at extremely heavy loads, subjecting it to security attacks, depriving it of resources at
critical times, and so forth. A crude analogy is that if you want to bring down a house, you can
whale away at random walls with a sledgehammer, but your task will be accomplished much
more efficiently if you consult the architecture first to find which of the walls is holding up the
roof. (The point of testing is, after all, to "bring down the house.")
Overlaying all of these types of testing is regression testing, which is testing that occurs after a
change has been made to the system. The name comes from the desire to uncover old bugs that might
resurface after a change, a sign that the software has "regressed" to a less mature state. Regression
testing can occur at any of the previously mentioned levels, and often consists of rerunning the bank of
tests and checking for the occurrence of old (or for that matter, new) faults.
Black-Box and White-Box Testing
Testing (at any level) can be "black box" or "white box." Black-box testing treats the software as an
opaque "black box," not using any knowledge about the internal design, structure, or implementation.
The tester's only source of information about the software is its requirements.
Architecture plays a role in black-box testing, because it is often the architecture document where
the requirements for a piece of the system are described. An element of the architecture is unlikely to
correspond one-to-one with a requirement nicely captured in a requirements document. Rather, when
the architect creates an architectural element, he or she usually assigns it an amalgamation of
requirements, or partial requirements, to carry out. In addition, the interface to an element also
constitutes a set of "requirements" for it the element must happily accept the specified parameters and
produce the specified effect as a result. Testers performing black-box testing on an architectural element
(such as a major subsystem) are unlikely to be able to do their jobs using only requirements published
in a requirements document. They need the architecture as well, because the architecture will help the
tester understand what portions of the requirements relate to the specified subsystem.
White-box testing makes full use of the internal structures, algorithms, and control and data flows of
a unit of software. Tests that exercise all control paths of a unit of software are a primary example of
white-box testing. White-box testing is most often associated with unit testing, but it has a role at higher
levels as well. In integration testing, for example, white-box testing can be used to construct tests that
·
·
·- 0 A a
attempt to overload the connection between two components by exploiting knowledge about how a
component (for example) manages multiple simultaneous interactions.
Gray-box testing lies, as you would expect, between black and white. Testers get to avail
themselves of some, but not all, of the internal structure of a system. For example, they can test the
interactions between components but not employ tests based on knowledge of a component's internal
data structures.
There are advantages and disadvantages with each kind of testing. Black-box testing is not biased
by a design or implementation, and it concentrates on making sure that requirements are met. But it can
be inefficient by (for example) running many unit tests that a simple code inspection would reveal to be
unnecessary. White-box testing often keys in on critical errors more quickly, but it can suffer from a
loss of perspective by concentrating tests to make the implementation break, but not concentrating on
the software delivering full functionality under all points in its input space.
Risk-based Testing
Risk -based testing concentrates effort on areas where risk is perceived to be the highest, perhaps
because of immature technologies, requirements uncertainty, developer experience gaps, and so forth.
Architecture can inform risk-based testing by contributing categories of risks to be considered.
Architects can identify areas where architectural decisions (if wrong) would have a widespread impact,
where architectural requirements are uncertain, quality attributes are demanding on the architecture,
technology selections risky, or third-party software sources unreliable. Architecturally significant
requirements are natural candidates for risk-based test cases. If the architecturally significant
requirements are not met, then the system is unacceptable, by definition.
Test Activities
Testing, depending on the project, can consume from 30 to 90 percent of a development's schedule and
budget. Any activity that gobbles resources as voraciously as that doesn' t just happen, of course, but
needs to be planned and carried out purposefully and as efficiently as possible. Here are some of the
activities associated with testing:
• Test planning. Test activities have to be planned so that appropriate resources can be allocated.
"Resources" includes time in the project schedule, labor to run the tests, and technology with
which the testing will be carried out. Technology might include test tools, automatic regression
testers, test script builders, test beds, test equipment or hardware such as network sniffers, and
so forth.
• Test development. This is an activity in which the test procedures are written, test cases are
chosen, test datasets are created, and test suites are scripted. The tests can be developed either
before or after development. Developing the tests prior to development and then developing a
module to satisfy the test is a characteristic of test-first development.
• Test execution. Here, testers apply the tests to the software and capture and record errors.
• Test reporting and defect analysis. Testers report the results of specific tests to developers, and
they report overall metrics about the test results to the project's technical management. The
analysis might include a judgment about whether the software is ready for release. Defect
·
·
·- 0 A a
analysis is done by the development team usually along with the customer, to adjudicate
disposition of each discovered fault: fix it now, fix it later, don't worry about it, and so on.
• Test harness creation. One of the architect's common responsibilities is to create, along with
the architecture, a set of test harnesses through which elements of the architecture may be
conveniently tested. Such test harnesses typically permit setting up the environment for the
elements to be tested, along with controlling their state and the data flowing into and out of the
elements.
Once again, architecture plays a role and informs each of these activities; the architect can
contribute useful information and suggestions for each. For test planning, the architecture provides the
list of software units and incremental subsets. The architect can also provide insight as to the
complexity or, if the software does not yet exist, the expected complexity of each of the software units.
The architect can also suggest useful test technologies that will be compatible with the architecture; for
example, Java's ability to support assertions in the code can dramatically increase software testability,
and the architect can provide arguments for or against adopting that technology. For test development,
the architecture can make it easy to swap datasets in and out of the system. Finally, test reporting and
defect analysis are usually reported in architectural terms: this element passed all of its tests, but that
element still has critical errors showing. This layer passed the delivery test, but that layer didn't. And so
forth.
The Architect's Role
Here are some of the things an architect can do to facilitate quality testing. First and foremost, the
architect can design the system so that it is highly testable. That is, the system should be designed with
the quality attribute of testability in mind. Applying the cardinal rule of architecture ("Know your
stakeholders!"), the architect can work with the test team (and, to the extent they have a stake in testing,
other stakeholders) to establish what is needed. Together, they can come up with a definition of the
testability requirements using scenarios, as described in Chapter 10. Testability requirements are most
likely to be a concern of the developing organization and not so much of the customer or users, so don't
expect to see many testing requirements in a requirements document. Using those testability
requirements, the testability tactics in Chapter 1 0 can be brought to bear to provide the testability
needed.
In addition to designing for testability, the architect can also do these other things to help the test
effort:
• Insure that testers have access to the source code, design documents, and the change records.
• Give testers the ability to control and reset the entire dataset that a program stores in a
persistent database. Reverting the database to a known state is essential for reproducing bugs or
running regression tests. Similarly, loading a test bed into the database is helpful. Even
products that don't use databases can benefit from routines to automatically preload a set of test
data. One way to achieve this is to design a "persistence layer" so that the whole program is
database independent. In this way, the entire database can be swapped out for testing, even
using an in-memory database if desired.
• Give testers the ability to install multiple versions of a software product on a single machine.
·
·
·- 0 A a
This helps testers compare versions, isolating when a bug was introduced. In distributed
applications, this aids testing deployment configurations and product scalability. This capability
could require configurable cotnmunication ports and provisions for avoiding collisions over
resources such as the registry.
As a practical matter, the architect cannot afford to ignore the testing process because if, after
delivery, something goes seriously wrong, the architect will be one of the first people brought in to
diagnose the problem. In one case we heard about, this involved flying to the remote mountains of Peru
to diagnose a problem with mining equipment.
19.3. Summary
·
·
·- 0 A a
Architecture plays a key role in both implementation and testing. In the implementation phase, letting
future readers of the code know what architectural constructs are being used, using frameworks, and
using code templates all make life easier both at implementation time and during maintenance.
During testing the architecture determines what is being tested at which stage of development.
Development quality attributes can be tested during unit test and runtime quality attributes can be tested
during integration testing.
Testing, as with other activities in architecture-based development, is a cost/benefit activity. Do not
spend as much time testing for faults whose consequences are small and spend the most time testing for
faults whose consequences are serious. Do not neglect testing for faults that show up after the system
has been executing for an extended period.
19.4. For Further Reading
·
·
·- 0 A a
George Fairbanks gives an excellent treatment of architecture and implementation in Chapter 1 0 of his
book Just Enough Software Architecture, which is entitled "The Code Model" [Fairbanks 1 OJ.
Mary Shaw long ago recognized the conceptual gap between architecture and implementation and
wrote about it eloquently in her article "Procedure Calls Are the Assembly Language of Software
Interconnections: Connectors Deserve First-Class Status" [Shaw 94]. In it she pointed out the disparity
between rich connectors available in architecture and the impoverished subroutine call that is the
mainstay of nearly every programming language.
Details about the AUTOSAR framework can be found at www.autosar.org.
Architecture-based testing is an active field of research. (Bertolino 96b], [Muccini 07], [Muccini
03], [Eickelman 96], (Pettichord 02], and [Binder 94] specifically address designing systems so that
they are more testable. In fact, the three bullets concerning the architect's role in Section 1 9.2 are drawn
from Pettichord's work.
Voas [Voas 95] defines testability, identifies its contributing factors, and describes how to measure
it.
Bertolino extends Voas' s work and ties testability to dependability [Bertolino 96].
Finally, Baudry et al. have written an interesting paper that examines the testability of well-known
design patterns [Baudry 03].
19 .5. Discussion Questions
·
·
·- 0 A a
1 . In a distributed system each computer will have its own clock. It is difficult to perfectly
synchronize those clocks. How will this complicate making performance measures of distributed
systems? How would you go about testing that the performance of a particular system activity is
adequate?
2. Plan and implement a modification to a module. Ask your colleagues to do the same modification
independently. Now compare your results to those of your colleagues. What is the mean and the
standard deviation for the time it takes to make that modification?
3. List some of the reasons why an architecture and a code base inevitably drift apart. What
processes and tools might address this gap? What are their costs and benefits?
4. Most user interface frameworks work by capturing events from the user and by establishing
callbacks or hooks to application-specific functionality. What limitations do these architectural
assumptions impose on the rest of the system?
5. Consider building a test harness for a large system. What quality attributes should this harness
exhibit? Create scenarios to concretize each of the quality attributes.
6. Testing requires the presence of a test oracle, which determines the success (or failure) of a test.
For scalability reasons, the oracle must be automatic. How can you ensure that your oracle is
correct? How do you ensure that its performance will scale appropriately? What process would
you use to record and fix faults in the testing infrastructure?
7. In embedded systems faults often occur "in the field" and it is difficult to capture and replicate
the state of the system that led to its failure. What architectural mechanisms might you use to
solve this problem?
8. In integration testing it is a bad idea to integrate everything all at once (big bang integration).
How would you use architecture to help you plan integration increments?
·
·
·- 0
20. Architecture Reconstruction and Conformance
It was six men of Indostan I To learning much
inclined, Who went to see the Elephant I (Though
all of them were blind), That each by observation
I Might satisfy his mind.
The First approach 'd the Elephant, I And
happening to fall Against his broad and sturdy
side, I At once began to bawl: ((God bless mel but
the Elephant I Is very like a wall I "
The Second, feeling of the tusk, I Cried, - ((Hoi
what have we here So very round and smooth and
sharp? I To me 'tis mighty clear This wonder of
an Elephant I Is very like a spear I "
The Third approached the animal, I And
happening to take The squirming trunk within his
hands, I Thus boldly up and spake: (J see, " quoth
he, ((the Elephant I Is very like a snake! "
The Fourth reached out his eager hand, I And felt
about the knee. ( ( What most this wondrous beast
is like I Is mighty plain, " quoth he, " 'Tis clear
enough the Elephant I Is very like a tree! "
The Fifth, who chanced to touch the ear, I Said:
('E 'en the blindest man Can tell what this
resembles most; I Deny the fact who can, This
marvel of an Elephant I Is very like a fan ! "
The Sixth no sooner had begun I About the beast
to grope, Then, seizing on the swinging tail I That
fell within his scope, (J see, " quoth he, ((the
Elephant I Is very like a rope! "
And so these men of Indostan I Disputed loud and
long, Each in his own opinion I Exceeding stif)
and strong, Though each was partly in the right, I
And all were in the wrong!
-"The Blind Men and the Elephant," by John
Godfrey Saxe
A a
·
·
·- 0 A a
Throughout this book we have treated architecture as something largely under your control and shown
how to make architectural decisions to achieve the goals and requirements in place for a system under
development. But there is another side to the picture. Suppose you have been given responsibility for a
system that already exists, but you do not know its architecture. Perhaps the architecture was never
recorded by the original developers, now long gone. Perhaps it was recorded but the documentation has
been lost. Or perhaps it was recorded but the documentation is no longer synchronized with the system
after a series of changes. How do you maintain such a system? How do you manage its evolution to
maintain the quality attributes that its architecture (whatever it may be) has provided for us?
This chapter surveys techniques that allow an analyst to build, maintain, and understand a
representation of an existing architecture. This is a process of reverse engineering, typically called
architecture reconstruction. Architecture reconstruction is used, by the architect, for two main purposes:
• To document an architecture where the documentation never existed or where it has become
hopelessly out of date
· To ensure conformance between the as-built architecture and the as-designed architecture.
In architecture reconstruction, the "as-built" architecture of an implemented system is reverseengineered
from existing syste1n artifacts.
When a system is initially developed, its architectural elements are mapped to specific
implementation elements: functions, classes, files, objects, and so forth. This is forward engineering.
When we reconstruct those architectural elements, we need to apply the inverses of the original
mappings. But how do we go about determining these mappings? One way is to use automated and
semiautomated extraction tools; the second way is to probe the original design intent of the architect.
Typically we use a combination of both techniques in reconstructing an architecture.
In practice, architecture reconstruction is a tool-intensive activity. Tools extract information about
the system, typically by scouring the source code, but they may also analyze other artifacts as well, such
as build scripts or traces from running systems. But architectures are abstractions they can not be seen
in the low-level implementation details, the programming constructs, of most systems. So we need tools
that aid in building and aggregating the abstractions that we need, as architects, on top of the ground
facts that we develop, as developers. If our tools are usable and accurate, the end result is an
architectural representation that aids the architect in reasoning about the system. Of course, if the
original architecture and its impletnentation are "spaghetti," the reconstruction will faithfully expose
this lack of organization.
Architecture reconstruction tools are not, however, a panacea. In some cases, it may not be possible
to generate a useful architectural representation. Furthermore, not all aspects of architecture are easy to
automatically extract. Consider this: there is no programming language construct in any major
programming language for "layer" or "connector" or other architectural elements; we can't simply pick
these out of a source code file. Similarly, architectural patterns, if used, are typically not explicitly
documented in code.
Architecture reconstruction is an interpretive, interactive, and iterative process involving many
activities; it is not automatic. It requires the skills and attention of both the reverse-engineering expert
and, in the best case, the architect (or smneone who has substantial knowledge of the architecture). And
·
·
·- 0 A a
whether the reconstruction is successful or not, there is a price to pay: the tools come with a learning
curve that requires time to climb.
20.1. Architecture Reconstruction Process
·
·
·- 0 A a
Architecture reconstruction requires the skillful application of tools, often with a steep learning curve.
No single tool does the entire job. For one reason, there is often diversity in the number of
implementation languages and dialects in which a software system is implemented a mature MRI
scanner or a legacy banking application may easily comprise more than ten different programming and
scripting languages. No tool speaks every language.
Instead we are inevitably led to a "tool set" approach to support architecture reconstruction
activities. And so the first step in the reconstruction process is to set up the workbench.
An architecture reconstruction workbench should be open (making it easy to integrate new tools as
required) and provide an integration framework whereby new tools that are added to the tool set do not
impact the existing tools or data unnecessarily.
Whether or not an explicit workbench is used, the software architecture reconstruction process
comprises the following phases (each elaborated in a subsequent section):
1 . Raw view extraction. In the raw view extraction phase, raw information about the architecture
is obtained from various sources, primarily source code, execution traces, and build scripts.
Each of these sets of raw information is called a view.l
1. This use of the te1m "view" is consistent with our definition in Chapter 1 8 : "a representation of a set of system
elements and relations among them."
2. Database construction. The database construction phase involves converting the raw
extracted information into a standard form (because the various extraction tools may each
produce their own form of output). This standardized form of the extracted views is then used
to populate a reconstruction database. When the reconstruction process is complete, the
database will be used to generate authoritative architecture documentation.
3. View fusion and manipulation. The view fusion phase combines the various views of the
information stored in the database. Individual views may not contain complete or fully
accurate information. View fusion can improve the overall accuracy. For example, a static
view extracted from source code might miss dynamically bound information such as calling
relationships. This could then be combined with a dynamic view from an execution trace,
which will capture all dynamically bound calling information, but which may not provide
complete coverage. The combination of these views will provide higher quality information
than either could provide alone. Furthermore, view creation and fusion is typically associated
with some expert interpretation and manipulation. For example, an expert might decide that a
group of elements should be aggregated together to form a layer.
4. Architecture analysis. View fusion will result in a set of hypotheses about the architecture.
These hypotheses take the form of architectural elements (such as layers) and the constraints
and relationships among them. These hypotheses need to be tested to see if they are correct,
and that is the function of the analysis step. Some of these hypotheses might be disproven,
requiring additional view extraction, fusion, and manipulation.
The four phases of architecture reconstruction are iterative. Figure 20. 1 depicts the major tasks of
·
·
·- 0 A a
architecture reconstruction and their relationships and outputs. Solid lines represent data flow and
dashed lines represent human interaction.
I
View Extraction
Lexical Parsing Instrumentation
#
;
#'
,' Database
' Construction 􀑈
,
Database
View Fusion
and Manipulation
'
'
'
'
'
• • •
'
,
,
'
'
'
'
'
,
,
,
,
,0
Architecture
Analysis
,
' Key:
,
'
'
1 ..1 -P-...- r_ooe_ s-s-te--;p --. 1
Data flow
1!!1 - - - - - - - -i>-
lntorms
Figure 20. 1 . Architecture reconstruction process
All of these activities are greatly facilitated by engaging people who are familiar with the system.
They can provide insights about what to look for that is, what views are amenable to extraction and
provide a guided approach to view fusion and analysis. They can also point out or explain exceptions to
the design rules (which will show up as violations of the hypotheses during the analysis phase). If the
experts are long gone, reconstruction is still possible, but it may well require more backtracking from
incorrect initial guesses.
20.2. Raw View Extraction
·
·
·- 0 A a
Raw view extraction involves analyzing a system's existing design and implementation artifacts to
construct one or more models of it. The result is a set of information that is used in the view fusion
activity to construct more-refined views of the system that directly support the goals of the
reconstruction, goals such as these:
• Extracting and representing a target set of architectural views, to support the overall
architecture documentation effort.
• Answering specific questions about the architecture. For example, "What components are
potentially affected if I choose to rewrite component X?" or "How can I refactor my layering to
remove cyclic dependencies?"
The raw view extraction process is a blend of the ideal (what information do you want to discover
about the architecture that will most help you meet the goals of your reconstruction effort?) and the
practical (what information can your available tools actually extract and present?).
From the source artifacts (code, header files, build files, and so on) and other artifacts (e.g.,
execution traces), you can identify and capture the elements of interest within the system (e. g., files,
functions, variables) and their relationships to obtain several base system views. Table 20. 1 shows a
typical list of the elements and several relationships among them that might be extracted.
Table 20.1. Examples of Extracted Elements and Relations
Source
Element
Ffle
File
File
Directory
Directory
Function
Function
Function
Relation
includes
' conca􀒕ns
defines -
• conta1.ns
contains
calls
access
-
access -
var
read
write
Target
Element
Flle
Function
Variable
Directory
File
Function
Variab'le
Variable
Description
C preprocessor #include of one
file by another
Definition of a function in a tHe
Definition of a variable in a file
Directory contains a subdirectory
Directory contains a file
Static function call
Read access on a variable
Wr1te access on a variable
Each of the relationships between the elements gives different information about the system:
• The c a l l s relationship between functions helps us build a call graph.
• The includes relationship between the files gives us a set of dependencies between system files.
• The access_react and access_write relationships between functions and variables show us how
data is used. Certain functions may write a set of data and others may read it. This information
is used to determine how data is passed between various parts of the system. We can determine
whether or not a global data store is used or whether most information is passed through
function calls.
·
·
·- 0 A a
• Certain elements or subsystems may be stored in particular directories, and capturing relations
such as dir_contains_file and dir_contains_dir is useful when trying to identify elements later.
• If the system to be reconstructed is object oriented, classes and methods are added to the list of
elements to be extracted, and relationships such as c l a s s_is_subclass_of_class and
c l a s s contains method are extracted and used. - -
Information obtained can be categorized as either static or dynamic. Static information is obtained
by observing only the system artifacts, while dynamic information is obtained by observing how the
system runs. The goal is to fuse both to create more accurate system views.
If the architecture of the system changes at runtime, that runtime configuration should be captured
and used when carrying out the reconstruction. For example, in some systems a configuration file is
read in by the system at startup, or a newly started system examines its operating environment, and
certain elements are executed or connections are made as a result.
Another reason to capture dynamic information is that some architecturally relevant information
may not exist in the source artifacts because of late binding. Examples of late binding include the
following:
• Polymorphism
• Function pointers
• Runtime parameterization
• Plug-ins
• Service interactions mediated by brokers
Further, the precise topology of a system may not be determined until runtime. For example, in
peer-to-peer systems, service-oriented architectures, and cloud computing, the topology of the system is
established dynamically, depending on the availability, loading, and even dynamic pricing of system
resources. The topology of such systems cannot be directly recovered from their source artifacts and
hence cannot be reverse-engineered using static extraction tools.
Therefore, it may be necessary to use tools that can generate dynamic information about the system
(e.g., profiling tools, instrumentation that generates runtime traces, or aspects in an aspect-oriented
programming language that can monitor dynamic activity). Of course, this requires that such tools be
available on the platforms on which the system executes. Also, it may be difficult to collect the results
from code instrumentation. For example, embedded systems often have no direct way to output such
information.
Table 20.2 summarizes some of the common categories of tools that might be used to populate the
views loaded into the reconstruction database.
Table 20.2. Tool Categories for Populating Reconstructed Architecture Views
Tool
Parsers
Abstract Syntax
Tree (AST)
Analyzers
Lexical Analyzers
Profilers
Code
Instrumentation
Tools
Static or
Dynamic
Static
Dynamic
Description
·
·
·- 0
Parsers analyze the code and generate internal
representations from it (for the purpose of
generating machine code). It Is possible to save
this internal representation to obtain a view.
AST analyzers do a similar job to parsers, but
they buHd an exp'licit tree representation of
the parsed information. We can build analysis
tools that traverse the AST and output selected
pieces of architecturally relevant information in
an appropriate format.
Lexical analyzers examine source artifacts
purely as strings of lexical .elements or tokens.
The user of a lexical analyzer can specify
a set of code patterns to be matched and
output. Similarly, a collection of ad hoc tool1s
such as grep and Perl can carry out pattern
matching and searching within the code to
output some required information. All of these
tools-code·generating parsers, AST-based
analyzers, lexical analyzers, and ad hoc pattern
matchers-are used to output static information.
Profiling and code coverage analysis tools can
be used to output information about the code as
it is being executed, and usually do not involve
addtng new code to the system.
Code instrumentation, which has wide
appticability in the field of testing, involves
addrng code to the system to output specific
information while the system is executing.
Aspects, rn an aspect-oriented programming
language, can serve the same purpose
and have the advantage of keeping the
instrumentation code separate from the code
being monitored.
A a
Tools to analyze design models, build files, and executables can also be used to extract further
information as required. For instance, build files include information on module or file dependencies
that exist within the system, and this information may not be reflected in the source code, or anywhere
else.
An additional activity that is often required prior to loading a raw view into the database is to prune
irrelevant information. For example, in a C code base there may be several main o routines, but only one
of those (and its resulting call graph) will be of concern for analysis. The others may be for test
harnesses and other utility functions. Similarly if you are building or using libraries that are operatingsystem
specific, you may only be interested in a specific OS (e.g., Linux) and thus want to discard the
libraries for other platforms.
20.3. Database Construction
·
·
·- 0 A a
Some of the information extracted from the raw view extraction phase, while necessary for the process
of reconstruction, may be too specific to aid in architectural understanding. Consider Figure 20.2. In
this figure we show a set of facts extracted from a code base consisting of classes and methods, and
inclusion and calling relations. Each element is plotted on a grid and each relation is drawn as a line
between the elements. This view, while accurate, provides no insight into the overarching abstractions
or coarse-grained structures present in the architecture.
A reType: level Fdlered: Ot , 'D arc:s
Figure 20.2. A raw extracted view: white noise
Thus we need to manipulate such raw views, to collapse information (for example, hiding methods
inside class definitions), and to show abstractions (for example, showing all of the connections between
business objects and user interface objects, or identifying distinct layers).
It is helpful to use a database to store the extracted information because the amount of information
being stored is large, and the manipulations of the data are tedious and error-prone if done manually.
Some reverse-engineering tools, such as Lattix, SonarJ, and Structure 1 0 1 , fully encapsulate the
·
·
·- 0 A a
database, and so the user of the tool need not be concerned with its operation. However, those who are
using a suite of tools together a workbench will need to choose a database and decide on internal
representations of the views.
20.4. View Fusion
·
·
·- 0 A a
Once the raw facts have been extracted and stored in a database, the reconstructor can now perform
view fusion. In this phase, the extracted views are manipulated to create fused views. Fused views
combine information from one or more extracted views, each of which may contain specialized
information. For example, a static call view might be fused with a dynamic call view. One might want
to combine these two views because a static call view will show all explicit calls (where method A calls
method B) but will miss calls that are made via late binding mechanisms. A dynamically extracted call
graph will never miss a call that is made during an execution, but it suffers the "testing" problem: it will
only report results from those paths through the system that are traversed during its execution. So a
little-used part of the system perhaps for initialization or error recovery might not show up in the
dynamic view. Therefore we fuse these two views to produce a more complete and more accurate graph
of system relationships.
The process of creating a fused view is the process of creating a hypothesis about the architecture
and a visualization of it to aid in analysis. These hypotheses result in new aggregations that show
various abstractions or clusterings of the elements (which may be source artifacts or previously
identified abstractions). By interpreting these fused views and analyzing them, it is possible to produce
hypothesized architectural views of the system. These views can be interpreted, further refined, or
rejected. There are no universal completion criteria for this process; it is complete when the
architectural representation is sufficient to support the analysis needs of its stakeholders.
For example, Figure 20.3 shows the early results of interacting with the tool SonarJ. SonarJ first
extracts facts from a set of source code files (in this case, written in Java) and lets you define a set of
layers and vertical slices through those layers in a system. SonarJ will then instantiate the user-specified
definitions of layers and slices and populate them with the extracted software elements.
􀞐. 􀃌 􀃮···
􀃍􀃏􀃎.􀃐 Trio Ill user ,,
IIJ <.􀒔• -•
'"=!' -·
- ·-:"::,! 􀑇" = <un-restr "'"''' ·-=. !'.! 11<---< • -' '.:ted. e. eon troller
r=unrestr lctE!d . .
-
p;i -· ·-
􀂵 <unrestr lctE!d. .
= -􀞏 - - = Doman
< <unrastr lctE!d ...
<<unrastrlcted . .
I
- ·=,. ..... I
<<unrestricted . .
-
-
Figure 20.3. Hypothesized layers and vertical slices
In the figure there are five layers: Controller, Data, Domain, DSI, and Service. And there are six
vertical slices defined that span these layers: Common, Contact, Customer, Distribution, Request, and
User. At this point, however, there are no relationships between the layers or vertical slides shownthis
is merely an enumeration of the important system abstractions.
20.5. Architecture Analysis: Finding Violations
·
·
·- 0 A a
Consider the following situation: You have designed an architecture but you have suspicions that the
developers are not faithfully implementing what you developed. They may do this out of ignorance, or
because they have differing agendas for the system, or simply because they were rushing to meet a
deadline and ignored any concern not on their critical path. Whatever the root cause, this divergence of
the architecture and the implementation spells problems for you, the architect. So how do you test and
ensure conformance to the design?
There are two major possibilities for maintaining conformance between code and architecture:
• Conformance by construction. Ensuring consistency by construction that is, automatically
generating a substantial part of the system based on an architectural specification is highly
desirable because tools can guarantee conformance. Unfortunately, this approach has limited
applicability. It can only be applied in situations where engineers can employ specific
architecture-based development tools, languages, and implementation strategies. For systems
that are composed of existing parts or that require a style of architecture or implementation
outside those supported by generation tools, this approach does not apply. And this is the vast
majority of systems.
• Conformance by analysis. This technique aims to ensure conformance by analyzing (reverseengineering)
system information to flag nonconforming elements, so that they can be fixed:
brought into conformance. When an implementation is sufficiently constrained so that
modularization and coding patterns can be identified with architectural elements, this technique
can work well. Unfortunately, however, the technique is limited in its applicability. There is an
inherent mismatch between static, code-based structures such as classes and packages (which
are what programmers see) and the runtime structures, such as processes, threads, clients,
servers, and databases, that are the essence of most architectural descriptions. Further
complicating this analysis, the actual runtime structures may not be known or established until
the program executes: clients and servers may come and go dynamically, components not under
direct control of the implementers may be dynamically loaded, and so forth.
We will focus on the second option: conformance by analysis.
In the previous step, view fusion gave us a set of hypotheses about the architecture. These
hypotheses take the form of architectural elements (sometimes aggregated, such as layers) and the
constraints and relationships among them. These hypotheses need to be tested to see if they are correct
-to see if they conform with the architect' s intentions. That is the function of the analysis step.
Figure 20.4 shows the results of adding relationships and constraints to the architecture initially
created in Figure 20.3. These relationship and constraints are information added by the architect, to
reflect the design intent. In this example, the architect has indicated the relationships between the layers
of Figure 20.3. These relationships are indicated by the directed lines drawn between the layers (and
vertical slices). Using these relationships and constraints, a tool such as SonarJ is able to automatically
detect and report violations of the layering in the software.
·loll's CRMi:Xa ...
Ill common
<<LnreslriCIIld.
<<pub Ia:>>
;::;; controller
i
I r=oara I
I
aoamain -t
- 051
' SServlce
'
,,
Fm. External
, Alnlt 1 Reflection
<<hl<;lden><>
,..
...-=:;f
Ill COntact 􀑉
' , l
I l J
I I I I
I I .T I
••
' -
UI CUSIDmer Ill OISITiboJI.lon .••
􀞎 .. l I I J I
I I I l
I I I
I
til Rllql.le$1
·
·
·-
III IJ$Er
I
I !
I I
I
0
l I l
i
I
Figure 20.4. Layers, vertical slices, relationships, and constraints
A a
We can now see that the Data layer (row 2 in Figure 20.4) can access, and hence depends on, the
DSI layer. We can further see that it may not access, and has no dependencies on, Domain, Service, or
Controller (rows 1 , 3 , and 5 in the figure).
In addition we can see that the JUnit component in the "External" component is defined to be
inaccessible. This is an example of an architectural constraint that is meant to pervade the entire system:
no portion of the application should depend upon JUnit, because this should only be used by test code.
Figure 20.5 shows an example of an architecture violation of the previous restriction. This violation
is found by SonarJ by searching through its database, applying the user-defined patterns, and finding
violations of those patterns. In this figure you can see an arc between the Service layer and JUnit. This
arc is highlighted to indicate that this is an illegal dependency and an architectural violation. (This
figure also shows some additional dependencies, to external modules.)
P' FJ�ia.ii•N-.
IS!
j.foa�a
[Si
·u 13051
13' I I '􀃭'
,,r \�􀀨·� ..
••
.....--
[li1'CO "' rnru av• I USer .. ;􀞌
􀃉 ....-
r
·
·
·-
·�
....-
0
􀃋􀞍AI'idllt den>> I fRe􀃊 I
Figure 20.5. Highlighting an architecture violation
A a
Architecture reconstruction is a means of testing the conformance to such constraints. The
preceding example showed how these constraints might be detected and enforced using static code
analysis. But static analysis is primarily useful for understanding module structures. What if one needed
to understand runtime information, as represented by C&C structures?
In the example given in Figure 20.6, an architecture violation was discovered via dynamic analysis,
using the research DiscoTect system. In this case an analysis of the runtime architecture of the Duke's
Bank application a simple Enterprise JavaBeans (EJB) banking application created by Sun
Microsystems as a demonstration of EJB functionality was performed. The code was "instrumented"
using AspectJ; instrumentation aspects were woven into the compiled bytecode of the EJB application.
These aspects emitted events when methods entered or exited and when objects were constructed.
I􀑅 end: Components
Entty&ean
[ ] Se<;t.bn """""
􀑆
f ..... 􀃬
-
lfk!gal Connactlon
Connector11 Ports
r.:-1 Bf!.>n Inttn!Ciiofl •
Diltaba51e Wl'b •
• Diltilbase R�lld
•
·
·
·-
Response
Request
Dat.llb<lse
0
oetaSOuree_o
Figure 20.6. An architecture violation discovered by dynamic analysis
A a
Figure 20.6 shows that a "database write" connector was discovered in the dynamic analysis of the
architecture. Sun's EJB specification and its documented architecture of Duke 's Bank forbid such
connections. All database access is supposed to be managed by entity beans, and only by entity beans.
Such architectural violations are difficult to find in the source code often just a single line of code is
involved and yet can substantially affect the quality attributes of the resulting system.
20.6. Guidelines
The following are a set of guidelines for the reconstruction process:
·
·
·- 0 A a
• Have a goal and a set of objectives or questions in mind before undertaking an architecture
reconstruction project. In the absence of these, a lot of effort could be spent on extracting
information and generating architecture views that may not be helpful or serve any useful
purpose.
• Obtain some representation, however coarse, of the system before beginning the detailed
reconstruction process. This representation serves several purposes, including the following:
• It identifies what information needs to be extracted from the system.
• It guides the reconstructor in determining what to look for in the architecture and what
views to generate.
Identifying layers is a good place to start.
• In many cases, the existing documentation for a system may not accurately reflect the syste1n as
it is implemented. Therefore it may be necessary to disregard the existing documentation and
use it only to generate the high-level views of the system, because it should give an indication
of the high-level concepts.
• Tools can support the reconstruction effort and shorten the reconstruction process, but they
cannot do an entire reconstruction effort automatically. The work involved in the effort requires
the involvement of people (architects, maintainers, and developers) who are familiar with the
system. It is important to get these people involved in the effort at an early stage as it helps the
reconstructor get a better understanding of the system being reconstructed.
20.7. Summary
·
·
·- 0 A a
Architecture reconstruction and architecture conformance are crucial tools in the architect's toolbox to
ensure that a system is built the way it was designed, and that it evolves in a way that is consistent with
its creators' intentions. All nontrivial long-lived systems evolve: the code and the architecture both
evolve. This is a good thing. But if the code evolves in an ad hoc manner, the result will be the big ball
of mud, and the system's quality attributes will inevitably suffer. The only defense against this erosion
is consistent attention to architecture quality, which implies the need to maintain architecture
conformance.
The results of architectural reconstruction can be used in several ways:
• If no documentation exists or if it is seriously out of date, the recovered architectural
representation can be used as a basis for documenting the architecture, as discussed in Chapter
1 8 .
• It can be used to recover the as-built architecture, or to check conformance against an "asdesigned"
architecture. Conformance checking assures us that our developers and maintainers
have followed the architectural edicts set forth for them and are not eroding the architecture by
breaking down abstractions, bridging layers, compromising information hiding, and so forth.
• The reconstruction can be used as the basis for analyzing the architecture or as a starting point
for reengineering the system to a new desired architecture.
• Finally, the representation can be used to identify elements for reuse or to establish an
architecture-based software product line (see Chapter 25).
The software architecture reconstruction process comprises the following phases:
1 . Raw view extraction. In the raw view extraction phase, raw information about the architecture
is obtained from various sources, primarily source code, execution traces, and build scripts.
Each of these sets of raw information is called a view.
2. Database construction. The database construction phase involves converting the extracted
information into a standard form (because the various extraction tools may each produce their
own form of output) and populating a reconstruction database with this information.
3. View fusion. The view fusion phase combines views of the information stored in the database.
4. Architecture analysis. View fusion has given us a set of hypotheses about the architecture.
These hypotheses take the form of architectural elements (sometimes aggregated, such as
layers) and the constraints and relationships among them. These hypotheses need to be tested
to see if they are correct, and that is the function of the analysis step.
20.8. For Further Reading
·
·
·- 0 A a
The Software Engineering Institute (SEI) has developed two reconstruction workbenches: Dali and
Armin. Dali was our first attempt at creating a workbench for architecture recovery and conformance
[Kazman 99]. Armin, a complete rewrite and rethink of Dali, is described in [O 'Brien 03].
Both Armin and Dali were primarily focused on module structures of an architecture. A later tool,
called DiscoTect, was aimed at discovering C&C structures. This is described in [Schmerl 06].
Many other architecture reverse-engineering tools have been created. A few of the notable ones
created in academia are [van Deursen 04], [Murphy 0 1], and [Storey 97].
In addition there are a number of commercial architecture extraction and reconstruction tools that
have been slowly gaining market acceptance in the past decade. Among these are the following:
• SonarJ (www.hello2morrow.com)
• Lattix ( www .lattix. com)
• Understand (www.scitools.com)
Cai et al. [Cai 20 1 1] compellingly demonstrate the need for architecture conformance testing in an
experimental study that they conducted, wherein they found that software engineering students, given
UML designs for a variety of relatively simple systems, violate those designs over 70 percent of the
time.
Finally, the set of guidelines presented in this chapter for how to go about reconstructing an
architecture was excerpted from [Kazman 02].
20.9. Discussion Questions
·
·
·- 0 A a
1 . Suppose that for a given system you wanted to extract the architectural structures (as
discussed in Chapter 1) listed in the table rows below. For each row, fill in each column to
appraise each strategy listed in the columns. "VH" (very high) means the strategy would be
very effective at extracting this structure; "VL" means it would be very ineffective; "H," M,"
and "L" have the obvious in-between values.
Architectural Slr\lctures
􀂅nterviewing experts on
the sy.stem
Decomposition
'
(/) 􀛗 Uses
::>
-
u 2 Layers
1i5
·a> Class -
::>
"0
0 :2 Data model
Service (for SOA
t/) systems) (!)
....
:::1
Concurrency -
(.) c.> 􀛘 2
() (i)
Deployment
U)
·a>
....
::>
-
0
:::;l
....
-
U)
􀑄
0 ·- - Implementation
ca
0
0 - - Work assignment
<r
Reconstruction Strategies
Analy;dng
structure of
source code files
Static
analysis of
source code
Dynamic
analysis of system's
execution
2 . Recall that in layered systems, the relationship among layers is allowed to use. Also recall that it
is possible for one piece of software to use another piece without actually calling it for example,
by depending on it leaving some shared resource in a usable state. Does this interpretation change
your answer above for the "Uses" and ''Layers" structures?
3. What inferences can you make about a systetn's module structures from examining a set of
behavioral traces gathered dynamically?
4. Suppose you believe that the architecture for a system follows a broker pattern. What information
would you want to extract from the source code to confirm or refute this hypothesis? What
behavioral or interaction pattern would you expect to observe at runtime?
5. Suppose you hypothesize that a system makes use of particular tactics to achieve a particular
quality attribute. Fill in the columns of the table below to show how you would go about
verifying your hypothesis. (Begin by filling in column 1 with a particular tactic for the named
quality attribute.)
Interviewing
experts on
Tactics for • • • the system
Availability
lnteroperabillty
Modifiability
Performance
Security
Testability
Usabili1y
·
·
·-
Reconstruction Strategies
Analyzing
structure of Static
source code analysis of
files source code
0 A a
Dynamic
analysis of
system's
execution
6. Suppose you want to confirm that developers and maintainers had remained faithful to an
architecture over the lifetime of the system. Describe the reconstruction and/or auditing processes
you would undertake.
2 1 . Architecture Evaluation
·
·
·- 0
Fear cannot be banished, but it can be calm and
without panic; it can be mitigated by reason and
evaluation.
-Vannevar Bush
A a
We discussed analysis techniques in Chapter 14. Analysis lies at the heart of architecture evaluation,
which is the process of determining if an architecture is fit for the purpose for which it is intended.
Architecture is such an important contributor to the success of a system and software engineering
project that it makes sense to pause and make sure that the architecture you've designed will be able to
provide all that's expected of it. That's the role of evaluation. Fortunately there are mature methods to
evaluate architectures that use many of the concepts and techniques you've already learned in previous
chapters of this book.
21.1. Evaluation Factors
Evaluation usually takes one of three forms:
• Evaluation by the designer within the design process
• Evaluation by peers within the design process
• Analysis by outsiders once the architecture has been designed
Evaluation by the Designer
·
·
·- 0 A a
Every time the designer makes a key design decision or completes a design milestone, the chosen and
competing alternatives should be evaluated using the analysis techniques of Chapter 14. Evaluation by
the designer is the "test" part of the "generate-and-test" approach to architecture design that we
discussed in Chapter 1 7.
How much analysis? This depends on the importance of the decision. Obviously, decisions made to
achieve one of the driving architectural requirements should be subject to more analysis than others,
because these are the ones that will shape critical portions of the architecture. But in all cases,
performing analysis is a matter of cost and benefit. Do not spend more time on a decision than it is
worth, but also do not spend less time on an important decision than it needs. Some specific
considerations include these:
• The importance of the decision. The more important the decision, the more care should be
taken in making it and making sure it's right.
• The number of potential alternatives. The more alternatives, the more time could be spent in
evaluating them. Try to eliminate alternatives quickly so that the number of viable potential
alternatives is small.
• Good enough as opposed to perfect. Many times, two possible alternatives do not differ
dramatically in their consequences. In such a case, it is more important to make a choice and
move on with the design process than it is to be absolutely certain that the best choice is being
made. Again, do not spend more time on a decision than it is worth.
Peer Review
Architectural designs can be peer reviewed just as code can be peer reviewed. A peer review can be
carried out at any point of the design process where a candidate architecture, or at least a coherent
reviewable part of one, exists. There should be a fixed amount of time allocated for the peer review, at
least several hours and possibly half a day. A peer review has several steps:
1 . The reviewers determine a number of quality attribute scenarios to drive the review. Most of
the time these scenarios will be architecturally significant requirements, but they need not be.
These scenarios can be developed by the review team or by additional stakeholders.
2. The architect presents the portion of the architecture to be evaluated. (At this point,
comprehensive documentation for it may not exist.) The reviewers individually ensure that
they understand the architecture. Questions at this point are specifically for understanding.
There is no debate about the decisions that were made. These come in the next step.
·
·
·- 0 A a
3. For each scenario, the designer walks through the architecture and explains how the
scenario is satisfied. (If the architecture is already documented, then the reviews can use it to
assess for themselves how it satisfies the scenario.) The reviewers ask questions to determine
two different types of information. First, they want to determine that the scenario is, in fact,
satisfied. Second, they want to determine whether any of the other scenarios being considered
will not be satisfied because of the decisions made in the portion of the architecture being
reviewed.
4. Potential problems are captured. The list of potential problems forms the basis for the
follow-up of the review. If the potential problem is a real problem, then it either must be
fixed or a decision must be explicitly made by the designers and the project manager that they
are willing to accept the problem and its probability of occurrence.
If the designers are using the ADD process described in Chapter 1 7, then a peer review can be done
at the end of step 3 of each ADD iteration.
Analysis by Outsiders
Outside evaluators can cast an objective eye on an architecture. "Outside" is relative; this may mean
outside the development project, outside the business unit where the project resides but within the same
company; or outside the company altogether. To the degree that evaluators are "outside," they are less
likely to be afraid to bring up sensitive problems, or problems that aren't apparent because of
organizational culture or because "we've always done it that way."
Often, outsiders are chosen because they possess specialized knowledge or experience, such as
knowledge about a quality attribute that's important to the system being examined, or long experience
in successfully evaluating architectures.
Also, whether justified or not, managers tend to be more inclined to listen to problems uncovered by
an outside team hired at considerable cost. (This can be understandably frustrating to project staff who
may have been complaining about the same problems to no avail for months.)
In principle, an outside team may evaluate a completed architecture, an incomplete architecture, or a
portion of an architecture. In practice, because engaging them is complicated and often expensive, they
tend to be used to evaluate complete architectures.
Contextual Factors
For peer reviews or outside analysis, there are a number of contextual factors that must be considered
when structuring an evaluation. These include the artifacts available, whether the results are public or
private, the number and skill of evaluators, the number and identity of the participating stakeholders,
and how the business goals are understood by the evaluators.
• What artifacts are available? To perform an architectural evaluation, there must be an artifact
that describes the architecture. This must be located and made available. Some evaluations may
take place after the system is operational. In this case, recovery tools as described in Chapter 20
may be used both to assist in discovering the architecture and to test that the as-built system
conforms to the as-designed system.
·
·
·- 0 A a
• Who sees the results ? Some evaluations are performed with the full knowledge and
participation of all of the stakeholders. Others are performed more privately. The private
evaluations tnay be done for a variety of reasons, ranging from corporate culture to (in one case
we know about) an executive wanting to determine which of a collection of competitive
systems he should back in an internal dispute about the systems.
• Who performs the evaluation? Evaluations can be carried out by an individual or a team. In
either case, the evaluator(s) should be highly skilled in the domain and the various quality
attributes for which the system is to be evaluated. And for carrying out evaluation methods with
extensive stakeholder involvement, excellent organizational and facilitation skills are a must.
• Which stakeholders will participate? The evaluation process should provide a method to elicit
the goals and concerns that the important stakeholders have regarding the system. Identifying
the individuals who are needed and assuring their participation in the evaluation is critical.
• What are the business goals? The evaluation should answer whether the system will satisfy the
business goals. If the business goals are not explicitly captured and prioritized prior to the
evaluation, then there should be a portion of the evaluation dedicated to doing so.
21.2. The Architecture Tradeoff Analysis Method
·
·
·- 0 A a
The Architecture Tradeoff Analysis Method (ATAM) has been used for over a decade to evaluate
software architectures in domains ranging from automotive to financial to defense. The AT AM is
designed so that evaluators need not be familiar with the architecture or its business goals, the system
need not yet be constructed, and there may be a large number of stakeholders.
Participants in the AT AM
The ATAM requires the participation and mutual cooperation of three groups:
• The evaluation team. This group is external to the project whose architecture is being
evaluated. It usually consists of three to five people. Each member of the team is assigned a
number of specific roles to play during the evaluation. (See Table 2 1 . 1 for a description of these
roles, along with a set of desirable characteristics for each. A single person may adopt several
roles in an ATAM.) The evaluation team may be a standing unit in which architecture
evaluations are regularly performed, or its members may be chosen from a pool of
architecturally savvy individuals for the occasion. They may work for the same organization as
the development team whose architecture is on the table, or they may be outside consultants. In
any case, they need to be recognized as competent, unbiased outsiders with no hidden agendas
or axes to grind.
Table 2 1 . 1 . ATAM Evaluation Team Roles
Ro·le 1Responsibilities
Team Leader Sets u p the evaluation; coordinates with client, making sure dient's
needs are met; establishes evaluation contract; forms evaluation
team; sees that final report fS produced and delivered (although the
writing may be delegated)
Evaluation Runs evaluation; facilitates elicitation of scenarios; administers
Lead·er scenario selection/prioritization process; facilitates evaluation ot scenarios
against architecture; facilitates on-site analysis
Scenario Writes sc·enarios on flipchart or whi'feboard during scenario elici1ation;
Scribe captures agreed-on wording of each scenario, halting discussion un1il
exact wording is captured
Proceedings Captures proceedings in electronic form on laptop or workstation:
Scribe raw scenario·s, issue(s) that motivate each scenario (often lost in the
wording of the scenario itself}1 and resolution of each scenario when
applied to architecture(s); also generates a printed list of adopted
scenarios for handout to all participants
Questioner Raises issues of architectural interest, usually related' to the quality
attributes in which he or she has expertise
• Project decision makers. These people are empowered to speak for the development project or
have the authority to mandate changes to it. They usually include the project manager, and if
there is an identifiable customer who is footing the bill for the development, he or she may be
present (or represented) as well. The architect is always included a cardinal rule of
architecture evaluation is that the architect must willingly participate.
·
·
·- 0 A a
• Architecture stakeholders. Stakeholders have a vested interest in the architecture performing as
advertised. They are the ones whose ability to do their job hinges on the architecture promoting
modifiability, security, high reliability, or the like. Stakeholders include developers, testers,
integrators, maintainers, performance engineers, users, builders of systems interacting with the
one under consideration, and others listed in Chapter 3 . Their job during an evaluation is to
articulate the specific quality attribute goals that the architecture should meet in order for the
system to be considered a success. A rule of thumb and that is all it is is that you should
expect to enlist 1 2 to 1 5 stakeholders for the evaluation of a large enterprise-critical
architecture. Unlike the evaluation team and the project decision makers, stakeholders do not
participate in the entire exercise.
Outputs of the AT AM
As in any testing process, a large benefit derives from preparing for the test. In preparation for an
ATAM exercise, the project's decision makers must prepare the following:
1 . A concise presentation of the architecture. One of the requirements of the ATAM is that the
architecture be presented in one hour, which leads to an architectural presentation that is both
concise and, usually, understandable.
2. Articulation of the business goals. Frequently, the business goals presented in the ATAM are
being seen by some of the assembled participants for the first time, and these are captured in
the outputs. This description of the business goals survives the evaluation and becomes part
of the project's legacy.
The AT AM uses prioritized quality attribute scenarios as the basis for evaluating the architecture,
and if those scenarios do not already exist (perhaps as a result of a prior requirements capture exercise
or ADD activity), they are generated by the participants as part of the ATAM exercise. Many times,
AT AM participants have told us that one of the most valuable outputs of AT AM is this next output:
3. Prioritized quality attribute requirements expressed as quality attribute scenarios. These
quality attribute scenarios take the form described in Chapter 4. These also survive past the
evaluation and can be used to guide the architecture's evolution.
The primary output of the AT AM is a set of issues of concern about the architecture. We call
these risks:
4. A set of risks and nonrisks. A risk is defined in the AT AM as an architectural decision that
may lead to undesirable consequences in light of stated quality attribute requirements.
Similarly, a nonrisk is an architectural decision that, upon analysis, is deemed safe. The
identified risks form the basis for an architectural risk mitigation plan.
5. A set of risk themes. When the analysis is complete, the evaluation teatn examines the full set
of discovered risks to look for overarching themes that identify systetnic weaknesses in the
architecture or even in the architecture process and team. If left untreated, these risk themes
will threaten the project's business goals.
Finally, along the way, other information about the architecture is discovered and captured:
6. Mapping of architectural decisions to quality requirements. Architectural decisions can be
·
·
·- 0 A a
interpreted in terms of the qualities that they support or hinder. For each quality attribute
scenario examined during an AT AM, those architectural decisions that help to achieve it are
detennined and captured. This can serve as a statement of rationale for those decisions.
7. A set of identified sensitivity and tradeoff points. These are architectural decisions that have a
marked effect on one or more quality attributes.
The outputs of the AT AM are used to build a final written report that recaps the method,
summarizes the proceedings, captures the scenarios and their analysis, and catalogs the findings.
There are intangible results of an AT AM-based evaluation. These include a palpable sense of
community on the part of the stakeholders, open communication channels between the architect and the
stakeholders, and a better overall understanding on the part of all participants of the architecture and its
strengths and weaknesses. While these results are hard to measure, they are no less important than the
others and often are the longest-lasting.
Phases of the AT AM
Activities in an AT AM-based evaluation are spread out over four phases:
• In phase 0, "Partnership and Preparation," the evaluation team leadership and the key project
decision makers informally meet to work out the details of the exercise. The project
representatives brief the evaluators about the project so that the team can be supplemented by
people who possess the appropriate expertise. Together, the two groups agree on logistics, such
as the time and place of meetings, who brings the flipcharts, and who supplies the donuts and
coffee. They also agree on a preliminary list of stakeholders (by name, not just role), and they
negotiate on when the final report is to be delivered and to whom. They deal with formalities
such as a statement of work or nondisclosure agreements. The evaluation team examines the
architecture documentation to gain an understanding of the architecture and the major design
approaches that it comprises. Finally, the evaluation team leader explains what information the
manager and architect will be expected to show during phase 1 , and helps the1n construct their
presentations if necessary.
• Phase 1 and phase 2 are the evaluation phases, where everyone gets down to the business of
analysis. By now the evaluation team will have studied the architecture documentation and will
have a good idea of what the system is about, the overall architectural approaches taken, and
the quality attributes that are of paramount importance. During phase 1 , the evaluation team
meets with the project decision makers (for one to two days) to begin information gathering and
analysis. For phase 2, the architecture 's stakeholders join the proceedings and analysis
continues, typically for two days. Unlike the other phases, phase 1 and phase 2 comprise a set
of specific steps; these are detailed in the next section.
• Phase 3 is follow-up, in which the evaluation team produces and delivers a written final report.
It is first circulated to key stakeholders to make sure that it contains no errors of understanding,
and after this review is complete it is delivered to the person who commissioned the evaluation.
Table 2 1 .2 shows the four phases of the AT AM, who participates in each one, and an approximate
timetable.
Phase
0
1
2
3
·
·
·- 0
Table 2 1 .2. ATAM Phases and Their Characteristics
Activity Partie ipants Typical Duration
Partnership and Evaluation team lead- Proceeds rnformally as
preparation ership and key project required, perhaps over
decision makers a few weeks
EvaJuation Evaluation team and 1-2 days ·tollowed by a
proj,ect decision makers hiatus ot 1-3 we·eks
EvaJuation Evaluation team, project 2 days
(continued) decision makers, and
stakeholders
Follow-up Evaluation team and 1 week
evaluation client
Source: Adapted from [Clements O l b].
Steps of the Evaluation Phases
A a
The ATAM analysis phases (phase 1 and phase 2) consist of nine steps. Steps 1 through 6 are carried
out in phase 1 with the evaluation team and the project's decision makers: typically, the architecture
team, project manager, and project sponsor. In phase 2, with all stakeholders present, steps 1 through 6
are summarized and steps 7 through 9 are carried out.
Table 2 1 .3 shows a typical agenda for the first day of phase 1 , which covers steps 1 through 5 . Step
6 in phase 1 is carried out the next day.
Table 2 1 .3. Agenda for Day 1 of the AT AM
Time
0830 - 1 000
1000 - 1 100
1100 - 1.130
1130 - 1:230
1230 - 1330
1330 - 1430
1430 - 1 530
1530 - 1600
1600 - 1700
Step 1 : Present the AT AM
Activity
Introductions; Step 1 : Present the ATAM
Step 2 : Present Business Drivers
Break
Step 3: Present Architecture
Lunch
Step 4: Identify Architectural Approaches
Step 5: Generate Utility Tree
Break
Step 5: Generate Utility Tree (continued)
The first step calls for the evaluation leader to present the AT AM to the assembled project
representatives. This time is used to explain the process that everyone will be following, to answer
questions, and to set the context and expectations for the remainder of the activities. Using a standard
presentation, the leader describes the AT AM steps in brief and the outputs of the evaluation.
Step 2: Present the Business Drivers
Everyone involved in the evaluation the project representatives as well as the evaluation team
members needs to understand the context for the system and the primary business drivers motivating
·
·
·- 0 A a
its development. In this step, a project decision maker (ideally the project manager or the system's
customer) presents a system overview from a business perspective. The presentation should describe the
following:
• The system' s most important functions
• Any relevant technical, managerial, economic, or political constraints
• The business goals and context as they relate to the project
• The major stakeholders
• The architectural drivers (that is, the architecturally significant requirements)
Step 3 : Present the Architecture
Here, the lead architect (or architecture team) makes a presentation describing the architecture at an
appropriate level of detail. The "appropriate level" depends on several factors: how much of the
architecture has been designed and documented; how much time is available; and the nature of the
behavioral and quality requirements.
In this presentation the architect covers technical constraints such as operating system, hardware, or
middleware prescribed for use, and other systems with which the system must interact. Most important,
the architect describes the architectural approaches (or patterns, or tactics, if the architect is fluent in
that vocabulary) used to meet the requirements.
To make the most of limited time, the architect's presentation should have a high signal-to-noise
ratio. That is, it should convey the essence of the architecture and not stray into ancillary areas or delve
too deeply into the details of just a few aspects. Thus, it is extremely helpful to brief the architect
beforehand (in phase 0) about the information the evaluation team requires. A template such as the one
in the sidebar can help the architect prepare the presentation. Depending on the architect, a dress
rehearsal can be included as part of the phase 0 activities.
Architecture Presentation (Approximately 20 slides; 60 Minutes)
Driving architectural requirements, the measurable quantities you associate with these
requirements, and any existing standards/models/approaches for meeting these (2-3
slides)
Important architectural information ( 4-8 slides):
• Context diagram the system within the context in which it will exist. Humans or
other systems with which the system will interact.
• Module or layer view the modules (which may be subsystems or layers) that
describe the system's decomposition of functionality, along with the objects,
procedures, functions that populate these, and the relations among them (e.g.,
procedure call, method invocation, callback, containment).
• Component-and-connector view processes, threads along with the synchronization,
data flow, and events that connect them.
• Deployment view CPUs, storage, external devices/sensors along with the networks
·
·
·- 0 A a
and communication devices that connect them. Also shown are the processes that
execute on the various processors.
Architectural approaches, patterns, or tactics employed, including what quality
attributes they address and a description of how the approaches address those attributes
(3-6 slides):
• Use of commercial off-the-shelf (COTS) products and how they are
chosen/integrated ( 1-2 slides).
• Trace of 1 to 3 of the most important use case scenarios. If possible, include the
runtime resources consumed for each scenario (1-3 slides).
• Trace of 1 to 3 of the most important change scenarios. If possible, describe the
change impact (estimated size/ difficulty of the change) in terms of the changed
modules or interfaces (1-3 slides).
• Architectural issues/risks with respect to meeting the driving architectural
requirements (2-3 slides).
• Glossary ( 1 slide).
Source: Adapted from [Clements 0 1 b].
As 1nay be seen in the presentation template, we expect architectural views, as described in Chapters
1 and 1 8 , to be the primary vehicle for the architect to convey the architecture. Context diagrams,
component-and-connector views, module decomposition or layered views, and the deployment view are
useful in almost every evaluation, and the architect should be prepared to show them. Other views can
be presented if they contain information relevant to the architecture at hand, especially information
relevant to achieving important quality attribute goals.
As a rule of thumb, the architect should present the views that he or she found most important
during the creation of the architecture and the views that help to reason about the most important quality
attribute concerns of the system.
During the presentation, the evaluation team asks for clarification based on their phase 0
examination of the architecture documentation and their knowledge of the business drivers from the
previous step. They also listen for and write down any architectural tactics or patterns they see
employed.
Step 4: Identify Architectural Approaches
The ATAM focuses on analyzing an architecture by understanding its architectural approaches. As we
saw in Chapter 1 3, architectural patterns and tactics are useful for (among other reasons) the known
ways in which each one affects particular quality attributes. A layered pattern tends to bring portability
and maintainability to a system, possibly at the expense of performance. A publish-subscribe pattern is
scalable in the number of producers and consumers of data. The active redundancy tactic pr01notes high
availability. And so forth.
By now, the evaluation team will have a good idea of what patterns and tactics the architect used in
·
·
·- 0 A a
designing the systetn. They will have studied the architecture documentation, and they will have heard
the architect's presentation in step 3 . During that step, the architect is asked to explicitly name the
patterns and tactics used, but the team should also be adept at spotting ones not mentioned.
In this short step, the evaluation team simply catalogs the patterns and tactics that have been
identified. The list is publicly captured by the scribe for all to see and will serve as the basis for later
analysis.
Step 5 : Generate Quality Attribute Utility Tree
In this step, the quality attribute goals are articulated in detail via a quality attribute utility tree. Utility
trees, which were described in Chapter 1 6, serve to make the requirements concrete by defining
precisely the relevant quality attribute requirements that the architects were working to provide.
The important quality attribute goals for the architecture under consideration were named in step 2,
when the business drivers were presented, but not to any degree of specificity that would permit
analysis. Broad goals such as "modifiability" or "high throughput" or "ability to be ported to a number
of platforms" establish important context and direction, and provide a backdrop against which
subsequent information is presented. However, they are not specific enough to let us tell if the
architecture suffices. Modifiable in what way? Throughput that is how high? Ported to what platforms
and in how much time?
In this step, the evaluation team works with the project decision makers to identify, prioritize, and
refme the system's most important quality attribute goals. These are expressed as scenarios, as
described in Chapter 4, which populate the leaves of the utility tree.
Step 6: Analyze Architectural Approaches
Here the evaluation team examines the highest-ranked scenarios (as identified in the utility tree) one at
a time; the architect is asked to explain how the architecture supports each one. Evaluation team
members especially the questioners probe for the architectural approaches that the architect used to
carry out the scenario. Along the way, the evaluation team documents the relevant architectural
decisions and identifies and catalogs their risks, nonrisks, sensitivity points, and tradeoffs. For wellknown
approaches, the evaluation team asks how the architect overcame known weaknesses in the
approach or how the architect gained assurance that the approach sufficed. The goal is for the
evaluation team to be convinced that the instantiation of the approach is appropriate for meeting the
attribute-specific requirements for which it is intended.
Scenario walkthrough leads to a discussion of possible risks, nonrisks, sensitivity points, or tradeoff
points. For example:
• The frequency of heartbeats affects the time in which the system can detect a failed c01nponent.
S01ne assignments will result in unacceptable values of this response these are risks.
• The number of simultaneous database clients will affect the number of transactions that a
database can process per second. Thus, the assignment of clients to the server is a sensitivity
point with respect to the response as measured in transactions per second.
• The frequency of heartbeats determines the time for detection of a fault. Higher frequency leads
to improved availability but will also consume more processing time and communication
·
·
·- 0
bandwidth (potentially leading to reduced performance). This is a tradeoff.
A a
These, in tum, may catalyze a deeper analysis, depending on how the architect responds. For
example, if the architect cannot characterize the number of clients and cannot say how load balancing
will be achieved by allocating processes to hardware, there is little point in a sophisticated performance
analysis. If such questions can be answered, the evaluation team can perform at least a rudimentary, or
back-of-the-envelope, analysis to determine if these architectural decisions are problematic vis-a-vis the
quality attribute requirements they are meant to address.
The analysis is not meant to be comprehensive. The key is to elicit sufficient architectural
information to establish some link between the architectural decisions that have been made and the
quality attribute requirements that need to be satisfied.
Figure 2 1 . 1 shows a template for capturing the analysis of an architectural approach for a scenario.
As shown, based on the results of this step, the evaluation team can identify and record a set of
sensitivity points and tradeoffs, risks, and nonrisks.
Scenario #; A 1' 2
Attribute(s)
Environment
Stimulus
Response
·
·
·- 0
Scenario: Detect and recover from HW failure
of main switch,
Availability
Normal operations
One of the CPUs fai􀆀s
0.999999 a.vaila,bility of switch
Architectural decisions Sensitjvity Tradeoff Risk Nonrisk
Backup CPU{s) S2 R8
No backup data onannel S3 T3 R9
Watchdog� S4 N 1 2
Heartbeat 85 N 1 3
Failover routing S6 N 1 4
Reasoning Ensures no comrnon mode failure by using different hardware
and operating system (see Risk 8)
Worst .. cas,e rollover is accomplished in 4 seconds as computing
state takes that long at worst
Guaranteed to detect failure within 2 seconds based on rates of
heartbeat and watchdog
Watchdog ls simple and has proved reliable
Availability requirement might be at risk due to 􀌀ack of backup
data channel . . • (see Risk 9)
Architecture
diagram Primar
CPU
(081 )
1 heartbeat Switch
1 ( 1 sec.) CPU
" (081)
Backup
CPU with
Watchdog
(OS2)
A a
Figure 21.1. Example of architecture approach analysis (adapted from [Clements 01b])
At the end of step 6, the evaluation team should have a clear picture of the most important aspects
of the entire architecture, the rationale for key design decisions, and a list of risks, nonrisks, sensitivity
points, and tradeoff points.
At this point, phase 1 is concluded.
Hiatus and Start of Phase 2
The evaluation team summarizes what it has learned and interacts informally (usually by phone) with
the architect during a hiatus of a week or two. More scenarios might be analyzed during this period, if
desired, or questions of clarification can be resolved.
·
·
·- 0 A a
Phase 2 is attended by an expanded list of participants with additional stakeholders attending. To
use an analogy from programming: Phase 1 is akin to when you test your own program, using your own
criteria. Phase 2 is when you give your program to an independent quality assurance group, who will
likely subject your program to a wider variety of tests and environments.
In phase 2, step 1 is repeated so that the stakeholders understand the method and the roles they are
to play. Then the evaluation leader recaps the results of steps 2 through 6, and shares the current list of
risks, nonrisks, sensitivity points, and tradeoffs. Now the stakeholders are up to speed with the
evaluation results so far, and the remaining three steps can be carried out.
Step 7: Brainstorm and Prioritize Scenarios
In this step, the evaluation team asks the stakeholders to brainstorm scenarios that are operationally
meaningful with respect to the stakeholders' individual roles. A maintainer will likely propose a
modifiability scenario, while a user will probably come up with a scenario that expresses useful
functionality or ease of operation, and a quality assurance person will propose a scenario about testing
the system or being able to replicate the state of the system leading up to a fault.
While utility tree generation (step 5) is used primarily to understand how the architect perceived and
handled quality attribute architectural drivers, the purpose of scenario brainstorming is to take the pulse
of the larger stakeholder community: to understand what system success means for them. Scenario
brainstorming works well in larger groups, creating an atmosphere in which the ideas and thoughts of
one person stimulate others' ideas.
Once the scenarios have been collected, they must be prioritized, for the same reasons that the
scenarios in the utility tree needed to be prioritized: the evaluation team needs to know where to devote
its limited analytical time. First, stakeholders are asked to merge scenarios they feel represent the same
behavior or quality concern. Then they vote for those they feel are most important. Each stakeholder is
allocated a number of votes equal to 30 percent of the number of scenarios,l rounded up. So, if there
were 40 scenarios collected, each stakeholder would be given 1 2 votes. These votes can be allocated in
any way that the stakeholder sees fit: all 1 2 votes for 1 scenario, 1 vote for each of 1 2 distinct scenarios,
or anything in between.
1. This is a common facilitated brainstorming technique.
The list of prioritized scenarios is compared with those from the utility tree exercise. If they agree, it
indicates good alignment between what the architect had in mind and what the stakeholders actually
wanted. If additional driving scenarios are discovered and they usually are this may itself be a risk,
if the discrepancy is large. This would indicate that there was some disagreement in the system's
important goals between the stakeholders and the architect.
Step 8: Analyze Architectural Approaches
After the scenarios have been collected and prioritized in step 7, the evaluation team guides the
architect in the process of carrying out the highest ranked scenarios. The architect explains how relevant
architectural decisions contribute to realizing each one. Ideally this activity will be dominated by the
architect's explanation of scenarios in terms of previously discussed architectural approaches.
In this step the evaluation team performs the same activities as in step 6, using the highest-ranked,
newly generated scenarios.
·
·
·- 0
Typically, this step might cover the top five to ten scenarios, as time permits.
Step 9: Present Results
A a
In step 9, the evaluation team groups risks into risk themes, based on some common underlying concern
or systemic deficiency. For example, a group of risks about inadequate or out-of-date documentation
might be grouped into a risk theme stating that documentation is given insufficient consideration. A
group of risks about the system's inability to function in the face of various hardware and/or software
failures might lead to a risk theme about insufficient attention to backup capability or providing high
availability.
For each risk theme, the evaluation team identifies which of the business drivers listed in step 2 are
affected. Identifying risk themes and then relating them to specific drivers brings the evaluation full
circle by relating the final results to the initial presentation, thus providing a satisfying closure to the
exercise. As important, it elevates the risks that were uncovered to the attention of management. What
might otherwise have seemed to a manager like an esoteric technical issue is now identified
unambiguously as a threat to something the manager is on record as caring about.
The collected information from the evaluation is summarized and presented to stakeholders. This
takes the form of a verbal presentation with slides. The evaluation leader recapitulates the steps of the
AT AM and all the information collected in the steps of the method, including the business context,
driving requirements, constraints, and architecture. Then the following outputs are presented:
• The architectural approaches documented
• The set of scenarios and their prioritization from the brainstorming
• The utility tree
• The risks discovered
• The nonrisks documented
• The sensitivity points and tradeoff points found
• Risk themes and the business drivers threatened by each one
" . . . but it was OK."
Years of experience have taught us that no architecture evaluation exercise ever goes
completely by the book. And yet for all the ways that an exercise might go terribly
wrong, for all the details that can be overlooked, for all the fragile egos that can be
bruised, and for all the high stakes that are on the table, we have never had an
architecture evaluation exercise spiral out of control. Every single one has been a
success, as measured by the feedback we gather from clients.
While they all turned out successfully, there were a few memorable cliffhangers.
More than once, we began an architecture evaluation only to discover that the
development organization had no architecture to be evaluated. Sometimes there was a
stack of class diagrams or vague text descriptions masquerading as an architecture.
·
·
·- 0 A a
Once we were promised that the architecture would be ready by the time the exercise
began, but in spite of good intentions, it wasn't. (We weren' t always so prudent about
pre-exercise preparation and qualification. Our current diligence was a result of
experiences like these.) But it was OK. In cases like these, the evaluation's main results
included the articulated set of quality attributes, a "whiteboard" architecture sketched
during the exercise, plus a set of documentation obligations on the architect. In all
cases, the client felt that the detailed scenarios, the analysis we were able to perform on
the elicited architecture, plus the recognition of what needed to be done, more than
justified the exercise.
A couple of times we began an evaluation only to lose the architect in the middle of
the exercise. In one case, the architect resigned between preparation and execution of
the evaluation. This was an organization in turmoil and the architect simply got a better
offer in a calmer environment elsewhere. Normally we don't proceed without the
architect, but it was OK. In this case the architect's apprentice stepped in. A little
additional prework to prepare him, and we were all set. The evaluation went off as
planned, and the preparation that the apprentice did for the exercise helped mightily to
prepare him to step into the architect's shoes.
Once we discovered halfway through an AT AM exercise that the architecture we
had prepared to evaluate was being jettisoned in favor of a new one that nobody had
bothered to mention. During step 6 of phase 1 , the architect responded to a problem
raised by a scenario by casually mentioning that "the new architecture" would not
suffer from that deficiency. Everyone in the room, stakeholders and evaluators alike,
looked at each other in the puzzled silence that followed. "What new architecture?" I
asked blankly, and out it came. The developing organization (a contractor for the U.S.
military, which had commissioned the evaluation), had prepared a new architecture for
the system, to handle the more stringent requirements they knew were coming in the
future. We called a timeout, conferred with the architect and the client, and decided to
continue the exercise using the new architecture as the subject instead of the old. We
backed up to step 3 (the architecture presentation), but everything else on the tablebusiness
drivers, utility tree, scenarios still were completely valid. The evaluation
proceeded as before, and at the conclusion of the exercise our military client was
extremely pleased at the knowledge gained.
In perhaps the most bizarre evaluation in our experience, we lost the architect
midway through phase 2. The client for this exercise was the project manager in an
organization undergoing a massive restructuring. The manager was a pleasant
gentleman with a quick sense of humor, but there was an undercurrent about him that
said he was not to be crossed. The architect was being reassigned to a different part of
the organization in the near future; this was tantamount to being fired from the project,
and the manager said he wanted to establish the quality of the architecture before his
architect's awkward departure. (We didn't find any of this out until after the
evaluation.) When we set up the AT AM exercise, the manager suggested that the j unior
·
·
·- 0 A a
designers attend. "They might learn something," he said. We agreed. As the exercise
began, our schedule (which was very tight to begin with) kept being disrupted. The
manager wanted us to meet with his company's executives. Then he wanted us to have
a long lunch with sotneone who could, he said, give us more architectural insights. The
executives, it turned out, were busy just now, and so could we come back and meet
with them a bit later? By now, phase 2 was thrown off schedule by so much that the
architect, to our horror, had to leave to fly back to his home in a distant city. He was
none too happy that his architecture was going to be evaluated without him. The junior
designers, he said, would never be able to answer our questions. Before his departure,
our team huddled. The exercise seemed to be teetering on the brink of disaster. We had
an unhappy departing architect, a blown schedule, and questionable expertise available.
We decided to split our evaluation team. One half of the team would continue with
phase 2 using the junior designers as our information resource. The second half of the
team would continue with phase 2 by telephone the next day with the architect.
Somehow we would make the best of a bad situation.
Surprisingly, the project manager seemed completely unperturbed by the turn of
events. "It will work out, I'm sure," he said pleasantly, and then retreated to confer with
various vice presidents about the reorganization.
I led the team interviewing the junior designers. We had never gotten a completely
satisfactory architecture presentation from the architect. Discrepancies in the
documentation were met with a breezy "Oh, well, that's not how it really works." So I
decided to start over with AT AM step 3 . We asked the half dozen or so designers what
their view of the architecture was. "Could you draw it?" I asked them. They looked at
each other nervously, but one said, "I think I can draw part of it." He took to the
white board and drew a very reasonable component-and-connector view. Someone else
volunteered to draw a process view. A third person drew the architecture for an
important offline part of the system. Others jumped in to assist.
As we looked around the room, everyone was busy transcribing the whiteboard
pictures. None of the pictures corresponded to anything we had seen in the
documentation so far. "Are these diagrams documented anywhere?" I asked. One of the
designers looked up from his busy scribbling for a moment to grin. "They are now," he
said.
As we proceeded to step 8, analyzing the architecture using the scenarios previously
captured, the designers did an astonishingly good job of working together to answer our
questions. Nobody knew everything, but everybody knew something. Together in a half
day, they produced a clear and consistent picture of the whole architecture that was
much more coherent and understandable than anything the architect had been willing to
produce in two whole days of pre-exercise discussion. And by the end of phase 2, the
design team was transformed. This erstwhile group of information-starved individuals
with limited compartmentalized knowledge became a true architecture team. The
members drew out and recognized each others' expertise. This expertise was revealed
·
·
·- 0 A a
and validated in front of everyone and most important, in front of their project
manager, who had slipped back into the room to observe. There was a look of supreme
satisfaction on his face. It began to dawn on me that you guessed it it was OK.
It turned out that this project manager knew how to manipulate events and people in
ways that would have impressed Machiavelli. The architect's departure was not
because of the reorganization, but merely coincident with it. The project manager had
orchestrated it. The architect had, the manager felt, become too autocratic and
dictatorial, and the manager wanted the j unior design staff to be given the opportunity
to mature and contribute. The architect's mid-exercise departure was exactly what the
project manager had wanted. And the design team's emergence under fire had been the
primary purpose of the evaluation exercise all along. Although we found several
important issues related to the architecture, the project manager knew about every one
of them before we ever arrived. In fact, he made sure we uncovered some of them by a
few discreet remarks during breaks or after a day's session.
Was this exercise a success? The client could not have been more pleased. His
instincts about the architecture' s strengths and weaknesses were confirmed. We were
instrumental in helping his design team, which would guide the system through the
stormy seas of the company's reorganization, come together as an effective and
cohesive unit at exactly the right time. And the client was so pleased with our final
report that he made sure the company's board of directors saw it.
These cliffhangers certainly stand out in our memory. There was no architecture
documented. But it was OK. It wasn't the right architecture. But it was OK. There was
no architect. But it was OK. The client really only wanted to effect a team
reorganization. In every instance we reacted as reasonably as we could, and each time it
was OK.
Why? Why, time after time, does it tum out OK? I think there are three reasons.
First, the people who have commissioned the architecture evaluation really want it
to succeed. The architect, developers, and stakeholders assembled at the client's behest
also want it to succeed. As a group, they help to keep the exercise marching toward the
goal of architectural insight. Second, we are always honest. If we feel that the exercise
is derailing, we call a timeout and confer among ourselves, and usually confer with the
client. While a small amount of bravado can come in handy during an exercise, we
never, ever try to bluff our way through an evaluation. Participants can detect that
instinctively, and the evaluation team must never lose the respect of the other
participants. Third, the methods are constructed to establish and maintain a steady
consensus throughout the exercise. There are no surprises at the end. The participants
lay down the ground rules for what constitutes a suitable architecture, and they
contribute to the risks uncovered at every step of the way.
So: Do the best job you can. Be honest. Trust the methods. Trust in the goodwill and
good intentions of the people you have assembled. And it will be OK. (Adapted from
[Clements 0 1 b])
·
·
·- 0 A a
-PCC
21.3. Lightweight Architecture Evaluation
·
·
·- 0 A a
Although we attempt to use time in an AT AM exercise as efficiently as possible, it remains a
substantial undertaking. It requires some 20 to 30 person-days of effort from an evaluation team, plus
even more for the architect and stakeholders. Investing this amount of time only makes sense on a large
and costly project, where the risks of making a major mistake in the architecture are unacceptable.
For this reason, we have developed a Lightweight Architecture Evaluation method, based on the
ATAM, for smaller, less risky projects. A Lightweight Architecture Evaluation exercise may take place
in a single day, or even a half-day meeting. It may be carried out entirely by members internal to the
organization. Of course this lower level of scrutiny and objectivity may not probe the architecture as
deeply, but this is a cost/benefit tradeoff that is entirely appropriate for many projects.
Because the participants are all internal to the organization and fewer in number than for the
AT AM, giving everyone their say and achieving a shared understanding takes much less time. Hence
the steps and phases of a Lightweight Architecture Evaluation can be carried out more quickly. A
suggested schedule for phases 1 and 2 is shown in Table 2 1 .4.
Table 21.4. A Typical Agenda for Lightweight Architecture Evaluation
Step
1 : Present 1he ATAM
2: Present Business
Drivers
3: Present Architecture
4: lden1ify Architectural
Approaches
5: Generate Quality
Attribute Utility Tree
0 hrs
0.25 hrs
0.5 hrs
0.25 hrs
Variable
0.5 hrs -
1 . 5 hrs
Notes
The participants are familfar with the process.
This step may be omitted.
The part1cipants are expected to understand
the system and its business goals and their
priorities. Fifteen minutes is allocated for a brief
review to ensure that these are fresh in everyonets
mind and that there are no surprises.
Again, all participants are expected to be familIar
with the system and so a brief overview of
the architecture, using at least module and C&C
views, is presented and 1 to 2 scenarios are
traced through these views.
The architecture approaches tor specific qual'lty
attribute concerns are identified by the architect.
This may be done as a po·rtion of step 3.
Scenarios might exist: part of previous evals,
part of design, part of requirements elicitation.
It you've got 'em, use 'em and make them into a
tree. Half hour. Otherwise, it will take longer.
A utility tree should already exist; the team reviews
the existing tree and updates It, if neededJ
with new scenarios, new response goals, or new
scenario priorities and risk assessments.
6: Analyze Architectural 2-3 hrs
Approaches
7: Brainstorm and 0 hrs
Prioritjze Scenarios
8: Analyze Architectural 0 hrs
Approaches
9: Present Results 0.5 hrs
TOTAL 4-6 hrs
·
·
·- 0
This step-mapping the highly ranked scenarios
onto the architecture-consumes the bulk of
the time and can be expanded or contracted as
needed.
This step can be omitted as the assembled {internal)
stakeholders are expected to contribute
scenarios expre·ssing their concerns in step 5.
This step is also omitted] since aiT analysis is
done in step 6.
At the end of an evaluation, the team reviews
the existing and newly discovered ri,sks, nonrisks,
sensitivi1ies, and tradeoffs and discusses
whether any new risk themes have arisen.
A a
There is no final report, but (as in the regular AT AM) a scribe is responsible for capturing results,
which can then be distributed and serve as the basis for risk remediation.
An entire Lightweight Architecture Evaluation can be prosecuted in less than a day perhaps an
afternoon. The results will depend on how well the assembled team understands the goals of the
method, the techniques of the method, and the system itself. The evaluation team, being internal, is
typically not objective, and this may compromise the value of its results one tends to hear fewer new
ideas and fewer dissenting opinions. But this version of evaluation is inexpensive, easy to convene, and
relatively low ceremony, so it can be quickly deployed whenever a project wants an architecture quality
assurance sanity check.
21.4. Summary
·
·
·- 0 A a
If a system is important enough for you to explicitly design its architecture, then that architecture should
be evaluated.
The number of evaluations and the extent of each evaluation may vary from project to project. A
designer should perform an evaluation during the process of making an important decision. Lightweight
evaluations can be performed several times during a project as a peer review exercise.
The ATAM is a comprehensive method for evaluating software architectures. It works by having
project decision makers and stakeholders articulate a precise list of quality attribute requirements (in the
form of scenarios) and by illuminating the architectural decisions relevant to carrying out each highpriority
scenario. The decisions can then be understood in terms of risks or nonrisks to find any trouble
spots in the architecture.
Lightweight Architecture Evaluation, based on the AT AM, provides an inexpensive, low-ceremony
architecture evaluation that can be carried out in an afternoon.
21.5. For Further Reading
·
·
·-
For a more comprehensive treatment of the ATAM, see [Clements Olb].
0
Multiple case studies of applying the ATAM are available. They can be found by going to
www.sei.cmu.edu/library and searching for "ATAM case study."
A a
To understand the historical roots of the ATAM, and to see a second (simpler) architecture
evaluation method, you can read about the software architecture analysis method (SAAM) in [Kazman
24}.
Several lighter weight architecture evaluation methods have been developed. They can be found in
[Bouwers 1 0], [Kanwal 1 0], and [Bachmann 1 1].
Maranzano et al. have published a paper dealing with a long tradition of architecture evaluation at
AT&T and its successor companies [Maranzano 05].
21.6. Discussion Questions
·
·
·- 0 A a
1 . Think of a software system that you're working on. Prepare a 30-minute presentation on the
business drivers for this system.
2. If you were going to evaluate the architecture for this system, who would you want to participate?
What would be the stakeholder roles and who could you get to represent those roles?
3. Use the utility tree that you wrote for the ATM in Chapter 1 6 and the design that you sketched for
the ATM in Chapter 17 to perform the scenario analysis step of the ATAM. Capture any risks and
nonrisks that you discover. Better yet, perform the analysis on the design carried out by a
colleague.
4. It is not uncommon for an organization to evaluate two competing architectures. How would you
modify the AT AM to produce a quantitative output that facilitates this comparison?
5. Suppose you've been asked to evaluate the architecture for a system in confidence. The architect
isn't available. You aren't allowed to discuss the evaluation with any of the system's
stakeholders. How would you proceed?
6. Under what circumstances would you want to employ a full-strength ATAM and under what
circumstances would you want to employ a Lightweight Architecture Evaluation?
22. Management and Governance
·
·
·-
How does a project get to be a year behind
schedule? One day at a time.
-Fred Brooks
0 A a
In this chapter we deal with those aspects of project management and governance that are important for
an architect to know. The project manager is the person with whom you, as the architect, must work
most closely, from an organizational perspective, and consequently it is important for you to have an
understanding of the project manager's problems and the techniques available to solve those problems.
We will deal with project management from the perspectives of planning, organizing, implementing,
and measuring. We will also discuss various governance issues associated with architecture.
In this chapter, we advocate a middleweight approach to architecture. It has the following aspects:
• Design the software architecture
• Use the architecture to develop realistic schedules
• Use incremental development to get to market quickly
Architecture is most useful in medium- to large-scale projects projects that typically have multiple
teams, too much complexity for any individual to fully comprehend, substantial investment, and
multiyear duration. For such projects the teams need to coordinate, quality attribute problems are not
easily corrected, and management demands both short time to market and adequate oversight.
Lightweight management methods do not provide for a framework to guide team coordination and
frequently require extensive restructuring to repair quality attribute problems. Heavyweight
management is usually associated with heavy oversight and a great emphasis on contractual
commitments. In some contexts, this is unavoidable, but it has an inherent overhead that slows down
development.
22.1. Planning
·
·
·- 0 A a
The planning for a project proceeds over time. There is an initial plan that is necessarily top-down to
convince upper management to build this system and give them some idea of the cost and schedule.
This top-down schedule is inherently going to be incorrect, possibly by large amounts. It does, however,
enable the project manager to educate upper managers as to different elements necessary in software
development. According to Dan Paulish, based on his experience at Siemens Corporation, some rules of
thumb that can be used to estimate the top-down schedule for medium-sized (,...., 150 KSLOC) projects
are these:
• Number of components to be estimated: ,...., 1 5 0
• Paper design time per component: ,4 hours
• Time between engineering releases: ,....,g weeks
• Overall project development allocation:
• 40 percent design: 5 percent architectural, 3 5 percent detailed
• 20 percent coding
• 40 percent testing
Once the system has been given a go-ahead and a budget, the architecture team is fonned and
produces an initial architecture design. The budget item deserves some further mention. One case is that
the budget is for the whole project and includes the schedule as well. We will call this case top-down
planning. The second case is that the budget is just for the architecture design phase. In this case, the
overall project budget emerges from the architecture design phase. This provides a gate that the team
has to pass through and gives the holders of the purse strings a chance to consider whether the value of
the project is worth the cost.
We now describe a merged process that includes both the top-down budget and schedule as well as
a bottom-up budget and schedule that emerges from the architecture design phase.
The architecture team produces the initial architecture design and the release plans for the system:
what features will be released and when the releases will occur. Once an initial architecture design has
been produced, then leads for the various pieces of the project can be assigned and they can build their
teams. The definition of the various pieces of the project and their assignment to teams is sometimes
called the work breakdown structure. At this point, cost and schedule estimates from the team leads and
an overall project schedule can be produced. This bottom-up schedule is usually much more accurate
than the top-down schedule, but there may also be significant differences due to differing assumptions.
These differences between the top-down and bottom-up schedules need to be discussed and sometimes
negotiated. Finally, a software development plan can be written that shows the initial (internal) release
as the architectural skeleton with feature-oriented incremental development after that. The features to be
in each release are developed in consultation with marketing.
Figure 22. 1 shows a process that includes both a top-down schedule and a bottom-up schedule.
Top-Down
Schedu'le
·
·
·- 0
First-Level
Decomposition
'
Bottom-Up
Schedule
....... ---• Reconciliation .._., __ ....
'
Software
Development Plan
Figure 22.1. Overview of planning process
A a
Once the software development plan has been written, the teams can determine times and groups for
integration and can define the coordination needs for the various projects. As we will see in the
subsection on global development in Section 22.2, the coordination needs for distributed teams can be
significant.
22.2. Organizing
·
·
·- 0 A a
Some of the elements of organizing a project are team organization, division of responsibilities between
project manager and software architect, and planning for global or distributed development.
Software Development Team Organization
Once the architecture design is in place, it can be used to define the project organization. Each member
of the team that designed the software architecture becomes the lead for a team whose responsibility is
to implement a portion of the architecture. Thus, responsibility for fleshing out and impletnenting the
design is distributed to those who had a role in its definition.
In addition, many support functions such as writing user documentation, system testing, training,
integration, quality assurance, and configuration management are done in a "matrix" form. That is,
individuals who are "matrixed" report to one person a functional manager for their tasking and
professional advancement and to another individual (or to several individuals) project managers for
their project responsibilities. Matrix organizations have the advantage of being able to allocate and
balance resources as needed rather than assign them permanently to a project that may have sporadic
needs for individuals with particular skills. They have the disadvantage that the people in them tend to
work on several projects simultaneously. This can cause problems such as divided loyalties or
competition among projects for resources.
Typical roles within a software development team are the following:
• Team leader manages tasks within the team.
• Developer designs and implements subsystem code.
• Configuration manager performs regular builds and integration tests. This role can frequently
be shared among multiple software development teams.
• System test manager system test and acceptance testing.
• Product manager represents marketing; defines feature sets and how system being developed
integrates with other systems in a product suite.
Division of Responsibilities between Project Manager and Software Architect
One of the important relations within a team is between the software architect and the project manager.
You can view the project manager as responsible for the external-facing aspects of the project and the
software architect as responsible for the internal aspects of the project. This division will only work if
the external view accurately reflects the internal situation and the internal activities accurately reflect
the expectations of the external stakeholders. That is, the project manager should know, and reflect to
management, the progress and the risks within the project, and the software architect should know, and
reflect to developers, stakeholder concerns. The relationship between the project manager and the
software architect will have a large impact on the success of a project. They need to have a good
working relation and be mindful of the roles they are filling and the boundaries of those roles.
We use the knowledge areas for project management taken from the Project Management Body of
Knowledge (PMBOK) to show the duties of these two roles in a variety of categories. Table 22. 1 (on
·
·
·- 0 A a
the next page) gives the knowledge area in the language of the PMBOK, defines the knowledge area in
English, what that means in software terms, and how the project manager and the software architect
collaborate to satisfy that category.
Table 22.1. Division of Responsibilities between Project Manager and Architect
PMBOK Knowledge Description
Area
Project Integration
Management
Project Scope
Management
Project Time
Management
Project Cost
Management
Project Quality
Management
Project Human
Resource
Management
Project
Communications
Management
Project Risk
Management
Project
Procurement
Management
Ensuring that the
various elements of
the project are properly
coordinated
Ensuring that the pro]·
ect includes all of the
work required and only
the work required
Ensuring that the
project completes in a
timely fashion
Ensuring that the project
is completed within
the required budget
Ensuring that the
project will satisfy the
needs for which it was
undertaken
Ensuring that the
project makes the most
effective use of the
people involved with the
project
Ensuring timely and
appropriate generation,
collection, dissemination,
storage, and
disposition of project
information
Identifying, analyzing,
and responding to
project risk
Acquiring goods and
services from outside
organization
Task
Developing, overseeing,
and updating the
project plan. Managing
change control
process.
Requirements
Work breakdown
structure and oomple·
lion tracking. Project
network diagram with
dates.
Resource planning,
cost estimation, cost
budgeting
Quality and metrics
Managing people and
their careers
Communicating
Risk management
Technology
Project Manager
Organize project, manage
resources, budgets
and schedules.
Define metrics and
metric collection strategy.
Oversee change
control process.
Negotiate project
scope with marketing
and software architect.
Oversee progress
against schedule. Help
define work breakdown
structure. Schedule
coarse activities to
meet deadlines.
Calculate cost to
completion at various
stages. Make deci·
sions regarding build/
buy and allocation of
resources.
Define productivity,
size, and project-level
quality measures.
Map skill sets of people
against required
skill sets. Ensure that
appropriate training is
provided. Monitor and
mentor career paths of
Individuals. Authorize
recruitment.
Manage communication
between team
and external entities.
Report to upper management.
Prioritize risks. Report
risks to management.
Take steps to mitigate
risks.
Procure necessary
resources. Introduce
new technology.
Software Architect
Create, design, and organize
team around design. Manage de·
pendencies. Implement the capture
of the metrics. Orchestrate
requests for changes. Ensure
that appropriate IT infrastructure
exists.
Elicit, negotiate, and review run·
time requirements and generate
development requirements.
Estimate cost, schedule, and risk
of meeting requirements.
Help define work breakdown
structure. Define tracking mea·
sures. Recommend assignment
of resources to software develop·
men! teams.
Gather costs from individual
teams. Make recommendations
regarding build/buy and resource
allocations.
Design for quality and track
system against design. Define
code-level quality metrics.
Deline requIred technical skill
sets. Mentor developers about ca·
reer paths. Recommend training.
Interview candidates.
Ensure communication and
coordination among developers.
Solicit feedback as to progress,
problems, and risks.
Identity and quantify risks. Adjust
architecture and processes to
mitigate risk.
Determine technology require·
ments. Recommend technology.
training, and tools.
Observe in this table that the project manager is responsible for the business side of the projectproviding
resources, creating and managing the budget and schedule, negotiating with marketing,
ensuring quality and the software architect is responsible for the technical side of the projectachieving
quality, determining measures to be used, reviewing requirements for feasibility, generating
develop-time requirements, and leading the development team.
Global Development
Most substantial projects today are developed by distributed teams. In many organizations these teams
are globally distributed. Some reasons for this trend are the following:
• Cost. Labor costs vary depending on location, and there is a perception that moving some
development to a low-cost venue will decrease the overall cost of the project. Experience has
shown that, at least for software development, savings may only be reaped in the long term.
·
·
·- 0 A a
Until the developers in the low-cost venue have a sufficient level of domain expertise and until
the management practices are adapted to compensate for the difficulties of distributed
development, a large atnount of rework must be done, thereby cutting into and perhaps
overwhelming any savings from wages.
• Skill sets and labor availability. Organizations may not be able to hire developers at a single
location: relocation costs are high, the size of the developer pool may be small, or the skill sets
needed are specialized and unavailable in a single location. Developing a system in a
distributed fashion allows for the work to move to where the workers are rather than forcing the
workers to move to the work location.
• Local knowledge of markets. Developers who are developing variants of a system to be sold in
their market have more knowledge about the types of features that are assumed and the types of
cultural issues that may arise.
A consequence of performing global or distributed development is that architecture and allocation
of responsibilities to teams is more important than in co-located development, where all of the
developers are in a single office, or at least in close proximity. For example, assume Module A uses an
interface from Module B. In time, as circumstances change, this interface may need to be modified.
This means the team responsible for Module B must coordinate with the team responsible for Module
A, as indicated in Figure 22.2.
Team A Team B
Coord 􀗔 n a ti c)n
-
- - -..
Dependency
Module B
Figure 22.2. If there is a dependency between Module A and Module B, then the teams must
coordinate to develop and modify the interface.
Methods for coordination include the following:
• Informal contacts. Informal contacts, such as 1neeting at the coffee roo1n or in the hallway, are
only possible if the teatns are co-located.
• Documentation. Documentation, if it is well written, well organized, and properly
disseminated, can be used as a means to coordinate the teams, whether co-located or at a
distance.
·
·
·- 0 A a
• Meetings. Teams can hold meetings, either scheduled or ad hoc and either face to face or
remote, to help bring the team together and raise awareness of issues.
• Electronic communication. Various forms of electronic communication can be used as a
coordination mechanism, such as email, news groups, blogs, and wikis.
The choice of method depends on many factors, including infrastructure available, corporate
culture, language skills, time zones involved, and the number of teams dependent on a particular
module. Until an organization has established a working method for coordinating among distributed
teams, misunderstandings among the teams are going to cause delays and, in some cases, serious
defects in a project.
22.3. Implementing
·
·
·- 0 A a
During the implementation phase of a project, the project manager and architect have a series of
decisions to make. In this section we discuss those involving tradeoffs, incremental development, and
managing risk.
Tradeoffs
From the project manager's perspective, tradeoffs are between quality, schedule, functionality, and cost.
These are the aspects of the project that are important to the external stakeholders, and the external
stakeholders are the project manager's constituency. Which of these aspects is most important depends
on the project context, and one of the project manager's major responsibilities is to make this
determination.
Over time, there is always new functionality that someone wants to have added to the project.
Frequently these requests come from the marketing department. It is important that the consequences of
these new requirements, in terms of cost and schedule, be communicated to all concerned stakeholders.
This is an area where the project manager and the architect must cooperate. What appear to be small
requirements changes from an outsider's perspective can, at times, require major modifications to the
architecture and consequently delay a project significantly.
The project manager's first response to creeping functionality is to resist it. Acting as a gatekeeper
for the project and shielding it from distractions is a portion of the job description. One technique that is
frequently used to manage change is a change control board. Bureaucracy can, at times, be your friend.
Change control boards are committees set up for the purpose of managing change within a project. The
original architecture team members are good candidates to sit on the change control board. Before
changing an interface, for example, the impact on those modules that depend on the interface needs to
be considered.
Any change to the architecture will incur costs, and it is the architect's responsibility to be the
gatekeeper for such changes. A change in the architecture implies changes in code, changes in the
architecture documentation, and perhaps changes in build-time tools that enforce architectural
conformance.
Documentation is especially important in distributed development. Co-located teams have a variety
of informal coordination possibilities, such as going to the next office or meeting in the coffee room or
the hall. Remote teams do not have these informal mechanisms and so must rely on more formal
mechanisms such as documentation; team members must have the initiative to talk to each other when
doubts arise. One company mounted a web cam on each developer's desktop to facilitate personal
communication.
Incremental Development
Recall that the software development plan lays out the overall schedule. Every six to eight weeks a new
release should be available and the specifics of the next release are decided. Forty percent of a typical
project's effort is devoted to testing. This means that testing should begin as soon as possible. Testing
for a release can begin once the forward development has begun on the next release. The schedule also
·
·
·- 0 A a
has to accommodate repairing the faults uncovered by the testing. This leads to a release being in one of
three states:
1 . Planning. This occurs toward the end of the prior development release. Enough of the prior
release must be completed to understand what will be unfinished in that release and must be
carried forward to the next one. At this stage, the software development plan is updated.
2. Development. The planned release is coded. We will discuss below how the project manager
and architect track progress on the release. Daily builds and automated testing can give some
insight into problems during development.
3. Test and repair. The release is tested through exercise of the test plan. In Chapter 1 9 we
described how the architecture can inform the test plan and even obviate the need for certain
types of testing. The problems found during test are repaired or are carried forward to the
next release.
Tracking Progress
The project manager can track progress through personal contact with developers (this tends to not scale
up well), formal status meetings, metrics, and risk management. Metrics will be discussed in the next
section. Personal contact involves checking with key personnel individually to determine progress.
These are one-on-one meetings, either scheduled or unscheduled.
Meetings, in general, are either status or working meetings. The two types of meetings should not be
mixed. In a status meeting, various teams report on progress. This allows for communication among the
teams. Issues raised at status meetings should be resolved outside of these meetings either by
individuals or by separately scheduled working meetings. When an issue is raised at a status meeting, a
person should be assigned to be responsible for the resolution of that issue.
Meetings are expensive. Holding effective meetings is an important skill for a manager, whether the
project manager or the architect. Meetings should have written agendas that are circulated before the
meetings begin, attendees should be expected to have done some prework for the meeting (such as readahead),
and only essential individuals should attend.
One of the outputs of status meetings is a set of risks. A risk is a potential problem together with the
consequences if it occurs. The likelihood of the risk occurring should also be recorded. Risks are also
raised at reviews. We have discussed architecture evaluation, and it is important from a project
managetnent perspective that reviews are included in the schedule. These can be code reviews,
architecture reviews, or requirements reviews. Risks are also raised by developers. They are the ones
who have the best perspective on potential problems at the implementation level. Architecture
evaluation is also important because it is a source of discovering risks.
The project manager must prioritize the risks, frequently with the assistance of the architect, and, for
the most serious risks, develop a mitigation strategy. Mitigating risks is also a cost, and so
implementing the strategy to reduce a risk depends on its priority, likelihood of occurrence, and cost if
it does occur.
22.4. Measuring
·
·
·- 0 A a
Metrics are an important tool for project managers. They enable the manager to have an objective basis
both for their own decision making and for reporting to upper management on the progress of the
project.
Metrics can be global pertaining to the whole project or they may depend on a particular phase
of the project. Another important class of metrics is "cost to complete." We discuss each of these
below.
Global Metrics
Global metrics aid the project manager in obtaining an overall sense of the project and tracking its
progress over time. All global metrics should be updated from time to time as the project proceeds.
First, the project manager needs a measure of project size. The three most common measures of
project size are lines of code, function points, and size of the test suite. None of these is completely
satisfactory as a predictor of cost or effort, but these are the most commonly used size metrics in
practice.
Schedule deviation is another global metric. Schedule deviation is measured by taking the
difference between the planned work time and the actual work time. Once time has passed, it can never
be recovered. If a project falls behind in its schedule, a tradeoff must be made between the aspects we
mentioned before: schedule, quality, functionality, and cost. As the project proceeds, schedule deviation
indicates a failure of estimation or an unforeseen occurrence. By drilling down in this metric and
discovering which teams are slipping, the project manager can decide to reallocate resources, if
necessary.
Developer productivity is another metric that the project manager can track. The project manager
should look for anomalies in the productivity of the developers. An anomaly indicates a potential
problem that should be investigated: perhaps a developer is inadequately trained for a task, or perhaps
the task was improperly estimated. Earned value management is one technique for measuring the
productivity of developers.
Finally, defects should be tracked. Again, anomalies in the number of defects discovered over time
indicate a potential problem that should be investigated. Not only defects but technical debt should be
tracked as well (see Chapter 1 5).
All of these measures have both a historical basis and a project basis. The historical basis is used to
make the initial estimates and then the project basis is used for ongoing management activities.
Phase Metrics and Costs to Complete
Open issues should be kept for each phase. For example, until a design is complete, there are always
open issues. These should be tracked and additional resources allocated if they are not resolved in a
timely fashion. Risks represent open issues that should be tracked, as does the project backlog.
Unmitigated risks from reviews are treated in a similar fashion. We have already discussed risk
managetnent, but one item a project manager can report to upper management is the number of highpriority
risks and the status of their mitigation.
·
·
·- 0 A a
Costs to complete is a bottom-up measure that derives from the bottom-up schedule. Once the
various pieces of the architecture have been assigned to teams, then the teams take ownership of their
schedule and the cost to complete their pieces.
22.5. Governance
·
·
·- 0 A a
Up to this point, we have maintained a focus in this chapter. We have focused on the project manager as
the embodiment of the project and have not discussed the external forces that act on the project
manager. The topic of governance deals directly with these other forces. The Open Group defines
architecture governance as "the practice and orientation by which enterprise architectures and other
architectures are managed and controlled." Implicit in this definition is the idea that the project which
is the focus of this book exists in an organizational context. This context will mediate the interactions
of the system being constructed with the other systems in the organization.
The Open Group goes on to identify four items as responsibilities of a governance board:
• Implementing a system of controls over the creation and monitoring of all architectural
components and activities, to ensure the effective introduction, implementation, and evolution
of architectures within the organization
• Implementing a system to ensure compliance with internal and external standards and
regulatory obligations
• Establishing processes that support effective management of the above processes within agreed
parameters
• Developing practices that ensure accountability to a clearly identified stakeholder community,
both inside and outside the organization
Note the etnphasis on processes and practices. Maintaining an effective governance process without
excessive overhead is a line that is difficult to maintain for an organization.
The problem comes about because each system that exists in an enterprise has its own stakeholders
and its own internal governance processes. Creating a system that utilizes a collection of other systems
raises the issue of who is in control.
Consider the following example. Company A has a collection of products that cover different
portions of a manufacturing facility. One collection of systems manages the manufacturing process,
another manages the processes by which various portions of the end product are integrated, and a third
collection manages the enterprise. Each of these collections of systems has its own set of customers.
Now suppose that the board of directors wishes to market the collection as an end-to-end solution
for a manufacturing facility. It further turns out that the systems that manage the manufacturing process
have a 6-month release cycle because the technology is changing quickly in this area. The systems that
manage the integration process have a 9-month release cycle because they are based on a widely used
commercial product with a 9-month release cycle. The enterprise systems have a 12-month release
cycle because they are based on an organization' s fiscal year and reflect tax and regulatory changes that
are likely to occur on fiscal year boundaries. Table 22.2 shows these schedules beginning at date 0.
Table 22.2. Release Schedules of Different Types of Products
Manufacturing Process Integration Process
·
·
·- 0
Control Control Enterprise Management
Vers􀀲on 1,-date 0 Version 1-dat,e 0 Version 1-date 0
Versfon 2-date 6 months Version 2-date 9 months Version 2-date 1 2 months
Verslon 3-date 1 2 months Version 3-date 1 8 months Version 3-date 24 months
A a
What should the release schedule be for the combined end-to-end solution? Recall that each of these
sets of products has reasons for their release schedule and each has its own set of customers who will
not be receptive to changes in the release schedule. This is typical of the sort of problem that a
governance committee deals with.
22.6. Summary
·
·
·-
A project must be planned, organized, implemented, tracked, and governed.
0 A a
The plan for a project is initially developed as a top-down schedule with an acknowledgement that it
is only an estimate. Once the decomposition of the system has been done, a bottom-up schedule can be
developed. The two must be reconciled, and this becomes the basis for the software development plan.
Teams are created based on the software development plan. The software architect and the project
manager must coordinate to oversee the implementation. Global development creates a need for an
explicit coordination strategy that is based on more formal methods than needed for co-located
development.
The implementation itself causes tradeoffs between schedule, function, and cost. Releases are done
in an incremental fashion and progress is tracked by both formal metrics and informal communication.
Larger systems require formal governance mechanisms. The issue of who has control over a
particular portion of the system may prevent some business goals from being realized.
22.7. For Further Reading
·
·
·- 0
Dan Paulish has written an excellent book on managing in an architecture-centric environment
[Paul ish 02] and much of the material in this chapter is adapted from his book.
A a
The Agile community has independently arrived at a medium-weight management process that
advocates up-front architecture. See [Coplein 1 0] for an example of the Agile community's description
of architecture.
The responsibilities of a governance board can be found in The Open Group Architecture
Framework (TOGAF), produced by the Open Group. www.opengroup.org/architecture/togaf8-
doc/arch/ chap26 .html
Basic concepts of project management are covered in [IEEE 1 1].
Using concepts of lean manufacturing, Kanban is a method for scheduling the production of a
system [Ladas 09].
Earned value management is discussed in en. wikipedia.org/wiki/Eamed _value_ management
22.8. Discussion Questions
·
·
·- 0 A a
1 . What are the reasons that top-down and bottom-up schedule estimates differ and how would you
resolve this conflict in practice?
2. Generic project management practices advocate creating a work breakdown structure as the first
artifact produced by a project. What is wrong with this practice from an architectural perspective?
3. If you were managing a globally distributed team, what architectural documentation artifacts
would you want to create first?
4. If you were managing a globally distributed team, what aspects of project management would
have to change to account for cultural differences?
5. Enumerate three different global metrics and discuss the advantages and disadvantages of each?
6. How could you use architectural evaluation as a portion of a governance plan?
7. In Chapter 1 we described a work assignment structure for software architecture, which can be
documented as a work assignment view. (Because it maps software elements modules to a
nonsoftware environmental structure the organizational units that will be responsible for
implementing the modules it is a kind of allocation view.) Discuss how documenting a work
assignment view for your architecture provides a vehicle for software architects and managers to
work together to staff a project. Where is the dividing line between the part of the work
assignment view that the architect should provide and the part that the manager should provide?
·
·
·- 0
Part Four. Architecture and Business
A a
Perhaps the most important job of an architect is to be a fulcrum where business and technical decisions
meet and interact. The architect is responsible for many aspects of the business, and must be continually
translating business needs and goals into technical realizations, and translating technical opportunities
and limitations into business consequences.
In this section of the book, we explore some of the most important business consequences of
architecture. This includes treating software architecture decisions as business investments, dealing with
the organizational aspects of architecture (such as organizational learning and knowledge management),
and looking at architecture as the key enabler of software product lines.
In Chapter 23 we discuss the economic implications of architectural decisions and provide a method
-called the CBAM, or Cost Benefit Analysis Method for making rational, business-driven
architectural choices. The CBAM builds upon other architecture methods and principles that you have
already seen in this book scenarios, quality attributes, active stakeholder involvement but adds a
new twist to the evaluation of architectural improvements: an explicit consideration of the utility that an
architectural improvement brings.
In Chapter 24 we consider the fact that architectures are created by actual human beings, called
architects working in actual organizations. This chapter considers the questions of how to foster
individual competence that is, how to create competent architects and how to create architecturally
competent organizations.
Finally, in Chapter 25 we look at the concept of software product lines. Not surprisingly, we find
that software architectures are the most important component of software-intensive product lines.
23. Economic Analysis of Architectures
·
·
·- 0
Arthur Dent: "I think we have different value
systems. " Ford Prefect: ''Well mine 's better. "
-Douglas Adams, Mostly Harmless
A a
Thus far, we have been primarily investigating the relationships between architectural decisions and the
quality attributes that the architecture's stakeholders have deemed important: If I make this architectural
decision, what effect will it have on the quality attributes? If I have to achieve that quality attribute
requirement, what architectural decisions will do the trick?
As important as this effort is, this perspective is missing a crucial consideration: What are the
economic implications of an architectural decision?
Usually an economic discussion is focused on costs, primarily the costs of building the system in the
first place. Other costs, often but not always downplayed, include the long-term costs incurred through
cycles of maintenance and upgrade. However, as we argue in this chapter, as important as costs are the
benefits that an architectural decision may bring to an organization.
Given that the resources for building and maintaining a system are finite, there must be a rational
process that helps us choose among architectural options, during both an initial design phase and
subsequent upgrade periods. These options will have different costs, will consume differing amounts of
resources, will implement different features (each of which brings some benefit to the organization),
and will have some inherent risk or uncertainty. To capture these aspects, we need economic models of
software that take into account costs, benefits, risks, and schedule implications.
23.1. Decision-Making Context
·
·
·- 0 A a
As we saw in Chapter 1 6, business goals play a key role in requirements for architectures. Because
major architectural decisions have technical and economic implications, the business goals behind a
software system should be used to directly guide those decisions. The most immediate economic
implication of a business goal decision on an architecture is how it affects the cost of implementing the
system. The quality attributes achieved by the architecture decisions have additional economic
implications because of the benefits (which we call utility) that can be derived from those decisions; for
example, making the system faster or more secure or easier to maintain and update. It is this interplay
between the costs and the benefits of architectural decisions that guides (and tonnents) the architect.
Figure 23 . 1 show this interplay.
Business
Goals
Arch i1ecture
Strategies
Cost
Performance
Secu(ity
Modifiability
Usability
•
•
•
Figure 23.1. Business goals, architectural decisions, costs, and benefits
For example, using redundant hardware to achieve a desired level of availability has a cost;
checkpointing to a disk file has a different cost. Furthermore, both of these architectural decisions will
result in (presumably different) measurable levels of availability that will have some value to the
organization developing the system. Perhaps the organization believes that its stakeholders will pay
more for a highly available system (a telephone switch or medical monitoring software, for example) or
that it will be sued if the system fails (for example, the software that controls antilock brakes in an
automobile).
Knowing the costs and benefits associated with particular decisions enables reasoned selection from
among competing alternatives. The economic analysis does not make decisions for the stakeholders,
just as a financial advisor does not tell you how to invest your money. It simply aids in the elicitation
and documentation of value for cost (VFC): a function of the costs, benefits, and uncertainty of a
"portfolio" of architectural investments. It gives the stakeholders a framework within which they can
apply a rational decision-making process that suits their needs and their risk aversion.
Economic analysis isn't something to apply to every architectural decision, but rather to the most
basic ones that put an overarching architectural strategy in place. It can help you assess the viability of
that strategy. It can also be the key to objective selection among competing strategies, each of which
might have advocates pushing their own self-interests.
23.2. The Basis for the Economic Analyses
·
·
·- 0 A a
We now describe the key ideas that form the basis for the economic analyses. The practical realization
of these ideas can be packaged in a variety of ways, as we describe in Section 23.3. Our goal here is to
develop the theory underpinning a measure of VFC for various architectural strategies in light of
scenarios chosen by the stakeholders.
We begin by considering a collection of scenarios generated as a portion of requirements elicitation,
an architectural evaluation, or specifically for the economic analysis. We examine how these scenarios
differ in the values of their projected responses and we then assign utility to those values. The utility is
based on the importance of each scenario being considered with respect to its anticipated response
value.
Armed with our scenarios, we next consider the architectural strategies that lead to the various
projected responses. Each strategy has a cost, and each impacts multiple quality attributes. That is, an
architectural strategy could be implemented to achieve some projected response, but while achieving
that response, it also affects some other quality attributes. The utility of these "side effects" must be
taken into account when considering a strategy's overall utility. It is this overall utility that we combine
with the project cost of an architectural strategy to calculate a final VFC measure.
Utility-Response Curves
Our economic analysis uses quality attribute scenarios (from Chapter 4) as the way to concretely
express and represent specific quality attributes. We vary the values of the responses, and ask what the
utility is of each response. This leads to the concept of a utility-response curve.
Each scenario's stimulus-response pair provides some utility (value) to the stakeholders, and the
utility of different possible values for the response can be compared. This concept of utility has roots
that go back to the eighteenth century, and it is a technique to make comparable very different concepts.
To help us make major architectural decisions, we might wish to compare the value of high
performance against the value of high modifiability against the value of high usability, and so forth. The
concept of utility lets us do that.
Although sometimes it takes a little prodding to get them to do it, stakeholders can express their
needs using concrete response measures, such as "99.999 percent available." But that leaves open the
question of how much they would value slightly less demanding quality attributes, such as "99. 99
percent available." Would that be almost as good? If so, then the lower cost of achieving that lower
value might make that the preferred option, especially if achieving the higher value was going to play
havoc with another quality attribute like performance. Capturing the utility of alternative responses of a
scenario better enables the architect to make tradeoffs involving that quality attribute.
We can portray each relationship between a set of utility measures and a corresponding set of
response measures as a graph a utility-response curve. Some examples of utility-response curves are
shown in Figure 23 .2. In each, points labeled a, b, or c represent different response values. The utilityresponse
curve thus shows utility as a function of the response value.
1 00 ..---------,
Utility
0 L__ _ L_..I -.L -'
{a)
a b
Quality attribute response
(c)
100 ,--------�-,
Utifity I
I
I
I
0 L.::.......:: ....:::. _.....!.... _...:.... _ J
a b
Quality attribute response
100
I
I
I
I Utility
I I
I I
I I
' I I I
I I I
I I I I
0 • •
(e)
a b c
Ouanty attribute response
·
·
·-
100 .--------,
Utility
(b)
0 �.::.....-:: :::::::::::::: _:::::_j ! l_ j
a b
Quality a #tribute response
100 ,...----------::>1
Utility
0 "--- -....L
a
--
b
.__ .
(d) QuaHty attribute response
Figure 23.2. Some sample utility-response curves
0 A a
The utility-response curve depicts how the utility derived from a particular response varies as the
response varies. As seen in Figure 23.2, the utility could vary nonlinearly, linearly, or even as a step
function. For example, graph (c) portrays a steep rise in utility over a narrow change in a quality
attribute response level. In graph (a), a modest change in the response level results in only a very small
change in utility to the user.
In Section 23.3 we illustrate some ways to engage stakeholders to get them to construct utility
curves.
Weighting the Scenarios
Different scenarios will have different importance to the stakeholders; in order to make a choice of
architectural strategies that is best suited to the stakeholders' desires, we must weight the scenarios. It
does no good to spend a great deal of effort optimizing a particular scenario in which the stakeholders
actually have very little interest. Section 23.3 presents a technique for applying weights to scenarios.
Side Effects
Every architectural strategy affects not only the quality attributes it was selected to achieve, but also
other quality attributes as well. As you know by now, these side effects on other quality attributes are
often negative. If those effects are too negative, we must make sure there is a scenario for the side effect
·
·
·- 0 A a
attribute and determine its utility-response curve so that we can add its utility to the decision-making
mix. We calculate the benefit of applying an architectural strategy by summing its benefits to all
relevant quality attributes; for some quality attributes the benefit of a strategy might be negative.
Determining Benefit and Normalization
The overall benefit of an architectural strategy across quality attribute scenarios is the sum of the utility
associated with each one, weighted by the importance of the scenario. For each architectural strategy i,
its benefit Bi over j scenarios (each with weight Wj) is
Bl. = I.. (b . . X W·) '} l'} J
Referring to Figure 23 .2, each bi,j is calculated as the change in utility (over whatever architectural
strategy is currently in place, or is in competition with the one being considered) brought about by the
architectural strategy with respect to this scenario:
biJ = Uexpected - Ucurrent
That is, the utility of the expected value of the architectural strategy minus the utility of the current
system relative to this scenario.
Calculating Value for Cost
The VFC for each architectural strategy is the ratio of the total benefit, Bi, to the cost, Ci, of
implementing it:
VFC = B· I C· l l
The cost Ci is estimated using a model appropriate for the system and the environment being
developed, such as a cost model that estimates implementation cost by measuring an architecture 's
interaction complexity. You can use this VFC score to rank-order the architectural strategies under
consideration.
Consider curves (a) and (b) in Figure 23.2. Curve (a) flattens out as the quality attribute response
improves. In this case, it is likely that a point is reached past which VFC decreases as the quality
attribute response improves; spending more money will not yield a significant increase in utility. On the
other hand, curve (b) shows that a small improvement in quality attribute response can yield a very
significant increase in utility. In that situation, an architectural strategy whose VFC is low might rank
significantly higher with a modest improvement in its quality attribute response.
23.3. Putting Theory into Practice: The CBAM
·
·
·- 0 A a
With the concepts in place we can now describe techniques for putting them into practice, in the form of
a method we call the Cost Benefit Analysis Method (CBAM). As we describe the method, remember
that, like all of our stakeholder-based methods, it could take any of the forms for stakeholder interaction
that we discussed in the introduction to Part III.
Practicalities of Utility Curve Determination
To build the utility-response curve, we first determine the quality attribute levels for the best-case and
worst-case situations. The best-case quality attribute level is that above which the stakeholders foresee
no further utility. For example, a system response to the user of 0. 1 second is perceived as
instantaneous, so improving it further so that it responds in 0.03 second has no additional utility.
Similarly, the worst-case quality attribute level is a minimum threshold above which a system must
perform; otherwise it is of no use to the stakeholders. These levels best-case and worst-case are
assigned utility values of 100 and 0, respectively. We then determine the current and desired utility
levels for the scenario. The respective utility values (between 0 and 1 00) for various alternative
strategies are elicited from the stakeholders, using the best-case and worst-case values as reference
points. For example, our current design provides utility about half as good as we would like, but an
alternative strategy being considered would give us 90 percent of the maximum utility. Hence, the
current utility level is set to 50 and the desired utility level is set to 90.
In this manner the utility curves are generated for all of the scenarios.
Show Business or Accounting?
As software architects, what kind of business are we in? One of Irving Berlin's most
famous songs is entitled "There's No Business Like Show Business." David Letterman,
riffing off this song title, once quipped, "There's no business like show business, but
there are several businesses like accounting."
How should we think of ourselves, as architects? Consider two more quotations
fro·m famous business leaders:
I never get the accountants in before I start up a business. It's done on gut
feeling.
-Richard Branson
It has been my experience that competency in mathematics, both in numerical
manipulations and in understanding its conceptual foundations, enhances a
person 's ability to handle the more ambiguous and qualitative relationships that
dominate our day-to-day financial decision-making.
-Alan Greenspan
Architectures are at the fulcrum of a set of business, social, and technical decisions.
·
·
·- 0
A poor decision in any dimension can be disastrous for an organization. A decision in
any one dimension is influenced by the other dimensions. So in our roles as architects,
which are we, Alan Greenspan or Richard Branson?
A a
My claim is that, as an industry, we in software are more like Richard Branson. We
make decisions that have enormous economic consequences on a gut feeling, without
ever examining their financial consequences in a disciplined way. This might be OK if
you are an intuitive genius, but it doesn' t work well for most of us. Engineering is
about making good decisions in a rational, predictable way. For this we need methods.
-RK
Practicalities of Weighting Determination
One method of weighting the scenarios is to prioritize them and use their priority ranking as the weight.
So for N scenarios, the highest priority one is given a weight of 1 , the next highest is given a weight of
(N-1 )IN, and so on. This turns the problem of weighting the scenarios into one of assigning priorities.
The stakeholders can determine the priorities through a variety of voting schemes. One simple
method is to have each stakeholder prioritize the scenarios (from 1 to N) and the total priority of the
scenario is the sum of the priorities it receives from all of the stakeholders. This voting can be public or
secret.
Other schemes are possible. Regardless of the scheme used, it must make sense to the stakeholders
and it must suit their culture. For example, in some corporate environments, everything is done by
consensus. In others there is a strict hierarchy, and in still others decisions are made in a democratic
fashion. In the end it is up to the stakeholders to make sure that the scenario weights agree with their
intuition.
Practicalities of Cost Determination
One of the shortcomings of the field of software architecture is that there are very few cost models for
various architectural strategies. There are many software cost models, but they are based on overall
system characteristics such as size or function points. These are inadequate to answer the question of
how much does it cost to, for example, use a publish-subscribe pattern in a particular portion of the
architecture. There are cost models that are based on complexity of modules (by function point analysis
according to the requirements assigned to each module) and the complexity of module interaction, but
these are not widely used in practice. More widely used in practice are corporate cost models based on
previous experience with the same or similar architectures, or the experience and intuition of senior
architects.
Lacking cost models whose accuracy can be assured, architects often tum to estimation techniques.
To proceed, remember that an absolute number for cost isn't necessary to rank candidate architecture
strategies. You can often say something like "Suppose strategy A costs $x. It looks like strategy B will
cost $2x, and strategy C will cost $0.5x." That' s enormously helpful. A second approach is to use very
coarse estimates. Or if you lack confidence for that degree of certainty, you can say something like
·
·
·- 0 A a
"Strategy A will cost a lot, strategy B shouldn't cost very much, and strategy C is probably somewhere
in the middle."
CBAM
Now we describe the method we use for economic analysis: the Cost Benefit Analysis Method. CBAM
has for the most part been applied when an organization was considering a major upgrade to an existing
system and they wanted to understand the utility and value for cost of making the upgrade, or they
wanted to choose between competing architectural strategies for the upgrade. CBAM is also applicable
for new systems as well, especially for helping to choose among competing strategies. Its key concepts
(quality attribute response curves, cost, and utility) do not depend on the setting.
Steps
A process flow diagram for the CBAM is given in Figure 23.3. The first four steps are annotated with
the relative number of scenarios they consider. That number steadily decreases, ensuring that the
method concentrates the stakeholders' time on the scenarios believed to be of the greatest potential in
terms of VFC.
Step 1 : Collate scenarios:
Prioritize to choose top one-third
•
Step 2: Refine scenarios: Determine quality
attribute response levels for best. worst,
current. and desired cases of the scenario
Step 3: Prioritize scenarios:
Eliminate halt of the scenarios
•
Step 4: Assign utility for the current and the
desired levels for each scenario
Step 5: Map architectural strategies to
scenarios and determine quality attribute
response levels
Step 6: Determine the expected utility value
of architectural strategy using interpolation
r
Step 7: Calculate total benefit obtained
trom an architectural strategy
Step 8: Choose architectural strategies
based on ROI subject to cost constraints
Step 9: Confirm results with intuition
·
·
·- 0
N
scenarios
N/3
scenarios
N/3
scenarios
N/6
scenarios
Figure 23.3. Process flow diagram for the CBAM
A a
This description of CBAM assumes that a collection of quality attribute scenarios already exists.
This collection might have come from a previous elicitation exercise such as an AT AM exercise (see
Chapter 2 1) or quality attribute utility tree construction (see Chapter 1 6).
The stakeholders in a CBAM exercise include people who can authoritatively speak to the utility of
various quality attribute responses, and probably include the same people who were the source of the
quality attribute scenarios being used as input. The steps are as follows:
1 . Collate scenarios. Give the stakeholders the chance to contribute new scenarios. Ask the
stakeholders to prioritize the scenarios based on satisfying the business goals of the system.
This can be an informal prioritization using a simple scheme such as "high, medium, low" to
·
·
·-
rank the scenarios. Choose the top one-third for further study.
0 A a
2. Refine scenarios. Refine the scenarios chosen in step 1 , focusing on their stimulus-response
measures. Elicit the worst-case, current, desired, and best-case quality attribute response level
for each scenario. For example, a refined performance scenario might tell us that worst-case
performance for our system's response to user input is 1 2 seconds, the best case is 0. 1
seconds, and our desired response is 0.5 seconds. Our current architecture provides a
response of 1 .5 seconds:
Scenario Worst Case Current
Scenario #17: 12 seconds 1. 5 seconds
Response to
user input
• • •
Desired Best Case
0.5 seconds 0.1 seconds
3. Prioritize scenarios. Prioritize the refined scenarios, based on stakeholder votes. You give
1 00 votes to each stakeholder and have them distribute the votes among the scenarios, where
their voting is based on the desired response value for each scenario. Total the votes and
choose the top 50 percent of the scenarios for further analysis. Assign a weight of 1 . 0 to the
highest-rated scenario; assign the other scenarios a weight relative to the highest rated. This
becomes the weighting used in the calculation of a strategy's overall benefit. Make a list of
the quality attributes that concern the stakeholders.
4. Assign utility. Determine the utility for each quality attribute response level (worst-case,
current, desired, best-case) for the scenarios from step 3. You can conveniently capture these
utility curves in a table (one row for each scenario, one column for each of the four quality
attribute response levels). Continuing our example from step 2, this step would assign utility
values from 1 to 1 00 for each of the latency values elicited for this scenario in step 2:
Scenario Worst Case Current Desired Best Case
Scenario #17: i 2 seconds 1 . 5 seconds 0.5 seconds 0.1 seconds
Response to
user input Utility 5 Utility 50 Utility 80 Utilfty 85
5. Map architectural strategies to scenarios and determine their expected quality attribute
response levels. For each architectural strategy under consideration, determine the expected
quality attribute response levels that will result for each scenario.
6. Determine the utility of the expected quality attribute response levels by interpolation. Using
the elicited utility values (that form a utility curve), determine the utility of the expected
quality attribute response level for the architectural strategy. Do this for each relevant quality
attribute enumerated in step 3. For example, if we are considering a new architectural strategy
that would result in a response time of 0. 7 seconds, we would assign this a utility
proportionately between 50 (which it exceeds) and 80 (which it doesn't exceed).
The formula for interpolation between two data points (xa, Ya) and (xb, Yb) is given by:
·
·
·- 0 A a
For us, the x values are the quality attribute response levels and the y values are the utility
values. So, employing this formula, the utility value of a 0.7-second response time is 74.
7. Calculate the total benefit obtained from an architectural strategy. Subtract the utility value
of the "current" level from the expected level and normalize it using the votes elicited in step
3 . Sum the benefit due to a particular architectural strategy across all scenarios and across all
relevant quality attributes.
8. Choose architectural strategies based on VFC subject to cost and schedule constraints.
Determine the cost and schedule implications of each architectural strategy. Calculate the
VFC value for each as a ratio of benefit to cost. Rank-order the architectural strategies
according to the VFC value and choose the top ones until the budget or schedule is
exhausted.
9. Confirm results with intuition. For the chosen architectural strategies, consider whether these
seem to align with the organization's business goals. If not, consider issues that may have
been overlooked while doing this analysis. If there are significant issues, perfonn another
iteration of these steps.
Computing Benefit for Architectural Variation Points
This chapter is about calculating the cost and benefit of competing architectural
options. While there are plenty of metrics and methods to measure cost, usually as
some function of complexity, benefit is more slippery. CBAM uses an assemblage of
stakeholders to work out jointly what utility each architectural option will bring with it.
At the end of the day, CBAM's measures of utility are subjective, intuitive, and
imprecise. That's all right; stakeholders seldom are able to express benefit any better
than that, and so CBAM takes what stakeholders know and formulates it into a justified
choice. That is, CBAM elicits inputs that are imprecise, because nothing better is
available, and does the best that can be done with the1n.
As a counterpoint to CBAM, there is one area of architecture in which the
architectural options are of a specific variety: product-line architectures. In Chapter 25,
you'll be introduced to software architectures that serve for an entire family of systems.
The architect introduces variation points into the architecture, which are places where it
can be quickly tailored in preplanned ways so that it can serve each of a variety of
different but related products. In the product-line context, the major architectural option
is whether to build a particular variation point in the architecture. Doing so isn't free;
otherwise, every product-line architecture would have an infinitude of variation points.
So the question becomes: When will adding a variation point pay off?
CBAM would work for this case as well; you could ask the product line's
stakeholders what the utility of a new variation point would be, vis-a-vis other options
·
·
·- 0
such as including a different variation point instead, or none at all. But in this case,
there is a quantitative formula to measure the benefit. John McGregor of Clemson
University has long been interested in the economics of software product lines, and
along with others (including me) invented SIMPLE, a cost-modeling language for
software product lines. SIMPLE is great at estimating the cost of product-line options,
but not so great at estimating its benefits. Recently, McGregor took a big step toward
remedying that.
Here is his formula for modeling the marginal value of building in an additional
variation point to the architecture:
T
v;(t,n = max (0, -E .·. c;(r)e'<r-n
T.=t
'JN • .. •
+ P;.r E[ ·. • max ( 0, . · Xu.;( 'l' )e -r( t--1)) l) ...I...
k -r:=T
Got that? No? Right, me neither. But we can understand it if we build it up from
• pieces.
A a
The equation says (as all value equations do) that value is benefit minus cost, and so
those are the two terms.
The first term measures the expected cost of building variation point i over a time
period from now until time T (some far-off horizon of interest such as fielding your
fifth system or getting your next round of venture capital funding). The function ct(r)
measures the cost of building variation point i at time 't; it's evaluated using
conventional cost-estimating techniques for software. The e factor is a standard
economic term that accounts for the net present value of money; r is the assumed
current interest rate. This is summed up over all time between now and T, and made
negative to reflect that cost is negative value. So here's the first term decomposed:
SYMBOLS FOR TIME
T = time variable
t :::: time now
T = target date
T* : modeling limit (􀁜forever)
i = index over variation points
r = assumed interest {ate
Cost spent to build variation point i at time r
Expected cost summed over
all r·etevant time intervals
I T
. . . adjusted by a factor to account
for net present value of money
E L c,.('r:)e-r<<-l}
. < = 1
The second term evaluates the benefit. The function Xi k('t) is the key; it measures
'
the value of the variation point in the kth product of the product line. This is equal to
the marginal value of the variation point to the kth product, minus the cost of using
(exercising) the variation point in that kth product. That first part is the hardest part to
·
·
·- 0 A a
come by, but your marketing department should be able to help by expressing the
marginal value in terms of the additional products that the variation point would enable
you to build and how much revenue each would bring in. (That's what marketing
departments are paid to figure out.)
. . . adjusted by a factor to account
for net present value of money
Xi.f/r:)e -r(r-t)
va1ue of variation point i in product k
at time r= VMP;;;(r) 􀀯 MCulr)
marQ'inal value of the ;rh variation
point in the kt11 product at time T
'
marginal cost of taU'oring variation
point i for use ·in product k
To the function X we apply the same factor as before to account for the net present
value of money, sum it up over all time periods between now and T, and make it
nonnegative:
SYMBOLS FOR TIME
r = time variable
t = time now
T = tar·get date
r = modeling limit (t:=forever)
i = index over variation points
r = assumed interest rate
k = indelC over products
Vatue cannot
be negative
. . . adjusted by a factor to account
for net present value of money
value of variation point ; in product k
at time -r= \1J\1P1..k(r') - MC1.ir)
marginal value of the t1h variatio? point in the k'b product at time T
marginal cost of tailoring variation
point i for use in product k
·
·
·- 0
Then we sutn that up over all products k in the product line and multiply it by the
probability that the variation point will in fact be ready by the time it's needed:
T.
Pn·E[ L max (0 L X;.lr)e -r(r-t))])
probability that variation point i
will be ready tor use by time T
k r=T
value of variation point i in product k over
all time . . .
. . . and over all products
A a
Add the two terms together and there you have it. It's easy to put this in a
spreadsheet that calculates the result given the relevant inputs, and it provides a
quantitative measurement to help guide selection of architectural options in this case,
introduction of variation points to support a product line.
-PCC
23.4. Case Study: The NASA ECS Project
·
·
·- 0
We will now apply the CBAM to a real-world system as an example of the method in action.
A a
The Earth Observing System is a constellation of NASA satellites that gathers data for the U.S.
Global Change Research Program and other scientific communities worldwide. The Earth Observing
System Data Information System (EOSDIS) Core System (ECS) collects data from various satellite
downlink stations for further processing. ECS 's mission is to process the data into higher-form
information and make it available to scientists in searchable form. The goal is to provide both a
common way to store (and hence process) data and a public mechanism to introduce new data formats
and processing algorithms, thus making the information widely available.
The ECS processes an input stream of hundreds of gigabytes of raw environment-related data per
day. The computation of 250 standard "products" results in thousands of gigabytes of information that
is archived at eight data centers in the United States. The system has important performance and
availability requirements. The long-term nature of the project also makes modifiability important.
The ECS project manager had a limited annual budget to maintain and enhance his current system.
From a prior analysis in this case an AT AM exercise a large set of desirable changes to the system
was elicited from the system stakeholders, resulting in a large set of architectural strategies. The
problem was to choose a (much) smaller subset for implementation, as only 1 0 to 20 percent of what
was being proposed could actually be funded. The manager used the CBAM to make a rational decision
based on the economic criterion of return on investment.
In the execution of the CBAM described next, we concentrated on analyzing the Data Access
Working Group (DAWG) portion of the ECS.
Step 1 : Collate Scenarios
A subset of the raw scenarios put forward by the DA WG team were as shown in Table 23 . 1 . Note that
they are not yet well formed and that some of them do not have defined responses. These issues are
resolved in step 2, when the number of scenarios is reduced.l
1. In the presentation of the DA WG case study, we only show the reduced set of scenarios.
Table 23.1. Collected Scenarios in Priority Order
Scenario Scenario Description
·
·
·- 0
1 Reduce data distribution failures that result in hung distribution requests
requiring manual intervention.
2 Reduce data distribution failures that result in lost distribution requests.
3 Reduce the number at orders that fail on the order submission process.
4 Reduce order failures that result in hung orders that require manual intervention.
5 Reduce order failures that r·esult in lost orders.
6 There is no good method of tracking ECSGuest failed/canceled orders without
much manual intervention (e.g. , spreadsheets).
7 Users need more information on why their orders for data failed.
8 Because of limitations, there is a need to artificially limit the size and number of
orders.
9 Small orders result in too many notifications to users.
1 0 The system should process a 50-GB user request in one day, and a 1-TB user
request in one week.
Step 2 : Refine Scenarios
A a
The scenarios were refined, paying particular attention to precisely specifying their stimulus-response
measures. The worst-case, current-case, desired-case, and best-case response goals for each scenario
were elicited and recorded, as shown in Table 23.2.
Table 23.2. Response Goals for Refined Scenarios
Response Goals
Scenario Worst Current Desired Best
1 1 0% hung 5% hung 1 % hung O% hung
2 > 5% lost < 1 % tlost 09/o lost O% lost
3 10% fail 5% fail 1 % fail 0% fail
4 10% hung 5% hung 1 % hung Oo/o hung.
5 1 0% lost < 1 % lost 0% lost 0% lost
6 50% need help 25% need help 0% need help 0% need help
7 1 0% gel 50% get 100% get 100% get
information information information information
8 50% limited 30% limited 0% limited 0% limited
9 1/granule 1/granule 1 / '1 00 granules 1/1 ,000 granules
1 0 < 50% meet goal 60% meet goal 80°/o meet goal > 90% meet goal
Step 3 : Prioritize Scenarios
In voting on the refined representation of the scenarios, the close-knit team deviated slightly from the
method. Rather than vote individually, they chose to discuss each scenario and arrived at a
determination of its weight via consensus. The votes allocated to the entire set of scenarios were
constrained to 1 00, as shown in Table 23.3. Although the stakeholders were not required to make the
votes 1nultiples of 5, they felt that this was a reasonable resolution and that more precision was neither
needed nor justified.
Scenario Votes
q 10
2 15
3 15
4 10
5 15
6 10
7 5
8 5
9 10
1 0 5
Step 4: Assign Utility
·
·
·-
Table 23.3. Refined Scenarios with Votes
Response Goals
Worst Current Desired
10% hung 5% hung 1 '% hung
> 5% lost < 􀋿0/o lost 0% lost
10% fail 5% fail t % tail
10% hung 5% hung 1 % hung
10% lost < 1% lost 0% lost
50% need help 25% need help 0% need help
10% get 50% get 100% get
information information information
50% limited 30% l'imited 0% limited
1/granule 1/granule 1/100 granules
< 50% meet 60% meet goal SO% meet goal
goal
0 A a
Best
0% hung
0% lost
0% fail
O% hung
0% lost
0% need help
100% get
information
0% limited
1 / 1 ,000 granules
> 90% meet goal
In this step the utility for each scenario was determined by the stakeholders, again by consensus. A
utility score of 0 represented no utility; a score of 1 00 represented the most utility possible. The results
of this process are given in Table 23.4.
Table 23.4. Scenarios with Votes and Utility Scores
UtiUty Scores
Scenario Votes Worst Current Desired Best
1 10 10 80 95 100
2 1 5 0 70 100 100
3 1 5 25 70 100 100
4 10 10 80 95 100
5 1 5 0 70 100 100
6 1 0 0 80 100 100
7 5 10 70 100 100
8 5 0 .20 100 100
9 10 50 50 80 90
1 0 5 50 50 80 90
Step 5: Develop Architectural Strategies for Scenarios and Determine Their Expected Quality
Attribute Response Levels
Based on the requirements implied by the preceding scenarios, a set of 1 0 architectural strategies was
developed by the ECS architects. Recall that an architectural strategy may affect more than one
scenario. To account for these complex relationships, the expected quality attribute response level that
·
·
·- 0 A a
each strategy is predicted to achieve had to be determined with respect to each relevant scenario. The
set of architectural strategies, along with the determination of the scenarios they address, is shown in
Table 23.5. For each architectural strategy/scenario pair, the response levels expected to be achieved
with respect to that scenario are shown (along with the current response, for comparison purposes).
Table 23.5. Architectural Strategies and Scenarios Addressed
Scenarios Current Expected
Strategy Name Description Affected Response Response
1 Order persistence on Store an order as soon as it arrives in the system. 3 5% fail 2% FaD
submission 5 <1% lost 0%1ost
6 25% need help 0% need help
2 Order chunking Allow operators to partition large orders into multiple small orders. 8 30% limited 15% limited
3 Order bundling Combine multiple small orders into one large order. 9 1 per granule 1 per 100
10 60% meet goal 55% meet goal
4 Order segmentation Allow an operator to skip items that cannot be retrieved due to data 4 5%hung 2%hung
quality or availability issues.
5 Order reassignment Allow an operator to reassign the media type for items in an order. 1 5%hung 2%hung
6 Order retry Allow an operator to retry an order or items in an order that may 4 5% hung 3%hung
have failed due to temporary system or data problems.
7 Forced order Allow an operator to override an item's unavailability due to data 1 5%hung 3%hung
completion quality constraints.
8 Failed order Ensure that users are notified only when part of their order has truly 6 25% need help 20% need help
notification failed and provide detailed status of each item; user notification
occurs only it operator okays notification; the operator may edit 7 50% get 90%get
notification. information information
9 Granule-level order An operator and user can determine the status tor each item in their 6 25% need help 10% need help
tracking order.
7 50% get 95% get
information information
1 0 links to user An operator can quickly locate a user's contact information. 7 50% get 60%get
information Server will access SDSRV Information to determine any data information information
restrictions that might apply and will route orders/order segments to
appropriate distribution capabilities, including DDIST. PDS, external
subsetters and data processing tools, etc.
Step 6: Determine the Utility of the ''Expected'' Quality Attribute Response Levels by
Interpolation
Once the expected response level of every architectural strategy has been characterized with respect to a
set of scenarios, their utility can be calculated by consulting the utility scores for each scenario's current
and desired responses for all of the affected attributes. Using these scores, we may calculate, via
interpolation, the utility of the expected quality attribute response levels for the architectural
strategy/scenario pair applied to the DA WG of ECS.
Step 7: Calculate the Total Benefit Obtained from an Architectural Strategy
Based on the infonnation collected, as represented in Table 23.6, the total benefit of each architectural
strategy can now be calculated, following the equation from Section 23.2, repeated here:
B l. = "£") . (b l., }. X W·) }
Table 23.6. Architectural Strategies and Their Expected Utility
Scenarios
Strategy Name Affected
1 Order persistence on submission 3
5
6
2 Order chunking 8
3 Order bundling 9
10
4 Order segmentation 4
5 Order reassignment 1
6 Order retry 4
7 Forced order comp·letion 1
8 Failed order notification 6
7
9 Granule-level order tracking 6
7
10 Links to user information 7
·
·
·-
Current
Utility
70
70
80
20
50
70
80
80
80
80
80
70
80
70
70
0 A a
Expected
Utility
90
100
100
60
80
65
90
92
85
87
85
90
90
95
75
This equation calculates total benefit as the sum of the benefit that accrues to each scenario,
normalized by the scenario's relative weight. Using this formula, the total benefit scores for each
architectural strategy are now calculated, and the results are given in Table 23.7.
Table 23.7. Total Benefit of Architectural Strategies
Raw N'ormal ized
Architectural Architectural
Scenario Scenar•o Strategy Strateg:y To·tal Architectural
Strategy Affected Wei·ght Benefit Benefit Strategy Benef·it
1 3 1 5 20 300
1 5 1 5 30 450
1 6 1 0 20 200 950
2 8 5 40 200 200
3 9 1 0 30 300
3 1 0 5 -5 -25 275
4 4 1 0 1 0 100 100
5 1 1 0 12 120 120
6 4 1 0 5 50 50
7 1 1 0 7 70 70
8 6 1 0 5 50
8 7 5 20 100 150
9 6 1 0 10 100
9 7 5 25 125 225
10 7 5 5 25 25
·
·
·- 0
Step 8: Choose Architectural Strategies Based on VFC Subject to Cost Constraints
A a
To complete the analysis, the team estimated cost for each architectural strategy. The estimates were
based on experience with the system, and a return on investment for each architectural strategy was
calculated. Using the VFC, we were able to rank each strategy. This is shown in Table 23.8. Not
surprisingly, the ranks roughly follow the ordering in which the strategies were proposed: strategy 1 has
the highest rank; strategy 3 the second highest. Strategy 9 has the lowest rank; strategy 8, the second
lowest. This simply validates stakeholders' intuition about which architectural strategies were going to
be of the greatest benefit. For the ECS these were the ones proposed first.
Table 23.8. VFC of Architectural Strategies
Strategy Cost Total Strategy Benefit Strategy VFC Strategy Rank
1 1200 950 0.79 1
2 400 200 0.5 3
3 400 275 0.69 2
4 200 100 0.5 3
5 400 120 0.3 7
6 200 50 0.25 8
7 200 70 0.35 6
8 300 150 0.5 3
9 1000 225 0.22 1 0
10 100 25 0.25 8
Results of the CBAM Exercise
The most obvious results of the CBAM are shown in Table 23 .8: an ordering of architectural strategies
based on their predicted VFC. However, just as for the ATAM method, the benefits of the CBAM
extend beyond the qualitative outcomes. There are social and cultural benefits as well.
Just as important as the ranking of architectural strategies in CBAM is the discussion that
accompanies the information-collecting and decision-making processes. The CBAM process provides a
great deal of structure to what is always largely unstructured discussions, where requirements and
architectural strategies are freely mixed and where stimuli and response goals are not clearly articulated.
The CBAM process forces the stakeholders to make their scenarios clear in advance, to assign utility
levels of specific response goals, and to prioritize these scenarios based on the resulting determination
of utility. Finally, this process results in clarification of both scenarios and requirements, which by itself
is a significant benefit.
23.5. Summary
·
·
·- 0 A a
Architecture-based economic analysis is grounded on understanding the utility-response curve of
various scenarios and casting them into a form that makes them comparable. Once they are in this
common form based on the common coin of utility the VFC for each architecture improvement,
with respect to each relevant scenario, can be calculated and compared.
Applying the theory in practice has a number of practical difficulties, but in spite of those
difficulties, we believe that the application of economic techniques is inherently better than the ad hoc
decision-making approaches that projects (even quite sophisticated ones) employ today. Our experience
with the CBAM tells us that giving people the appropriate tools to frame and structure their discussions
and decision making is an enormous benefit to the disciplined development of a complex software
system.
23.6. For Further Reading
·
·
·- 0
The origins of the CBAM can be found in two papers: [Moore 03] and Kazman [0 1].
A a
A more general background in economic approaches to software engineering may be found in the
now-classic book by Barry Boehm [Boehm 8 1].
And a more recent, and somewhat broader, perspective on the field can be found in [Biffl 1 0].
The product-line analysis we used in the sidebar on the value of variation points came from a paper
in the 201 1 International Software Product Line Conference by John McGregor and his colleagues
[McGregor 1 1].
23.7. Discussion Questions
·
·
·- 0 A a
1 . This chapter is about choosing an architectural strategy using rational, economic criteria. See how
many other ways you can think of to make a choice like this. Hint: Your candidates need not be
"rational."
2. Have two or more different people generate the utility curve for a quality attribute scenario for an
ATM. What are the difficulties in generating the curve? What are the differences between the two
curves? How would you reconcile the differences?
3. Discuss the advantages and disadvantages of the method for generating scenario priorities used in
the CBAM. Can you think of a different way to prioritize the scenarios? What are the pluses and
minuses of your method?
4. Using the results of your design exercise for the ATM from Chapter 1 7 as a starting point,
develop an architectural strategy for achieving a quality attribute scenario that your design does
not cover.
5. Generate the utility curves for two different systems in the same domain. What are the
differences? Do you believe that there are standard curves depending on the domain? Defend
your answer.
24. Architecture Competence
·
·
·- 0
The ideal architect should be a man of letters, a
skillful draftsman, a mathematician, familiar with
historical studies, a diligent student of
philosophy, acquainted with music, not ignorant
of medicine, learned in the responses of
jurisconsults, familiar with astronomy and
astronomical calculations.
-Vitruvius, De Architectura (25 B.C.)
The lyf so short, the craft so long to Ierne.
-Geoffrey Chaucer
A a
If software architecture is worth "doing," surely it's worth doing well. Most of the literature about
architecture concentrates on the technical aspects. This is not surprising; it is a deeply technical
discipline. There is little information that speaks to the fact that architectures are created by architects
working in organizations, full of actual human beings. Dealing with these humans is decidedly
nontechnical. What can be done to help architects, especially architects-in-training, be better at this
important dimension of their job? And what can be done to help architecture organizations do a better
job in fostering their architects to produce their best work?
An organization's ability to routinely produce high-quality architectures that are aligned with its
business goals well cannot be understood simply through examination of past architectures and
measurement of their deficiencies. The organizational and human causes of those deficiencies also need
to be understood.
This chapter is about the competence of individual architects and the organizations that wish to
produce high-quality architects. We define the architecture competence of an organization as follows:
The architecture competence of an organization is the ability of that organization to grow,
use, and sustain the skills and knowledge necessary to effectively carry out architecturecentric
practices at the individual, team, and organizational levels to produce architectures
with acceptable cost that lead to systems aligned with the organization's business goals.
Because the architecture competence of an organization depends, in part, on the competence of
architects, we begin by asking what it is that architects are expected to do, know, and be skilled at. Then
we'll look at what organizations can and should do to help their architects produce better architectures.
Individual and organizational competencies are intertwined. Studying only one or the other won't do.
·
·
·- 0 A a
24.1. Competence of Individuals: Duties, Skills, and Knowledge of Architects
Architects perform many activities beyond directly producing an architecture. These activities, which
we call duties, form the backbone of an individual's architecture competence. We surveyed a broad
body of information aimed at architects (such as websites, courses, books, and position descriptions for
architects). We also surveyed practicing architects. These surveys tell us that duties are but one aspect
of individual competence. Writers about architects also speak of skills and knowledge. For example, the
ability to communicate ideas clearly and to negotiate effectively are skills often ascribed to competent
architects. In addition, architects need to have up-to-date knowledge about patterns, database platforms,
web services standards, quality attributes, and a host of other topics.
Duties, skills, and knowledgel form a triad upon which architecture competence for individuals
rests. The relationship among these three is shown in Figure 24. 1 namely, skills and knowledge
support the ability to perform the required duties. Infinitely talented architects are of no use if they
cannot (for whatever reason) perform the duties required of the position; we would not say they were
competent.
1. Some writers speak of the importance of experience. We count experience as a form of knowledge.
Duties
Skills Knowledge
Support
Figure 24. 1 . Skills and knowledge support the execution of duties.
To give examples of these concepts:
• ''Design the architecture" is a duty.
• "Ability to think abstractly" is a skill.
• "Patterns and tactics" is a part of the body of knowledge.
This example purposely illustrates that skills and knowledge are important (only) for supporting the
ability to carry out duties effectively. As another example, "documenting the architecture" is a duty,
"ability to write clearly" is a skill, and "ISO Standard 4201 0" is part of the related body of knowledge.
Of course, a skill or knowledge area can support more than one duty.
Knowing the duties, skills, and know ledge of architects (or, more precisely, the duties, skills, and
knowledge that are needed of architects in a particular organizational setting) can help establish
measurement and improvement strategies for individual architects. If you want to improve your
·
·
·- 0 A a
individual architectural competence, you should do the following:
Duties
1 . Gain experience carrying out the duties. Apprenticeship is a productive path to achieving
experience. Education alone is not enough, because education without on-the-job application
merely enhances knowledge.
2. Improve your nontechnical skills. This dimension of improvement involves taking
professional development courses, for example, in leadership or time management. Some
people will never become truly great leaders or communicators, but we can all improve on
these skills.
3. Master the body of knowledge. One of the most itnportant things a competent architect must
do is master the body of knowledge and retnain up to date on it. To emphasize the importance
of remaining up to date, consider the advances in knowledge required for architects that have
emerged in just the last few years. For example, the cloud and edge computing that we
discuss in Chapters 26 and 21 were not important topics several years ago. Taking courses,
becoming certified, reading books and journals, visiting websites and portals, reading blogs,
attending architecture-oriented conferences, joining professional societies, and meeting with
other architects are all useful ways to improve knowledge.
This section summarizes a wide variety of an architect's duties. Not every architect in every
organization will perform every one of these duties on every project. But a competent architect should
not be surprised to find himself or herself engaged in any of the activities listed here. We divide the
duties into technical duties (Table 24. 1) and nontechnical duties (Table 24.2). One immediate
observation you should make is how many nontechnical duties there are. An obvious implication, for
those of you who wish to be architects, is that you must pay adequate attention to the nontechnical
aspects of your education and your professional activities.
Table 24.1. The Technical Duties of a Software Architect
General Duty
Area
Architecting
Specific Duty
Area
Creating an
arcMitectu re
Evaluating and
analyzing an
architecture
Documenting an
architecture
Working with
and transforming
existing
system(s)
Performing other
architecting
duties
Example
Duties
·
·
·- 0
Design or select an arch.itecture. Create software
archit·ecture design pran. Build product line or product
architecture. Make design decisions. Expand detail
and reflne design to converg.e on fi11al design. Identity
the patterns and tactics and articulate the principles
and key mechanisms of the architecture. Partition the
system. Define how the components fit together and
interact.
Evaluate an architecture (tor your current system
or for other systems) to determine sati>sfaction of
use cases and quality attribute scenarios. Create
prototypes. Participate in design reviews. Review
construction-level designs. Review the designs
of the components designed by junior engineers.
Revrew designs for compliance with the architecture.
Compare software architecture evaluation
techniques. Apply value-based architecting
techniques to evaluate architectural decisions. Model
alternatives. Perform tradeoff analysis.
Prepare architectural documents and presentations
useful to stakeholders. Oocument software
interfaces. Produce documentation standards.
Document variability and dynamic behav,ior.
Maintain and evolve existing system and its
architecture. Red.esign existing. architecture{s) for
migration to new technology and p.latforms.
Sell the vision, keep the vision alive. Participate in
product design meetings. Give technical advice
on architecture, design, and development. Provide
architectural guidelines for software design activities.
Lead architecture improvement activities. Participate
in software process definition and improvement.
Define philosophy and principles for global
architecture. Oversee or manage the architecture
definition process. Provide architecture oversight of
software development projects.
A a
Duties
concerned
with life-cycle
activities
other than
architecting
Managing the
requi:rements
Implementing
the product
Testing the
product
Evaluating future
techn ologles
Selecting tools
and technology
·
·
·- 0
Analyze functional and quality attribute software
requirements. Understand business and customer
needs and ensure that the requirements meet
these needs. Capture customer, organizational, and
business re·quirements on the architecture. Create
software speclfications from business requirements.
ArHculate and refine architectural requirements.
Listen to and understand the scope of the project.
Understand the client's key design needs and
expectations.
Produce code. Conduct code reviews. Develop
reusable software components. Analyze, select,
and integrate software components. Set and ensure
adherence to coding guidelines. Recommend
deve􀋽opment methodologies and coding standards.
Monitor, mentor, and review the work of outside
consultants and vendors.
Establish architecture-based testlng procedures.
Support system tasters. Support field testing. Support
bug fixing and maintenance.
Eva.luate and recommend enterprise's software
solutions. Manage the introduction of new software
solutions. Analyze current IT environment and
recommend so:lutions for deficiencies. Work with
vendors to represent organization's requirements
and influence future products. Develop and present
technical white papers.
Perform technical feasibility studies of new
technologies and archttectures. Evaluate commercial
tools and software components from an architectural
perspective. Deve:lop internal technical standards and
contribute to the development of external technic a'!
standards.
Table 24.2. The Nontechnical Duties of a Software Architect
A a
Skills
General Duty
Area
Management
Specific Duty
Area
Managing the
project
Managing the
people
Supporting the
management
Organization and Supporting the
business-related organization
duties
Leadership and
team building
Supporting the
business
Providing
technrcal
leadership
Example
Duties
·
·
·- 0
Help with budgeting and planning. Follow budgetary
constraints. Manage resources. Perform sizing and
estimation. Perform migration planning and risk
assessment. Take care of or oversee configuration
control. Create development schedules. Measure
results using quantitative metrfcs and improve both
personal results and teams' productivity. Identify and
schedule architectural releases.
Build 􀋾·trusted advisor'' relationships. Coordinate.
Motivate. Advocate. Delegate. Act as a supervisor.
Provide feedback on appropriateness and difficulty of
project. Advise the project manager on the tradeoffs
between software design choices and requirements
choices. Provide input to software project manager in
the software project planni·ng and estimation process.
Serve as a ''bridge" between the technical team and
the project manager.
Grow an architecture evaluation capability in the
organization. Review and contribute to research and
development efforts. Participate in the hiring process
for the team. Help with product marketrng. Institute
and oversee cost-effective software architecture
design reviews. He·lp develop intellectual property.
Translate business strategy into technical vfsion and
strategy. Influence the business strategy. Understand
and evaluate business processes. Understand
and communicate the business value of software
architecture. Help the organization meet its business
goals. Understand customer and market trends.
Identify, understand, and resolve business issues.
Align architecture with the business g.oals and
objectives.
Mentor other architects. Produce technology trend
analysis or roadmaps.
Building a team Set team context (vision). Build the architecture
team and align them with the vision. Mentor junior
architects. Educate the team on the use of the
architecture. Maintain morale, both within and outside
the architecture group. Foster the professional
development of team members. Coach teams of
software design engineers for planning, tracking, and
completion of work within the agreed plan. Mentor
and coach staff in the use of software technologies.
Work both as a leader and as an individual
contributor.
A a
Given the wide range of duties enumerated in the previous section, what skills (beyond mastery of the
technical body of knowledge) does an architect need to possess? Much has been written about the
·
·
·- 0 A a
architect's special role of leadership in a project; the ideal architect is an effective cotnmunicator,
manager, team builder, visionary, and mentor. Some certificate or certification programs emphasize
nontechnical skills. Common to these certification programs are nontechnical assessment areas of
leadership, organization dynamics, and communication.
Table 24.3 enumerates the set of nontechnical skills most useful to an architect.
Table 24.3. The Nontechnical Skills of a Software Architect
Gen·eral Skill
Area
Specific Skill Example
Area Skins
Communication Outward
skills
Ability to make oral and written communications
and presentations. Ability to present and explain
technical information to diverse audiences. Ability
to transfer knowledge. Ability to persuade. AbiHty to
se·e from and sell to multiple vJ,ewpoints.
Interpersonal
skills
Work skills
Inward AbiUty to listen, interview􀍕 consult􀍖 and' negotiate.
AbiHty to understand and express complex topics.
Wahin team Ability to be a team player. Ability to work
effectively with superiors, colleagues, and
customers. Ability to maintain constructive
working relationships. Ability to work in a diverse
team environment. Ability to inspire creative
collaboration. Ability to build consensus.
wah other
people
Leadership
Ability to demonstrate interpersonal skills. Ability to
be diplomatic and respect others. Abiltty to mentor
others. Ability to handle and resolve conflict.
Abiltty to make decisions. Ability to take initiative
and be innovative. Ability to demonstrate
independent judgment, be influential, and
command respect
Knowledge
Workload
management
Skills to excel
In corporate
environment
Skills for
handling
information
Skills for
handling the
unexpected
·
·
·- 0
Abili.ty to work well under pressure, plan, manage
tlme, estimate. Abitity to support a wide range
of issues and work on multiple complex projects
concurrently. Ability to effectively prioritize and
execute tasks in a high-pressure environment.
Ability to think strategically. Ability to work under
general supervision and under given constraints.
Ability to organize workflow. Abil'ity to sense where
the power is and how rt flows in an organization.
Ability to do what it takes to get the job done. Abifity
to be entrepreneurial, be assertive without being
aggressive, and receive constructive criticism.
Ability to be detail oriented while maintaining
overall vision and focus. Ability to see the big
picture. AbUity to deal with abstraction.
Ability to tolerate ambiguity. Ability to take and
manage risks. Ability to solve problems.
Abmty to 'be adaptablet ffexible, open minded, and
resilient. Ability to do the juggling necessary to
deploy successful software pro􀍗ects.
A a
A competent architect has an intimate familiarity with the architectural body of knowledge. The
software architect should
• Be comfortable with all branches of software engineering from requirements definition to
implementation, development, verification and validation, and deployment
• Be familiar with supporting disciplines such as configuration managetnent and project
management
• Understand current design and implementation tools and technologies
Knowledge and experience in one or more application domains is also necessary.
Table 24.4 is the set of knowledge areas for an architect.
Table 24.4. The Knowledge Areas of a Software Architect
General
Knowledge
Area
Computer
science
knowledge
Specific
Knowledge
Area
Knowledge of
architecture
concepts
Knowledge
of software
• • engtneenng
Design know!edge
Programming
knowledge
Specific
Knowledge
Examples
·
·
·- 0
Knowledge of architecture frameworks,
architectural patterns, tactics, viewpoints,
standard architecturesT relationship
to system and enterprise architecture,
architecture description languages, emerging
technologies, architecture evaluation models
and methods, and quality attributes.
Knowledge of systems e:n·glneering.
Knowledge of software development life
cycle, software process management,
and improvement techniques. Knowledge
of requirements analysis, mathematics.
·
development methods and modeling
techniques, elicitation techniques. Knowledg.e
of component􀍘based software development,
reuse methods and techniques, software
product-line techniques, documentation,
testing and debugging tools.
Knowledge of different tools and design
techniques. Knowledge of how to design
complex multi-product systems. Knowledge
of object-oriented tmalysis and design, U M L
diagrams, and UML analysis modeling.
Knowledge of programming ranguages and
programming language models. Knowledge
of specialized programming techniques for
security, real time, etc.
A a
Knowledge of
technologies
and platforms
Knowledge
about the
organization's
context and
management
Knowledge
of specific
technologies and
platforms
General
knowledge of
technologies and
platforms
Domain
knowledge
Industry
knowledge
Enterprise
knowledge
Leadership and
management
techniques
·
·
·- 0
Knowledge of hardware/software interfaces,
web􀆦based applications, and Internet
technologies. Knowledge of specific software/
operating systems, such as RDBMS concepts,
cloud platforms, and SOA impJementations.
Knowledge of IT industry future directions and
the ways in which infrastructure impacts an
application.
Knowledge of the most relevant domain(s)
and domain􀆦speciflc technologies.
Knowledge of the industry's best practices and
industry standards. Knowledge of how to work
in onshore/offshore team environment.
Knowledge of the company's busfoess
practices, and your competition's products,
strategies, and processes. Knowledge of
business and technical strategy, and business
reengineering principles and processes.
Knowledge of strategic planning, financial
models, and budgeting.
Krmwledge of coaching, mentoring, and
training software developers. 'Knowledge of
project management. Knowledge of project
engineering.
A a
·
·
·-
24.2. Competence of a Software Architecture Organization
0 A a
Organizations by their practices and structure can help or hinder architects in perfonning their duties.
For example, if an organization has a career path for architects, that will motivate employees to become
architects. If an organization has a standing architecture review board, then the project architect will
know how and with whom to schedule a review. Lack of these items will mean that an architect has to
fight battles with the organization or determine how to carry out a review without internal guidance. It
makes sense, therefore, to ask whether a particular organization is architecturally competent and to
develop instruments whose goal is measuring the architectural competence of an organization. The
architectural competence of organizations is the topic of this section.
Activities Carried Out by a Competent Organization
Organizations have duties, skills, and knowledge for architecture as well. For example, adequately
funding the architecture effort is an organizational duty, as is effectively using the available architecture
workforce (by appropriate teaming, and so on). These are organizational duties because they are outside
the control of individual architects. An organization-level skill might be effective knowledge
management or human resource management as applied to architects. An example of organizational
knowledge is the composition of an architecture-based life-cycle model that software projects may
employ.
Here are some things duties that an organization can perform to help improve the success of its
architecture efforts:
• Hire talented architects.
• Establish a career track for architects.
• Make the position of architect highly regarded through visibility, reward, and prestige.
• Establish a clear statement of responsibilities and authority for architects.
• Establish a mentoring program for architects.
• Establish an architecture training and education program.
• Establish an architect certification program.
• Have architects receive external architect certifications.
• Measure architects' performance.
• Establish a forum for architects to communicate and share information and experience.
• Establish a repository of reusable architectures and architecture-based artifacts.
• Develop reusable reference architectures.
• Establish organization-wide architecture practices.
• Establish an architecture review board.
• Measure the quality of architectures produced.
• Provide a centralized resource to analyze and help with architecture tools.
• Hold an organization-wide architecture conference.
• Have architects join professional organizations.
• Bring in outside expert consultants on architecture.
• Include architecture milestones in project plans.
• Have architects provide input into product definition.
• Have architects advise on the development team structure.
·
·
·-
• Give architects influence throughout the entire project life cycle.
• Reward or penalize architects based on project success or failure.
Assessment Goals
0 A a
The activities enumerated above can be assessed. What are the potential goals from an assessment of an
organization's architecture competence? There are at least four sets of reasons for assessing
organizational architectural competence:
1 . There are goals relevant to any business that wishes to improve their architectural
competence. Businesses regularly assess their own performance in a variety of meanstechnical,
fiscal, operational (for example, consider the widespread use of tnulti-criteria
techniques such as the Balanced Scorecard or Business/IT Alignment in industry) for a
variety of reasons. These include determining whether they are meeting industry norms and
gauging their progress over time in meeting organizational goals.
2. There are goals relevant to an acquisition organization. For example, an organization can use
an assessment of architecture competence to assess a contractor in much the same way that
contractors are scrutinized with respect to their CMMI level. Or an organization might use an
assessment of architecture competence to aid in deciding among competing bids from
contractors. All other things being equal, an acquiring organization would prefer a contractor
with a higher level of architectural competence because this typically means fewer
downstream problems and rework. An acquisition organization might assess the contractors
directly, or hire a third party to do the assessment.
3. There are goals relevant to service organizations: such organizations might be motivated to
maintain, measure, and advertise their architectural competence as a means of attracting and
retaining customers. In such a case they would typically rely on outside organizations to
assess their level of competence in an objective fashion.
4. Finally, there are goals that are relevant to product builders: these organizations would be
motivated to assess, monitor (and, over time, increase) their level of architectural competence
as it would ( 1) aid in advertising the quality of their products and (2) aid in their internal
productivity and predictability. In fact, in these ways their motivations are aligned with those
of service organizations.
Assessing an Organization's Competence
In addition to duties, skills, and knowledge, there are other models of individual and organizational
competence that are helpful in building an instrument for assessing an organization's competence. They
are the following:
·
·
·- 0 A a
• The Human Performance Technology model, which measures the value of an individual or
department's output and the cost of producing that output. It holds that competent people
produce the most value for every organizational dollar spent.
• The Organizational Coordination model, which measures how teams in multiple sites
developing a single product or related set of products cooperate to produce a functioning
product. An organization that is architecturally competent will have more effective and efficient
coordination mechanisms than an organization that is not architecturally competent.
• The Organizational Learning model, which measures how well an organization's learning
processes transform experience into knowledge, moderated by context.
We have created a framework for organizational architecture competence that forms the foundation
for a competence assessment procedure. A small team of trained assessors can use the framework to
conduct interviews (with architects, developers, and technical and organizational managers), examine
current practices, read documents and evidentiary artifacts (such as organizational standards), and
investigate architecture-based successes and failures in the recent past. They can use their findings to
identify systemic trouble spots and recommend improvement strategies.
Table 24.5 shows the framework. For convenience, it is divided into practice areas that relate to
software and system engineering, technical management (which is by and large the management of
single projects or small numbers of related projects), and organizational management (which is
management at a scope more broad than that of projects).
Table 24.5. Framework for Organizational Architecture Competence
Software
Engineering
Practice Areas
Quality Attribute Elicitation Practices
Tools and Tec,hnology Selection
Modeling and Prototyping Practices
Architecture Design Practices
Architecture Description Practices
Architecture Evaluation Practices,
System Implementation Practices
·
·
·-
• Software design practices (design conforms to
architecture)
0
• Software coding practices (code conforms to design and
architecture)
Software Verification Practices
• Proving properties of the software
• Software testing
Architectu re Reconstruction Practices
Business or Mission Goals Practices
• Setting goals
• Measur[ng achievement of organization's goals
• Performance-based compensation
Product or System Definition Practices
• Setting functional requirements
A a
Technicaf
Management
Practice Areas
Organizational
Management
Practice Areas
Allocating Resources
·
·
·-
• Setting architect's workload and schedule
• Funding stakeholder involvement
Project Management Practioes
0
• Project plan structure aligned with architecture structure
• Adequate time planned for architecture evaluation
Process Discipline Practices
• Establish organizatlon-wide arct1itecture practices
• Process monitoring and improvement practices
• Reuse practices
Collaboration with Manager Practices
• Architects advise managers
• Architects support manage.rs
Hire Talented Architects
Establish a Career Track for Architects
• Leadership roles for architects
• Succession planning
Professional Development Practices
• Ongoing training
• Creating and sustaining an internal community of
architects
• Supporting participation in external communities
Organizational Planning Practices
Technology Planning and Forecasting Practices
A a
The framework is populated by questions inspired by the four models of competence that we
previously described. Each question has a set of answers that we might expect to see in a competent
organization. For example, a question associated with the practice area "Hire talented architects" deals
with how a candidate architect's experience and capabilities are assessed. Expected answers might
include having the architect take a test, requiring that candidates possess an architecture certification, or
examining previous architectures designed by the candidate. (Our expected answers grow as we visit
more and more organizations. It's a pleasant surprise when we find an organization carrying out a
practice area in a clever way that we hadn't thought of.)
Questions Based on the Duties, Skills, and Knowledge Model
Our assessment framework contains dozens of questions related to duties, skills, and knowledge. The
questions are posed in terms of the organization: The questions ask how the organization ensures that
the architectural duties are carried out in a competent manner, and how the organization measures and
nurtures its architects' skills and knowledge. Here is a small set of example questions based on the
Duties, Skills, and Knowledge model (chosen from among the dozens that populate our assessment
framework) that we use in an architecture competence assessment exercise with an organization.
Duty: Creating an Architecture
Question: How do you create an architecture?
·
·
·- 0
• How do you ensure that the architecture is aligned with the business goals?
A a
• What is the input into the architecture creation process? What inputs are provided to the
architect?
• How does the architect validate the information provided? What does the architect do in case
the input is insufficient or inadequate?
Duty: Architecture Evaluation and Analysis
Question: How do you evaluate and analyze an architecture?
• Are evaluations part of the normal software development life cycle or are they done when
probletns are encountered?
• Is the evaluation incremental or "big bang"? How is the timing determined?
• Does the evaluation include an explicit activity relating architecture to business goals?
• What are the inputs to the evaluation? How are they validated?
• What are the outputs from an evaluation? How are the outputs of the evaluation utilized? Are
the outputs differentiated according to itnpact or importance? How are the outputs validated?
Who is c01nmunicated what outputs?
Knowledge: Architecture Concepts
Question: How does your organization ensure that its architects have adequate architectural knowledge?
• How are architects trained in general knowledge of architecture?
• How do architects learn about architectural frameworks, patterns, tactics, standards,
documentation notations, and architecture description languages?
• How do architects learn about new or emerging architectural technologies (e.g., multi-core
processors)?
• How do architects lean1 about analysis and evaluation techniques and methods?
• How do architects learn quality attribute-specific knowledge, such as techniques for analyzing
and managing availability, performance, modifiability, and security?
• How are architects tested to ensure that their level of knowledge is adequate, and remains
adequate, for the tasks that they face?
Questions Based on the Organizational Coordination Model
Questions based on the Organizational Coordination model focus on how the organization establishes
its teams and what support it provides for those teams to coordinate effectively. Here are a couple of
example questions:
Question: How is the architecture designed with distribution of work to teams in mind?
• How available or broadly shared is the architecture to various teams?
• How do you manage the evolution of architecture during development?
·
·
·- 0 A a
• Is the work assigned to the teams before or after the architecture is defined, and with due
consideration of the architectural structure?
Question: Are the aspects of the architecture that will require a lot of interteam coordination
supported by the organization's coordination/communication infrastructure?
• Do you co-locate teams with high coordination? Or at least put them in the same time zone?
• Must all coordination among teams go through the architecture team?
Questions Based on the Human Performance Technology Model
The Human Performance Technology questions deal with the value and cost of the organization's
architectural activities. Here are examples of questions based on the Human Performance Technology
model:
Question: Do you track how much the architecture effort costs, and how it impacts overall project
cost and schedule?
• How do you track the end of architecture activities?
• How do you track the impact of architecture activities?
Question: Do you track the value or benefits of the architecture?
• How do you measure stakeholder satisfaction?
• How do you measure quality?
Questions Based on the Organizational Learning Model
Finally, a set of example questions, based on the Organizational Learning model, which deal with how
the organization systematically internalizes knowledge to its advantage:
Question: How do you capture and share experiences, lessons learned, technological decisions,
techniques and methods, and knowledge about available tooling?
• Do you use any knowledge management tools?
• Is capture and use of architectural knowledge embedded in your processes?
• Where is the information about "who knows what" captured and how is this information
maintained?
• How complete and up to date is your architecture documentation? How widely disseminated is
it?
Performing an Assessment
Our organizational competence assessment is carried out using a team of three to four assessors. The
exercise is set up by establishing the scope of the review: Are we assessing the entire company? One of
its divisions? Or perhaps a single important project?
After we establish the scope, we identify the groups we wish to interview. Of course, we'll want to
interview the architecture team(s) within the scope. From there we identify groups both upstream and
downstream of the architects. Upstream are groups that manage the architects or provide organizationwide
architecture training. Downstream, we interview the "consumers" of the architectures, such as
·
·
·- 0 A a
developers, integrators, testers, and maintainers. We interview small groups, making sure that no
members of an interview group have reporting relationships with each other.
We try hard to establish an informal atmosphere in the group interviews, to avoid inhibiting the
participants. The tone is conversational, not inquisitional. We begin each interview by reminding the
participants of the purpose of the exercise, and to assure them that nothing they say will be quoted to
anyone outside the group in any way that could identify them.
For each group, we have planned which parts of the framework we wish to discuss with that group.
We won't ask testers, for example, questions intended for managers, and vice versa. We use the
questions as a guide for the conversation, but not a rigid script. Whenever we pose a question in the
assessment instrument, there are a number of meta-questions that automatically accompany it. For
example:
• What evidence could you show us to support your answer? Supporting evidence might include
a software development plan that lays out the role of architecture in a project, an organization's
architecture-based training curriculum, or many other kinds of documentation.
• How sustainable is your answer over time, over different systems, and across different
architects? For example, we might ask how an answer might change if a different architect
came on board the project.
The outcome of an assessment is organized by the practice areas of the framework. For each
practice, we assign one of three values that correspond to "you're doing this well," "you could be doing
this better (and experiencing more benefit)," and "this is an area of high risk." Graphically, we show
this as green light, yellow light, red light. We have found that this simple metric provides organizations
with enough granularity to turn their attention to problem areas, which is the whole point of the
assessment. We do not give an overall rating. Thus, we closely mirror the "continuous representation"
option of maturity models such as CMMI, in which the result is a vector rather than a scalar.
We present the fmdings in a written report and a slide presentation. In both, we describe and j ustify
each finding, based on what we were told in the interviews and/or read in provided documents.
24.3. Summary
·
·
·- 0 A a
The vast majority of work on software and systems architecture (including our own) has focused on the
technical aspects. But an architecture is much more than a technical "blueprint" for a system. This has
led us to try to understand, in a more holistic way, what an architect and an architecture-centric
organization must do to succeed. To this end, we have developed a framework that aids us in assessing
an organization for competence.
We use the framework to ask questions about an organization's practices. We can also ask about
recent architecture successes and failures, and investigate the causes of each. The output of this exercise
is a formal report that assesses competence at organization, team, and individual levels. Along with this
report we make improvement recommendations based on assessment results; these, too, are tied to the
underlying competence models.
You can do the same sort of evaluation on your own organization. The key to the process is in
understanding the various models and in creating questions based on these models that aid you in
assessing how well you are doing in those areas that you care about. Given this knowledge, you can
create your own improvement plan, as an individual architect or for an entire organization.
24.4. For Further Reading
·
·
·- 0 A a
The four models that underlie the assessment framework presented here are described in more detail in
the Technical Note "Models for Evaluating and Improving Architecture Competence" [Bass 08]. These
models are the following:
• Duties, Skills, and Knowledge (DSK) model of competence. This model is predicated on the
belief that architects and architecture-producing organizations are useful sources for
understanding the tasks necessary to the job of architecting. To assemble a comprehensive set
of duties, skills, and knowledge for architects, we surveyed approximately 200 sources of
information targeted to professional architects books, websites, blogs, position descriptions,
and more. The results of this survey can be found in [Clements 07].
• Human Performance model of competence. This model is based on the human performance
engineering work of Thomas Gilbert [Gilbert 07]. This model is predicated on the belief that
competent individuals in any profession are the ones who produce the most valuable results at a
reasonable cost. Using this model will involve figuring out how to measure the value and cost
of the outputs of architecture efforts, finding areas where that ratio can be improved, and
crafting improvement strategies based on environmental and behavioral factors.
• Organizational Coordination model of competence. The focus of this model is on creating an
interteam coordination model for teams developing a single product or a closely related set of
products. The architecture for the product induces a requirement for teams to coordinate during
the realization or refinement of architectural decisions. The organizational structure, practices,
and tool environtnent of the teams allow for particular types of coordination with a particular
interteam communication bandwidth. The coordination model of competence compares the
requirements for coordination that the architecture induces with the bandwidth for coordination
supported by the organizational structure, practices, and tool environment [Cataldo 07].
• Organizational Learning model of competence. This model is based on the concept that
organizations, and not just individuals, can learn. Organizational learning is a change in the
organization that occurs as a function of experience. This change can occur in the
organization's cognitions or knowledge (e.g., as presented by Fiol and Lyles [Fiol 85]), its
routines or practices (e.g., as demonstrated by Levitt and March [Levitt 88]), or its performance
(e.g., as presented by Dutton and Thomas [Dutton 84]). Although individuals are the medium
through which organizational learning generally occurs, learning by individuals within the
organization does not necessarily imply that organizational learning has occurred. For learning
to be organizational, it has to have a supra-individual component [Levitt 88]. There are three
approaches to measure organizational learning: ( 1 ) measure knowledge directly through
questionnaires, interviews, and verbal protocols; (2) treat changes in routines and practices as
indicators of changes in knowledge; or (3) view changes in organizational performance
indicators associated with experience as reflecting changes in knowledge [Argote 07].
The Open Group offers a certification program for qualifying the skills, knowledge, and experience
of IT, business, and enterprise architects, which is related to measuring and certifying an individual
architect's competence. Visit opengroup.org for details. The International Association of Software
Architects (IASA) offers a similar certification; see iasahome.org.
·
·
·- 0 A a
Dana Bredemeyer and Ruth Malan have written many articles on the role of the software architect
(www.bredemeyer.com/who.htm), including their duties and skills [Bredemeyer 1 1]
(www.bredemeyer.com/Architect/ArchitectSkillsLinks.htm). They have their own competence
framework as well as a skills development program.
The U.K. Chapter of the International Council on Systems Engineering (INCOSE) maintains a
"Core Competencies Framework" for systems engineers that includes a "Basic Skills and Behaviours"
section listing "the usual common attributes required by any professional engineer" [INCOSE 05]. The
list includes coaching, communication, negotiation and influencing, and "team working."
The classic work on the Balanced Scorecard was created by Kaplan and Norton [Kaplan 92] and the
classic work on Business/IT Alignment was originally created by Luftman [Luftman 00], although this
has been updated to explicitly consider the role of architecture in alignment [Chen 1 0].
Boehtn, V alerdi, and Honour [Boehm 07] provide one of the few empirical studies of systems
engineering and how an investment in "better" engineering pays off (or doesn't) in the future.
24.5. Discussion Questions
·
·
·- 0 A a
1 . In which skills and knowledge discussed in this chapter do you think you might be deficient?
How would you reduce these deficiencies?
2. Which duties, skills, or know ledge do you think are the most important or cost-effective to
improve in an individual architect? Justify your answer.
3. How would you measure the specific value of architecture in a project? How would you
distinguish the value added by architecture from the value added by other activities such as
quality assurance or configuration management?
4. How do you measure items such as "customer satisfaction" or "negotiation skills"? How would
you validate such measurements?
5. How would you distinguish benefits caused by systematic organizational learning from the
benefits due to heroic efforts by individuals within the organization?
6. Section 24.2 listed a number of practices of an architecturally competent organization. Prioritize
that list based on expected benefit over expected cost.
7. Suppose you are in charge of hiring an architect for an important system in your company. How
would you go about it? What would you ask the candidates in an interview? Would you ask them
to produce anything? If so, what? Would you have them take a test of some kind? If so, what?
Who in your company would you have interview them? Why?
·
·
·-
25. Architecture and Software Product Lines
0
Coming together is a beginning. Keeping together
is progress. Working together is success.
-Henry Ford
A a
A software architecture represents a significant investment of time and effort, usually by senior talent.
So it is natural to want to maximize the return on this investment by reusing an architecture across
multiple systems.
There are many ways this happens in practice. The patterns we discussed in Chapter 1 3 are a big
step in this direction; using a pattern is reusing a package of architectural decisions (albeit not a
complete architecture). And strictly speaking, every time you make a change to a system, you are
reusing its architecture (or whatever portion of its architecture you don't have to change).
This chapter shows yet another way to reuse a software architecture (and many other assets as well)
across a family of related systems, and the benefits that doing so can bring. Many software-producing
organizations tend to produce systems or products that resemble each other more than they differ. This
is an opportunity for reusing the architecture across these similar products. These software product lines
simplify the creation of new members of a family of similar systems.
This kind of reuse has been shown to bring substantial benefits that include reduced cost of
construction, higher quality, and greatly reduced time to market. This is the lure of the software product
line approach to system building.
The Software Engineering Institute defines a software product line as "a set of software-intensive
systems sharing a common, managed set of features that satisfy the specific needs of a particular market
segment or mission and that are developed from a common set of core assets in a prescribed way."
The vision is of a set of reusable assets (called core assets) based on a common architecture and the
software elements that populate that architecture. The core assets also include designs and their
documentation, user manuals, project management artifacts such as budgets and schedules, software test
plans and test cases, and more.
The product line approach works because the core assets were built specifically to support multiple
members of the same family of products. Hence, reusing them is faster and less expensive than
reinventing those software assets for each new product or system in the organization's portfolio. Core
assets, including the architecture, are usually designed with built-in variation points places where they
can be quickly tailored in preplanned ways.
Once the core assets are in place, syste1n building beco1nes a matter of
• Accessing the appropriate assets in the core asset base
• Exercising the variation points to configure them as required for the system being built
• Assembling that system
·
·
·- 0 A a
In the ideal case, this can be done automatically. Additional software developed for an individual
product, if needed at all, tends to account for a small fraction of the total software. Integration and
testing replace design and coding as the predominant activities.
Product lines are nothing new in manufacturing. Many historians trace the concept to Eli Whitney' s use
of interchangeable parts to build rifles in the early 1 800s, but earlier examples also exist. Today, there
are hundreds of examples in manufacturing: think of the products of companies like General Motors,
Toyota, Boeing, Airbus, Dell, even McDonald's, and the portfolio of similar products that each one
produces. Each company exploits commonality in different ways. Boeing, for example, developed the
757 and 767 in tandem, and the parts lists of these two very different aircraft overlap by about 60
percent.
The improvements in cost, time to market, and productivity that come with a successful software
product line can be breathtaking. Consider:
• Nokia credits the software product line approach with giving it flexibility to bring over a dozen
phones to market each year, as opposed to the three or so it could manage before, all with an
unprecedented variety of features.
• Cummins, Inc., was able to reduce the time it takes to produce the software for a diesel engine
from about a year to about a week.
• Hewlett-Packard builds products using one-quarter of the staff, in one-third of the time, and
with one twenty-fifth the number of defects, compared with software built before the advent of
software product line engineering.
• Deutsche Bank estimates $4 million in savings per year realized from building its global
transaction and settlement software as a product line.
• Philips reports reduced faults during integration in its high-end television portfolio by adopting
the product line approach. Product diversity used to be one of the top three concerns of their
architects. Now it doesn't even make the list of concerns at all; the product line approach has
taken software development off the critical path the software no longer determines the
delivery date of the product.
• With a product line of satellite ground control systems it c01nmissioned, the U.S. National
Reconnaissance Office reported the first product requiring 1 0 percent the expected number of
developers and having one-tenth the expected nutnber of defects.
• In Philips's medical systems product line, the software product line approach has cut both
software defects and time to market by more than half.
Creating a successful product line depends on a coordinated strategy involving software engineering,
technical management, and organization management. Because this is a book on software architecture,
we focus on the architectural aspects of software product lines, but all aspects must work together in
order for an organization to successfully create a product line.
That Silver Lining Might Have a Cloud
The software product line paradigm is a powerful way to leverage an investment in
architecture (and other core assets) into a family of related systems and thus see order·

·
·- 0 A a
of-magnitude improvements in time to market, quality, and productivity. These results
are possible and have been demonstrated by companies large and small in many
different domains. The effects are real. Further, data from many sources and companies
confirms with astonishing consistency that, to make the investment pay off, an
organization needs to build only three products. This is the minimum number we would
expect to have in a product line.
But other results are possible as well, and a spectacular crash-and-burn is not out of
the question when trying to adopt this approach. Product line practice, like any
technology, needs careful thought given to its adoption, and a company's history,
situation, and culture must be taken into account. Factors that can contribute to product
line failure include these:
• Lack of a champion in a position of sufficient control and visibility
• Failure of management to provide sustained and unwavering support
• Reluctance of middle managers to relinquish autocratic control of projects
• Failure to clearly identify business goals for adopting the product line approach
• Abandoning the approach at the first sign of difficulty
• Failure to adequately train staff in the approach and failure to explain or justify the
change adequately
• Lack of discipline in managing the architecture's variation points
• Scoping the product line too broadly or too narrowly
• Lack of product line tooling to help manage and exercise the variation points
Fortunately, there are strategies for overcoming most of these factors. One good
strategy is to launch a small but visible pilot project to demonstrate the quantitative
benefits of software product lines. The pilot can be staffed by those most willing to try
something new while the skeptics go about their business. It can work out process
issues, clarify roles and responsibilities, and in general work out the bugs before the
approach is transitioned to a wider setting.
-PCC
25.1. An Example of Product Line Variability
·
·
·- 0 A a
The following example will help us illustrate the concept of product line variability. In a product line of
software to support U.S. bank loan offices, suppose we have a software module that calculates what a
customer owes in the current month. For 1 8 of the 2 1 products in our product line, this module is
completely adequate. However, our company is about to enter the market in the state of Delaware,
which has certain laws that affect what a customer can owe. For the three products we plan to sell in
Delaware, we need a module that differs from the "standard" module. Analysis shows that the
difference will affect about 250 lines of source code in our 8,000-line module.
To build one of the Delaware products, what do we do? An obvious option is to copy the module,
change the 250 or so lines, and use the new version in the three products. This practice is called "cloneand-
own" the new projects "clone" the module, change it, and then "own" the new version. Most
companies, when faced with this situation, resort to clone-and-own. It's expedient in that it provides a
quick start to a new product, but it comes with a substantial cost down the road.
The probletn with clone-and-own is that it doesn't scale. Suppose each of our 2 1 products comprises
roughly 1 00 1nodules. If each module is allowed to diverge for each product, that's potentially 2 , 1 00
modules that the 1naintenance staff has to deal with, each one spiraling off on its own separate
maintenance trajectory based on the needs of the lone project each version is used in. Many companies'
growth in a market is limited brought to a halt, in fact by their inability to staff the maintenance of
so many separate versions of so many different assets composing the products in their portfolio. An
organization fielding several versions of several products finds itself dealing with a staggeringly
complex code base. The strain begins to show when a systematic change needs to be made to all of the
products for example, to add a new feature, or migrate to a new platform, or make the user interface
work in a different language. Because each version of each component used in each product has been
allowed to evolve separately, now suddenly making a systematic change becomes prohibitively
expensive (and only gets worse each time a new product is added the labor involved grows as the
square of the number of products). It only takes a few such portfolio-wide changes before organizations
feel that they've hit a wall of complexity and expense.
So much for clone-and-own. What else can we do? Instead of allowing up to 2 1 versions of each
module, we would much rather find a way to take advantage of the fact that these nearly identical
modules vary only in small, well-defined ways. To take advantage of their similarities, we introduce a
variation mechanism into the module. (Variation mechanisms are often realized as tactics, such as the
"defer binding" set of tactics described in Chapter 7.) This variation mechanism will let us maintain a
single module that can adapt to the range of variations in the applications (in our example, the 2 1
banking products) that it has to support. If we plan to market our products in states that, like Delaware,
have their own laws affecting what a customer owes, we may need to support additional variations of
the module. So our variation mechanism should be able to accommodate those possibilities as well.
The payoff for this up-front planning is that an asset used in any of the products exists as a single
version that (through the exercising of built-in variation mechanisms) works for all of the products in
the product line. And now, making a portfolio-wide change merely consists of changing the core assets
that are affected. Because all future versions of all products use the same core assets, changing the core
·
·
·- 0
asset base has the effect of changing all of the products in the organization's portfolio.
A a
25.2. What Makes a Software Product Line Work?
·
·
·- 0 A a
What makes product lines succeed is that the commonalities shared by the products can be exploited
through reuse to achieve production economies. The potential for reuse is broad and far-ranging,
including the following:
• Requirements. Most of the requirements are common with those of earlier systems and so can
be reused. In fact, many organizations simply maintain a single set of requirements that apply
across the entire family as a core asset; the requirements for a particular system are then written
as "delta" documents off the full set. In any case, most of the effort consumed by requirements
analysis is saved from system to system.
• Architectural design. An architecture for a software system represents a large investment of
time from the organization's most talented engineers. As we have seen, the quality goals for a
system performance, reliability, modifiability, and so forth are largely promoted or inhibited
once the architecture is in place. If the architecture is wrong, the system cannot be saved. For a
new product, however, this most important design step is already done and need not be
repeated.
• Software elements. Software elements are applicable across individual products. Element reuse
includes the (often difficult) initial design work. Design successes are captured and reused;
design dead ends are avoided, not repeated. This includes design of each element' s interface, its
documentation, its test plans and procedures, and any models (such as performance models)
used to predict or measure its behavior. One reusable set of elements is the system's user
interface, which represents an enormous and vital set of design decisions. And as a result of this
interface reuse, products in a product line usually enjoy the same look and feel as each other, an
advantage in the 1narketplace.
• Modeling and analysis. Performance models, schedulability analysis, distributed system issues
(such as proving the absence of deadlock), allocation of processes to processors, fault tolerance
schemes, and network load policies all carry over frmn product to product. Companies that
build real-time distributed systems report that one of the major headaches associated with
production has all but vanished. When they field a new product in their product line, they have
high confidence that the timing problems have been worked out and that the bugs associated
with distributed computing synchronization, network loading, and absence of deadlock have
been eliminated.
• Testing. Test plans, test processes, test cases, test data, test harnesses, and the communication
paths required to report and fix problems are already in place.
• Project planning artifacts. Budgeting and scheduling are more predictable because experience
is a high-fidelity indicator of future performance. Work breakdown structures need not be
invented each time. Teams, team size, and team composition are all easily determined.
All of these represent valuable core assets, each of which can be imbued with its own variation
points that can be exercised to build a product. We'll look at architectural variation points later in this
chapter, but for now imagine that any artifact represented by text can consist of text blocks that are
·
·
·- 0 A a
exposed or hidden for a particular product. Thus, the artifact that is maintained in the core asset base
represents a superset of any version that will be produced for a product.
Artifact reuse in turn enables reuse of knowledge:
• Processes, methods, and tools. Configuration control procedures and facilities, documentation
plans and approval processes, tool environments, system generation and distribution
procedures, coding standards, and many other day-to-day engineering support activities can all
be carried over from product to product. The software development process is in place and has
been used before.
Giving Software Reuse a New Lease on Life
Software product lines rely on reuse, but reuse has a long but less than stellar history in
software engineering, with the prmnise almost always exceeding the payoff. One reason
for this failure is that until now reuse has been predicated on the idea of "If you build it,
they will come." A reuse library is stocked with snippets from previous projects, and
developers are expected to check it first before coding new elements. Almost
everything conspires against this model. If the library is too sparse, the developer will
not find anything of use and will stop looking. If the library is too rich, it will be hard to
understand and search. If the elements are too small, it is easier to rewrite them than to
find them and carry out whatever modifications they might need. If the elements are too
large, it is difficult to determine exactly what they do in detail, which in any case is not
likely to be exactly right for the new application. In most reuse libraries, pedigree is
hazy at best. The developer cannot be sure exactly what the element does, how reliable
it is, or under what conditions it was tested. And there is almost never a match between
the quality attributes needed for the new application and those provided by the elements
in the library.
In any case, it is common that the elements were written for a different architectural
model than the one the developer of the new system is using. Even if you find
something that does the right thing with the right quality attributes, it is doubtful that it
will be the right kind of architectural element (if you need an object, you might fmd a
process), that it will have the right interaction protocol, that it will comply with the new
application's error-handling or failover policies, and so on.
This has led to so many reuse failures that many project managers have given up on
the idea. "Bah!" they exclaim. "We tried reuse before, and it doesn't work!"
Software product lines make reuse work by establishing a strict context for it. The
architecture is defined; the functionality is set; the quality attributes are known. Nothing
is placed in the reuse library or "core asset base" in product line terms that was not
built to be reused in that product line. Product lines work by relying on strategic or
planned, not opportunistic, reuse.
-PCC
·
·
·- 0 A a
• People. Because of the commonality of applications, personnel can be transferred among
projects as required. Their expertise is applicable across the entire line.
• Exemplar systems. Deployed products serve as high-quality demonstration prototypes or
engineering models of performance, security, safety, and reliability.
• Defect elimination. Product lines enhance quality because each new system takes advantage of
the defect elimination in its forebears. Developer and customer confidence both rise with each
new instantiation. The more complicated the system, the higher the payoff for solving vexing
performance, distribution, reliability, and other engineering issues once for the entire family.
All of this reuse helps products launch more quickly, with higher quality, lower cost, and more
predictable budget and schedule. This is critical for getting a product to market in a timely fashion.
However, these benefits do not come for free. A product line may require a substantial up-front
investment of time and effort to set up and manage, as well as to keep the core assets responsive to
changing market needs.
25.3. Product Line Scope
·
·
·- 0 A a
One of the most important inputs to an architect building an architecture for a software product line is
the product line's scope. A product line 's scope is a statement about what systems an organization is
willing to build as part of its line and what systems it is not willing to build. Defining a product line's
scope is like drawing a doughnut in the space of all possible systems, as shown in Figure 25 . 1 . The
doughnut' s center represents the systems that the organization could easily build using its base of core
assets; these are within its production capability. Systems outside the doughnut are out of scope because
they are ones the product line 's core assets are not well equipped to handle; this would be like asking
Toyota to build, say, apple pies on one of its automotive assembly lines.
• • • • • • • • • • • • •
• • • • • • • • • • • • •
•
• •
•
• •
•
•
•
•
•
• •
•
• •
•
•
•
•
• • •
• • •
• • •
• • •·
• •
• • .,
• • •
• • •
• •
• • •
• • •
�------�·· . . . . .
• • • • • • • • • • • • •
• • • • • • • • • • • • • •
• • • • • • • • • • • • •
Figure 25.1. The space of all possible systems is divided into areas within scope (white}, areas
outside of scope (speckled}, and areas that require case-by-case disposition (gray).
Systems on the doughnut itself could be handled, but with some effort. These often represent
invitations from the marketplace asking the organization to extend its product line. To take advantage of
such an opportunity, the organization would have to broaden its production capability that is, make its
core asset base able to handle the new product. These opportunities require case-by-case disposition as
they arise, to see if the potential payoff (such as entry into a slightly different area of the market) would
outweigh the cost to modify the core assets. This would be like asking Toyota to build a riding
lawnmower.
The scope represents the organization's best prediction about what products it will be asked to build
in the foreseeable future. Input to the scoping process comes from the organization's strategic planners,
marketing staff, domain analysts who can catalog similar systems (both existing and on the drawing
board), and technology experts.
A product line scope is a critical factor in the success of the product line. Scope too narrowly (the
products only vary in a stnall number of features) and an insufficient number of products will be
derived to justify the investment in development. Scope too broadly (the products vary in kind as well
as in features) and the effort required to develop individual products from the core assets is too great to
lead to significant savings. Scope can be refined as a portion of the initial establishment of the product
·
·
·- 0 A a
line or opportunistically depending on the product line adoption strategy (see the section on adoption
strategies in Section 25.8).
The problem in defining the scope is not in finding commonality a creative architect can find
points of commonality between any two systems but in finding commonality that can be exploited to
substantially reduce the cost of constructing the systems that an organization intends to build. When
considering scope, more than just the systems being built should be considered. Market segmentation
and types of customer interactions assumed will help determine the scope of any particular product line.
For example, Philips, the Dutch manufacturer of consumer electronics, has distinct product lines for
home video electronic systems and digital video communication. Video is the common thread, but one
is a mass market, where the customer is assumed to have very little video sophistication, and the other
is a much smaller market consisting purely of video professionals. The products being developed reflect
these assumptions about the sophistication of customers and the amount of care each customer will
receive. These differences were sufficient to keep Philips from attempting to develop a single product
line for both markets.
Narrowly scoped product lines offer opportunities to build specialized tools to support the
specification of new products. For example, General Motors' Powertrain division builds a software
product line of automotive software. It makes an individual product from its product line core assets
based on contracts stored in a database. Each ele·ment has well-defined interfaces and possible variation
points. A tool searches the database based on desired features and assembles the product.
The scope definition is vital to the product line architect because the scope defines what is common
across all members of the product line, and the specific ways in which the products differ from each
other. The fixed part of a product line architecture reflects what is constant, and the architecture's
variation points accommodate the variations among products.
25.4. The Quality Attribute of Variability
·
·
·- 0 A a
Scoping decisions, which tell the product line architect what kinds of systems are "in" and what kinds
of systems are "out" of the product line, lead to the introduction of variability in the core assets. In fact,
the quality attribute of variability is most closely associated with product lines. Some may feature highperformance
products, or high-security products, or high-availability products, but all product lines
feature variability aimed at satisfying the commonalities and variations identified by the product line's
scope.
We introduced variability in Chapter 12. There we said that variability is a special form of
modifiability, pertaining to the ability of a core asset to adapt to usages in the different product contexts
that are within the product line scope. The goal of variability in a software product line is to make it
easy to build and maintain products in the product line over time.
Table 25 . 1 gives the general scenario for variability. The source is some actor in the product line
organization who identifies a need for variation; this actor is probably someone involved in setting the
product line's scope, such as a marketer.
Table 25.1. The General Scenario for Variability
Portion of Scenario
Source
Stimulus
Environment
Artifact
Response
Response measure
Possible Values
Actor requesting variability
·
·
·-
Requests to support variations in the· foll'owlng:
• Hardware
• Feature sets
• Technologies
• User interfaces
• Quality attributes
• . . . and more
for the range of products affected, such as:
• All
• A specified subset
• 􀍙hose that include feature set x
• New products
Variants are to be created at
• Runtime
• Build time
• Development time
Asset(s} affected􀍚 such as:
• Requirements
• Architecture
• Component x
• Test suite v
•
• Project plan z
• . . . and more
The requested variants can be created.
0
A specified cost and/or time to create the core assets and
to create the variants using these core assets
A a
Identifying variation is a constant, iterative process in the life of a software product line. Because of
the many different ways a product can vary, particular variants can be identified at virtually any titne
during the development process. Some variations are identified during product line requirement
elicitations; others, during architecture design; and still others, during implementation. Variations may
also be identified during implementation of the second (and subsequent) products as well.
Product line architectures feature variability as an important quality attribute. They achieve this by
incorporation of variation mechanisms, which we will discuss in more detail shortly.
25.5. The Role of a Product Line Architecture
·
·
·- 0 A a
Of all of the assets in a core asset repository, the software architecture plays the most central role. There
is both a tactical and a strategic reason for this.
The tactical reason is the importance the architecture plays in building products in a product line.
The essence of building a successful software product line is discriminating between what is expected
to remain constant across all family members and what is expected to vary. Software architecture is
ideal for handling this variation, because all architectures are abstractions that admit multiple instances.
By its very nature every architecture is a statement about what we expect to remain constant and what
we admit may vary. For example, interfaces to components are designed to remain stable, with
anticipated changes hidden behind those interfaces.
In a software product line, the architecture has to encompass both the varying and the nonvarying
aspects. A product line architecture must be designed to accommodate a set of explicitly allowed
variations. Thus, identifying the allowable variations is part of the architect's responsibility, as is
providing built-in mechanistns for achieving them. Those variations may be substantial. Products in a
software product line exist simultaneously and may vary in terms of their behavior, quality attributes,
platform, network, physical configuration, middleware, scale factors, and so forth.
The strategic reason has to do with the capability it imparts to an organization outside the realm of
an existing product line. As we saw in Chapters 2 and 3., an architecture can serve as a technical
platform for launching new applications and even new business models, and it can serve as a
springboard for an organization diving into a new business area. This seems to be especially true for
product line architectures. There are many cases where an organization has taken advantage of its
production capability that is, its core asset base crowned by a product line architecture by using that
capability to enter new markets. For example, Cummins took its product line of automotive diesel
engines to enter and quickly dominate the neighboring market for industrial diesel engines. Industrial
diesel engines power things like rock crushers and ski lifts, markets of low volume and high
specialization. Systems in that market built uniquely for each application are expensive and don't yield
a high return. But a product line that includes industrial diesel engines in its scope, and whose
production capability supports industrial diesel engines, is a recipe for rapid market capture.
A product line architect needs to consider three things that are unique to product line architectures:
• Identifying variation points. This is done by using the scope definition and product line
requirements as input. The product line architect determines where in the architecture variation
points should be made available to support the rapid building of products.
• Supporting variation points. This is done by introducing variation mechanisms, which will be
discussed in the next section.
• Evaluating the architecture for product line suitability, which will be discussed later in this
chapter.
25.6. Variation Mechanisms
·
·
·- 0 A a
In a conventional architecture, the mechanism for achieving different instances often comes down to
modifying the code. But in a software product line, modifying code is undesirable, because this leads to
a large number of separately maintained implementations that quickly outstrip an organization's ability
to keep them up to date and consistent.
Three primary architectural variation mechanisms are these:
• Inclusion or omission of elements. This decision can be reflected in the build procedures for
different products, or the implementation of an element can be conditionally compiled based on
some parameter indicating its presence or absence.
• Inclusion of a difef rent number of replicated elements. For instance, high-capacity variants
might be produced by adding more servers the actual number should be unspecified, as a
point of variation, and may be done dynamically.
• Selection of different versions of elements that have the same interface but different behavioral
or quality attribute characteristics. Selection can occur at compile time, build time, or runtime.
Selection mechanisms include static libraries, which contain external functions linked after
compilation time; dynamic link libraries, which have the flexibility of static libraries but defer
the decision until runtime based on context and execution conditions; and add-ons (e.g., plugins,
extensions, and themes), which add or modify application functionality at runtime. By
changing the libraries, we can change the implementation of functions whose names and
signatures are known.
Some variation mechanisms can be introduced that change aspects of a particular software element.
Modifying the source code each time the element is used in a new product that is, clone-and-ownfalls
into this category, although it is undesirable. More sophisticated techniques include the following:
• Extension points. These are identified places in the architecture where additional behavior or
functionality can be safely added.
• Reflection. This is the ability of a program to manipulate data on itself or its execution
environment or state. Reflective programs can adjust their behavior based on their context.
• Overloading. This is a means of reusing a named functionality to operate on different types.
Overloading promotes code reuse, but at the cost of understandability and code complexity.
Other commonly used variation mechanisms include those in Table 25.2.
Table 25.2. Common Variation Mechanisms
Variation
Mechanism
Inheritance;
specialtzing or
generalizing a
particular class
Component
substitution
Add-ons, plugins
Templates
Parameters
(including text
preprocessors)
Generator
Aspects
Runtime
condihonals
Config:urator
Properties Relevant to
Building the Core Assets
Cost: Medium
Skills: Object-oriented
languages
Cost: Medium
Ski.lls: Interface definitions
Cost: High
Skills: Framework
• programming
Cost: Medium
SkUis: Abstractions
Cost: Medium
Skills: No special skills
required
Cost: High
Skills: Generative
• programm1ng
Cost: Medium
Skills: Aspect-ori·ented
• programming
Cost: Medium
SkUis: No special skills
required
Cost: Medium
Skills: No special skills
required
·
·
·- 0
Properties Relevant to Exerci,sing
the Variation Mechanism When
Building Products
Stakeholder: Product developers
Tools.: Compiler
Cost: Medium
Stakeholder: Product developer, system
admi nlstrator
Tools: Compiler
Cost: Low
Stakeholder: End user
Tools: None
Cost: Low
Stakeholder: Product devefoper, system
administrator
Tools: None
Cost: Medium
Stakeholder: Product developer, system
administrator, end user
Tools: None
Cost: Low
Stakeholde,r: System admlnistrator, end
user
Tools: Generator
Cost: Low
Stakeholder: Product developer
Tools: Aspect-oriented language
compiler
Cost: Medium
Stakeholder: None
Tools: None
Cost: No development cost; some
performance cost
Stakeholder: Product developer
Tools: Configurator
Cost: Low to medium
Choosing the right variation mechanism affects numerous costs:
A a
• The skill set required to implement, or learn and use, the specific variation mechanism, such as
server or framework programming
• The one-time costs of building or acquiring the tools (such as compilers or generators) required
to create the variation mechanism
• The recurring cost and time to exercise the variation mechanism
The choice of variation mechanism also affects downstream users and developers:
• The targeted group of users that use the mechanism for product-specific adaptation, such as
product developer, integrator, system administrator, and end user
Finally, the choice of variation mechanism affects product quality:
• The impact of the variation mechanism on quality, such as possible performance penalties or
memory consumption
• The impact on the mechanism' s maintainability
·
·
·- 0 A a
The architect should document the choice of variation mechanisms. In fact, the documentation of
variation mechanisms is the primary way in which the documentation for a product line architecture
differs from that of a conventional architecture. In the documentation template we presented in Chapter
1 8 , the section called the variability guide is reserved for exactly this purpose. The variability guide
should describe each variation mechanism, how and when to exercise it, and what allowed variations it
supports. The architecture documentation should also describe the architecture' s instantiation process,that
is, how its variation points are exercised. Also, if certain combinations of variations are disallowed,
then the documentation needs to explain valid and invalid variation choices.
25.7. Evaluating a Product Line Architecture
·
·
·- 0 A a
Like any other, the architecture for a software product line should be evaluated for fitness of purpose.
The architecture should be evaluated for its robustness and generality, to make sure it can serve as the
basis for products in the product line's envisioned scope. It should also be evaluated to make sure it
meets the specific behavioral and quality requirements of the product at hand. We begin by focusing on
the what and how of the evaluation and then tum to when it should take place.
What and How to Evaluate. The evaluation will have to focus on the variation points to make sure
they are appropriate, that they offer sufficient flexibility to cover the product line's intended scope, that
they allow products to be built quickly, and that they do not impose unacceptable runtime performance
costs. If your evaluation is scenario based, expect to elicit scenarios that involve instantiating the
architecture to support different products in the family. Also, different products in the product line may
have different quality attribute requirements, and the architecture will have to be evaluated for its ability
to provide all required combinations. Here again, try to elicit scenarios that capture the quality attributes
required of family members.
Often, some of the hardware and other performance-affecting factors for a product line architecture
are unknown to begin with. In this case, evaluation can establish bounds on the performance that the
architecture is able to achieve, assuming bounds on hardware and other variables. The evaluation can
identify potential contention so that you can put in place the policies and strategies to resolve it.
When to Evaluate. An evaluation should be performed on an instance or variation of the
architecture that will be used to build one or more products in the product line. The extent to which this
is a separate, dedicated evaluation depends on the extent to which the product's requirements differ
from the product line architecture envelope. If it does not differ, the product architecture evaluation can
be abbreviated, because many of the issues normally raised in a single product evaluation will have
been dealt with in the product line evaluation. In fact, just as the product architecture is a variation of
the product line architecture, the product architecture evaluation is a variation of the product line
architecture evaluation. Therefore, depending on the evaluation method used, the evaluation artifacts
(scenarios, checklists, and so on) will have reuse potential, and you should create them with that in
mind. The results of evaluation of product architectures often provide useful feedback to the product
line architects and fuel architectural improvements.
When a new product is proposed that falls outside the scope of the original product line (for which
the architecture was presumably evaluated), the product line architecture can be reevaluated to see if it
will suffice for it. If it does, the product line 's scope can be expanded to include the new product, or to
spawn a new product line. If it does not, the evaluation can determine how the architecture will have to
be modified to accommodate the new product. The product line and product instance architectures can
be evaluated not only to determine architectural risks but also to understand economic consequences
(see Chapter 23), to determine which products will yield the most return.
25.8. Key Software Product Line Issues
·
·
·- 0 A a
It takes considerable maturity in the developing organization to successfully field a product line.
Technology is not the only barrier to this; organization, process, and business issues are equally vital to
master to fully reap the benefits of the software product line approach.
Architecture definition is an important activity for any project, but as we saw in the previous
section, it needs to emphasize variation points in a software product line. Configuration management is
also an important activity for any project, but it is more complex for a software product line because
each product is the result of binding a large number of variations. The configuration management
problem for product lines is to reproduce any version of any product delivered to any customer, where
"product" means code and supporting artifacts ranging from requirement specs and test cases to user
manuals and installation guides. This involves knowing what version of each core asset was used in a
product's construction, how every asset was tailored, and what special-purpose code or documentation
was added.
Examining every facet of launching a product line and institutionalizing a product line culture is
outside the scope of this book, but the next sections will examine a few of the key areas that must be
addressed. These are issues that an organization will have to face when considering whether to adopt a
product line approach for software development and, if so, how to go about it.
Adoption Strategies
An organization's culture and context will dramatically affect how it goes about adopting a product line
approach. Here are some of the important organizational and process factors that we have seen in
practice.
Top-Down vs. Bottom-Up
Top-down adoption arises when a (typically high level) manager decrees that the organization will use
the approach. The problem is to get employees in the trenches to change the way they work. Bottom-up
adoption happens when designers and developers working at the product level realize that they are
needlessly duplicating each other's work and begin to share resources and develop generic core assets.
The probletn is finding a manager willing to sponsor the work and spread the technique to other parts of
the organization. Both approaches work; both are helped enormously by the presence of a strong
champion someone who has thoroughly internalized the product line vision and can share that
compelling vision with others. (It works better if the champion is in a position of some authority.)
Proactive vs. Reactive
There are two primary models for how an organization may grow a product line:
• In a proactive product line, an organization defines the family using a comprehensive definition
of scope. They do this not with a crystal ball but by taking advantage of their experience in the
application area, their knowledge about the market and technology trends, and their good
business sense. The proactive model allows the organization to make the most far-reaching
strategic decisions. Explicitly scoping the product line allows you to look at areas that are
underrepresented by products already in the marketplace, make small extensions to the product
·
·
·- 0 A a
line, and move quickly to fill the gap. In short, proactive product line scope allows an
organization to take charge of its own fate. Sometimes an organization does not have the ability
to forecast the needs of the market with the certainty suggested by the proactive model. The
proactive model also takes some titne to define and implement, and in that titne the
organization needs to continue to construct products.
• In a reactive product line, an organization builds the next member or members of the product
family from earlier products. This is best used when there is uncertainty of requirements.
Perhaps the domain is a new one. Perhaps the market is in flux. Or perhaps the organization
cannot afford to build a core asset base that will cover the entire scope all at once. In the
reactive model, with each new product the architecture is extended as needed and the core asset
base is built up from what has turned out to be common. The reactive model puts much less
emphasis on up-front planning and strategic direction setting. Rather, the organization lets itself
be taken where the market dictates. This is an example of agile architecting, as described in
Chapter 15.
Incremental vs. Big Bang
If you are proactively building a product line, you still need to choose how to populate it: all at once or
incretnentally over time. Populating the core asset base all at once is a strategy that has worked
successfully for some organizations. However, it tends to require all or nearly all of the organization's
resources be focused on that task, at the expense of new product production. A different approach is to
populate the core asset base incrementally, as circumstances and resources permit. Each product that
goes out the door is built with whatever core assets are available at the time. That means that early
products will include software not derived from core assets. But those products will still be better off
(that is, faster to market, of higher quality, and easier to maintain) than products built entirely from
unique code. And it's entirely possible that some of the software unique to those early products can be
extracted, adapted, and generalized to becotne core assets themselves, thus helping populate the core
asset base in a reactive fashion.
Knowing the various adoption models can help an organization choose the one that is right for it.
For example, the proactive model requires a heavier initial investment but less rework than the reactive
model. The reactive model relies exclusively on rework with little initial investment. Which model
should act as a guide for a particular organization depends on the business situation.
Creating Products and Evolving a Product Line
An organization that has a product line will have an architecture and a collection of elements associated
with it. From time to time, the organization will create a new member of the product line that will have
features both in common with and different from those of other members.
One problem associated with a product line is managing its evolution. As time passes, the line or,
more precisely, the set of core assets from which products are built must evolve. That evolution will
be driven by both external and internal sources:
External sources
• New versions of existing elements within the line will be released by their vendors, and future
products will need to be constructed from them.
·
·
·- 0 A a
• New externally created elements may be added to the line. Thus, for example, functions that
were previously performed by internally developed elements may now be performed by
elements acquired externally, or vice versa. Or future products will need to take advantage of
new technology, as embodied in externally developed elements.
• New features may be added to the product line to keep it responsive to user needs or
competitive pressures.
Internal sources
• Some entity within the organization must determine if new functions added to a product are
within the product line 's scope. If so, they can simply be built from the asset base. If not, a
decision must be made: either the enhanced product spins off from the product line, following
its own evolutionary path, or the asset base must be expanded to include it. Updating the line
may be the wisest choice if the new functionality is likely to be used in future products, but this
capability comes at the cost of the time necessary to update the core assets.
• An organization may wish to replace old products with ones built from the most up-to-date
version of the asset base. Keeping products compatible with the product line takes time and
effort. But not doing so may make future upgrades more time consuming, because either the
product will need to be brought into compliance with the latest product line elements or it will
not be able to take advantage of improvements in the line.
Organizational Structure
An asset base on which products depend, but which has its own evolutionary path (perhaps driven by
technology change), requires an organization to decide how to manage both it and product
development. There are two main organizational strategies from which to choose, plus a number of
minor variations. The two main structures reflect different answers to the question "Shall we have a
dedicated group whose sole job is to build and maintain our core asset base?"
1 . We 're all in this together. In this scheme, there is no separate core asset group. The productbuilding
development teams coordinate closely, and divide up the core asset responsibilities
among themselves. That is, Product Team 1 might be assigned responsibility for the
development and maintenance of Core Assets 3, 6, 9, 12, and 1 5 ; Product Team 2 might take
Core Assets 1 , 4, and 8; and so forth. This works well enough for small organizations, but as
size grows the communication channels become untenable. Also, each team has to resist the
temptation to build core assets that are especially appropriate to its needs, but less so to other
teams' needs.
2. Separate core asset unit. In this scheme, a special unit is given responsibility for the
development and maintenance of the core asset base. Separate development teams in the
organization's business units build the products. In this scheme, the core asset unit
(sometitnes called a domain engineering unit) assumes the responsibility for the overall
strategic direction of the product line. To the product teams, they appear almost like an
exten1al supplier. The product teams coordinate among themselves to set the core asset
·
·
·- 0
team's development and test priorities, based on product delivery obligations.
A a
25.9. Summary
·
·
·- 0 A a
This chapter presented an architecture-based development paradigm known as software product lines.
The product line approach is steadily climbing in popularity as more organizations see true order-ofmagnitude
improvements in cost, schedule, and quality from using it.
Like all technologies, however, this one holds some surprises for the unaware. Architecturally, the
key is identifying and managing commonalities and variations, but nontechnical issues must be
addressed as well, including how the organization adopts the model, structures itself, and maintains its
external interfaces.
25.10. For Further Reading
·
·
·- 0 A a
[Clements 0 1 a] is a comprehensive treatment of software product lines. It includes a number of case
studies as well as a thorough discussion of product line "practice areas," which are areas of expertise a
product line organization should have (or should develop) to help bring about product line success.
[van der Linden 07] contains a rich set of product line case studies.
[Anastasopoulos 00] presents a good list of variation mechanisms, as do [Jacobson 97] and
[Svahnberg 00]. [Bachmann 05] provides a list of their own, as well as a treatment of each in terms of
cost (it was the source for Table 25.2). Organizational models for software product lines are treated in
[Bosch 00].
There is an active software product line community of research and practice. The Software Product
Line Conference (SPLC) is the mainstream forum for new software product line research and success
stories. You can find it at www.splc.net. SPLC maintains a "Software Product Line Hall of Fame,"
which showcases successful software product lines that can serve as engineering models (and
inspiration) to aspiring product line organizations. Each year, new members of the Hall of Fame are
nominated, and in most years a new candidate is inducted. You can see the winners at
www .splc.netlfame.html.
The SEI's website contains a wealth of material about software product lines, including a collection
of "getting started" material: www. sei. emu. edu/productlines.
25.11. Discussion Questions
·
·
·- 0 A a
1 . Variability is achieved by adding variation mechanisms to a system. Variation mechanisms
include inheritance, component substitution, plug-ins, templates, parameters (including text
preprocessors), generators, aspects, runtime conditionals, and a configura tor tool. Because
variability can be seen as a kind of modifiability, see if you can map each of these variation
mechanisms to one or more modifiability tactics given in Chapter 7.
2. Suppose a company builds two similar systems using a large set of common assets, including an
architecture. Which of the following would you say constitutes a product line?
• Sharing only an architecture but no elements.
• Sharing only a single eletnent.
• Sharing the same operating system and programming language runtime libraries.
• Sharing the same team of developers.
Defend your answer.
3 . Pick a type of system you're familiar with for example, an automobile or a smartphone. Think
of three instances of that kind of system. Make a list of all of the things the three instances have
in common. Now make a list of all of the things that distinguish the three instances from each
other (that is, their variation points). If automobiles turn out to be too complex, start with a
simpler kind of "system," such as an electric light.
4. Write some concrete scenarios to express the variability you identified in the previous question.
5. Do the list of variation mechanisms in this chapter constitute tactics for variability? Discuss.
6. In many software product lines, products differ by the quality attributes they exhibit. For instance,
a company might sell a cheap, low-security version of its product alongside a more expensive,
high-security version of the same product. Which variation mechanisms might you choose to
achieve this kind of variability?
·
·
·- 0
Part Five. The Brave New World
A a
Parts I through IV of this book have dealt with the technical, organizational, and business perspectives
on software architecture. In this part, we tum our attention to emerging technologies. We have often
been asked whether principles or technology is more important, and the answer, of course, is "Yes, they
are both important." Principles have a long lifetime; technology that affects architects tends to change
every decade or so. In this part, we provide brief introductions to two technologies that we believe will
last and have a significant impact on architects the cloud and the edge. We also discuss one of the
continuing problems of many architects: How do I get my organization to embrace architectural
principles?
The cloud provides you with the option of outsourcing your data center. The vision is that
computing resources are available to an application as electricity is available to a consumer. That is, one
plugs in an appliance and electricity is available. In data center terms, you hook your web browser up to
an application and computation power is available. All of the capacity, management, and operational
issues of a data center are taken care of by a third party, and all you, as an architect, need to do is to
utilize the resources you need. This trend has been accompanied by a vast expansion of the amount of
data that organizations manage. Google, Yahoo!, Facebook, and the other web giants all must manage
petabytes of data. In Chapter 26, we provide a brief introduction to the technologies associated with the
cloud and with managing these vast amounts of data.
Cloud computing is associated with the world of social networks and open source. The term "edgeperiphery"
is used to describe both the crowdsource and the open source movements. The term refers
simultaneously to crowdsourced systems such as Facebook and Wikipedia and open source systems
such as the Apache Web Server and Hadoop. In Chapter 27, we describe this phenomenon and explore
some of the architectural implications of this aspect of the brave new world.
We end by discussing "adoption." It describes an approach to dealing with the following problem:
"OK, you guys have convinced me. Now I need to convince my organization of the importance of
architectural principles. How do I do that?"
26. Architecture in the Cloud
·
·
·- 0
There was a time when every household, town,
farm or village had its own water well. Today,
shared public utilities give us access to clean
water by simply turning on the tap; cloud
computing works in a similar fashion.
-Vivek Kundra
A a
If you have read anything about the history of computing, you will have read about time-sharing. This
was the era, in the late 1 960s and the 1 970s, sandwiched between eras when individuals had sole,
although limited, access to multimillion-dollar computers and when individuals had access to their own
personal computers. Time-sharing involved multiple users (maybe as many as several hundred)
simultaneously accessing a powerful mainframe computer through a terminal, potentially remote from
the mainframe. The operating system on the mainframe made it appear as if each user had sole access to
that computer except, possibly, for performance considerations. The driving force behind the
development of time-sharing was economic; it was infeasible to provide every user with a multimilliondollar
computer, but efficiently sharing this expensive but powerful resource was the solution.
In some ways, cloud computing is a re-creation of that era. In fact, some of the basic techniques.such
as virtualization that are used in the cloud today date from that period. Any user of an
application in the cloud does not need to know that the application and the data it uses are situated
several time zones away, and that thousands of other users are sharing it. Of course, with the advent of
the Internet, the availability of much more powerful computers today, and the requirement for
controlled sharing, designing the architecture for a cloud-based application is much different from
designing the architecture for a time-sharing-based application. The driving forces, however, remain
much the same. The economics of using the cloud as a deployment platform are so compelling that few
organizations today can afford to ignore this set of technologies.
In this chapter we introduce cloud concepts, and we discuss various service models and deployment
options for the cloud, the economic justification for the cloud, the base architectures and mechanisms
that make the cloud work, and some sample technologies. We will conclude by discussing how an
architect should approach building a system in the cloud.
26.1. Basic Cloud Definitions
·
·
·- 0 A a
The essential characteristics of cloud computing (based, in part, on definitions provided by the U.S.
National Institute of Standards and Technology, or NIST) are the following:
1 . On-demand self-service. A resource consumer can unilaterally provision computing services,
such as server time and network storage, as needed automatically without requiring human
interaction with each service's provider. This is sometimes called empowerment of end users
of computing resources. Examples of resources include storage, processing, memory,
network bandwidth, and virtual machines.
2. Ubiquitous network access. Cloud services and resources are available over the network and
accessed through standard networking mechanisms that promote use by a heterogeneous
collection of clients. For example, you can effectively run large applications on small
platforms such as smart phones, laptops, and tablets by running the resource-intensive portion
of those applications on the cloud. This capability is independent of location and device; all
you need is a client and the Internet.
3. Resource pooling. The cloud provider's computing resources are pooled. In this way they can
efficiently serve multiple consumers. The provider can dynamically assign physical and
virtual resources to consutners, according to their instantaneous demands.
4. Location independence. The location independence provided by ubiquitous network access is
generally a good thing. It does, however, have one potential drawback. The consumer
generally has less control over, or knowledge of, the location of the provided resources than
in a traditional implementation. This can have drawbacks for data latency. The consumer may
be able to ameliorate this drawback by specifying abstract location information (e.g., country,
state, or data center).
5. Rapid elasticity. Due to resource pooling, it is easy for capabilities to be rapidly and
elastically provisioned, in some cases automatically, to quickly scale out or in. To the
consumer, the capabilities available for provisioning often appear to be virtually unlimited.
6. Measured service. Cloud systems automatically control and optimize resource use by
leveraging a metering capability for the chosen service (e.g., storage, processing, bandwidth,
and user accounts). Resource usage can be monitored, controlled, and reported so that
consumers of the services are billed only for what they use.
7. Multi-tenancy. Multi-tenancy is the use of a single application that is responsible for
supporting distinct classes or users. Each class or user has its own set of data and access
rights, and different users or classes of users are kept distinct by the application.
26.2. Service Models and Deployment Options
·
·
·- 0 A a
In this section we discuss more terminology and basic concepts. First we discuss the most important
models for a consumer using the cloud.
Cloud Service Models
Software as a Service (SaaS)
The consumer in this case is an end user. The consumer uses applications that happen to be running on
a cloud. The applications can be as varied as email, calendars, video streaming, and real-time
collaboration. The consumer does not manage or control the underlying cloud infrastructure, including
network, servers, operating systems, storage, or even individual application capabilities, with the
possible exception of limited user-specific application configuration settings.
Platform as a Service (PaaS)
The consumer in this case is a developer or system administrator. The platform provides a variety of
services that the consumer may choose to use. These services can include various database options,
load-balancing options, availability options, and development environments. The consumer deploys
applications onto the cloud infrastructure using programming languages and tools supported by the
provider. The consumer does not manage or control the underlying cloud infrastructure, including
network, servers, operating systems, or storage, but has control over the deployed applications and
possibly application hosting environment configurations. Some levels of quality attributes (e.g., uptime,
response time, security, fault correction time) may be specified by service-level agreements (SLAs).
Infrastructure as a Service (IaaS)
The consumer in this case is a developer or system administrator. The capability provided to the
consumer is to provision processing, storage, networks, and other fundamental computing resources
where the consumer is able to deploy and run arbitrary software, which can include operating systems
and applications. The consumer can, for example, choose to create an instance of a virtual computer and
provision it with some specific version of Linux. The consumer does not manage or control the
underlying cloud infrastructure but has control over operating systems, storage, deployed applications,
and possibly limited control of select networking components (e.g., host firewalls). Again, SLAs are
often used to specify key quality attributes.
Deployment Models
The various deployment models for the cloud are differentiated by who owns and operates the cloud. It
is possible that a cloud is owned by one party and operated by a different party, but we will ignore that
distinction and assume that the owner of the cloud also operates the cloud.
There are two basic models and then two additional variants of these. The two basic models are
private cloud and public cloud:
• Private cloud. The cloud infrastructure is owned solely by a single organization and operated
solely for applications owned by that organization. The primary purpose of the organization is
not the selling of cloud services.
·
·
·- 0 A a
• Public cloud. The cloud infrastructure is made available to the general public or a large
industry group and is owned by an organization selling cloud services.
The two variants are community cloud and hybrid cloud:
• Community cloud. The cloud infrastructure is shared by several organizations and supports a
specific c01nmunity that has shared concerns (e.g., mission, security requirements, policy, and
compliance considerations).
• Hybrid cloud. The cloud infrastructure is a composition of two or more clouds (private,
community, or public) that remain unique entities. The consumer will deploy applications onto
some combination of the constituent cloud. An example is an organization that utilizes a private
cloud except for periods when spikes in load lead to servicing some requests from a public
cloud. Such a technique is called "cloud bursting."
26.3. Economic Justification
·
·
·- 0 A a
In this section we discuss three economic distinctions between (cloud) data centers based on their size
and the technology that they use:
1 . Economies of scale
2. Utilization of equipment
3. Multi-tenancy
The aggregated savings of the three items we discuss may be as large as 80 percent for a 100,000-
server data center compared to a 1 0,000-server data center. Economic considerations have made almost
all startups deploy into the cloud. Many larger enterprises deploy a portion of their applications into the
cloud, and almost every enterprise with substantial computation needs at least considers the cloud as a
deployment platform.
Economies of Scale
Large data centers are inherently less expensive to operate per unit measure, such as cost per gigabyte,
than smaller data centers. Large data centers may have hundreds of thousands of servers. Smaller data
centers have servers numbered in the thousands or maybe even the hundreds. The cost of maintaining a
data center depends on four factors:
1 . Cost of power. The cost of electricity to operate a data center currently is 1 5 to 20 percent of
the total cost of operation. The per-server power usage tends to be significantly lower in large
data centers than in smaller ones because of the ability to share items such as racks and
switches. In addition, large power users can negotiate significant discounts (as much as 50
percent) compared to the retail rates that operators of small data centers must pay. Some areas
of the United States provide power at significantly lower rates than the national average, and
large data centers can be located in those areas. Finally, organizations such as Google are
buying or building innovative and cheaper power sources, such as on- and offshore wind
farms and rooftop solar energy.
2. Infrastructure labor costs. Large data centers can afford to automate many of the repetitive
management tasks that are performed manually in smaller data centers. In a traditional data
center, an administrator can service approximately 140 servers, whereas in a cloud data
center, the same administrator can service thousands of servers.
3. Security and reliability. Maintaining a given level of security, redundancy, and disaster
recovery essentially requires a fixed level of investment. Larger data centers can amortize
that investment over their larger number of servers and, consequently, the cost per server will
be lower.
4. Hardware costs. Operators of large data centers can get discounts on hardware purchases of
up to 30 percent over smaller buyers.
These economies of scale depend only on the size of the data center and do not depend on the
deployment model being used. Operators of public clouds have priced their offerings so that many of
the cost savings are passed on to their consumers.
Utilization of Equipment
·
·
·- 0 A a
Common practice in non virtualized data centers is to run one application per server. This is caused by
the dependency of many enterprise applications on particular operating systems or even particular
versions of these operating systems. One result of the restriction of one application per server is
extremely low utilization of the servers. Figures of 1 0 to 1 5 percent utilization for servers are quoted by
several different vendors.
Use of virtualization technology, described in Section 26.4, allows for easy co-location of distinct
applications and their associated operating systems on the same server hardware. The effect of this colocation
is to increase the utilization of servers. Furthermore, variations in workload can be managed to
further increase the utilization. We look at five different sources of variation and discuss how they
might affect the utilization of servers:
1 . Random access. End users may access applications randomly. For example, the checking of
email is for some people continuous and for others time-boxed into a particular time period.
The more users that can be supported on a single server, the more likely that the randomness
of their accesses will end up imposing a uniform load on the server.
2. Time of day. Those services that are workplace related, unsurprisingly, tend to be more
heavily used during the work day. Those that are consumer related tend to be heavily used
during evening hours. Co-locating different services with different time-of-day usage patterns
will increase the overall utilization of a server. Furthermore, time differences among
geographically distinct locations will also affect utilization patterns and can be considered
when planning deployment schedules.
3. Time of year. Smne applications respond to dates as well as time of day. Consumer sites will
see increases during the Christmas shopping season, and floral sites will see increases around
Valentine's Day and Mother's Day. Tax preparation software will see increases around the
tax return submission due date. Again, these variations in utilization are predictable and can
be considered when planning deployment schedules.
4. Resource usage patterns. Not all applications use resources in the same fashion. Search, for
example, is heavier in its usage of CPU than email but lighter in its use of storage. Colocating
applications with complementary resource usage patterns will increase the overall
utilization of resources.
5. Uncertainty. Organizations must maintain sufficient capacity to support spikes in usage. Such
spikes can be caused by news events if your site is a news provider, by marketing events if
your site is consumer-facing, or even sporting events because viewers of sporting events may
turn to their computers during breaks in the action. Startups can face surges in demand if their
product catches on more quickly than they can build capacity.
The first four sources of variation are supported by virtualization without reference to the cloud or
the cloud deployment model. The last source of variation (uncertainty) depends on having a deployment
model that can accommodate spikes in demand. This is the rationale behind cloud bursting, or keeping
applications in a private data center and offloading spikes in demand to the public cloud. Presumably, a
public cloud provider can deploy sufficient capacity to accommodate any single organization's spikes in
demand.
Multi-tenancy
·
·
·- 0 A a
Multi-tenancy applications such as Salesforce.com or Microsoft Office 365 are architected explicitly to
have a single application that supports distinct sets of users. The economic benefit of multi-tenancy is
based on the reduction in costs for application update and management. Consider what is involved in
updating an application for which each user has an individual copy on their own desktop. New versions
must be tested by the IT department and then pushed to the individual desktops. Different users may be
updated at different times because of disconnected operation, user resistance to updates, or scheduling
difficulties. Incidents result because the new version may have some incompatibilities with older
versions, the new version may have a different user interface, or users with old versions are unable to
share information with users of the new version.
With a multi-tenant application, all of these problems are pushed from IT to the vendor, and some of
them even disappear. Any update is available at the same instant to all of the users, so there are no
problems with sharing. Any user interface changes are referred to the vendor's hotline rather than the IT
hotline, and the vendor is responsible for avoiding incompatibilities for older versions.
The problems of upgrading do not disappear, but they are amortized over all of the users of the
application rather than being absorbed by the IT department of every organization that uses the
application. This amortization over more users results in a net reduction in the costs associated with
installing an upgraded version of an application.
26.4. Base Mechanisms
·
·
·- 0 A a
In this section we discuss the base mechanisms that clouds use to provide their low-level services. In an
IaaS instance, the cloud provides to the consumer a virtual machine loaded with a machine image.
Virtualization is not a new concept; it has been around since the 1 960s. But today virtualization is
economically enticing. Modem hardware is designed to support virtualization, and the overhead it adds
has been measured to be just 1 percent per instance running on the bare hardware.
We will discuss the architecture of an IaaS platform in Section 26.5. In this section, we describe the
concepts behind a virtual machine: the hypervisor and how it manages virtual machines, a storage
system, and the network.
Hypervisor
A hypervisor is the operating system used to create and manage virtual machines. Because each virtual
machine has its own operating system, a consumer application is actually managed by two layers of
operating system: the hypervisor and the virtual machine operating system. The hypervisor manages the
virtual machine operating system and the virtual machine operating system manages the consumer
application. The key services used by the hypervisor to support the virtual machines it manages are a
virtual page mapper and a scheduler. A hypervisor, of course, provides additional services and has a
much richer structure than we present here, but these key services are the two that we will discuss.
Page Mapper
We begin by describing how virtual memory works on a bare (nonvirtualized) machine. All modem
servers utilize virtual memory. Virtual memory allows an application to assume it has a large amount of
memory in which to execute. The assumed memory is mapped into a much smaller physical memory
through the use of page tables. The consumer application is divided into pages that are either in physical
memory or temporarily residing on a disk. The page table contains the mapping of logical address
(consumer application address) to physical address (actual machine address) or disk location. Figure
26. 1 shows the consumer application executing its next instruction. This causes the CPU to generate a
target address from which to fetch the next instruction or data item. The target address is used to
address into a page table. The page table provides a physical address within the computer where the
actual instruction or data item can be found if it is currently in main memory. If the physical address is
not currently resident in the main memory of the computer, an interrupt is generated that causes a page
that contains the target address to be loaded. This is the mechanism that allows a large (virtual) address
space to be supported on much smaller physical memory.
��
CPU
Key: Software
component
Hardware
component
Data flow
Page table
used to
convert
target
Target address of address to
next instruction physical
address
Physical
address
inside
current
address
space
Physical
address
outside
current
address
space
·
·
·- 0
Fetch next
instruction
from
physical
address
Fetch next
instruction
from
inte rrupt
handler
Figure 26.1. Virtual memory page table
A a
Turning the virtual memory mechanism into a virtualization mechanism involves adding another
level of indirection. Figure 26.2 shows a logical sequence that maps from the consumer application to a
physical machine address. Modern processors contain many optimizations to make this process more
efficient. A consumer application generates the next instruction with its target address. This target
address is within the virtual machine in which the consumer application is executing. The virtual
machine page table maps this target address to an address within the virtual machine based on the target
address as before (or indicates that the page is not currently in memory). The address within the virtual
machine is converted to a physical address by use of a page table within the hypervisor that manages
the current virtual machines.
Key: Software
compone-nt
Har<lware
component
Data ftow
CPU
·
·
·-
Host Server
next
Instruction
TPaabg􀂊Je :J
0 0 0
Hypervtaor
r 􀞋 Host page table 1-------:-� poi,nts to VM
Target address of
'===--'next instruction
page table \. 1
0 A a
Figure 26.2. Adding a second level of indirection to determine which virtual machine the address
references
Scheduler
The hypervisor scheduler operates like any operating system scheduler. Whenever the hypervisor gets
control, it decides on the virtual machine to which it will pass control. A simple round-robin scheduling
algorithtn assigns the processor to each virtual machine in tum, but many other possible scheduling
algorithms exist. Choosing the correct scheduling algorithm requires you to make assumptions about
the demand characteristics of the different virtual machines hosted within a single server. One area of
research is the application of real-time scheduling algorithms to hypervisors. Real-time schedulers
would be appropriate for the use of virtualization within embedded systems, but not necessarily within
the cloud.
Storage
A virtual machine has access to a storage system for persistent data. The storage system is managed
across multiple physical servers and, potentially, across clusters of servers. In this section we describe
one such storage system: the Hadoop Distributed File System (HDFS).
We describe the redundancy mechanism used in HDFS as an example of the types of mechanisms
used in cloud virtual file systems. HDFS is engineered for scalability, high performance, and high
availability.
A component-and-connector view ofHDFS within a cluster is shown in Figure 26.3. There is one
NameNode process for the whole cluster, multiple DataNodes, and potentially multiple client
applications. To explain the function of HDFS, we trace through a use case. We describe the successful
use case for "write." HDFS also has facilities to handle failure, but we do not describe these. See the
·
·
·- 0
"For Further Reading" section for a reference to the HDFS failure-handling mechanisms.
RPC
Application layer
Client layer
Streaming Protocol
Key: ( Process )
-
• •
Connector
NameNode Process
RPC
DataNode Processes
A a
Figure 26.3. A component-and-connector view of an HDFS deployment. Each process exists on a
distinct computer.
For the "write" use case, we will assume that the file has already been opened. HDFS does not use
locking to allow for simultaneous writing by different processes. Instead, it assumes a single writer that
writes until the file is complete, after which multiple readers can read the file simultaneously. The
application process has two portions: the application code and a client library specific to HDFS. The
application code can write to the client using a standard (but overloaded) Java I/0 call. The client
buffers the information until a block of 64 MB has been collected. Two of the techniques used by
HDFS for enhancing performance are the avoidance of locks and the use of 64-MB blocks as the only
block size supported. No substructure of the blocks is supported by HDFS. The blocks are
undifferentiated byte strings. Any substructure and typing of the information is managed solely by the
application. This is one example of a phenomenon that we will notice in portions of the cloud: moving
application-specific functionality up the stack as opposed to moving it down the stack to the
infrastructure.
For reliability purposes each block is replicated a parameterizable number of times, with a default of
three. For each block to be written, the NameNode allocates DataNodes to write the replicas. The
DataNodes are chosen based on two criteria: ( 1 ) their location replicas are spread across racks to
protect against the possibility that a rack fails; and (2) the dynamic load on the DataNode. Lightly
loaded DataNodes are given preference over heavily loaded DataNodes to reduce the possibility of
contention for the DataNodes among different files being simultaneously accessed.
Once the client has collected a buffer of 64 MB, it asks the NameNode for the identities of the
DataNodes that will contain the actual replicas. The NameNode manages only metadata; it is not
·
·
·- 0 A a
involved in the actual transfer or recording of data. These DataNode identities are sent from the
NameNode to the client, which then treats them as a pipeline. At this point the client streams the block
to the first DataNode in the pipeline. The first DataNode then streams the data to the second DataNode
in the pipeline, and so forth until the pipeline (of three DataN odes, unless the client has specified a
different replication value) is completed. Each DataN ode reports back to the client when it has
successfully written the block, and also reports to the NameNode that it has successfully written the
block.
Network
In this section we describe the basic concepts behind Internet Protocol (IP) addressing and how a
message arrives at your computer. In Section 26.5 we discuss how an IaaS system manages IP
addresses.
An IP address is assigned to every "device" on a network whether this device is a computer, a
printer, or a virtual machine. The IP address is used both to identify the device and provide instructions
on how to find it with a message. An IPv4 address is a constrained 32-bit number that is, typically,
represented as four groups for human readability. For example, 1 92.0.2.235 is a valid IP address. The
familiar names that we use for URLs, such as "http://www.pearsonhighered.coin/", go through a
translation process, typically through a domain name server (DNS), that results in a numeric IP address.
A message destined for that IP address goes through a routing process to arrive at the appropriate
location.
Every IP message consists of a header plus a payload. The header contains the source IP address
and the destination IP address. 1Pv6 replaces the 32-bit number with a 1 28-bit number, but the header of
an IP message still includes the source and destination IP addresses.
It is possible to replace the header of an IP message for various reasons. One reason is that an
organization uses a gateway to manage traffic between external computers and computers within the
organization. An IP address is either "public," meaning that it is unique within the Internet, or "private,"
meaning that multiple copies of the IP address are used, with each copy owned by a different
organization. Private IP addresses must be accessed through a gateway into the organization that owns
it. For outgoing messages, the gateway records the address of the internal machine and its target and
replaces the source address in the TCP header with its own public IP address. On receipt of a return
message, the gateway would determine the internal address for the message and overwrite the
destination address in the header and then send the message onto the internal network. Network address
translation (NAT) is the name of this process of translation.
26.5. Sample Technologies
·
·
·- 0 A a
·